[
    {
        "id": 1,
        "CONTEXT": "\"## \n\n## Description\n\nIssue Definition: Bug in OOB Script (Schedule Job: Auto Close Case) - HRSD  \ngrContext.query(); is not being used, after adding queries.  \n  \nObserved in: Orlando Patch 4, Orlando Patch 7  \n  \n\n\n## Steps to Reproduce\n\n Steps to Reproduce:  \n1. On any OOB instance, open the below OOB scheduled script:  \n\/sysauto\\_script.do?sys\\_id=8d5f5dfb53032300c128ddeeff7b12bd&sysparm\\_record\\_target=sysauto&sysparm\\_record\\_row=1&sysparm\\_record\\_rows=6&sysparm\\_record\\_list=nameCONTAINScase%5EORDERBYname\n\n\n  \n2. In the script block, after line 56 :  \n\"grContext.query();\" is missing  \n  \n\n\n## Workaround\n\n1. Make changes to the auto close case: ( the scheduled job with sys\\_id\u00a0 - 8d5f5dfb53032300c128ddeeff7b12bd) in line number 53 - 56\n\n\n`if(grCase.hr_service.le_type) {`  \n`if(new sn_hr_core.hr_CaseUtils()._getActivitySetContextCases(caseId))`  \n`return;`  \n`}`\n\n\n2. Make changes to the script include\u00a0hr\\_CaseUtils\u00a0 (with sys\\_id 24c782869f202200d9011977677fcf89) as this :  \nadd a method -\n\n\n`_getActivitySetContextCases: function(caseId){  \n var grContext = new GlideRecord('sn_hr_le_activity_set_context');  \n grContext.addQuery('hr_case',caseId);  \n grContext.addQuery('state','awaiting_trigger');  \n grContext.query();  \n return grContext.hasNext();  \n },`  \n3. Run the following background script:\u00a0 `new sn_hr_core.hr_CaseUtils()._getActivitySetContextCases('123');`  \nThis should generate an RCA. Make sure there are two RCAs - one for the script include, and one for the scheduled script.\n\n  \n**Related Problem: PRB1442036**\"\n\n",
        "QUERY": " 'How to generate a RCA in OOB script?'",
        "GROUND_TRUTH": "To generate a RCA in OOB script, follow these steps:\n\n1. Open the OOB scheduled script on any OOB instance using the following link: \/sysauto_script.do?sys_id=8d5f5dfb53032300c128ddeeff7b12bd&sysparm_record_target=sysauto&sysparm_record_row=1&sysparm_record_rows=6&sysparm_record_list=nameCONTAINScase%5EORDERBYname\n\n2. In the script block, after line 56, you will notice that \"grContext.query();\" is missing.\n\n3. Make changes to the auto close case (the scheduled job with sys_id - 8d5f5dfb53032300c128ddeeff7b12bd) in line number 53 - 56 as follows:\n\n```\nif(grCase.hr_service.le_type) {\nif(new sn_hr_core.hr_CaseUtils()._getActivitySetContextCases(caseId))\nreturn;\n}\n```\n\n4. Make changes to the script include hr_CaseUtils (with sys_id 24c782869f202200d9011977677fcf89) by adding a method:\n\n```\n_getActivitySetContextCases: function(caseId){\n var grContext = new GlideRecord('sn_hr_le_activity_set_context');\n grContext.addQuery('hr_case',caseId);\n grContext.addQuery('state','awaiting_trigger');\n grContext.query();\n return grContext.hasNext();\n },\n```\n\n5. Run the following background script: new sn_hr_core.hr_CaseUtils()._getActivitySetContextCases('123');\nThis should generate an RCA. Make sure there are two RCAs - one for the script include, and one for the scheduled script."
    },
    {
        "id": 2,
        "CONTEXT": "\"## \n\n**Identify installation error**\n\n\n\u00a0\n\n\nAfter activating Conversational Integration with Microsoft Teams users will get an option to install MS teams from messaging app integration page. During installation when the user clicks on install and if bellow error page is displayed with error code = 9, this is because the user on MS Team tenant does not have one of the required mentioned roles.\n\n\n\u00a0\n\n\nPlease follow the [doc](https:\/\/docs.servicenow.com\/bundle\/quebec-now-intelligence\/page\/administer\/virtual-agent\/concept\/teams-conv-integration.html \"doc\") to know more about Conversational Integration with Microsoft Teams plugin\n\n\nHere is a [link](https:\/\/store.servicenow.com\/sn_appstore_store.do#!\/store\/application\/8be385e4776110105d7b3882a910610e\/2.0.0?sl=sh \"link\") for store app plugin for Conversational Integration with Microsoft Teams plugin\n\n\n\u00a0\n\n\n![](sys_attachment.do?sys_id=4a9550e7db86f410bb4a474d13961958)\n\n\n\u00a0\n\n\n**Required Roles**\n\n\nServicenow: virtual\\_agent\\_admin and external\\_app\\_install\\_admin or admin\n\n\nMicrosoft Teams administrator (one of the following): Global Administrator, Application Administrator, or Cloud Application Administrator\n\n\n\u00a0\n\n\n**How to resolve the above error**\n\n\nHere is a [Microsoft document](https:\/\/docs.microsoft.com\/en-us\/microsoft-365\/admin\/add-users\/assign-admin-roles?view=o365-worldwide \"Microsoft document\") that will help to add required roles to users. Once the user is assigned one of the above roles, they should be able to install\/override MS Teams configuration.\"\n\n",
        "QUERY": " 'Why Global Administrator is required for MS team integration'",
        "GROUND_TRUTH": "The Global Administrator role is required for MS Teams integration because it is one of the roles that has the necessary permissions to install and override MS Teams configuration. When integrating with MS Teams, the user on the MS Team tenant needs to have one of the required roles, which include Global Administrator, Application Administrator, or Cloud Application Administrator. If the user does not have one of these roles, they may encounter an installation error with error code = 9. Therefore, having the Global Administrator role is crucial for successful integration with MS Teams."
    },
    {
        "id": 3,
        "CONTEXT": "\"## \n\n### Details\n\nThis module will replace the below section on [Instance upgrade FAQ - Frequently Asked Questions](https:\/\/support.servicenow.com\/kb?id=kb_article_view&sysparm_article=KB0678054) from the T+ release\n\n\n[15. How to use an Update Set to capture reverted customisations made after an upgrade?](https:\/\/support.servicenow.com\/kb?id=kb_article_view&sysparm_article=KB0678054#q15)\n\n\n\u00a0\n\n\nProduct documentation:- [Upgrade Plan Overview](https:\/\/docs.servicenow.com\/csh?topicname=uc-upgrade-plan.html&version=latest)\n\n\n* This feature can only be used from the Tokyo release.\n* The customer should be creating an Upgrade plan for every upgrade on the instance from X to Y\/XPatch\\* to XPatch\\*\/XPatch\\* to YPatch\\*.\n* If the customer plan to upgrade from T to U; they need to create it on their BUILDER instance once so that it can be used for all their instances going on the same upgrade patch where all other instances except the builder instance will be CONSUMER.\n* If the customer is going from UP1 to UP6; another plan should be created.\n* This feature is part of an OOB plugin that will be installed with your Tokyo upgrade \u00a0\"com.glide.upgrade\\_to\\_customized\".\n* This upgrade Plan will reduce the time and man-hours consumed for Post upgrade activities as the below-listed activities will be automated by the upgrade engine using the AppRepo feature which is already on the instance as a base feature.\n* This might increase the total Upgrade time ( as post-upgrade manual activities are implemented by the Upgrade Engine).\n\n\n\u00a0\n\n\nUpgrade plan focus on automating the following which is currently a manual task after an upgrade.\n\n\n\u00a0\n\n\n1. Upgrade installed applications ( Custom or Store)\n\n\n2. Install new applications ( Custom or Store)\n\n\n3. Install new Store application ( Target upgrade Release specific)\n\n\n4. Automatically review=> Resolve your Post upgrade Skipped records( these will be captured as scope specific app updates)\n\n### Additional Information\n\n## The High-Level Data flow is as follows-\n\n\n\u00a0\n\n\n1. Configure your DEV instance as a BUILDER instance.\n2. Upgrade your instance (T==> TPatch\\*\/U\/V)\n3. Review and [Resolve all your Skipped](https:\/\/docs.servicenow.com\/csh?topicname=uc-process-skipped-records.html&version=latest) records ( POST upgrade)\\*\\*\\* This is very important. If there are no action items for a skipped configuration and there is a new version coming in the upgrade, the file will be reverted to the base version ( this can be revisited later). But it is recommended to address all the skipped items.\n4. Install new Plugins\/applications ( Custom or Store) if any.\n5. Upgrade Centre=> Upgrade Plan\n6. Created upgrade plan when ready ( after making necessary changes) should be published which will create an app for your AppRepo. This will create a Global Scoped Application \"Upgrade Plan \u0096 Target Version name\"\n7. Upgrade plan can be retrieved on any Consumer instance from \" My Company Applications\"\n8. Install the new version if you have made any changes to the upgrade plan. Any changes to the upgrade plan ( Refresh) will create a new version and publish Automatically. The name will be \"Upgrade Plan \u0096 Version name\". \\*\\*\\* We recommend testing this on a SUB PROD consumer instance before installing this.\n9. Review the Upgrade plan and the Related list ( Individual versions for each application). This will contain installed apps as well as custom apps. If you do not want any of these apps to be excluded from the upgrade ( please mark them inactive).Instance Administrators can compare the versions and mark them as per the organisational requirement\/development plan\n10. Create an upgrade change for your CONSUMER instance.\n11. \"Upgrade Preview\" and this will Preview the Upgrade with Upgrade Plan. Predictions will give you the count. It says how many are Automatically resolved.\n12. During upgrade, the Upgrade engine will take care of App Updates\/Commit Customisation\/Resolve Skipped post upgrade files( as you did in the BUILDER instance)\n13. Upgrade Monitor will show you the relevant information.\n\n\n\u00a0\n\n\n![](\/sys_attachment.do?sys_id=fc5df7e1474f39103542f24c736d435d)\n\n\n## Points to remember\n\n\n\u00a0\n\n\n1. Upgrade Plan is created as a\u00a0 GLOBAL AppRepo Application with version incremented for each of Refresh( link)\n2. Upgrade plan is a GLOBAL application which hold config files which are in their own respective scope. GLOBAL application is to create\/host\/publish\/transfer\/install the config files in an easy way to consumer instances.\n3. Upgrade Plan should be installed on your CONSUMER instance before the upgrade.\n4. The upgrade plan will include all your Global customizations ( there will be a new app created for this as \"Global Customizations - Upgrade Plan\"). This app will remain on your app repository as an app and will be used for future upgrades to new versions. For every family releases the version number will be incremented.\n5. All Skipped records post-upgrade when addressed\/Resolved will be included in the sys\\_update\\_xml. These will be captured in the upgrade plan as customisations for respective Scope\/Package and will be included in Upgrade Plan\n6. This will also create applications with versions as installed on your BUILDER instance.\n7. This will create new versions for Store installed apps\/plugins if these are customised on that instance on their respective SCOPE. It will list the current version that is installed and the Customised version if available. Product doc: [Manage customizations to applications](https:\/\/docs.servicenow.com\/csh?topicname=manage-customizations-store-apps.html&version=latest)\n8. If you have multiple DEV instances publishing to AppRep and if the current BUILDER instance does not have up-to-date custom applications which were not installed ( but published) on App Repo from other DEV; this will not be included in the Upgrade Plan. It will give a warning saying there is a new version available in App Repo.\n9. The upgrade plan only contains the config. It does not contain any metadata. So if there is a Scoped app update coming in via Upgrade Plan; the metadata is going to be its customised package even though the Upgrade plan is installed as an App from AppRep.\n10. Every time you refresh the Upgrade Plan; it will automatically get published to AppRepo. We need to make sure the same version is installed on the CONSUMER instance.\n11. Soon after the Upgrade Plan is installed on the CONSUMER it will start downloading the individual components for the Upgrade Engine to Consume. These are stored in the Attachment table.\n12. During the Upgrade preview; It will list out the number of records that get auto-resolved and records that will not. You need to take action on the records which say to be reverted.\u00a0 This happens if your Consumer instance has extra customisations which were not on your BUILDER instance. You can take note of these records and bring the customisations back to BUILDER and Refresh Upgrade Plan which will include these NEW customisations. Then this new version should be installed on the consumer instance before the upgrade.\n13. The upgrade plan will be consumed during the upgrade and will do the activities which are captured. These config files will be updated to their metadata on that respective SCOPE.\n14. Upgrade Plan will be considered for any Schema changes and there will be only one online alter if there is a schema change coming in via upgrade and another one on the Upgrade plan\n15. If there are any items in an Error state during Upgrade validation on the CONSUMER instance; the customer can remediate the issue and reprocess the Upgrade Plan to rectify this before the Version Upgrade. There will be a Status column that will specify the issue to remediate.\n\n\n\u00a0\n\n\n## FAQ\n\n\n\u00a0\n\n\n**1. Which instance should I configure as Builder?**\n\n\nYou should be making your DEV instance the builder instance. If you have multiple DEV instances; you should configure the MASTER\/MAIN DEV instance as your BUILDER instance\n\n\nBy Default all instances are marked as \"Consumer\" Driven by property glide.upgrade.plan.instance\\_type ( only available from T+)\n\n\nThese instances should stay as a Consumer instance\n\n\nTEST\n\n\nPROD.\n\n\n\u00a0\n\n\nOnce the instance is configured as a BUILDER; you cannot change it to consumer and the clone preserves this setting.\n\n\n\u00a0\n\n\n**2. Can I make my PROD a BUILDER instance?**\n\n\nYou should NOT\u00a0 make your PROD instance a Builder instance\n\n\nThe upgrade plan is linked to each upgrade. So once there is a plan created; your Global customisations will be linked to a \"Global Customisations - Upgrade Plan\" Application. You do not want that in PROD\n\n\n\u00a0\n\n\n**3. What happens to the Store application installation on the upgrade plan for which we do not have entitlement on the Consumer ( PROD)**\n\n\nEntitlement issue\/Licensing issue ( State will be Error). These will not be auto-installed, and this should be addressed manually.\n\n\nThis will be the same for MAINT-only plugins. This will be changed to Error.\n\n\n\u00a0\n\n\n**4. What happens if I do not want to install an application\/update which is already on a tested upgrade plan**\n\n\nWe have the option to make individual updates Inactive before your Actual Version upgrade on the target instance. So these will be skipped.\n\n\n\u00a0\n\n\n**5. Can I disable the Installed upgrade plan in case of any last-minute change of plans?**\n\n\nWe can disable the upgrade plan together. Also, there is an option to create another upgrade(Refresh) plan ( new version) and install it as an Update on the consumer before the upgrade\n\n\n\u00a0\n\n\n**6. When we install the customisation via the Upgrade plan; will it leave an entry on sys\\_update\\_version and sys\\_update\\_xml?**\n\n\nIf it is a new App installation without any customisation; it will not leave an entry on sys\\_update\\* tables. If it is a Store App\/Plugin and is customised; it will have those customisation records on sys\\_update\\* tables on the consumer instance\n\n\n\u00a0\n\n\n**7. Can I install a deactivated component from the Upgrade plan at a later stage on the consumer?**\n\n\nThis is not possible at the moment and it will be there from U+. This will again dependent on if there is a new version in App Repo\/was it already upgraded on the instance from App Repo\/Store.\n\n\n\u00a0\n\n\n**8. What will happen to an application which is already in a newer version installed than the version coming in Upgrade Plan?**\n\n\nIf the upgrade plan is having the same or an older version; it will not be consumed during the Upgrade.\n\n\n\u00a0\n\n\n**9. How can I find out if the Upgrade plan was executed?**\n\n\nThis will be listed on the sys\\_upgrade\\_history\\_log table\" Auto resolved from\" column.\n\n\n\u00a0\n\n\n**10. Is this available for Self Hosted customers?**\n\n\nIf the self-hosted customer has AppRepo connectivity they can utilise this feature.\n\n\n\u00a0\n\n\n**11. What if there is a plugin\/app dependency on an app\/plugin on the upgrade plan?**\u00a0\n\n\nThe upgrade plan will not look into the dependencies and auto-install them. The dependent plugins should be a part of the Upgrade Plan\n\n\n\u00a0\n\n\n**12. Will this increase the time of the total upgrade?**\n\n\nThis will increase the Upgrade change duration and increase the time to upgrade, but post-upgrade activities are managed by the Upgrade engine automatically.\n\n\n\u00a0\n\n\n**13. Will the Upgrade Plan capture the Update-sets?**\n\n\nThe upgrade plan does not capture all the updates made by the customer. While building we pick the skipped records and corresponding scopes will be included in the plan along with the customer installed\/upgraded\/published applications.\n\n\n  \nWe do not have any support for update set commits to capture in the upgrade plan at the moment. Post-upgrade update-set commits will be manual.\n\n\n\u00a0\n\n\n**14. Can I uninstall the Upgrade plan after WAR upgrade?**\n\n\nFrom the instance where the Upgrade Plan is installed, it can be uninstalled if the instance is not yet upgraded(WAR version).\n\n\nFrom the instance where the Upgrade plan is installed and WAR upgraded, It cannot be uninstalled as the Upgrade plan is a bunch of configuration Records that will be copied over to the instance during upgrade Plan installation as a Global App containing files in their respective scope. These will be consumed when the Upgrade happens. So, during the upgrade these configs that came over via the Upgrade plan would have been already consumed and changes made to sys\\_metadata of those respective updates in their respective scopes. An uninstallation on the upgrade plan after the WAR upgrade will not help here as the configuration record is already updated with\/after the upgrade and this might mess up the instance if attempted.\n\n\n\u00a0\n\n\n**15. Why there are multiple \\*global\\* scope entries on my sys\\_scope\/sys\\_app table on the builder instance?**\n\n\n\u00a0\n\n\n\u00a0\n\n\nThis is expected behaviour as the Upgrade plan creates multiple \"global\" apps with the same sys\\_scope(global) but with different sys\\_id's. The scope will remain global even though there are OOB \"global\" records linked to these newly created Global scopes.\n\n\n\u00a0\n\n\n![](\/sys_attachment.do?sys_id=855df7e1474f39103542f24c736d4399 \"Screenshot 2023-10-26 at 1.42.12 pm.png\")\n\n\n\u00a0\n\n\n**16. Why OOB \"global\" records are linked to \"Global Customizations \u0096 Upgrade Plan\" (Application) on my builder and consumer instance?**\n\n\n\u00a0\n\n\nThis is an expected behaviour where this is a new \"global\" scope created with the upgrade plan and any global updates that get added to the upgrade plan will be linked to the Application as \"Global Customizations \u0096 Upgrade Plan\". This is a cosmetic change and will not affect the functionality of the instance as the records will stay \"global\"\n\n\n\u00a0\n\n\n\u00a0\n\n\n**17. There is a new App \"Global Customizations \u0096 Upgrade Plan\" on the \"Not installed\" section of my company application list.**\n\n\n\u00a0\n\n\nThis is an expected behaviour, and this will be auto-installed when the Upgrade Plan is consumed during the Upgrade. You do not have to install manually, and you only need to install \"Upgrade Plan \\*RELEASE VERSION\\*\n\n\n\u00a0\n\n\n![](\/sys_attachment.do?sys_id=855df7e1474f39103542f24c736d4363 \"Screenshot 2023-10-26 at 1.27.10 pm.png\")\n\n\n\u00a0\n\n\n\u00a0\n\n\n**18. The WAR version for which the Upgrade plan was created on the builder instance is not available for selection on the Consumer instance Upgrade Change.**\n\n\n\u00a0\n\n\nIf there is a mismatch in the WAR version name on the upgrade plan the upgrade plan will not be consumed.\n\n\n\u00a0\n\n\nEg: XP7HF1 upgrade plan WILL NOT be consumed during a XP7HF1a upgrade.\n\n\n\u00a0\n\n\n\u00a0\n\n\nWhen a new Patch\/HotFix#\/HotFix #\\* gets released to remediate security issues, the old version will be taken out of the available list for Upgrade selection. If the Upgrade plan was already published from the OLD version; the remediation steps will be as follows:-\n\n\n\u00a0\n\n\nOn the builder version where the Upgrade plan was published; please upgrade to the new available Patch\/Hotfix version via the NowSupport change.\n\n\nPublish the new Upgrade plan once it is installed with the new WAR version.\n\n\nInstall the new upgrade plan on the consumer instance.\n\n\n\u00a0\n\n\n**19. What issues can be anticipated if the upgrade plan was not consumed during an upgrade?**\n\n\n\u00a0\n\n\nIf it is not consumed, you can have sys\\_scope issues when trying to commit Global update-sets captured from the builder version on the consumer instance as the new \\*global\\* sys\\_scope records (Point 16) will not be committed as the Upgrade plan was not consumed which can give you preview errors for sys\\_scope as it is not found on the consumer.\n\n\n\u00a0\n\n\n**20.\u00a0Why is there a customised version for the OOB\/installed apps\/plugins on the Upgrade plan even though we haven\u0092t made any customisation?**\n\n\n\u00a0\n\n\nWhat the Upgrade plan does is package all the previous customisations you had on the instance and make it into a version. So these records you see there will not be a recent one and they might have been updated before(maybe years back). The upgrade plan basically packages it as a 1.0.\\* version so that it will be hosted in AppRepo in its individual scope.\n\n\n\u00a0\n\n\nThe next upgrade plan will have an incremented version from the current one.\n\n\nYou can use this feature to transfer updates to an OOB module\/app\/plugin by Publishing them separately to AppRepo irrespective of whether it is linked to the Upgrade plan by not using the update-set feature. If you install them as packaged versions; uninstalling them will be easy instead of backing up an update set.\n\n\n![](\/sys_attachment.do?sys_id=895df7e1474f39103542f24c736d439d \"Screenshot 2023-11-02 at 1.58.55 pm.png\")\n\n\n\u00a0\n\n\n**21.\u00a0Why does the customisation record show as Inserted\/Unchanged in sys\\_upgrade history disposition?**\n\n\n\u00a0\n\n\nWhen the upgrade plan is consumed during the upgrade; the customisations packaged for OOB\/Store Apps\/Plugins will be added to the instance sys\\_update\\_version linked to the new Packaged version of the application.\n\n\nSo if it was previously brought over by an update-set; there will be a new entry on sys\\_update\\_version and it will be the same file, but with a new entry. This should not be confu...\"\n\n",
        "QUERY": " 'Why do I have skipped errors in the upgrade monitor after upgrade to tokyo'",
        "GROUND_TRUTH": "Skipped errors in the upgrade monitor after upgrading to Tokyo can occur due to unresolved skipped records during the upgrade process. According to KB12345, it is crucial to review and resolve all your skipped records post-upgrade. If there are no action items for a skipped configuration and there is a new version coming in the upgrade, the file will be reverted to the base version. However, it is recommended to address all the skipped items. If you do not do this, it may result in skipped errors in the upgrade monitor."
    },
    {
        "id": 4,
        "CONTEXT": "\"## \n\nDelete demands\n[Strategic Portfolio Management](..\/..\/..\/product\/it-business-management\/reference\/r_ITBusinessManagement.html \"Align work with business goals to deliver products and services in a way that supports your strategic priorities. ServiceNow Strategic Portfolio Management (SPM, formerly IT Business Management, enables you to use digital transformation to drive more customer value faster. Plan, deliver, and track value across different methodologies.\") > [Project Portfolio Management](..\/..\/..\/product\/project-portfolio-suite\/concept\/c_ProjectPortfolioSuite.html \"Project Portfolio Management provides a simplified, team-oriented approach to Project Portfolio Management and IT development by combining several individual applications.\") > [Demand Management](..\/..\/..\/product\/planning-and-policy\/concept\/c_DemandManagement.html \"The Demand Management application consists of tools for capturing, centralizing, and assessing strategic and operational demands. It also provides a single location for managing all the demand information.\") > \n# Delete demands\n\n\nDemands can be deleted only while in the Pending state.\n\n\n\nRole required: it\\_demand\\_manager or it\\_demand\\_user\n\n\n\n\nWhen you delete a demand, all data related to the demand, such as risks, demand\n tasks, requirements, and decisions are deleted, but the stakeholders are not deleted\n from the Stakeholder Register [dmn\\_stakeholders\\_register] table.\n\n\nIf a project is already created from a demand, its reference is removed from the\n project along with the data related to the demand, however, the project is not\n deleted from the database.\n\n\nTo delete a demand:\n\n\n\n1. Navigate to All > Demand > Demands > All.\n2. Do:\n\t* Click the demand to open the demand form and then click\n\t Delete.\n\t* Select the check box next to the demand and then select\n\t Delete from the Actions\n\t choice list.\n\n\n\n\n\n**Parent Topic:** [Editing demands](..\/..\/..\/product\/planning-and-policy\/concept\/editing-demands.html \"Users with the demand manager role can view and modify demands using the Demand Management application.\")\"\n\n",
        "QUERY": " 'when project is deleted from a demand all information is deleted'",
        "GROUND_TRUTH": "According to KB12345, when a demand is deleted, all data related to the demand such as risks, demand tasks, requirements, and decisions are deleted. However, if a project has already been created from a demand, its reference is removed from the project along with the data related to the demand, but the project itself is not deleted from the database. Therefore, when a project is deleted from a demand, not all information is deleted; the project remains in the database."
    },
    {
        "id": 5,
        "CONTEXT": "\"## \n\nTransfer WhatsApp chat conversations to live agents\n# Transfer WhatsApp chat conversations to live agents\n\n\nConfigure the Advanced Work Assignment application to transfer a WhatsApp chat conversation initiated by a requester to a live\n agent.\n\n\nYour administrator must have completed the following tasks:* [Install Conversational Integration with WhatsApp (powered by Twilio)](messg-whatsapp-install.html \"Install the Conversational Integration with WhatsApp (powered by Twilio) so that your business can interact with requesters on the WhatsApp app. The application includes demo data and installs related ServiceNow Store applications and plugins if they are not already installed.\").\n* [Activate Advanced Work\n Assignment](..\/..\/advanced-work-assignment\/task\/awa-activate.html \"You can activate the Advanced Work Assignment plugin (com.glide.awa) if you have the admin role.\").\n* Set the application scope to Conversational Integration with WhatsApp (powered by Twilio)\n using the application picker. For more information, see [Application\n picker](..\/build\/applications\/concept\/c_ApplicationPicker.dita\/c_ApplicationPicker.html).\n* Activate the Conversational Messaging plugin (com.glide.messaging.awa). For more\n information, see [Activate Conversational Messaging](activate-messaging-actions.html \"You can activate the Conversational Messaging plugin (com.glide.messaging.awa) if you have the admin role.\").\n\n\nRole required: admin\n\n\n\n\n1. Enable the WhatsApp service channel.\n\t1. Navigate to Advanced Work Assignment > Settings > Service Channels.\n\t2. In the Name column of the Queues list, search\n\t for WhatsApp.\n\t3. Click WhatsApp.\n\t4. On the Service Channel form, select the Active\n\t check box.\n\t5. Click Update.For more information, see [Service\n channels](..\/..\/advanced-work-assignment\/concept\/awa-service-channels.html \"Provide customer support by automatically routing incoming work to agents through service channels.\").\n2. Automatically route work items for WhatsApp chat conversations\n to agents by configuring the queue for the WhatsApp service\n channel.\n\t1. Navigate to Advanced Work Assignment > Settings > Queues.\n\t2. In the Name column of the Queues list, search\n\t for Agent WhatsApp Queue.\n\t3. Click Agent WhatsApp Queue.\n\t4. On the Queue form, select the Active check\n\t box.\n\t5. Click Update.\nNote: By default, the WhatsApp - Most Capacity assignment\n rule is associated with the Agent WhatsApp Queue. You can create another\n assignment rule and associate it with the queue. For more information, see\n [Configure agent\n assignment rules](..\/..\/advanced-work-assignment\/task\/awa-create-assignment-rule.html \"Set the Advanced Work Assignment criteria for assigning work items to agents. Choose the assignment rule that considers the agent with the most capacity or the agent who has gone the longest without work. Establish the settings for the auto-assign handling, reject handling, and skill handling related lists.\") and [Work item\n queues](..\/..\/advanced-work-assignment\/reference\/awa-queues.html \"In Advanced Work Assignment, queues store a specific type of work item for a service channel.\").\n3. Configure agent presence states for the WhatsApp service\n channel.\n\t1. Navigate to Advanced Work Assignment > Settings > Presence States.\n\t2. In the Name column of the Presence States list,\n\t click an existing state.\n\t3. In the Apply to groups section of the Presence State form, click the\n\t add icon (>) to move the Agent WhatsApp Group\n\t group from the Available column to the\n\t Selected column.\n\t\n\tNote: You can ignore this step if you have selected the Apply\n\t to all groups check box.\n\t4. Click Update.For more information, see [Configure agent presence\n states](..\/..\/advanced-work-assignment\/task\/awa-configure-agent-presence.html \"Create or modify the availability states that agents use to indicate whether they can receive work or are offline or away. Agents set these states in their Workspace Inbox.\").\"\n\n",
        "QUERY": " 'whatsapp service channel in presence state'",
        "GROUND_TRUTH": "To configure the WhatsApp service channel in the presence state, follow these steps:\n\n1. Navigate to Advanced Work Assignment > Settings > Presence States.\n2. In the Name column of the Presence States list, click an existing state.\n3. In the Apply to groups section of the Presence State form, click the add icon (>) to move the Agent WhatsApp Group group from the Available column to the Selected column. Note: You can ignore this step if you have selected the Apply to all groups check box.\n4. Click Update.\n\nFor more information, see [Configure agent presence states](..\/..\/advanced-work-assignment\/task\/awa-configure-agent-presence.html \"Create or modify the availability states that agents use to indicate whether they can receive work or are offline or away. Agents set these states in their Workspace Inbox.\")."
    },
    {
        "id": 6,
        "CONTEXT": "\"## \n\nUse this KB article <https:\/\/docs.servicenow.com\/bundle\/vancouver-mobile\/page\/administer\/tablet-mobile-ui\/task\/sg-configure-multiscan.html> to configure grouped input for multiple scans.\n\n\n\u00a0\n\n\nField Service Management provides Scan Multiple Parts and Scan Remove Multiple Parts functions Out of the Box. Both of these functions are used to perform asset usage\/removal on the Work order task by scanning the asset tags of the asset. Both of these functions are present as overflow actions in the Use Part\/Remove part screens on the Work order task Parts tab. Make sure the Work order task to be in work in progress state to see these functions.\n\n\n**Scan Multiple Parts:**\n\n\n1. This function is used to record asset usage on a work order task by scanning multiple asset tags on an asset.\n\n\n2. This action item for this function is my\\_inventory\\_use\\_multiple\\_parts[\/sys\\_sg\\_write\\_back\\_action\\_item.do?sys\\_id=95a73df95b691010400f53643381c7c8] takes the multi scan input using the Multiple\\_scan\\_part variable.\n\n\n![](\/sys_attachment.do?sys_id=7fc7c15d97bebd505ad8f6e11153af6b)\n\n\nIn the above Snippet of code, Multiple\\_scan\\_part[index][\"Scan Asset Tag\"] gives the asset tag details of the asset scanned using Grouped input.\n\n\n3. Each scanned input is sent to\u00a0 **FSMMultiPartsScan.multiplePartsScan** to record asset usage.\u00a0\n\n\n4. To perform asset usage, the asset should be in In Stock\/Available or In stock\/Reserved status.\n\n\n\u00a0\n\n\n**Scan Remove Multiple Parts:**\n\n\n1. This function is used to record asset removal on a work order task by scanning multiple asset tags on an asset.\n\n\n2. This action item for this function is my\\_inventory\\_remove\\_multiple\\_parts[\/sys\\_sg\\_write\\_back\\_action\\_item.do?sys\\_id=0be0e41adbae1090f047d8fdd39619db] takes the multi scan input using the Multiple\\_scan\\_remove\\_part variable.\n\n\n![](\/sys_attachment.do?sys_id=37c7c15d97bebd505ad8f6e11153af5a)\n\n\nIn the above Snippet of code, Multiple\\_scan\\_remove\\_part[index][\"Scan Asset Tag\"] gives the asset tag details of the asset scanned using Grouped input.\n\n\n3. Each scanned input is sent to\u00a0 FSMMultiPartsScan.multiplePartsScan to record asset removal.\u00a0\n\n\n4. To perform asset removal, the asset should be in use or Consumed substate.\"\n\n",
        "QUERY": " 'What scanning does midserver perform'",
        "GROUND_TRUTH": "no answer found"
    },
    {
        "id": 7,
        "CONTEXT": "\"## \n\nCreate a risk event task\n[Governance, Risk, and Compliance](..\/..\/..\/product\/grc-common\/reference\/r_WhatIsGRC.html \"Respond to business risks in real time. Connect security and IT with an integrated risk program offering continuous monitoring, prioritization, and automation.\") > [Using Risk Management](..\/..\/..\/product\/grc-risk\/concept\/using-risk-mgmt.html \"You can use the features and capabilities of the Risk Management application perform various activities such as creating issues, reporting risk events, performing risk assessments and so on.\") > \n# Create a risk event task\n\n\nA risk event might require associated tasks. Unless these tasks are created and\n eventually closed, the risk event cannot be closed.\n\n\n\nRole required: sn\\_risk.manager\n\n\n\n1. Navigate to All > Risk Events > All Events.\n2. Select the record for which a task must be created.\n3. Select the Tasks related list and then select New.\n4. On the form, fill the fields.\n\n\n\nTable 1. Risk event task form\n| Field | Description |\n| --- | --- |\n| Assigned to | Person that the task is assigned to. |\n| Priority | Level of importance given to a task, which indicates how  readily a task or assignment can be delayed. |\n| Short description | Brief description of the task. |\n| Description | Detailed description of the task. |\n| Work notes | Any relevant information for your reference. |\n| Additional comments | Any additional information that might be  necessary. |\n5. Select Submit.\n\n\n\nReview the associated issues of a risk event or create a new issue during approval.\n For more information, see [Manually create GRC issues](..\/..\/grc-common\/task\/t_CreateAnIssue.html \"As a GRC user, you can manually create issues to document policy, risk, or audit observations, or to accept any GRC problems. You can also identify the source of the issue to help analyze and classify the issues.\").\n\n\n\n\n\n\n**Parent Topic:** [Use Risk Events](..\/..\/..\/product\/grc-risk\/concept\/use-risk-events.html \"Report risk events and monitor their workflow to prevent losses in your organization.\")\"\n\n",
        "QUERY": " 'what role can be assigned a risk event task'",
        "GROUND_TRUTH": "According to KB12345, the role that can be assigned a risk event task is 'sn_risk.manager'. This role is required to navigate to All > Risk Events > All Events, select the record for which a task must be created, select the Tasks related list and then select New. They then fill in the fields on the form and select Submit."
    },
    {
        "id": 8,
        "CONTEXT": "\"## \n\nEnable Dynamic Translation for an unsupported language\n[Now Platform capabilities](..\/..\/..\/administer\/general\/reference\/r_ServiceNowPlatform.html \"Digitize your workflows and run them at scale by using the core capabilities of the Now Platform. When you activate and configure these features, you can create custom business logic, manage and secure data, and build alternate user interfaces.\") > [Conversational Interfaces](..\/..\/..\/administer\/conversational-interfaces\/concept\/ci-landing-page.html \"Enable your users to engage with live agents, virtual agents, or both using ServiceNow Conversational Interfaces. These interfaces provide tools for creating and managing optimal conversational experiences for your users, in the chat channels that they prefer.\") > [Virtual Agent](..\/..\/..\/administer\/virtual-agent\/concept\/virtual-agent-landing-page.html \"Unlock your enterprise productivity with ServiceNow Virtual Agent. Provide your employees and customers with a friendly messaging interface, featuring pre-built conversations powered by artificial intelligence.\") > [Building and deploying Virtual Agent](..\/..\/..\/administer\/virtual-agent\/concept\/using-virtual-agent.html \"Convert the time your agents and technicians spend handling low-impact user requests into an intelligently managed interaction. Enable Virtual Agent with NLU to understand the intent of what people are looking for and provide them with more relevant answers.\") > [Localization options for Virtual Agent](..\/..\/..\/administer\/virtual-agent\/concept\/multi-language-options-va.html \"The ServiceNow platform provides several methods for serving your international Virtual Agent users, regardless of their language and locale. Professionally localize your conversations, use dynamic machine translation, or a combination of the two.\") > \n# Enable Dynamic Translation for an unsupported language\n\n\nThe ServiceNow\u00ae platform supports 22 languages, but you can enable\n dynamic machine translation for additional languages in Virtual Agent.\n\n\n\nInstall and configure the plugins for Dynamic Translation, as described in\n [Prerequisites for using Dynamic Translation in Virtual Agent](..\/concept\/prereq-using-dt-va.html \"To enable dynamic machine translation, install ServiceNow language plugins and Dynamic Translation plugins. Then configure Dynamic Translation for your instance.\").\n\n\nRole required: virtual\\_agent\\_admin or admin\n\n\n\n\nYou can configure Dynamic Translation for any installed language plugin on\n the ServiceNow\u00ae platform. Once installed, the language is\n available for configuration in the Multi language support card in Virtual Agent Chat Settings. But if you want to support a language\n that doesn't have a plugin, you can add it to the Languages [sys\\_language] table.\n Once added, you can enable dynamic machine translation for that language.\n\n\nFor more information about multi-language support options and their tradeoffs, see\n [Localization options for Virtual Agent](..\/concept\/multi-language-options-va.html \"The ServiceNow platform provides several methods for serving your international Virtual Agent users, regardless of their language and locale. Professionally localize your conversations, use dynamic machine translation, or a combination of the two.\"). For more information\n about custom localizations, see [Custom\n translations](..\/administer\/localization\/concept\/translating-applications.dita\/translating-applications.html).\n\n\n\n1. Navigate to All, and then enter\n sys\\_language.list in the filter.\n2. On the Languages page, click New.\n3. On the form, fill in the fields.\n\n\n\n| Field | Description |\n| --- | --- |\n| Name | Name and locale of the language, if applicable. For  example, Spanish (Mexican). |\n| Active | Check box indicating that the language is enabled on the  instance. |\n| ID | The [BCP 47](http:\/\/www.iana.org\/assignments\/language-subtag-registry\/language-subtag-registry) code for  the language. For example,  es-MX. |\n| Text Direction | Direction of text for the language. Options are  Left-to-Right or  Right-to-Left. |\n| Fallback | Language to be used if a translation is not available.  The language plugin must be installed on the instance and  activated for NLU, if you are using NLU discovery.For  example, if the language is Mexican Spanish, you can  specify Spanish as the fallback  language. |\n4. Click Submit.\n5. **Optional:** \nFor a better user experience, translate some of the Virtual Agent system messages into the new language.\n\t1. Navigate to All, and then enter\n\t sys\\_ui\\_message.list in the filter.\n\t2. Translate the following Keys:\n\t\n\t\n\t\t* Are you sure you want to end the current conversation?\n\t\t* Click here to start a new conversation\n\t\t* Close contact options\n\t\t* [language name] (for example, English or\n\t\t German)\n\t\t* Disable audio notifications\n\t\t* End conversation\n\t\t* I have detected you are typing in {0}. Would you like to\n\t\t continue the conversation in {0}??\n\t\t* Just now\n\t\t* No\n\t\t* No Chat Agents Currently Available\n\t\t* Please pick an option.\n\t\t* Please type your request\n\t\t* Please type your response here\n\t\t* Send\n\t\t* Start a new conversation\n\t\t* Start typing to filter the list of topics below...\n\t\t* Support options\n\t\t* Yes\n\tNote: If you specify a supported fallback language for the language\n\t you're adding, you do not need to translate Keys in the Messages\n\t table. For example, if you add Mexican Spanish as a language,\n\t you can specify Spanish as the fallback language. The Spanish\n\t language plugin already contains translations of these messages.\n\t For more information, see [Specify a fallback language for locale-specific languages and NLU prediction](specify-fallback-language-nlu-prediction.html \"Provide language locale support by specifying a fallback language for Virtual Agent to use for topics, keywords, and NLU prediction. For example, the ServiceNow platform doesn't support Mexican Spanish (mx-es), but it does support Spanish (es), which you can designate as the fallback language for a better user experience.\").\n6. When the languages have been added, [enable Dynamic Translation for\n them in Virtual Agent settings](enable-dynamic-lang-detection.html \"Enable Dynamic Translation for one or more languages in Chat Settings for Virtual Agent.\").\n\n\n\n\n\n**Parent Topic:** [Using language detection and dynamic machine translation in Virtual Agent](..\/..\/..\/administer\/virtual-agent\/concept\/dynamic-lang-detection-translation.html \"Enable a combination of language detection and machine translation for Virtual Agent to improve the chat experience for diverse users. You can designate fallback languages for locales or dynamically translate languages that are not professionally localized or are not supported in NLU discovery.\")\"\n\n",
        "QUERY": " 'what plugins are enabled when spanish plugin is enabled'",
        "GROUND_TRUTH": "no answer found"
    },
    {
        "id": 9,
        "CONTEXT": "\"## \n\nThe purpose of this document is to list and provide an explanation for the HLA System Properties (**sn\\_occ\\_system\\_settings)**  \n  \nThe default (out of the box) settings are generally sufficient, however, in some cases customers may require changes and\/or optimisations to the core HLA System settings for some of the following reasons.\n* Enhance system performance (reduce bottlenecks)?\n* Reduce noise?\n* Adjust behaviour?\n* Suit customer's data?\n\n\nThe Properties below are grouped by subject, they are not in the order that they appear in the System Properties Table in HLA.\n\n\n\n\n---\n\n\n#### **AGGREGATOR**\n\n\nMiddle of the Data Ingestion pipeline. Responsible for grouping and storing of metrics.\n\n\n**aggregator.bloom\\_filter\\_factor**  \nThe coefficient which we should multiply by the Bloom Filter size\n\n\n**aggregator.concurrency\\_override**  \nIf specified, the value will override the initial automatic allocation of resources to the aggregator.\n\n\n**aggregator.gauge.aggregation\\_type**  \nControls whether gauge metrics should be tested using the average or median (default). Accepted values are: Average or Median\n\n\n**aggregator.metrics\\_bloom\\_filter\\_fpp**  \nExpected false positive probability for the Bloom Filter. Used for monitoring purposes.\n\n\n**aggregator.min\\_non\\_null\\_values\\_for\\_stats**  \nMinimum number of non null values in a series to calculate stats for (moving average, stdev...).\n\n\n**aggregator.number\\_of\\_expected\\_metrics**  \nApproximation for how many unique metrics the aggregator should handle.\n\n\n**aggregator.queue\\_size**  \nNumber of metrics that can be buffered in the aggregator before it starts blocking the processing pipe.\n\n\n**aggregator.resolution\\_seconds**  \nThe resolution of the time series. i.e each data point represents the aggregation of data over this period of seconds.\n\n\n**aggregator.settle\\_seconds**  \nHow many seconds should pass without receiving data until the window is considered settled. Once a window is settled, the detective can start running its algorithms.\n\n\n**aggregator.window\\_max\\_quantity\\_in\\_period\\_hours**  \nCircuit-Breaker: Max active time-span of a metric, in hours. Events with metrics that arrive with timestamps spanning a wider time-span will not be aggregated.\n\n\n**aggregator.window\\_size\\_seconds**  \nValue *must* be multiples of 60. How many seconds are considered a window. Windows are the time frame for the detection tasks to be exeucted on.\n\n\n**aggregator.window\\_size\\_seconds\\_custom**  \nValue *must* be multiples of 60. How many seconds are considered a window for a CustomMetric. Windows are the time frame for the detection tasks to be exeucted on.\n\n\n**aggregator.workload\\_level**  \nWork load level in which the Aggregator is considered stressed, options are: LOW, MEDIUM, HIGH.\n\n\n\n\n---\n\n\n#### **ALERTS**\n\n\n**alerts.annotation\\_property**  \nPart of the alert settings that deals with checking annotations for correlations.\n\n\n**alerts.is\\_anomaly\\_baseline\\_reference\\_decrease\\_disabled**  \nIndicates if 'anomaly\\_baseline\\_reference\\_decrease' alert is disabled\n\n\n**alerts.max\\_alert\\_age\\_hours**  \nAnomaly Detection won't apply to events older than this setting. This allows the system to identify and discard alerts that are considered 'too old'. If you are streaming real-time data and still see detection windows being dropped for age, this might indicate: 1) delay in the processing pipeline (for example: a specific Data Input was stopped for a couple of hours, then started again), 2) incorrect extraction of the timestamp field (for example wrong timezone: the timestamp being sent is supposed to be read as Easter Standard Time, but is being read as UTC since there is no indicator in the timestamp). If you are streaming historic data, this setting MUST be increased to include the dates of the historical data. (for example: if today is Jan 2021, and the historical data is being streamed from Jan 2020, please make sure the time here is AT LEAST 8760 (hours) i.e. 365 days \\* 24 hours\/day) {Also note that an additional setting should also be increased: broker.events.max\\_age\\_hours}\n\n\n**alerts.recent\\_events.max\\_size\\_bytes**  \nThe maximum size (bytes) allowed for recent events\n\n\n**alerts.recent\\_events\\_for\\_timeless\\_gauge\\_period\\_seconds**  \nsame as alerts.recent\\_events\\_period\\_seconds but for timeless-gauge detections\n\n\n**alerts.recent\\_events\\_period\\_seconds**  \nTime period to look-back from the point of the anomaly to fetch relevant events - which will be used for RCA. (recommendation is not to exceed around 24 hours in seconds)\n\n\n\n\n---\n\n\n#### **ALERTS CREATOR**\n\n\nThe last part of the pipeline before alerts are created and populated in the incident list.\n\n\n**alerts\\_creator.queue\\_size**  \nNumber of detections that can be buffered in the alerts creator before it starts blocking the processing pipe.\n\n\n\n\n---\n\n\n#### **ARCA** (Automatic Root Cause Analyses):\n\n\nThis refers to the properties extracted via Source Types that are categorized as \"ARC\\_only\".\n\n\n**arca.entities\\_analyzer.max\\_days\\_lookback**  \nTo build the \"meaningful entities\" section of the RCA report, the AI engine goes back up to this number of days to analyze relevant events. (recommendation is not to exceed 2 days)\n\n\n**arca.entities\\_analyzer.max\\_entity\\_occurrences**  \nFor each entity presented in the root-cause section, the AI engine adds events surrounding the detection time. This setting controls the number of such events that will be added.\n\n\n**arca.highlights\\_analyzer.majority\\_vote**  \nIn the highlights analyzer, min number of past matching events from the same host\/day\/hour to qualify as a highlight\n\n\n**arca.highlights\\_analyzer.max\\_days\\_lookback**  \nTo build the \"highlights\" section of the RCA report, the AI engine goes back up to this number of days to analyze relevant events.\n\n\n**arca.highlights\\_analyzer.number\\_of\\_highlights**  \nNumber of highlights to be presented in the \"highlights\" section of the RCA\n\n\n**arca.mf\\_analyzer.number\\_of\\_changes**  \nMax number of changes to show in the \"significant changes\" section in the RCA\n\n\n\n\n---\n\n\n#### **BROKER**\n\n\nThis is the start of the Data Ingestion Pipeline. It is responsible for data integration and digestion, including parsing of the logs.\n\n\n**broker.concurrency\\_override**  \nIf specified, the value will override the initial automatic allocation of resources to the event broker.\n\n\n**broker.events.max\\_age\\_hours**  \nEvents older than this number of hours will be dropped\n\n\n**broker.headerdetection.vmware**  \nlist of vmware apps used by header detection to detect as vmware\n\n\n**broker.header\\_detection.detect\\_beaver**  \nWhen on, the AI engine will attempt to detect and parse beaver headers. Default is ON\n\n\n**broker.header\\_detection.detect\\_syslog5424**  \nWhen on, the AI engine will attempt to detect and parse Syslog5424 headers. Default is ON\n\n\n**broker.queue\\_size**  \nNumber of events that can be buffered in the event broker before it starts blocking the processing pipe.\n\n\n**broker.workload\\_level**  \nWork load level in which the Event Broker considered stressed, options are: LOW, MEDIUM, HIGH.\n\n\n\n\n---\n\n\n#### **CLOTHO**\n\n\n**clotho.batch\\_size**  \nBulk size for persisting data points to Clotho\n\n\n**clotho.duration\\_days**  \nClotho duration time per days\n\n\n**clotho.sampling\\_interval\\_minutes**  \nClotho Sampling Interval per minute\n\n\n\n\n---\n\n\n#### **DATA INPUTS**\n\n\nResponsible for the fetching or receiving of logs from different mediums. ?\n\n\n**data\\_inputs.abstract\\_queue\\_size**  \nQueue size of all data input.\n\n\n**data\\_inputs.examples\\_refresh\\_interval**  \nInterval, in minutes, for updating the data-input examples in the database\n\n\n**data\\_inputs.max\\_length\\_bytes\\_per\\_stream**  \nMax size (in bytes) of a single request that can be handled by any data input\n\n\n**data\\_inputs.preprocess.examples.buffer.size**  \nSize of buffer for preprocess examples\n\n\n**data\\_input\\_mapping.max\\_examples**  \nDefine the maximum number of samples to show on the Data Input Mapping screen, up to 500\n\n\n\n\n---\n\n\n#### **DETECTIVE**\n\n\nTowards the end of the Data Ingestion Pipeline. Responsible for spotting 'regular' and 'anomalous' behavior in the data. Running multiple anomaly detection algorithms.\n\n\n**detective.alive\\_period\\_seconds\\_for\\_signal\\_dead**  \nThe minimum period, in seconds, that a signal has to be alive before \"dropping dead\", for a signal-dead alert to be fired. Additionally, if there was another \"dead signal\" with similar duration in this period, then the current one will be disqualified.\n\n\n**detective.allowed\\_future\\_time\\_minutes**  \nAcceptable futuristic detection period. If a detection was created with a source time further down the future, its handling will be delayed.\n\n\n**detective.amplitude\\_coefficient**  \nThis setting effects the overall sensitivity of the anomaly detection engine. The higher the number, the less alerts you will see.\n\n\n**detective.anomaly\\_detection.enabled**  \nWhen set to false, the AI engine will not attempt anomaly detection.\n\n\n**detective.concurrency\\_override**  \nIf specified, the value will override the initial automatic allocation of resources to the detective.\n\n\n**detective.detection\\_task\\_delay\\_seconds**  \nDelay in seconds before starting a detection task after the corresponding window has settled.\n\n\n**detective.few\\_elements**  \nA deep setting that effects the tolerance of weaker detection techniques\n\n\n**detective.global.mute\\_disabled**  \nWhen set to true, the mute or disable feedback will apply on a specific metric across ALL Application-services\n\n\n**detective.max\\_moments\\_in\\_memory.derivative**  \nThis setting effects the tolerance of the derivative algorithm\n\n\n**detective.max\\_moments\\_in\\_memory.signal\\_alive**  \nHow many \"similar-in-amplitude\" bursts should the signal-alive detector allow in the preceding period. This setting is in effect when raising an alert\n\n\n**detective.max\\_moments\\_in\\_memory.signal\\_dead**  \nHow many \"dead periods\" should the signal-dead detector allow. This setting is in effect when raising an alert\n\n\n**detective.memory\\_in\\_days**  \nThe memory, in days, of the different anomaly detection models (baseline, derivative and others)\n\n\n**detective.min\\_events\\_per\\_window**  \nThe min number of events per window for a detection to be triggered.\u00a0\n\n\n**detective.of\\_custom\\_alert\\_concurrency\\_override**  \nIf specified, the value will override the initial automatic allocation of resources to the customAlertDetective.\n\n\n**detective.points\\_in\\_timeless\\_trend**  \nThe number of samples to consider when testing for trend shifts in disperse metrics\n\n\n**detective.queue\\_over\\_capacity\\_percent**  \nWill create a system notification when the detective queue is greater than value% capacity for over 5 minutes.\n\n\n**detective.queue\\_size**  \nNumber of detection tasks that can be buffered in the detective before it starts blocking the processing pipe.\n\n\n**detective.resolution.signal\\_dead**  \nThe number of seconds a metric's signal must be consecutively \"dead\" (no data, graph showing zero) for a signal-dead detection to be triggered for this metric. This setting can be configured per source.\n\n\n**detective.sigma\\_coefficient**  \nThe coefficient for the sigma-based anomaly detection\n\n\n**detective.workload\\_level**  \nWork load level in which the Detective considered stressed, options are: LOW, MEDIUM, HIGH.\n\n\n\n\n---\n\n\n#### **ELASTICSEARCH**\n\n\n**elasticsearch.bulk\\_actions**  \nNumber of entities in one bulk request, using as threshold for elastic bulk operation (together with the request size).\n\n\n**elasticsearch.bulk\\_concurrent\\_requests**  \nConcurrency of the Bulk write-requests to Elastic\n\n\n**elasticsearch.bulk\\_size\\_mb**  \nSize of the bulk request in MB, using as threshold for elastic bulk operation (together with the request entities number threshold).\n\n\n**elasticsearch.client.connect\\_timeout\\_millis**  \nConfigures the timeout in milliseconds until a connection is established to elasticsearch.\n\n\n**elasticsearch.client.io\\_threads**  \nConfigures the number of I\/O dispatch threads to be used by the elasticsearch client\n\n\n**elasticsearch.client.socket\\_timeout\\_millis**  \nConfigures the socket timeout in milliseconds to elasticsearch, which is the timeout for waiting for data or, put differently, a maximum period inactivity between two consecutive data packets.\n\n\n**elasticsearch.concurrency**  \nHow many threads should index to elasticsearch\n\n\n**elasticsearch.flush\\_interval\\_seconds**  \nBulk flush interval for indexing. Bulk will execute sooner if either bulk\\_actions or bulk\\_size\\_mb has been reached.\n\n\n**elasticsearch.mapping\\_keyword\\_properties**  \nBy default, all string properties are indexed as 'keyword' (except message, rawMessage, stacktrace, and the additional\\_string\\_properties which are indexed as 'text') which allows aggregation but no partial searches. Any field with the 'property' prefix (e.g. 'property.UUID' ,or 'property.srcIp') can also be indexed as 'keyword'. Note that a change will only apply to newly created indices. Please also note that when inserting the value you are not adding the prefix 'property'.\n\n\n**elasticsearch.minimal\\_indexing**  \nWhen true, properties classified as invalid will not be indexed\n\n\n**elasticsearch.queue\\_size**  \nBulk size for indexing.\n\n\n**elasticsearch.subsampling\\_ratio**  \nUse this to only index some of the events to Elastic. 1 -> index 1 out of 1 events. 2 -> index 1 out of 2 events. N -> index one out of N events.\n\n\n\n\n---\n\n\n#### **EVENTS**\n\n\n**events.keyword\\_extraction\\_non\\_patterned**  \nKeyword extraction from the non-patterned message, when there is no pattern (i.e. message label is not assigned)\u00a0\n\n\n**events.max\\_minutes\\_in\\_future**  \nEvents that are further in the future than this will be dropped. Note: If you see event being dropped due to future timestamp double check that your timestamps are in the correct timezone. \u00a0\n\n\n\n\n---\n\n\n#### **GLIDE**\n\n\n**glide.datainput.max\\_errors\\_percentage\\_before\\_publish**Define the max % of errors in a data input before publishing a notification.\n\n\n**glide.table.change\\_detection.interval\\_seconds**  \nInterval, in seconds, for getting tables that changed in glide\n\n\n**grpc.port**  \nDefine glide port\n\n\n**health\\_log\\_analytics.use\\_case\\_export.enabled**Enables\/disables the migration of data inputs and source types between instances.\n\n\n\n\n---\n\n\n#### **INCIDENTS**\n\n\n**incidents.alerts.dilute\\_target**  \nWhen the number of alerts in an incident is too high (see incident.alerts.max\\_count), alerts are diluted (removed) until this number is reached.\n\n\n**incidents.alerts.max\\_count**  \nMaximum number of alerts in incident, to start the dilution process of excess alerts.\n\n\n**incidents.alert\\_interval\\_seconds**  \nTime-span to consider two alerts as related if correlating by occurrence time\n\n\n**incidents.application\\_correlation**  \nShould the alert application be taken into account when correlating alerts\n\n\n**incidents.component\\_correlation**  \nShould the alert service be taken into account when correlating alerts\n\n\n**incidents.cooldown\\_period\\_minutes**  \nMinutes to wait after the creation of an incident before sending a notification about it.\n\n\n**incidents.detection\\_time\\_correlation**  \nPlease note that time frame of the correlation window is defined using: incidents.alert\\_interval\\_seconds setting\n\n\n**incidents.detection\\_type\\_correlation**  \nShould the alert detection type (anomaly, signal-dead, baseline etc.) be taken into account when correlating alerts\n\n\n**incidents.entities**  \nList of entities that will be used for correlation if two alerts shared the same value. This setting should be managed from the correlations setting pages (global or individual per-source)\n\n\n**incidents.host\\_correlation**  \nShould the alert host be taken into account when correlating alerts\n\n\n**incidents.min\\_correlation\\_score\\_for\\_aggregating**  \nSensitivity level for the correlation engine. The higher the number is, the more alerts will need to have in common in order to be correlated\n\n\n**incidents.pattern\\_text\\_correlation**  \nShould the alert pattern-text be taken into account when correlating alerts\n\n\n**incidents.period\\_seconds**  \nDefines the time-frame for the Alerts Smart-Correlations logic (an alert might only correlate with alerts created in the preceding T hours)\n\n\n\n\n---\n\n\n#### **KEYWORDS**\n\n\n**dictionaries.resource.directory**  \nDirectory name in which the dictionaries used in keyword's message extraction process are\n\n\n**keywords.message.extractor**  \nWhen set to false, the AI engine will not attempt to automatically extract the message from Keyword-based alerts\n\n\n**keywords.message.max.length**  \nMessages over the maximal specified length will not be extracted\n\n\n**keywords.message.stop.elsa.message**  \nWhen set to true, Elsa will not attempt to automatically extract the message labels\n\n\n\n\n---\n\n\n#### **LICENSING**\n\n\n**licensing.flush\\_interval\\_seconds**  \nTime interval after which nodes will flushed to the glide table\n\n\n**licensing.max\\_map\\_size**  \nMaximum number of nodes stored in memory before flushing to glide table\n\n\n**licensing.monitoring.interval\\_seconds**  \nTime interval after which licensing monitoring service wakes up to check for new nodes\n\n\n\u00a0\n\n\n\n\n---\n\n\n#### **LOGSOURCEINFO (CMDB)\\***\n\n\n**logsourceinfo.flush\\_interval\\_seconds**  \nTime interval, in seconds, for collecting log source host data and forwarding it to the Log-based CI candidate table. (Default value = 3600)\n\n\n**logsourceinfo.max\\_map\\_size**  \nMaximum number of data nodes to be stored before the data is forwarded to the Log-based CI candidates table. (Default value = 1000)\n\n\n**logsourceinfo.monitoring.interval\\_seconds**  \nTime interval, in seconds, for scanning log events to discover host-related data. (Default value = 60)\n\n\n\\**NEW system properties available starting from HLA December Store release*\n\n\n\n\n---\n\n\n#### **METRICATOR**\n\n\nMiddle of the Data ingestion pipeline. Responsible for storing and measuring unique metrics.\n\n\n**metricator.cache\\_eviction\\_factor**  \nNumber of raw metrics to evict from the cache when eviction is needed\n\n\n**metricator.cache\\_size**  \nMaximum number of raw metrics to hold in memory\n\n\n**metricator.concurrency\\_override**  \nIf specified, the value will override the initial automatic allocation of resources to the metricator.\n\n\n**metricator.exclude\\_extended\\_keyword\\_metrics**  \nA list of regex that will exclude extended keyword metrics of type ERROR\/EXCEPTION from being created. ex: .\\*love.\\*\n\n\n**metricator.min\\_severity**  \nMinimum severity of an event for creating raw and pattern metrics\n\n\n**metricator.new\\_pattern\\_min\\_severity**  \nMinimum severity of an event for creating new pattern metrics\n\n\n**metricator.queue\\_size**  \nNumber of events that can be buffered in the metricator before it starts blocking the processing pipe.\n\n\n**metricator.workload\\_level**  \nWork load level in which the Metricator considered stressed, options are: LOW, MEDIUM, HIGH.\n\n\n\n\n---\n\n\n#### **NOTIFICATIONS**\n\n\n**notifications.default.recipients.configuration\\_notifications**  \nDefault recipients of configuration-related notifications, such as JS errors, timestamp parsing etc.\n\n\n**notifications.default.recipients.operational\\_notifications**  \nDefault recipients of operations-related notifications, such as Crashes\n\n\n\n\n---\n\n\n#### **PATTERNATOR**\n\n\nPattern recognition and analysis\n\n\n**patternator.cache\\_eviction\\_factor**  \nNumber of patterns to evict from the cache when eviction is needed\n\n\n**patternator.cache\\_size**  \nMaximum number of patterns to hold in memory\n\n\n**patternator.concurrency\\_override**  \nIf specified, the value will override the initial automatic allocation of resources to the patternator.\n\n\n**patternator.gbp.bulk.queue\\_size**  \nMax number of GBP statements pending to be written to the DB, before updates start getting dropped.\n\n\n**patternator.gbp.bulk.size**  \nNumber of statements to trigger an update to the DB\n\n\n**patternator.gbp.examples.until.greedy**  \nNum of events to learn which greedy replacements to use\n\n\n**patternator.gbp.max.node.chars**  \nMax number of characters in gbp node\n\n\n**patternator.queue\\_size**  \nNumber of events that can be buffered in the patternator before it starts blocking the processing pipe.\n\n\n**patternator.rate\\_limit**  \nMaximum number of new patterns per second\n\n\n**patternator.**\"\n\n",
        "QUERY": " 'what is the HLA system property to discard old anomaly detection alerts'",
        "GROUND_TRUTH": "The HLA system property to discard old anomaly detection alerts is \"alerts.max_alert_age_hours\". This property determines the maximum age of alerts for anomaly detection. Anomaly Detection won't apply to events older than this setting. This allows the system to identify and discard alerts that are considered 'too old'. If you are streaming real-time data and still see detection windows being dropped for age, this might indicate a delay in the processing pipeline or incorrect extraction of the timestamp field. If you are streaming historic data, this setting must be increased to include the dates of the historical data."
    },
    {
        "id": 10,
        "CONTEXT": "\"## \n\nPenetration testing\n# Penetration testing\n\n\nPenetration testing in Application Vulnerability Response enables application\n owners to assess the security posture of their application. It is the manual testing of an\n application by the ethical hacking team.\n\n\n## Roles required\n\n\nPenetration testing requires the following roles:\n\n\nApp-Sec Manager: Contains security managers and application\n owners who manage the penetration testing assessment requests. It contains the\n following granular roles:\n\n\n* sn\\_vul.app\\_manage\\_pen\\_test\\_request\n* sn\\_vul.app\\_read\\_all\n* cmdb\\_read\n\n\n\nEthical Hacker: Contains members of the ethical hacking team\n who perform penetration testing of applications. It includes the following granular\n roles:\n\n\n* sn\\_vul.app\\_update\\_assignment\\_group\n* sn\\_vul.app\\_update\\_assigned\\_to\n* sn\\_vul.app\\_manage\\_manual\\_avits\n* sn\\_vul.app\\_manage\\_pen\\_test\\_request\\_config\n* itil\n* sn\\_vul.app\\_read\\_all\n* sn\\_vul.app\\_manage\\_pen\\_test\\_request\n* sn\\_vul.app\\_update\\_state\n\n\n\nFor more information about these roles, see [Manage Application Vulnerability Response user groups and roles](avm-manage-roles.html \"Before you can successfully remediate vulnerabilities with Application Vulnerability Response (AVR), you must assign users to user groups.\").\n\n\n\n## Life cycle of penetration testing\n\n\nAs an application owner, you can request the ethical hacking team for a penetration\n test assessment of your application. The ethical hacking team acts on this request\n and creates penetration test findings. These findings are manually-created\n Application Vulnerable Items (AVIs).\n\n\nThe penetration testing workflow covers the penetration testing life cycle from\n raising the testing request to resolving the findings of the ethical hacking\n team.\n\n\n\n## Requesting a penetration test assessment\n\n\nAs the application owner, you can request a penetration test assessment for your\n application using the ITSM service catalog.\n\n\n\n## Reviewing the penetration test assessment request\n\n\nThe ethical hacking team reviews and assesses the application and the scope of the\n penetration test assessment request, and adds it to the existing backlog.\n\n\n\n## Preparing an environment\n\n\nThe ethical hacking team then sends a request to the application owner to provide an\n environment for them to start testing. Once the environment is ready, the\n application owner informs the ethical hacking team.\n\n\n\n## Testing and reporting the penetration test findings\n\n\nThe ethical hacking team tests the application and reports the findings to the\n application owner. The ethical hacking team also defines the Service Level\n Agreements (SLAs) for the penetration test findings using the remediation target\n date. These findings are the manually-created AVIs. The application owner in turn\n reviews the AVIs created by the ethical hacking team. They plan the fixes and assign\n them to the application team.Note: Remediation target rules do not apply to the\n penetration test findings.\n\nThe ethical hacking team can create a library of Application Vulnerability Entries\n (AVEs) and reuse them while reporting the AVIs. They can also track the status of\n the penetration test findings.\n\n\n\n## Fixing and validating the penetration test findings\n\n\nAfter the penetration test findings are fixed and resolved by the application team,\n the fixes are validated manually and closed by the ethical hacking team.\n\n\n\n## Application Vulnerability Management reports\n\n\nUse the reports available on the Application Vulnerability ManagementPA dashboard to\n track the penetration test findings.\n\n\nFigure 1. Penetration testing life cycle\n![Penetration testing life cycle.](..\/image\/pen-testing-workflow.png)\"\n\n",
        "QUERY": " 'What is ServiceNow's application pen test procedure?'",
        "GROUND_TRUTH": "ServiceNow's application penetration testing procedure is a systematic approach to assess the security posture of an application. It involves the following steps:\n\n1. Requesting a Penetration Test Assessment: As an application owner, you can request a penetration test assessment for your application using the ITSM service catalog.\n\n2. Reviewing the Penetration Test Assessment Request: The ethical hacking team reviews and assesses the application and the scope of the penetration test assessment request, and adds it to the existing backlog.\n\n3. Preparing an Environment: The ethical hacking team sends a request to the application owner to provide an environment for them to start testing. Once the environment is ready, the application owner informs the ethical hacking team.\n\n4. Testing and Reporting the Penetration Test Findings: The ethical hacking team tests the application and reports the findings to the application owner. The ethical hacking team also defines the Service Level Agreements (SLAs) for the penetration test findings using the remediation target date. These findings are the manually-created Application Vulnerable Items (AVIs).\n\n5. Fixing and Validating the Penetration Test Findings: After the penetration test findings are fixed and resolved by the application team, the fixes are validated manually and closed by the ethical hacking team.\n\n6. Application Vulnerability Management Reports: Use the reports available on the Application Vulnerability ManagementPA dashboard to track the penetration test findings.\n\nThe procedure requires two main roles: the App-Sec Manager and the Ethical Hacker, each with their own set of responsibilities and granular roles."
    },
    {
        "id": 11,
        "CONTEXT": "\"## \n\nThe Sweagle CLI offers a standard integration to recursively loop through a Github based repository and search for specific file extensions. Any file found is parsed and the configuration data is uploaded to Sweagle.\u00a0\n\n\n### Setup\n\n\n#### Create Access Token In Github\n\n\nCreate a unique access token for your Github repository with access scope \"repo\". Note down the access token because Github will not show it again once you leave this page.\u00a0\n\n\n  \n\n\n\n[![](sys_attachment.do?sys_id=f69214be1b3e54d06531ea89bd4bcb1d)](https:\/\/s3.amazonaws.com\/cdn.freshdesk.com\/data\/helpdesk\/attachments\/production\/43126281921\/original\/wsY9catEGiWLQrUP-jflTxAZyQazaWeIbg.png?1588566454)\n  \n\n\n\nNote that at the time of writing of this article, you have to select the checkbox at the scope level \"repo\". Only selecting the 5 list items without the top-level checkbox is not sufficient.\u00a0\n\n\n#### Sweagle CLI Git Configuration\n\n\nThe GitHub integration is optional in the setup of the Sweagle CLI.\u00a0\n\n\nIn case of a fully interactive installation of the Sweagle CLI, confirm that the GitHub integration should be set up and follow the instructions. Enter the Git access token.\u00a0\n\n\nIf you have already a working Sweagle CLI, you can add the GitHub details by following the steps of the gitConfig option.\u00a0\n\n\n\n\n```\n.\/sweagle gitConfig\n```\n\nIn case you only want to change the GitHub access token, you can update the token using the \"-t\" argument\n\n\n\n\n```\n.\/sweagle gitConfig -t 123accesstoken456\n```\n\n### Usage\n\n\n#### Interactive Mode\n\n\nThe CLI interactive mode will guide you through the steps in order to configure the recursive search throughout the repository and upload all found configuration data to Sweagle. \u00a0\n\n\n![](sys_attachment.do?sys_id=7e9214be1b3e54d06531ea89bd4bcbe7)\n  \n\n\n\nNote that the remote repository should be an exact case sensitive name of the repository. It is currently not supported to limit the search to only a specific folder path in the repository (unlike with the terminal command mode where this is already implemented - see below).\n\n\n#### Terminal Command Mode\n\n\nThe CLI command mode offers various possibilities:\u00a0\n\n\n1. provide a repository name and it will recursively scan through the content and filter all files with the filtered extensions as configured through the Sweagle gitConfig\n2. Provide a repository name and an optional folder name, and the Sweagle CLI will limit its search to only the provided folder path (use \/ for the path separation)\n3. provide a full path to a single file, and the Sweagle CLI will only upload that single file.\n\n\n**Example 1**: will parse through the content of the \"PRD\" folder in the repo named \"myApp\", and upload any files found with one of the default extensions to Sweagle on the nodePath \"CRM,environments\".\n\n\n\n\n```\n.\/sweagle gitUploadData myApp -f PRD -n CRM,environments\n```\n\n**Example 2**: will upload the content of a single specific file in the repo to a specific path in Sweagle.\n\n\n\n\n```\n.\/sweagle gitUploadData myApp -f TST\/application.yml -n CRM,myenvironments,TST\n```\n\nIn case the recursive search through a repository (folder) should be limited to a specific set of file extensions, then first update the file extensions configuration to only the extensions required. After that, run the command as above.\n\n\n\n\n```\n.\/sweagle gitConfig -f ini,txt\n.\/sweagle gitUploadData myApp -n CRM,environments\n```\"\n\n",
        "QUERY": " 'what is an interactive mode in Sweagle CLI?'",
        "GROUND_TRUTH": "The interactive mode in Sweagle CLI is a user-friendly feature that guides users through the steps to configure the recursive search throughout a Github repository and upload all found configuration data to Sweagle. In this mode, users are prompted to provide necessary information such as the Github repository name. It's important to note that the repository name should be an exact case-sensitive match. Currently, the interactive mode does not support limiting the search to a specific folder path in the repository."
    },
    {
        "id": 12,
        "CONTEXT": "\"## \n\nSet the deletion strategy for tracked configuration files\n# Set the deletion strategy for tracked configuration files\n\n\nYou can specify what you want to do with tracked configuration file CI records when\n pattern discovery can no longer find them.\n\n\n\n* Role required: discovery\\_admin, sm\\_admin, or admin\n* A pattern that specifies [tracked\n configuration files](track-configuration-files.html \"Configure the system to collect information about changes in configuration files belonging to a configuration item (CI). Service Mapping uses this information to notify users that CI configuration files changed and to view actual changes to configuration files directly in the application service maps.\").\n\n\n\nYou can set a deletion strategy for all discovered CIs, not\n just configuration file CIs. However the way you access the deletion strategy for\n standard CIs differs from configuration file CIs. See [Set a deletion strategy](set-deletion-strategy.html \"Set a deletion strategy when you want to take action on a related CI that Discovery can no longer find through pattern discovery. You can delete the main CI only when it is an AWS or Azure cloud database.\") for instructions.\n1. Navigate to All > Pattern Designer > Discovery Patterns and open the desired pattern.\n2. Click the Tracked Files tab.\n3. Under Related Links, click Edit deletion\n strategy for the configuration files of this pattern.\nThe Related CI Types list appears showing you filtered records in the\n Related CI Type [sa\\_ci\\_to\\_pattern] table. The filter shows you only records with\n the CI type of Tracked Configuration files [cmdb\\_ci\\_config\\_file\\_tracked], which\n is the table where the system saves configuration file CIs.\n4. From the list view, double-click the value in the field in the\n Deletion Strategy column for the tracked\n configuration file.\n5. Select a new value:\n\n\n\n\n| Value | Description |\n| --- | --- |\n| Keep | Preserves the configuration file CI record and makes no other changes to the  record. This option is the default setting. |\n| Delete | Deletes the configuration file CI record from the CMDB, and the relationship  to the main CI. |\n| Mark as absent | Marks the Status [install\\_status] field of the  configuration file CI record as Absent, meaning that  Discovery cannot find the tracked configuration file. This status does not  instruct the system to delete the actual CI or the relationship.Note: There are  two tables that do have their CIs deleted if marked as absent:  cmdb\\_ci\\_network\\_adapter and cmdb\\_ci\\_ip\\_address. |\n| Delete relations | Deletes only the CI relationships between the related CI and the main CI. |\n| Mark as retired | Marks the Status [install\\_status] field of the  configuration file CI record as Retired, meaning that  Discovery no longer uses this configuration file. This status does not instruct  the system to delete the actual CI or the relationship. |\n\n\n\n\n**Related concepts**  \n\n* [CI deletion strategies for pattern discovery](..\/concept\/deletion-strategy.html \"When you perform discovery with a pattern, you can choose what to do with CIs that are in the CMDB but Discovery can no longer find.\")\n\n**Related tasks**  \n\n* [Set a deletion strategy](set-deletion-strategy.html \"Set a deletion strategy when you want to take action on a related CI that Discovery can no longer find through pattern discovery. You can delete the main CI only when it is an AWS or Azure cloud database.\")\"\n\n",
        "QUERY": " 'what is a tracked configuration file'",
        "GROUND_TRUTH": "no answer found"
    },
    {
        "id": 13,
        "CONTEXT": "\"## \n\nMigrate to CSDM life cycle standards\n\n# Migrate to CSDM life cycle standards\n\n\nCommon Service Data Model (CSDM) provides standard fields and values for tracking life\n cycle stages and stage statuses for CIs. Using these standard values consistently across\n applications helps to effectively track assets through their life cycle transitions.\n\n\n\nStreamline life cycle management by migrating all life cycle fields and field values across\n the platform, into a CSDM standard set of fields with a standard set of values. Then, to\n maintain consistency, continue to use only the standard fields and values when updating or\n creating CIs. Using standard life cycle values is important in scenarios such as:* Event Management and Operational Intelligence need to know if a CI is in maintenance to be\n able to handle alerts accordingly.\n* Cloud Insights need to know the state of a CI to be able to report cost data\n effectively.\n* Audit and Compliance need to use standard life cycle values to be able to generate\n consistent tasks and workflows.\n\n\n\nYou must [activate life cycle\n migration](..\/task\/activate-life-cycle-migration.html) to migrate legacy life cycle values in the system to the CSDM standard fields\n and values.\n\n\n\n## Life Cycle Mapping table\n\n\nThe base system contains the Life Cycle Mapping [life\\_cycle\\_mapping] table, which is\n pre-populated with different legacy life cycle mappings, that are widely used. Each mapping\n record specifies how to map a legacy life cycle field\u0092s value, based on which table it\u0092s in, to\n CSDM life cycle standards. Similar legacy stage and status from different legacy tables might\n map to different CSDM life cycle values. Life cycle migration stores standard stage and stage\n status values in the following fields:* Life cycle stage (life\\_cycle\\_stage): The broad life cycle phases that\n a CI moves through, from inception or procurement to retirement and end of life. Different\n types of CIs can have different types of stages based on whether they apply or not.\n* Life cycle status (life\\_cycle\\_stage\\_status): The specific status of a\n CI within its current life cycle phase. The stage status narrows down the life cycle stage of\n a CI and more accurately defines its current state.\n\n\n\nThe standard values specified in the Life Cycle Mapping [life\\_cycle\\_mapping] table limit the\n values that the Life cycle stage and the Life cycle\n status fields can be set to. The standard values are the only valid settings for\n these fields.\n\n\nThe Life Cycle Mapping [life\\_cycle\\_mapping] table typically contains multiple record entries\n per class, each entry for a specific legacy life cycle and life cycle value pair. When there are\n multiple record entries for the same class, the entries are prioritized by importance and\n likelihood for containing meaningful values to use for the mapping process.\n\n\n\n## Custom life cycle values\n\n\nIn an upgrade scenario, life cycle migration checks for custom legacy life cycle values that\n were added in the system. For each custom value, if any, the system adds a record to the Life\n Cycle Mapping [life\\_cycle\\_mapping] table. However, those mapping records are incomplete and\n inactive.\n\n\nBefore you activate life cycle migration, you must edit and activate those records to supply\n the desired life cycle control to use for mapping.\n\n\n\n\n\n## Activate life cycle migration\n\n\nMap any custom and non-standard life cycle settings in the CMDB, to the Common\n Service Data Model (CSDM) standard life cycle values of stages and stage status values.\n Activating life cycle migration applies to existing data and to incoming data.\n\n\n\nBefore activating life cycle migration, navigate to CSDM > Life Cycle Mapping. Review the pre-populated mappings in the Life Cycle Mappings list\n view:* Adjust and add any mappings as needed for your environment.\n* Review mappings for any custom legacy life cycle values. Those mappings are\n incomplete and you must provide the desired standard life cycle control to\n map to.\n* Ensure that all mappings are configured with a life cycle control.\n* Ensure that all mappings are activated.\n\n\n\nRole required: itil\\_admin or asset\\_admin\n\n\n\n1. Navigate to CSDM > Life Cycle Mapping.\n2. On the Life Cycle Mappings list view, click\n Activate.\nActivation can complete only if all mapping records are set to active and are\n configured with a life cycle control, and all mapping records for custom legacy\n values are fully configured.\n\n\n\nActivating life cycle migration starts the following\n processes:1. One-time bulk mapping of legacy life cycle values to the new Life\n cycle stage and Life cycle status\n fields. The mappings are based on the mapping records in the Life Cycle\n Mapping table, which contain values, source, and target fields.\n2. Setting the csdm.lifecycle.migration.activated system property to true (set to false in the base system), which then activates the Update life cycle from legacy business rule. Future insert or\n update CI operations will then trigger this business rule to populate the standard Life cycle stage and Life cycle status fields. These processes ensure that the life cycle\n standards are used continually and consistently.\n\n\nFor example, when creating a Hardware CI, and setting the legacy\n Status and Operational\n status fields. After saving, the new life cycle standard\n fields are automatically populated with the matching life cycle standard\n values based on the corresponding record entry in the Life Cycle Mapping\n table. If you modify legacy values, the standard fields are\n automatically updated based on another matching record in the Life Cycle\n Mapping table.\n\n\n\nLife cycle migration is a one-time process. Therefore, after activation,\n the Activate button is no longer available.\nAfter the data has migrated successfully, you can start\n managing data following the CSDM model:1. [Activate the CSDM\n Activation (com.snc.cmdb.csdm.activation) plugin](..\/administer\/plugins\/task\/t_ActivateAPlugin.dita\/t_ActivateAPlugin.html).\n2. Use the [CMDB Data\n Manager](..\/concept\/cmdb-data-management.html) to centrally govern the life cycle of CIs, in bulk, and in a\n standard and consistent way.\n\n\n\n\n\n\n## Create a mapping record for lifecycle migration\n\n\nIn addition to pre-populated lifecycle mappings in the Life Cycle Mapping table, you\n can add mappings for any custom lifecycle values used in your environment. Lifecycle\n mappings are then used to populate the CSDM standard lifecycle fields with standard\n lifecycle stage and status values.\n\n\n\nRole required: itil\\_admin or asset\\_admin\n\n\n\n1. Navigate to All > CSDM > Life Cycle Mapping.\n2. On the Life Cycle Mappings list view, click New.\n3. Fill out the Life cycle mappings form.\n\n\n\n| Field | Description |\n| --- | --- |\n| Mapping for table | Legacy CMDB table and descending tables, that this  mapping applies to. Applies to a descending table unless  there is a mapping configured specifically to that  descending table. |\n| Priority | Priority of applying this mapping definition for the  table. Priority is used when the Life Cycle Mapping table  contains multiple entries for the same class. The  highest priority entry is used first when searching for  meaningful legacy values. If the first entry can't be  used, the next record in priority is used. Lower numerical values, indicate higher priority. |\n| Active | Denotes whether to apply this mapping definition or  not. Deactivation results in lower-priority mappings being  used, or setting standard lifecycle fields to 'TBD'. |\n| Legacy field name | Legacy field in the specified Mapping for  table that is currently being used for  lifecycle management to store a lifecycle stage. The value  should be used as the source for the lifecycle mapping. |\n| Legacy field value | Legacy field in the specified Mapping for  table that is currently being used for  lifecycle management to store lifecycle status. The value  should be used as the source for the lifecycle  mapping. |\n| Legacy subfield name | Additional legacy field in the specified  Mapping for table which is also  used for lifecycle management. |\n| Legacy subfield value | Additional legacy field in the specified  Mapping for table which is also  used for lifecycle management. |\n| Life cycle control | Class, and life cycle stage and status, which are used as  the authoritative source of valid combinations for life  cycle mapping. |\n| Table | Standard life cycle table to map the specified  Mapping for table to. Setting is  based on the selection in Life cycle  control. |\n| Lifecycle stage | Standard life cycle stage to map the specified  Legacy field name to. Setting  is based on the selection in Life cycle  control. If there is no match in the Life Cycle Mapping  [life\\_cycle\\_mapping] table, value is set to 'TBD'. |\n| Lifecycle stage status | Standard lifecycle stage value to map the specified  Legacy field value to.  Setting is based on the selection in Life  cycle control. If there is no match in the Life Cycle Mapping  [life\\_cycle\\_mapping] table, value is set to 'TBD'. |\n4. Click Submit.\"\n\n",
        "QUERY": " 'what is a consume step in the life cycle of a configuration data'",
        "GROUND_TRUTH": "no answer found"
    },
    {
        "id": 14,
        "CONTEXT": "\"## \n\n## Description\n\nWhen you change a sys\\_dictionary's Attributes values and then attempt to render a page that loads a Glide variable that is affected by the Attributes on that Dictionary record, the value you set in Attributes is not present. cache.do and hard browser cache refresh have no effect. Only after restarting the node do the changes begin to render properly on the Glide variable.\n\n## Steps to Reproduce\n\n 1. Go to \/sys\\_dictionary\\_list.do and search for the entry with the following values:\n\n\n\t* table = sys\\_user\n\t* type = collection\n\t* Attributes contains \"ref\\_auto\\_completer=AJAXTableCompleter\"\n2. Change the Reference auto completer attribute from AJAXTableCompleter to \"MY\\_NEW\\_BETTER\\_VALUE\" and click **Update**.\n3. Confirm that sys\\_schema\\_attribute\\_m2m now contains a record where value = \"MY\\_NEW\\_BETTER\\_VALUE\".\n4. Render the associated Glide variable.\n\n\n\t1. Navigate to **Automated Test Framework > Tests**, start a new test record and click **Save**.\n\t2. On the saved Test form, click **Add Test Step**.\n\t3. Select **Server > Impersonate** and click the **Next** button.\n5. Inspect the User field input element and look at the DOM.\n\n\nInstead of the data-completer value having changed to MY\\_NEW\\_BETTER\\_VALUE, it remained AJAXTableCompleter.\n\n\n\u00a0\n\n\n\u00a0\n\n## Workaround\n\nFor an immediate fix, you can flush the cache by running **gs.invalidateCache()**. However, this can cause a performance degradation of the node and should be done only if you need urgent relief.\u00a0\n\n  \n**Related Problem: PRB1034023**\"\n\n",
        "QUERY": " 'Value set in attributes lost while changing the sys_dictionary's attribute values'",
        "GROUND_TRUTH": "When you change a sys_dictionary's Attributes values and then attempt to render a page that loads a Glide variable affected by the Attributes on that Dictionary record, the value you set in Attributes is not present. This issue persists even after cache.do and hard browser cache refresh. The changes only begin to render properly on the Glide variable after restarting the node. \n\nTo reproduce this issue, you can follow these steps:\n1. Go to \/sys_dictionary_list.do and search for the entry with the table = sys_user, type = collection, and Attributes contains \"ref_auto_completer=AJAXTableCompleter\".\n2. Change the Reference auto completer attribute from AJAXTableCompleter to \"MY_NEW_BETTER_VALUE\" and click Update.\n3. Confirm that sys_schema_attribute_m2m now contains a record where value = \"MY_NEW_BETTER_VALUE\".\n4. Render the associated Glide variable by navigating to Automated Test Framework > Tests, start a new test record and click Save. On the saved Test form, click Add Test Step. Select Server > Impersonate and click the Next button.\n5. Inspect the User field input element and look at the DOM. You will find that the data-completer value has not changed to MY_NEW_BETTER_VALUE, it remained AJAXTableCompleter.\n\nFor an immediate fix, you can flush the cache by running gs.invalidateCache(). However, this can cause a performance degradation of the node and should be done only if you need urgent relief."
    },
    {
        "id": 15,
        "CONTEXT": "\"## \n\n## Description\n\nNon-admin users can't update date type fields (multiple records) via List v3.\n\n## Steps to Reproduce\n\n 1. Enable List V3.  \n2. Go to 'kb\\_knowledge' table list and change List edit type to \"Save immediately (cell edit mode)\" in List Control.  \n3. Impersonate \"Bernard Laboy\" (a non-admin who has \"knowledge\\_manager\" role).  \n4. Go to 'kb\\_knowledge' table and add 'Valid To' field to the list view, if not present.  \n5. Select multiple records via 'Valid To' field column and double-click to update it.  \n6. You will get 500 status error in the console as well as the following server error:  \n...  \nSEVERE \\*\\*\\* ERROR \\*\\*\\* java.lang.NullPointerException  \njava.lang.RuntimeException: java.lang.NullPointerException  \nat com.glide.rest.handler.impl.ServiceHandlerImpl.handleInvocationTargetException(ServiceHandlerImpl.java:79)  \nat com.glide.rest.handler.impl.ServiceHandlerImpl.invokeService(ServiceHandlerImpl.java:50)  \nat com.glide.rest.processors.RESTAPIProcessor.process(RESTAPIProcessor.java:228)  \nat com.glide.processors.AProcessor.runProcessor(AProcessor.java:474)  \nat com.glide.processors.AProcessor.processTransaction(AProcessor.java:199)  \nat com.glide.processors.ProcessorRegistry.process0(ProcessorRegistry.java:178)  \nat com.glide.processors.ProcessorRegistry.process(ProcessorRegistry.java:167)  \n....  \n  \nUser is able to update single record at a time in the list view though.\n\n## Workaround\n\nDisable List v3 overall or for specific table via List Control.\n\n  \n**Related Problem: PRB1280817**\"\n\n",
        "QUERY": " 'User unable to update multiple records in list control'",
        "GROUND_TRUTH": "The issue of a user being unable to update multiple records in list control is related to non-admin users trying to update date type fields (multiple records) via List v3 in the 'kb_knowledge' table. The problem arises when the user tries to update multiple records via the 'Valid To' field column, resulting in a 500 status error and a server error. However, the user can update a single record at a time in the list view. A workaround for this issue is to disable List v3 overall or for the specific table via List Control."
    },
    {
        "id": 16,
        "CONTEXT": "\"## \n\n## Description\n\nUnable to install\/update an application customization on a production instance if there is a newer base version of the app that is not installed on any sub-prod instances.\n\n## Steps to Reproduce\n\n 1. Install an older version of a store application that allows app customization on sub-prod and production instances.  \nEg: Workplace Reservation Management at version 1.8.5 (new version is 1.9.7)  \n  \nOn Sub-Prod instance:  \n2. Create an application customization for the app in Studio.  \n3. Add some changes to the app (eg: table, fields, BR, etc).  \n4. Publish the customized version of the app to application repository at version 1.0.0  \n  \nOn Production instance:  \n5. Navigate to System Applications > All and search for the app (Workplace Reservation Management)  \n6. Select the Customized Version as \"1.0.0 [Based on 1.8.5]\" and the Base Version as \"1.8.5\"  \n7. Click \"Update\".  \n  \nExpected: The customized version of the app is installed on Production instance.  \nActual: The install does not trigger as the \"Update\" button on the page doesn't work.\n\n## Workaround\n\n1. Upgrade the store application to the latest version (1.9.7) on a sub-prod instance.  \n2. Install application customization version (1.0.0) for the app on production.\n\n  \n**Related Problem: PRB1536634**\"\n\n",
        "QUERY": " 'Update not working if the newer base version of app not installed on sub-prod'",
        "GROUND_TRUTH": "According to KB12345, the issue of the 'Update' button not working if the newer base version of the app is not installed on sub-prod arises when you try to install or update an application customization on a production instance. This happens if there is a newer base version of the app that is not installed on any sub-prod instances. \n\nThe steps to reproduce this issue are as follows:\n1. Install an older version of a store application that allows app customization on sub-prod and production instances.  \n2. Create an application customization for the app in Studio on a sub-prod instance.  \n3. Add some changes to the app.  \n4. Publish the customized version of the app to the application repository.  \n5. Navigate to System Applications > All and search for the app on the production instance.  \n6. Select the Customized Version and the Base Version.  \n7. Click \"Update\".  \n\nThe expected result is that the customized version of the app is installed on the Production instance. However, the actual result is that the install does not trigger as the \"Update\" button on the page doesn't work.\n\nThe workaround for this issue is to upgrade the store application to the latest version on a sub-prod instance and then install the application customization version for the app on production."
    },
    {
        "id": 17,
        "CONTEXT": "\"## \n\n## Description\n\nIf you have performed a bulk restore of archived records (as described in [KB0680127](\/kb_view.do?sysparm_article=KB0680127 \"KB0680127\")), you might expect that these restored records will be archived again when the Archive schedule job (Archiver) runs. However, Archiver by design,\u00a0will not re-archive records that have been manually restored by a user. There might be various reasons that previously archived records are restored, so the\u00a0Archiver cannot determine whether these restored records should be re-archived. You therefore need to take action to re-archive them, for example, clicking on the Restore record UI action on each record. Currently, there is no option to re-archive manually restored records in bulk.\n\n## Steps to Reproduce\n\n 1. Archive a record using an active Archive rule.\n2. Restore that record using the Restore Record UI Action.\n\n\nNote that the regularly scheduled job - Archive runs and does not re-archive that record.\n\n\n  \n\n\n\n  \n\n\n## Workaround\n\n\n**Workaround (Pre Paris release):**\n\n\nTo re-archive restored records in bulk, consider adjusting the script provided below to clear entries from sys\\_archive\\_log ONLY for those restored records you want re-archived. You can put it in a scheduled script execution to make it run periodically to batch through a large recordset.\n\n\n**Notes**:\n\n\n* This script will NOT re-archive the records. It sets up the log so that on the next run of the Archive job, these records will get picked up and archived along with the other records that meet the criteria specified in the active Archive rules.\n* Because the sys\\_archive\\_log table could be large, you might want to consider options such as the following:\n\n\n  \n\n\t+ Adjust the script to filter on the required records using fields that are indexed as part of the filter criteria, such as a particular datetime when an archive was run (sys\\_archive\\_run) or the created datetime. (The fields that are indexed are listed in the Database Indexes tab that you can access by navigating to System Definition > Tables and filtering for name=sys\\_archive\\_log.)\n\t+ Limiting the number of records retrieved using gr.setLimit();\n\t+ Running the script in batches.\n* Because the OOB Archive job runs hourly, the script when executed will increase the number of records that fulfill the criteria specified in the active archive rule. This ight cause the Archive job to run past one hour if the default archive rule processing behavior or the number of active archive rules have been increased recently. This increases the likelihood of running into the known issue described in [KB0550967](\/kb_view.do?sysparm_article=KB0550967 \"KB0550967\") \"Archive threads can run in parallel on separate nodes for long running archive processes, degrading performance and adding noise errors to the log.\"\n* Test in subproduction before implementing in Production.\n\n\n**Script**:\n\n\n\n```\nvar gr = new GlideRecord('sys_archive_log');  \ngr.addQuery('id', recordToArchive);   \/\/ Use this to specify a record to re-archive  \ngr.addNotNullQuery('restored');\u00a0 \u00a0 \u00a0  \/\/ Use this to limit to records which have been restored (i.e. records that are currently NOT archived)  \ngr.addQuery('from_table', tableName); \/\/ Use this to limit to records on the table you want to re-archive  \ngr.query();  \n  \nwhile(gr.next()) {  \n\u00a0 \u00a0 gs.print(\"Resetting archive log to re-archive record: \" + gr.id);  \n\u00a0 \u00a0 if (gr.deleteRecord()) {  \n        gs.print(\"Log entry removed, record ready for re-archive\");  \n    } else {  \n        gs.print(\"Failed to remove entry from sys_archive_log\");  \n    }  \n}\n```\n\n**Paris Release and later:**\n\n\nIf the [Auto Rearchive checkbox](https:\/\/docs.servicenow.com\/csh?topicname=t_CreateAnArchiveRule.html&version=latest \"Auto Rearchive checkbox\") is checked on an archive rule, the restored record will be automatically rearchived after the specified auto rearchive duration (as part of the Archive job). Note that the Auto rearchive option will rearchive all restored records regardless of the condition in the rule.\n\n\nTo rearchive restored records conditionally, you can use the below script.\n\n\nvar gr = new GlideRecord(\u0091Table\\_To\\_Archive\\_From\u0092);\u00a0  \ngr.addQuery(); Write Query to fetch records you want to rearchive  \ngr.query();\n\n\nwhile (gr.next()) {\u00a0  \nif (gr.get(gr.sys\\_id))  \n{  \nvar archiver = new GlideArchiveRecord();  \narchiver.archive(gr);  \n}  \n}\n\n\n  \n\n\n  \n**Related Problem: PRB629054**\"\n\n",
        "QUERY": " 'unable to archive records that were restored earlier'",
        "GROUND_TRUTH": "According to KB12345, the inability to re-archive records that were restored earlier is by design. The Archiver will not re-archive records that have been manually restored by a user. This is because there might be various reasons that previously archived records are restored, so the Archiver cannot determine whether these restored records should be re-archived. \n\nHowever, there are workarounds to re-archive these records. For versions prior to the Paris release, you can adjust a provided script to clear entries from sys_archive_log ONLY for those restored records you want re-archived. This script sets up the log so that on the next run of the Archive job, these records will get picked up and archived along with the other records that meet the criteria specified in the active Archive rules.\n\nFor the Paris release and later, if the Auto Rearchive checkbox is checked on an archive rule, the restored record will be automatically rearchived after the specified auto rearchive duration (as part of the Archive job). Note that the Auto rearchive option will rearchive all restored records regardless of the condition in the rule. To rearchive restored records conditionally, you can use the provided script."
    },
    {
        "id": 18,
        "CONTEXT": "\"## \n\n## Description\n\nThe only way to link attachments to KB articles is via URL link\u00a0in the format: **\/sys\\_attachment.do?sys\\_id=<sys\\_id>**\n\n\nThis\u00a0makes\u00a0end\u00a0users not necessarily authenticated to an instance, but with access to public articles,\u00a0unable to access the attachment link they receive from emails on public KB articles.\n\n\n\u00a0\n\n## Steps to Reproduce\n\n - Login as KM administrator on any current instance.\n- Create a KB article and upload a document\/picture as attachment.\u00a0\n- Mark some of the text and create a link\u00a0to the attachment through the HTML editor.\u00a0\n- Choose the Attachment link, as opposed to URL.\n-\u00a0Select the document uploaded earlier.\u00a0\n- Save the KB article.\n- Open the KB article in edit mode and select the link to edit it.\u00a0\n\u00a0\nNotice\u00a0the Attachment link has been reset to be an URL link.\u00a0It is not possible to make it stay as an Attachment link, even deleting and recreating the attachment.\u00a0\nIf you send the KB article as email to users with no access to the instance,\u00a0they cannot access the attachment.\n\u00a0## Workaround\n\nThis is expected behaviour and by design. In\u00a0all current releases\u00a0an attachment link to an article is always\u00a0converted to a URL link\u00a0before being\u00a0stored into the database.\n\n\n\u00a0\n\n  \n**Related Problem: PRB686107**\"\n\n",
        "QUERY": " 'Unable to access the KB attachment if the KB article is sent as email to users'",
        "GROUND_TRUTH": "According to KB12345, the issue of users being unable to access the KB attachment when the KB article is sent as an email is due to the system design. When a KB article is created and a document or picture is uploaded as an attachment, the attachment link is automatically converted to a URL link before being stored into the database. This is the standard behavior across all current releases. Therefore, if the KB article is sent as an email to users who do not have access to the instance, they will not be able to access the attachment as they are not authenticated to the instance. This is an expected behavior and there is no workaround provided in the KB article."
    },
    {
        "id": 19,
        "CONTEXT": "\"## \n\n## Description\n\nCatalog UI Policies not working in Service Portal after Rome upgrade, when the variable internal name is not matching the one in the Catalog UI policy action.\n\n## Steps to Reproduce\n\n * Open any catalog item and create a variable with name \"u\\_test\"\n* Create a UI Policy and UI Policy Action for the above variable to hide it.\n* Export the UI Policy Action as XML and change the name (variable) to u\\_test\\_abc\n* Import back the UI Policy Action with changed name so that there is an UI Policy Action on a variable with name not matching with name of the variable\n* Test the catalog item in the native UI using try it button, you will see that the variable is hidden\n* Now open the catalog item in portal and see that the variable is not hidden\n\n\nThe reason this isn't working is, there is a mismatch of the variable internal name and the one defined in the catalog UI policy action.  \nEven with this mismatch, this works in the previous release Quebec, and still works in DesktopUI.\n\n## Workaround\n\nThis issue can happen if\n\n\n1. There is a miss match between\u00a0 item\\_option\\_new (variable) record's name and `variable` field on catalog\\_ui\\_policy\\_action record.\n\n\n2. There is a miss match between item\\_option\\_new\\_set (variable set) record's `internal\\_name` and `variable` field on catalog\\_ui\\_policy\\_action record (In this case policy action is on entire variable set).\n\n\n\u00a0\n\n\nThere are two possible workarounds for this issue\n\n\n\u00a0\n\n\n1. Customer can (create and) set `glide.sc.ui\\_policy.use\\_cache` property value to `false`\n\n\n* This will take customer back to pre Rome behaviour where we do not use cache for fetching UI policies, so if there are many UI policies there might be performance some performance impact\n\n\n2. Customer can correct (sync) `variable` field on `catalog\\_ui\\_policy\\_action` record to match with `name`\/`internal\\_name` of corresponding Variable\/Variable Set\n\n\nTo sync, use below script. Replace `:itemId` with the sys\\_id of catalog item where UI policy issues are found\n\n\n\u00a0\n\n\n\/\/ replace catalog item sys\\_id with :itemId  \nvar catItemId = ':itemId';  \nvar policyIds = [];  \nvar itemPolicies = new GlideRecord('catalog\\_ui\\_policy');  \nitemPolicies.addQuery('applies\\_to', 'item');  \nitemPolicies.addQuery('catalog\\_item', catItemId);  \nitemPolicies.query();  \nwhile(itemPolicies.next()) {  \n\u00a0 policyIds.push(itemPolicies.getUniqueValue());  \n}\n\n\nvar catItem = new sn\\_sc.CatItem(catItemId);  \nvar sets = catItem.getVariableSet();\n\n\nif(sets && sets.length) {  \n\u00a0 var setPolicies = new GlideRecord('catalog\\_ui\\_policy');  \n\u00a0 setPolicies.addQuery('applies\\_to', 'set');  \n\u00a0 setPolicies.addQuery('variable\\_set', 'IN', sets);  \n\u00a0 setPolicies.query();\n\n\n\u00a0 while(setPolicies.next()) {  \n\u00a0 \u00a0 policyIds.push(setPolicies.getUniqueValue());  \n\u00a0 }  \n}  \ngs.info('policy to be synced = '+ policyIds)  \nif (policyIds.length) {  \n\u00a0 var policyActions = new GlideRecord('catalog\\_ui\\_policy\\_action');  \n\u00a0 policyActions.addQuery('ui\\_policy', 'IN', policyIds);  \n\u00a0 policyActions.query();\n\n\n\u00a0 while(policyActions.next()) {  \n\u00a0 \u00a0 var variable = policyActions.catalog\\_variable.getDisplayValue().split(\"IO:\")[1];  \n\u00a0 \u00a0if (!variable)  \n\u00a0 \u00a0 variable = policyActions.catalog\\_variable.split(\"IO:\")[1];  \n\u00a0 \u00a0var option = new GlideRecord('item\\_option\\_new');  \n\u00a0 \u00a0if (option.get(variable)) {  \n\u00a0 \u00a0 policyActions.variable = option.name;  \n\u00a0 \u00a0} else {  \n\u00a0 \u00a0 option = new GlideRecord(\"item\\_option\\_new\\_set\");  \n\u00a0 \u00a0 if (option.get(variable))  \n\u00a0 \u00a0 \u00a0policyActions.variable = option.internal\\_name;  \n\u00a0 \u00a0}  \n\u00a0 \u00a0policyActions.update();  \n\u00a0 }  \n}\n\n\n\u00a0\n\n\nNote: There is also a `catalog\\_variable` field on `catalog\\_ui\\_policy\\_action` which is in form of `IO:sys\\_id\\_of\\_variable`, No need to touch that.\n\n\n\u00a0\n\n\n\u00a0\n\n\nIf customer is choosing workaround 1, they should delete the property once they are in version where fix is available.\n\n  \n**Related Problem: PRB1524152**\"\n\n",
        "QUERY": " 'ui policies are not working on the service portal'",
        "GROUND_TRUTH": "The issue of UI policies not working on the Service Portal after a Rome upgrade occurs when the variable internal name does not match the one in the Catalog UI policy action. This mismatch can occur in two scenarios: \n1. A mismatch between the item_option_new (variable) record's name and the `variable` field on the catalog_ui_policy_action record.\n2. A mismatch between the item_option_new_set (variable set) record's `internal_name` and `variable` field on the catalog_ui_policy_action record.\n\nThere are two possible workarounds for this issue:\n1. You can create and set the `glide.sc.ui_policy.use_cache` property value to `false`. This will revert to the pre-Rome behavior where cache is not used for fetching UI policies. However, if there are many UI policies, there might be some performance impact.\n2. You can correct (sync) the `variable` field on the `catalog_ui_policy_action` record to match with the `name`\/`internal_name` of the corresponding Variable\/Variable Set. A script is provided in the KB12345 to help with this process.\n\nIf you choose the first workaround, remember to delete the property once you are in a version where the fix is available."
    },
    {
        "id": 20,
        "CONTEXT": "\"## \n\n## Description\n\n* The SAMP reconciliation fails after 100% progress due to a race condition when the Import User Subscriptions job is also running at the same time.\n* The progress summary shows:\n\n\nTypeError: Cannot read property \"install\\_condition\" from null\n\n\nat sys\\_script\\_include.444d8294c32222006081face81d3aebf.script:514 (anonymous)  \nat sys\\_script\\_include.444d8294c32222006081face81d3aebf.script:266 (anonymous)  \nat sys\\_script\\_include.444d8294c32222006081face81d3aebf.script:247 (anonymous)  \nat sys\\_script\\_include.30bbdf9587f52300923aa75fe5cb0b97.script:413 (anonymous)  \nat sys\\_script\\_include.30bbdf9587f52300923aa75fe5cb0b97.script:400 (anonymous)  \nat sys\\_script\\_include.6761b0dd0b1232001a17650d37673a77.script:868 (anonymous)  \nat sys\\_script\\_include.6761b0dd0b1232001a17650d37673a77.script:824 (anonymous)  \nat sys\\_script\\_include.6761b0dd0b1232001a17650d37673a77.script:54 (anonymous)  \nat sys\\_trigger.f7006c6e471e59d0e1ce8a12736d4378:1\n\n\n\u00a0\n\n\nPossibly related problems:  \n[Reconciliation errors in the form 'Cannot read property <someSysId> from undefined' caused by SamNewPerCoreForMicrosoftLicenseCalculator loading all optimization records and Per Core VM allocations missing relationships](https:\/\/support.servicenow.com\/kb_view.do?sysparm_article=KB1227294 \"Reconciliation errors in the form 'Cannot read property <someSysId> from undefined' caused by SamNewPerCoreForMicrosoftLicenseCalculator loading all optimization records and Per Core VM allocations missing relationships\")  \n[Workday reconciliation is failing if subscriptions are pulled from SSO or custom integration profile](https:\/\/support.servicenow.com\/kb_view.do?sysparm_article=KB1316426 \"Workday reconciliation is failing if subscriptions are pulled from SSO or custom integration profile\")\n\n## Steps to Reproduce\n\n 1. Schedule the Reconciliation job to run at the same time as the SAM - Import User Subscriptions job.  \n2. Check Recon Results for recon with status 'Failed' and Progress summary 'Reconciliation Complete'.  \n3. Check the progress summary for the TypeError: cannot read property 'install\\_condition' from null.\n## Workaround\n\n- Use Case 1 -  \nMake sure the Import User Subscription and Software Reconciliation jobs are never scheduled to run at the same time.\n\n\n- Use Case 2 -\n\n\nIf you have verified both Import User Subscription and Software Reconciliation are running at different times, then consider changing the \"**SamAllocationSuiteEngine**\" Script Include ( \/sys\\_script\\_include.do?sys\\_id=444d8294c32222006081face81d3aebf ) to add the code below line at 435, so that the final code looks like this:  \n  \ngetSubscriptionSuitesOrComponents: function(publisherId, getSuites) {  \n\/\/ First get all the licensable software models of this publisher  \nvar subscriptionModels = [];  \nvar subscriptionGa = new SampAggregate('samp\\_sw\\_subscription');  \nsubscriptionGa.addQuery('software\\_model.manufacturer', publisherId);  \nsubscriptionGa.addNotNullQuery('user');  \nsubscriptionGa.addNotNullQuery('licensable\\_software\\_model'); --- We are filtering record where licensable\\_software\\_model is empty.  \nsubscriptionGa.groupBy('licensable\\_software\\_model');  \nsubscriptionGa.query();\n\n\nNOTE: The reconciliation job processes the valid record where you already have a valid licensable\\_software\\_model value and show the data. This same job also populates the licensable\\_software\\_model value for the records where licensable\\_software\\_model is empty. Thus the records skipped in the current run will be automatically processed in the next run.\n\n\nif you see any record with licensable\\_software\\_model as empty and publisher as empty, then those records will not be processed. Those do not have any direct impact on the job execution. This needs to be investigated separately.\n\n  \n**Related Problem: PRB1610760**\"\n\n",
        "QUERY": " 'typeerror: cannot read property \"install_condition\" from null software reconciliation'",
        "GROUND_TRUTH": "The error 'TypeError: Cannot read property \"install_condition\" from null' occurs during the SAMP reconciliation process when the Import User Subscriptions job is running simultaneously. This is due to a race condition. To reproduce this error, you need to schedule the Reconciliation job to run at the same time as the SAM - Import User Subscriptions job. Then, check the Recon Results for recon with status 'Failed' and Progress summary 'Reconciliation Complete'. The progress summary will show the TypeError.\n\nTo resolve this issue, there are two use cases provided in the article. In the first use case, you should ensure that the Import User Subscription and Software Reconciliation jobs are never scheduled to run at the same time. In the second use case, if you have verified that both jobs are running at different times, you should consider changing the \"SamAllocationSuiteEngine\" Script Include to add a specific code at line 435. This code filters out records where the licensable_software_model is empty. The reconciliation job will then process valid records where you already have a valid licensable_software_model value and show the data. This job also populates the licensable_software_model value for the records where it is empty. Thus, the records skipped in the current run will be automatically processed in the next run."
    },
    {
        "id": 21,
        "CONTEXT": "\"## \n\n### Description\n\n\n  \n  \n\n# Description\n\n\n\n\n---\n\n\nLazy Writer is a mechanism (background thread) that we use in the platform to defer updates to the database so that user sessions can go on about their business (not unlike an ASYNC business rule).\n\u00a0\nEach application server node will have a glide.lazy.writer thread that handles these deferred updates.  \n  \nLazy\u00a0writer handles asynchronous writes to sysevent, audit, sys\\_user\\_presence. When the instance is processing a large number of user presences updates and sysevent updates,\u00a0lazy\u00a0writer can cause row lock contention as all the rows in a large batch are locked until the transaction is committed.\u00a0 User presence updates have direct impact on UI transactions, and end users\u00a0will start to experience slowness.\n  \nWhen the Lazy Writer queue is full then its writes become\u00a0Sync writes and\u00a0that\u00a0causes the lock contention  \n  \n\n# Procedure\n\n\n\n\n---\n\n\n1. Check threads.do and localhost logs  \n  \nCheck the \/threads.do output and see if multiple threads are in the following stack trace:\nat com.glide.db.DBCompositeAction.executeLazy(DBCompositeAction.java:291)\n  \n\nCheck the localhost\\_log.$(date +\"%Y-%m-%d\").txt and look for threads with the following error:\nFAILED TRYING TO EXECUTE ON CONNECTION 15: UPDATE sys\\_user\\_presence\n  \n  \n2018-04-03 07:36:28 (640) Presence-thread-3 58A5E0FCDB9517C8DAF72FEB0B961945 SEVERE \\*\\*\\* ERROR \\*\\*\\* FAILED TRYING TO EXECUTE ON CONNECTION 15: UPDATE sys\\_user\\_presence SET `ua\\_time` = '2018-04-03T14:35:36.978Z', `path` = '\/hrportal', `sys\\_updated\\_by` = '\\_\\_USERID\\_\\_', `sys\\_mod\\_count` = 29230, `sys\\_updated\\_on` = '2018-04-03 14:35:37', `user\\_agent` = 'Mozilla\/5.0 (Windows NT 10.0; WOW64; Trident\/7.0; rv:11.0) like Gecko', `status`= NULL WHERE sys\\_user\\_presence.`sys\\_id` = '2327c028db4cba00b9b178f9bf9619d5' \/\\* dell079, gs:58A5E0FCDB9517C8DAF72FEB0B961945, tx:35e34dfcdbd957c8daf72feb0b9619e2 \\*\/\u00a0  \n**Lock wait timeout exceeded; try restarting transaction**  \n**java.sql.SQLException: Lock wait timeout exceeded; try restarting transaction**  \nat org.mariadb.jdbc.internal.SQLExceptionMapper.get(SQLExceptionMapper.java:149)\u00a0  \nat org.mariadb.jdbc.internal.SQLExceptionMapper.throwException(SQLExceptionMapper.java:106)\u00a0  \nat org.mariadb.jdbc.MySQLStatement.executeQueryEpilog(MySQLStatement.java:268)\u00a0  \nat org.mariadb.jdbc.MySQLStatement.execute(MySQLStatement.java:296)\u00a0  \nat org.mariadb.jdbc.MySQLStatement.execute(MySQLStatement.java:387)\u00a0  \nat sun.reflect.GeneratedMethodAccessor22.invoke(Unknown Source)\u00a0  \nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\u00a0  \nat java.lang.reflect.Method.invoke(Method.java:498)\u00a0  \nat com.glide.db.StatementWrapper.invoke(StatementWrapper.java:40)\u00a0  \nat com.sun.proxy.$Proxy7.execute(Unknown Source)\u00a0  \nat com.glide.db.DBI.executeStatement0(DBI.java:921)\u00a0  \nat com.glide.db.DBI.executeStatement(DBI.java:877)\u00a0  \nat com.glide.db.DBI.executeStatement(DBI.java:850)\u00a0  \nat com.glide.db.DBAction.executeAsResultSet(DBAction.java:283)\u00a0  \nat com.glide.db.DBCompositeAction.executeAsResultSet(DBCompositeAction.java:139)\u00a0  \nat com.glide.db.DBCompositeAction.executeAsResultSet0(DBCompositeAction.java:92)\u00a0  \nat com.glide.db.DBAction.executeAndReturnTable(DBAction.java:247)\u00a0  \nat com.glide.db.DBAction.executeNormal(DBAction.java:236)\u00a0  \nat com.glide.db.DBAction.executeAndReturnException(DBAction.java:197)\u00a0  \nat com.glide.db.DBAction.execute(DBAction.java:136)\u00a0  \n**at com.glide.db.DBCompositeAction.executeLazy(DBCompositeAction.java:291)**  \n**at com.glide.db.DBCompositeAction.executeLazy(DBCompositeAction.java:275)**  \n**at com.glide.db.DBCompositeAction.executeLazy(DBCompositeAction.java:271)**  \nat com.glide.ui.ng.NGPresenceService.update(NGPresenceService.java:132)\u00a0  \nat com.glide.ui.ng.NGPresenceService.updatePresence(NGPresenceService.java:92)\u00a0  \nat sun.reflect.GeneratedMethodAccessor470.invoke(Unknown Source)\u00a0  \nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\u00a0  \nat java.lang.reflect.Method.invoke(Method.java:498)\u00a0  \nat com.glide.rest.handler.impl.ServiceHandlerImpl.invokeService(ServiceHandlerImpl.java:43)\u00a0  \nat com.glide.rest.processors.RESTAPIProcessor.process(RESTAPIProcessor.java:228)\u00a0  \nat com.glide.processors.AProcessor.runProcessor(AProcessor.java:415)\u00a0  \nat com.glide.processors.AProcessor.processTransaction(AProcessor.java:186)\u00a0  \nat com.glide.processors.ProcessorRegistry.process0(ProcessorRegistry.java:178)\u00a0  \nat com.glide.processors.ProcessorRegistry.process(ProcessorRegistry.java:167)\u00a0  \nat com.glide.ui.GlideServletTransaction.process(GlideServletTransaction.java:49)\u00a0  \nat com.glide.sys.Transaction.run(Transaction.java:1977)\u00a0  \nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\u00a0  \nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\u00a0  \nat java.lang.Thread.run(Thread.java:748)  \n  \n  \n1.1 Check the issue is not due to excessive sys\\_user\\_presence updates, ie: via the localhost on the application nodes:  \n  \ngrep \"UPDATE sys\\_user\\_presence\" localhost\\_log.$(date +\"%Y-%m-%d\").txt |awk '{ print $4 }' |sort |uniq -c  \n  \nsample output:  \n\n[app128153.sjc111:\/glide\/nodes]$ grep \"UPDATE sys\\_user\\_presence\" localhost\\_log.$(date +\"%Y-%m-%d\").txt |awk '{ print $4 }' |sort |uniq -c\n\u00a023936 glide.lazy.writer[glide]\n\u00a0 \u00a012299 Presence-thread-1\n\u00a0 \u00a012291 Presence-thread-2\n\u00a0 \u00a012289 Presence-thread-3\n\u00a0 \u00a012290 Presence-thread-4\n  \nWe would expect to\u00a0only see glide.lazy.writer updates.\u00a0   \nIn the above example, this shows excessive updates by Presence Threads\n2. Check INNOTOP \/ Process List and also observe the INSERT\/UPDATE\/DELETE lock contention\n3. Check the application node logs, and observe errors similar to;  \n  \n\n2018-04-03 07:58:18 (254) Presence-thread-3 CDE47834DB9113C84C931AAC0B961959 **WARNING \\*\\*\\* WARNING \\*\\*\\* Current lazy writer delay: 0:01:18.809**\n2018-04-03 07:58:19 (087) **glide.lazy.writer[glide] SYSTEM SEVERE \\*\\*\\* ERROR \\*\\*\\* Exception during batch statement commit to** glide:dell\\_1:emc:jdbc:mysql:\/\/db160020.iad106.service-now.com:3402\/, **falling back to single commits: Deadlock found when trying to get lock; try restarting transaction**\n2018-04-03 07:58:19 (092) glide.lazy.writer[glide] SYSTEM [0:00:01.394] Statement batcher: 1000\n2018-04-03 07:58:19 (389) glide.lazy.writer[glide] SYSTEM Time: 0:00:00.223 id: dell\\_1[glide.17] for: UPDATE sys\\_user\\_presence\u00a0 SET `ua\\_time` = '2018-04-03T14:56:27.335Z', `path` = '\/hrportal', `sys\\_updated\\_by` = ...\n  \n  \n4.1 Check SPLUNK (Visualization tab)  \nsourcetype=appnode\\_localhost\\_log instance=\\_\\_INSTANCE\\_\\_ \"lock\\_wait\" | timechart count span=1m  \n  \n![](sys_attachment.do?sys_id=c83f7426db0ab450e515c223059619fd)  \n  \n  \n  \n4.2 Check Big Data  \n![](sys_attachment.do?sys_id=cc3fb426db0ab450e515c22305961902)\n4. Once confirmed, review binlogs and application node logs to determine if single (small group) of sessions are the cause.\u00a0 Use logs to identify the table(s) being updated too frequently.\u00a0 \u00a0 \u00a0See\u00a0[KB0657202](https:\/\/support.servicenow.com\/kb_view.do?sysparm_article=KB0657202 \"KB0657202\")\n5. Work backwards to establish the root cause of the\u00a0update flood\u00a0- initiating transactions from that sessionID in the application node logs.\u00a0 Possible causes could be widgets on \/sp Service Portal pages generating excessive 'sys\\_user\\_presence' updates.  \n  \nComparitive numbers:  \nGE during peak times sees around 100 per second (no issue)  \nDell during outage event P1s saw 500 updates per second to sys\\_user\\_presence which was the cause of slow performance  \n  \n  \n6.\u00a0Upgrade to a fixed release.  \nPRB1267824 optimizes the LazyWriter to avoid using db transactions unnecessarily so that\u00a0it does not\u00a0hold row locks for an unnecessarily long period of time.\n\n\n\u00a0\n\n\n\u00a0\n\n\n# Applicable Versions\n\n\n\n\n---\n\n\nFixed in:\u00a0\n\n\nJakarta Patch 9  \nKingston Patch 5\n\n\n\u00a0\n\n\n# Additional Information\n\n\n\n\n---\n\n\nRelated properties:\n\n\nglide.db.lazy.writer.wait\\_threads\u00a0\nglide.db.lazy.writer.debug\u00a0\nglide.db.lazy.writer.use\\_transaction\u00a0\nglide.db.allow.lazy.payload  \n  \n  \n[KB0657202](https:\/\/support.servicenow.com\/kb_view.do?sysparm_article=KB0657202 \"KB0657202\") TSE Binlog Query Examples  \n\n#\"\n\n",
        "QUERY": " 'troubleshoot performance issues in a instance'",
        "GROUND_TRUTH": "To troubleshoot performance issues in an instance, follow these steps:\n\n1. Check the \/threads.do output and localhost logs for multiple threads in the stack trace at com.glide.db.DBCompositeAction.executeLazy(DBCompositeAction.java:291). Look for threads with errors related to executing on connection 15: UPDATE sys_user_presence.\n\n2. Check if the issue is not due to excessive sys_user_presence updates via the localhost on the application nodes. You can do this by using the grep command to filter the logs.\n\n3. Check INNOTOP \/ Process List and observe the INSERT\/UPDATE\/DELETE lock contention.\n\n4. Check the application node logs for errors similar to \"Current lazy writer delay\" and \"Exception during batch statement commit to glide\".\n\n5. Use SPLUNK (Visualization tab) and Big Data to visualize the issue.\n\n6. Once confirmed, review binlogs and application node logs to determine if a single (small group) of sessions are the cause. Use logs to identify the table(s) being updated too frequently.\n\n7. Work backwards to establish the root cause of the update flood - initiating transactions from that sessionID in the application node logs. Possible causes could be widgets on \/sp Service Portal pages generating excessive 'sys_user_presence' updates.\n\n8. Upgrade to a fixed release. PRB1267824 optimizes the LazyWriter to avoid using db transactions unnecessarily so that it does not hold row locks for an unnecessarily long period of time.\n\nApplicable versions for the fix include Jakarta Patch 9 and Kingston Patch 5. Related properties include glide.db.lazy.writer.wait_threads, glide.db.lazy.writer.debug, glide.db.lazy.writer.use_transaction, and glide.db.allow.lazy.payload."
    },
    {
        "id": 22,
        "CONTEXT": "\"## \n\n## Description\n\nFor update sets that contain a delete for a table that does not exist, the update set will complete successfully but the progress worker that was doing the commit is never marked as complete and sits at 90-something percent complete. This is a non-fatal error.\n\n## Steps to Reproduce\n\n 1. Import an update set that contains a table delete for a table not in the target system.\n2. Commit the update set.\n\n\nNote that the progress worker dialog never disappears.\n3. Refresh the form.\n\n\nNote that the status is complete.\n4. Find the root sys\\_execution\\_tracker record for this operation.\n\n\nNote that the tracker is still running and one of its children for \"dropping table\" is marked as failed.\n\n\n\u00a0\n\n## Workaround\n\nClose (do **not** cancel) the dialog window and refresh the form to see the status marked as completed.\n\n  \n**Related Problem: PRB714572**\"\n\n",
        "QUERY": " 'Tracker of update set with a table delete stuck at 90 percent though update set is success'",
        "GROUND_TRUTH": "The issue of the tracker of an update set with a table delete getting stuck at 90 percent even though the update set is successful is a known problem. This happens when an update set contains a delete for a table that does not exist in the target system. The update set completes successfully, but the progress worker that was doing the commit is never marked as complete and remains at 90-something percent. This is a non-fatal error. The progress worker dialog never disappears, but if you refresh the form, the status is marked as complete. However, the root sys_execution_tracker record for this operation shows that the tracker is still running and one of its children for \"dropping table\" is marked as failed. The workaround for this issue is to close (do not cancel) the dialog window and refresh the form to see the status marked as completed."
    },
    {
        "id": 23,
        "CONTEXT": "\"## \n\n### Description\n\nChanging the \"Week Starts On\" field on the Time Card does not save\n\n### Release or Environment\n\nJakarta Patch 9c\n\n### Cause\n\nNo Time Sheet on the time card so \"Populate week starts on\" Business Rule was running.\n\n### Resolution\n\nThere is not a timesheet attached, the \"Populate week starts on\" Business Rule will run and assign the \"Week Starts on\" field with what is defined in the Time Sheet Policy.\n\n\nIf you have a Time Sheet and want to change the date in a Time Card, this should be changed first on the Time Sheet.\"\n\n",
        "QUERY": " 'Time card with \"week starts on\" not working as expected'",
        "GROUND_TRUTH": "The issue with the \"Week Starts On\" field on the Time Card not saving is due to the absence of a Time Sheet on the time card. When there is no timesheet attached, the \"Populate week starts on\" Business Rule runs and assigns the \"Week Starts on\" field with what is defined in the Time Sheet Policy. If you have a Time Sheet and want to change the date in a Time Card, this should be changed first on the Time Sheet."
    },
    {
        "id": 24,
        "CONTEXT": "\"## \n\n\u00a0\n\n\n\n\n\n\n| Note | **Note**: The information below is intended for readers in the SWAT staff. |\n| --- | --- |\n\n\n\u00a0\n\n\n\n## Table of Contents\n\n\n* [1. Overview](#OVERVIEW)\n* [2. Correlated errors](#CORRELATED_ERRORS)\n* [3. Common \"PostgresDB Errors\"](#POSTGRES_LOG)\n* [4. Common \"App Logs Errors\"](#APPNODE_LOG)\n* [5. Common \"SNOW API Errors\"](#SNOWAPI_LOG)\n* [6. Additional Resources](#ADDITIONAL_RESOURCES)\n\n\n\n# 1. Overview\n\n\nThe purpose of this article is to provide information about the common errors found by the \"[Tembo Splunk Dashboard - Error View](.\/kb?id=kb_article_view&sysparm_article=KB1553837)\".\n\n\n\u00a0\n\n\n# 2. Correlated errors\n\n\nHere are a list of known errors that are correlated across the different logs:\n\n\n\u00a0\n\n\n\n\n| PostgresDB Errors | App Logs Errors | SNOW API Errors | Remarks |\n| --- | --- | --- | --- |\n| aborting backup due to backend exiting before pg\\_backup\\_stop was called |  | \/glide\/snc-provision\/puppet\/modules\/backup\/lib\/backup\/backup-store.rb:<XXX>:in `<FUNCTION>': Backup not found: <FILENAME> (RuntimeError) | [PRB1709902](.\/problem.do?sysparm_query=number=PRB1709902) |\n|  |  |  |  |\n\n\n\u00a0\n\n\n# 3. Common \"PostgresDB Errors\"\n\n\nHere are a list of common errors listed on the [\"PGS DB errors\" section on the Tembo Splunk Dashboard - Error View](.\/kb?id=kb_article_view&sysparm_article=KB1553837#POSTGRES_LOG).\n\n\nThe following sets of data, can also be retrieved by using the Splunk search via the [\"Research & Development\" section for \"PostgresDB Errors\"](.\/kb?id=kb_article_view&sysparm_article=KB1554168#POSTGRES_LOG)\n\n\n\u00a0\n\n\n\n\n| Error\\_message | User | Remarks |\n| --- | --- | --- |\n| canceling statement due to lock timeout |  | Could be related to:[DEF0425255](https:\/\/buildtools1.service-now.com\/x_snc_defect_defect.do?sys_id=843a786293043910e3e7b0627cba1053) \/ [PRB1691462](.\/problem.do?sysparm_query=number=PRB1691462)If it happens after AHA Transfer and via \"autovacuum worker\", it could be [PRB1713550](.\/problem.do?sysparm_query=number=PRB1713550) |\n| you don't own a lock of type ExclusiveLock |  | Likely related to:[DEF0425255](https:\/\/buildtools1.service-now.com\/x_snc_defect_defect.do?sys_id=843a786293043910e3e7b0627cba1053) \/ [PRB1691462](.\/problem.do?sysparm_query=number=PRB1691462)If it happens after AHA Transfer and via \"autovacuum worker\", it could be [PRB1713550](.\/problem.do?sysparm_query=number=PRB1713550) |\n| no pg\\_hba.conf entry for host \"[local]\", user \"root\", database \"root\", no encryption | root |  |\n| no pg\\_hba.conf entry for host \"[local]\", user \"dbi\\_backup\", database \"postgres\", no encryption | dbi\\_backup |  |\n| current transaction is aborted, commands ignored until end of transaction block |  |  |\n| duplicate key value violates unique conORduplicate key value violates unique constraint \"XXX\" |  |  |\n| canceling statement due to user request |  |  |\n| deadlock detected |  |  |\n| column sys\\_user2.roles does not exist at character 8 |  | [PRB1711568](.\/problem.do?sysparm_query=number=PRB1711568) |\n| aborting backup due to backend exiting before pg\\_backup\\_stop was called |  | [PRB1709902](.\/problem.do?sysparm_query=number=PRB1709902) |\n| recovery is not in progress | dbi\\_monitoring | [PRB1710675](.\/problem.do?sysparm_query=number=PRB1710675) |\n| function pg\\_terminate\\_backend() does not exist at character 38 | dbi\\_monitoring | [PRB1711771](.\/problem.do?sysparm_query=number=PRB1711771) |\n| Function swarm64da.to\\_decimal\\_mariadb() is deprecated. Remove this function call as pgNOW can perform the conversion implicitly. |  | [PRB1610441](.\/problem.do?sysparm_query=number=PRB1610441) |\n| out of memory |  | **If** be=\"autovacuum worker\"then it is [PRB1711773](.\/problem.do?sysparm_query=number=PRB1711773) |\n|  |  |  |\n\n\n\u00a0\n\n\n# 4. Common \"App Logs Errors\"\n\n\nHere are a list of common errors listed on the [\"App Logs Errors\" section on the Tembo Splunk Dashboard - Error View](.\/kb?id=kb_article_view&sysparm_article=KB1553837#APPNODE_LOG).\n\n\nThe following sets of data, can also be retrieved by using the Splunk search via the [\"Research & Development\" section for \"App Logs Errors\"](.\/kb?id=kb_article_view&sysparm_article=KB1554168#APPNODE_LOG)\n\n\n\u00a0\n\n\n\n\n| Error\\_message | Remarks |\n| --- | --- |\n| Cannot map objectcom.glide.rest.util.RESTRuntimeException | **Note:**Only happens on buildtools1backend315with tx\\_pattern\\_hash=-1243197801via \/api\/x\\_snc\\_devops\\_auto\/github\\_trigger**ToDo**:Open incident\/case. |\n| Exception during batch statement commit to glide | ERROR: duplicate key value violates unique constraint \"<INDEX\\_NAME>\"**If** the SQL is \"INSERT INTO ire\\_mutex\", then it is **Expected**. |\n| The column index is out of range |  |\n| Replica pool (postgresql\\_<HOST>\\_<PORT>) query execution failedorg.postgresql.util.PSQLException | ERROR: canceling statement due to user request**To Check & Fix**:1) May have some slow queries to review. |\n| Replica pool (postgresql\\_<HOST>\\_<PORT>) query execution failedcom.glide.db.GlideSQLException | FAILED TRYING TO EXECUTE ON CONNECTION**To Check & Fix**:1) \"Syntax Error or Access Rule Violation detected by database\"2) \"General Data Exception detected by database\" |\n| Unable to check autocommit state of the connection <NUMBER>org.postgresql.util.PSQLException | This connection has been closed.**If** StackTrace goes through \"ConnectionWrapper.java:494\"==> it is an **expected** fix for [DEF0280841](https:\/\/buildtools1.service-now.com\/x_snc_defect_defect.do?sys_id=d2657b541bca09907a36db56dc4bcb58) \/ [PRB1559725](.\/problem.do?sysparm_query=number=PRB1559725) \/ [KB1113084](.\/kb?id=kb_article_view&sysparm_article=KB1113084) |\n|  |  |\n\n\n\u00a0\n\n\n# 5. Common \"SNOW API Errors\"\n\n\nHere are a list of common errors listed on the [\"SNOW API Errors\" section on the Tembo Splunk Dashboard - Error View](.\/kb?id=kb_article_view&sysparm_article=KB1553837#SNOWAPI_LOG).\n\n\nThe following sets of data, can also be retrieved by using the Splunk search via the [\"Research & Development\" section for \"SNOW API Errors\"](.\/kb?id=kb_article_view&sysparm_article=KB1554168#SNOWAPI_LOG)\n\n\n\u00a0\n\n\n\n\n| Error\\_message | User | Remarks |\n| --- | --- | --- |\n| ERROR:\u00a0 relation \"information\\_schema.innodb\\_lock\\_waits\" does not exist | snccid |  |\n| ERROR:\u00a0 syntax error at or near \".\" | snccid | [PRB1710273](.\/problem.do?sysparm_query=number=PRB1710273) |\n| ERROR:\u00a0 column \"table\\_rows\" does not exist | snccid | [PRB1710528](.\/problem.do?sysparm_query=number=PRB1710528) |\n| relation \"sn\\_app\\_ism\\_m5r\\_check\\_result\" does not exist | snccid | The \"sn\\_app\\_ism\\_m5r\\_check\\_result\" is \"sn\\_app\\_ism\\_monitor\\_check\\_result\". It is part of \"ISM Monitoring\" (sn\\_app\\_ism\\_monitor) plugin.There is an ongoing program to rollout this plugin. This error only highlight that the specific instance, has yet to get the plugin.**Action**: just ignore.[PRB1620748](.\/problem.do?sysparm_query=number=PRB1620748) \/ [KB1226357](.\/kb?id=kb_article_view&sysparm_article=KB1226357) - ISM Rollout for Internal Instances.Rollouts are in phases:[CHG48880066](.\/change_request.do?sysparm_query=number=CHG48880066) \/ [CHG48518975](.\/change_request.do?sysparm_query=number=CHG48518975) \/ [CHG48691062](.\/change_request.do?sysparm_query=number=CHG48691062) \/ [CHG47023323](.\/change_request.do?sysparm_query=number=CHG47023323) \/ [CHG47023298](.\/change_request.do?sysparm_query=number=CHG47023298) |\n| \/glide\/snc-provision\/puppet\/modules\/backup\/lib\/backup\/backup-store.rb:227:in `close': Backup not found: <FILENAME> (RuntimeError) | dbi\\_backup | [PRB1709902](.\/problem.do?sysparm_query=number=PRB1709902) |\n| \/glide\/snc-provision\/puppet\/modules\/backup\/lib\/backup\/backup-store.rb:<XXX>:in `validate': Backup not found: <BACKUP\\_FILE\\_NAME> (RuntimeError) | dbi\\_backup | [PRB1711501](.\/problem.do?sysparm_query=number=PRB1711501) (duplicate of [PRB1709902](.\/problem.do?sysparm_query=number=PRB1709902)) |\n| \/glide\/snc-provision\/puppet\/modules\/backup\/lib\/backup\/backup.rb:1130:in `close': Backup <BACKUP\\_FILE\\_NAME> truncated: reserved size (5G) reached (RuntimeError) | dbi\\_backup | [PRB1711502](.\/problem.do?sysparm_query=number=PRB1711502) |\n| \/glide\/snc-provision\/.rbenv\/linux-2.6-libc-2.5-x86\\_64\/versions\/2.5.1\/lib\/ruby\/gems\/2.5.0\/gems\/system\\_sumo-1.0.12\/lib\/system\\_sumo.rb:401:in `join': execution expired (Timeout::Error) | dbi\\_backup | [PRB1711499](.\/problem.do?sysparm_query=number=PRB1711499) |\n|  |  |  |\n\n\n\u00a0\n\n\n# 6. Additional Resources\n\n\n* [KB1553837](.\/kb?id=kb_article_view&sysparm_article=KB1553837) - Reference | Tembo Splunk Dashboard - Error View\n* [KB1554168](.\/kb?id=kb_article_view&sysparm_article=KB1554168) - Reference | Analysis with \"Tembo Splunk Dashboard - Error View\"\n* [KB1558863](.\/kb?id=kb_article_view&sysparm_article=KB1558863) - Reference | Common Errors from \"Tembo Splunk Dashboard - Error View\"\n* [List of Problems](https:\/\/support.servicenow.com\/problem_list.do?sysparm_query=u_tags%3D%23Tembo_Splunk_Dashboard&sysparm_view=) created from this \"[Tembo Splunk Dashboard - Error View](.\/kb?id=kb_article_view&sysparm_article=KB1553837)\" project\n\n\n\u00a0\n\n\n\nRevision Log...\u00a0 **(Last updated: 20-Dec-2023)**\n  \n\n\n\n| Version | Published | Summary of Changes |\n| --- | --- | --- |\n| 1.0 | 20-Dec-2023 | Initial version |\"\n\n",
        "QUERY": " 'Syntax error or Access Rule violation detected by database'",
        "GROUND_TRUTH": "The 'Syntax error or Access Rule violation detected by database' error is a common error listed under the \"App Logs Errors\" section. This error can occur when the Replica pool (postgresql_<HOST>_<PORT>) query execution fails. The error message is 'FAILED TRYING TO EXECUTE ON CONNECTION'. To check and fix this error, you need to look for two things: \n1) \"Syntax Error or Access Rule Violation detected by database\"\n2) \"General Data Exception detected by database\"\nIf you encounter these issues, it indicates that there may be some slow queries to review."
    },
    {
        "id": 25,
        "CONTEXT": "\"## \n\n## Description\n\nScheduled job \"SAM - Subscription Maintenance\" can't run properly in SAM Foundation so that it fails to update entitlement status\n\n## Steps to Reproduce\n\n Go to the scheduled job \"SAM - Subscription Maintenance\". Click \"Execute Now\".  \n  \nGo to System logs and observe that there is warning as below:  \n------  \norg.mozilla.javascript.EcmaError: \"SAMPReserveEntitlementUtil\" is not defined.  \nCaused by error in <refname> at line 10  \n  \n7: }  \n8:  \n9: function sampUpdateStateStatus() {  \n==> 10: SAMPReserveEntitlementUtil.processExpiredReserveEntitlements();  \n11:  \n12: var today = gs.daysAgo(0);  \n13: var grEnt = new GlideRecord('alm\\_license');  \n-------\n\n## Workaround\n\nComment out the line below under the function sampUpdateStateStatus() of the \"SAM - Subscription Maintenance\" Scheduled Script Execution:\n\n\nSAMPReserveEntitlementUtil.processExpiredReserveEntitlements();\n\n  \n**Related Problem: PRB1475316**\"\n\n",
        "QUERY": " 'SAM subscription maintenance failing to update entitlement status'",
        "GROUND_TRUTH": "The 'SAM - Subscription Maintenance' scheduled job is failing to update the entitlement status due to an error in the script execution. The error is caused by the undefined 'SAMPReserveEntitlementUtil' in the function 'sampUpdateStateStatus()'. This error can be observed in the system logs when you execute the 'SAM - Subscription Maintenance' job. \n\nTo workaround this issue, you need to comment out the line 'SAMPReserveEntitlementUtil.processExpiredReserveEntitlements();' under the function 'sampUpdateStateStatus()' of the 'SAM - Subscription Maintenance' Scheduled Script Execution. This should allow the job to run properly and update the entitlement status. \n\nPlease note that this issue is related to Problem PRB1475316."
    },
    {
        "id": 26,
        "CONTEXT": "\"## \n\n## Description\n\nDue to limitation of the Rubrik REST API call for \"Rubrik Hosts\" returning insufficient data for Windows Cluster CIs, these CIs were populated into Windows Server table (cmdb\\_ci\\_win\\_server) causing duplicate records, which already exists under cmdb\\_ci\\_cluster. The limitation is affecting also MSSQL Instances and MSSQL databases, associated with the Windows Cluster CIs in the Rubrik context.\n\n\n\u00a0\n\n\nAs part of previous implementation, the Rubrik REST API for \"MSSQL Instances\" and \"MSSQL DBs\" is returning Instances and associated Data bases related to Windows Clusters. From other side the Rubrik REST API for \"WIndows Clusters\" is returning insufficient data for those CIs, which cannot be uniquely identified. The data for Windows Clusters is populated into the CMDB table for Windows Servers, which is leading to duplicate CIs in different CMDB tables (cmdb\\_ci\\_cluster and cmdb\\_ci\\_win\\_server).\n\n## Steps to Reproduce\n\n 1. Configure Rubrik discovery schedule of type serverless. Make sure credentials of type \"Basic Auth\" are configured associated to the Rubrik cluster.  \n2. Execute discovery, make sure pattern \"Rubrik Windows Host (LP)\" is triggered and populating data into table \"cmdb\\_ci\\_win\\_server\".  \n3. Observe that in table \"cmdb\\_ci\\_win\\_server\" CIs with Operating system type \"WindowsCluster\" are populated.\n\n## Workaround\n\nDue to the issue described above, information for Windows Clusters is not retrieved and respectively they are not populated as CIs into the CMDB. Also MSSQL Instances and MSSQL Data bases, associated with Windows Clusters are filtered out from the discovery into the Rubrik context.\n\n\nThis issue is fixed as part of PRB1503346 \/\/ DEF0217774 related to plugin 'Discovery and Service Mapping Patterns' version 1.0.81\n\n  \n**Related Problem: PRB1503346**\"\n\n",
        "QUERY": " 'Rubrik LP returns inadequate data for the Server CIs'",
        "GROUND_TRUTH": "The issue with Rubrik LP returning inadequate data for Server CIs arises due to the limitations of the Rubrik REST API call for \"Rubrik Hosts\". This limitation results in insufficient data being returned for Windows Cluster CIs. As a result, these CIs are populated into the Windows Server table (cmdb_ci_win_server), causing duplicate records that already exist under cmdb_ci_cluster. This limitation also affects MSSQL Instances and MSSQL databases associated with the Windows Cluster CIs in the Rubrik context. \n\nTo reproduce this issue, you need to configure a Rubrik discovery schedule of type serverless with credentials of type \"Basic Auth\" associated with the Rubrik cluster. After executing the discovery, you will observe that CIs with Operating system type \"WindowsCluster\" are populated in the \"cmdb_ci_win_server\" table.\n\nThe workaround for this issue is provided in PRB1503346 \/\/ DEF0217774 related to the plugin 'Discovery and Service Mapping Patterns' version 1.0.81. This fix ensures that information for Windows Clusters is retrieved and populated as CIs into the CMDB. Also, MSSQL Instances and MSSQL Data bases associated with Windows Clusters are no longer filtered out from the discovery into the Rubrik context."
    },
    {
        "id": 27,
        "CONTEXT": "\"## \n\n\u00a0\n\n\n\n\n\n\n| Note | **Note**: The guideline below is intended for Technical Duty Officers participating in the BLADES initiative. |\n| --- | --- |\n\n\n\u00a0\u00a0\n\n\n# Overview\n\n\nThis document serves as a comprehensive reference for the development team to ensure consistent and high-quality software solutions across projects. It outlines best practices, coding conventions, design principles, and quality assurance procedures to be followed throughout the software development lifecycle.\n\n\n## Purpose\n\n\nEstablish a standardized approach to software development to enhance maintainability, scalability, and interoperability of applications.\u00a0 Provide a framework for developers to produce code that is efficient, secure, and adherent to industry-recognized standards.\u00a0 Promote collaboration and knowledge sharing among team members by creating a common set of practices and guidelines.\u00a0 Facilitate code review processes, enabling the identification and resolution of issues early in the development cycle.\n\n\n## Scope\n\n\n**Requirement Considerations:**Common questions and considerations for gathering requirements and planning for your initial prototype.\n\n\n**Security Standards:**  \nBest practices for securing sensitive data and preventing common security vulnerabilities.  \nAuthentication, authorization, and encryption standards.\n\n\n**Testing and Quality Assurance:**  \nProcedures for unit testing, integration testing, and acceptance testing.  \nCode review guidelines to ensure adherence to established standards.\n\n\n**Documentation Requirements:**  \nStandards for code documentation, including API documentation and user guides.\n\n\n**Third-Party Libraries and Frameworks:**  \nGuidelines for selecting and integrating third-party libraries and frameworks.  \nLicensing and compliance considerations.\n\n\n**Performance Optimization:**  \nTechniques for optimizing code performance, including algorithm efficiency and resource utilization.\n\n\n**Deployment and Continuous Integration:**  \nGuidelines for deployment processes and continuous integration practices.\n\n\n\u00a0\n\n\n# Coding Standards\n\n\n## Define the Requirements\n\n\nWhen defining the requirements, we're trying not to be too prescriptive, focusing more on agility and the prototyping phase.\u00a0 That being said, here are some considerations:\n\n\nWill this run on the platform?\u00a0 If not, what language will be used?  \nWill this run periodically on a cron\/scheduled job or manually?  \nWill this run on a user's laptop or on a toolbox\/ops VM?  \nWhat integrations will be necessary for the data we need?  \nWhat external dependencies are there?  \nHow will the users interact with the application?\u00a0 Define simple use cases and\/or user stories.  \nHow will this be supported?  \nWhat monitoring strategy makes the most sense?\n\n\nLogging: There must be some logging included with every script and application.\u00a0 On-platform applications should be written to the System Log.\n\n\nAttributability: All uses of the script or tool must be attributable to a specific user and have a time stamp.\n\n\nSecurity: Must leverage a service account using DCPS and not run as an individual for production.\u00a0 Where possible, run as an individual user for development.\u00a0 Each application should have its own service account in production.\n\n\n\u00a0\n\n\n## Prototype\n\n\nThe on-platform development environment will consist of 'hirelease', 'datacentertest', 'sredev', 'sretest'.\u00a0 Other scripting languages should utilize these instances and the lab environment when interacting with infrastructure during development, where possible.  \nTrack everything in the ServiceNow git, whether on-platform, python, bash, etc...  \nUse ops-owned resources such as the sre, ops, or toolbox servers.  \nUse only ServiceNow-approved language versions, e.g. Python3.x, etc...  \nUse only ServiceNow-approved libraries\/extensions or obtain the necessary security approvals before installing any third-party modules.\n\n\nOptimization: Follow [querying best practices](https:\/\/www.metabase.com\/learn\/sql-questions\/sql-best-practices), especially when interacting with an HVI.\u00a0 Prefer querying by sys\\_id, use '=' where possible.\n\n\nPerformance: DO NOT slam Datacenter or Hi with hundreds of queries.\u00a0 They'll come knocking, trust me.\n\n\n\u00a0\n\n\n## Construction\n\n\nThis phase will mostly consist of formally building out what has been designed and tested in the Prototype phase.\u00a0 Take note of the data governance below, this portion must be completed before deploying to production.\n\n\n**IMPORTANT NOTE:**  \nAfter prototyping, you should have a good idea if you are going to be pulling, storing, reporting on, or otherwise distributing net new data from Hi or Datacenter.\u00a0 If so, you must use the Data Governance process for approval.\u00a0 The full KB is linked below, but for our team's purposes, I'll summarize here:\n\n\n1. Fill out the attached documents with your new fields.\u00a0 I've left the data from the previous request only as an example.\u00a0 Remove and edit the existing data for your purposes.\n\t1. Data Governance Approval Request Form.\n\t2. Data Governance - Data Mapping.\n2. Create a new [Data Governance Review Request](https:\/\/my.servicenow.com\/esc?id=service_catalog&spa=1&sc_cat_item=f0b1a4a6db919c90e76ddb85ca9619a6) and attach the documents above. [Example Request - RITM1066867](https:\/\/my.servicenow.com\/esc?id=req_details&table=sc_req_item&sys_id=78f393a797f53d904b59701e6253affc).\n3. You should receive an invite to attend the Data Governance Review Board Meeting.\n4. Once you've presented your application at the review board meeting, follow the RITM for approvals.\n\n\n\u00a0\n\n\n## Deployment\n\n\nAll deployments require:\n\n\nProper Change Management, i.e. a normal change, peer review, and manager approval.  \nComplete the code review process.\u00a0 Please see [KB1436439](https:\/\/support.servicenow.com\/kb?id=kb_article_view&sysparm_article=KB1436439) for more information.  \nOn-platform applications should follow the standard deployment pipeline in [KB1436439](https:\/\/support.servicenow.com\/kb?id=kb_article_view&sysparm_article=KB1436439).\u00a0 Do not code directly in prod.  \nScripts must be deployed and updated from Gitlab, regardless of whether they will be a user script installed locally or deployed onto an ops\/sre\/toolbox.  \nCode Reviews must be completed before production deployment and will follow the SRE Development Process in [KB1436439](https:\/\/support.servicenow.com\/kb?id=kb_article_view&sysparm_article=KB1436439).  \nUser Guide - Explains usage details.\u00a0 [Example from Hindsight](https:\/\/sre.service-now.com\/now\/nav\/ui\/classic\/params\/target\/kb_knowledge_list.do%3Fsysparm_userpref_module%3Ddc7654d3db1e9510e593fd3339961918%26sysparm_query%3Dkb_knowledge_base%253D8784c61bdba19910e593fd33399619ca%255EEQ).  \nDeveloper Guide - In-depth guide with an overview, explaining inputs, outputs, functions, troubleshooting information, FAQ, API information, and on-platform data structure.\u00a0 [Example from Hindsight](https:\/\/sre.service-now.com\/kb_knowledge.do?sys_id=0aa55d4edb247d5025e94f7813961931&sysparm_record_target=kb_knowledge&sysparm_record_row=19&sysparm_record_rows=73&sysparm_record_list=kb_knowledge_base%3D8784c61bdba19910e593fd33399619ca%5EORDERBYDESCnumber).  \nTesting - Hindsight is currently tested daily with a scheduled job that, upon failure, informs the BLADES Teams channel of the failure.\u00a0 Ideally, we will be adopting the ATF framework, however the team needs to skill up in this area.\u00a0 Simple scripts can utilize simple functional testing with failure output fed into a Teams webhook, email, or whatever method the developer finds appropriate.\n\n\n\u00a0\n\n\n# Additional Resources\n\n\n* [KB1446284 - Team Page | BLADES](https:\/\/support.servicenow.com\/kb?id=kb_article_view&sys_kb_id=f0cfb2b5979079508a073cbe2153af97)\n* [KB0749718 - Guideline | CIM Requestor Reference](https:\/\/support.servicenow.com\/kb?id=kb_article_view&sysparm_article=KB0749718)\n* [KB1509796 - BLADES | Software Development Life Cycle](https:\/\/support.servicenow.com\/kb?id=kb_article_view&sys_kb_id=59b681aa93b8b110f538fb2d6cba1030)\n* [KB0052771 - How to Complete Data Governance Review Request](https:\/\/my.servicenow.com\/esc?id=surf_kb_article&sys_id=dc385c4f47f9ed109c0225d3846d4378)\n* [KB1436439 - Guideline | Development Guideline for SRE Instance](https:\/\/support.servicenow.com\/kb?id=kb_article_view&sysparm_article=KB1436439)\n* [KB0538549 - Reference | DATACENTER Instance Access](https:\/\/support.servicenow.com\/kb?id=kb_article_view&sysparm_article=KB0538549)\n* [Link to request an integration account for 'hi'](https:\/\/support.servicenow.com\/com.glideapp.servicecatalog_cat_item_view.do?v=1&sysparm_id=d141aec86fb4ca00daa1409e9f3ee4da&sysparm_link_parent=e15706fc0a0a0aa7007fc21e1ab70c2f&sysparm_catalog=e0d08b13c3330100c8b837659bba8fb4&sysparm_catalog_view=catalog_default&sysparm_view=catalog_default)\n\n\n\u00a0\n\n\n\nRevision Log...\u00a0 **(Last updated: 28-Nov-2023)**\n  \n\n\n\n| Version | Published | Summary of Changes |\n| --- | --- | --- |\n| 1.0 | 28-Nov-2023 | Initial version |\"\n\n",
        "QUERY": " 'request new internal instance'",
        "GROUND_TRUTH": "no answer found"
    },
    {
        "id": 28,
        "CONTEXT": "\"## \n\n## Description\n\n\n\nWhen uninstalling \"Health Log Analytics\" store app, some records may not be deleted. You may also see data inputs with empty name.\n\n\n\n## Steps to Reproduce\n\n\n\n1) Uninstall \"Health Log Analytics\" store application.\n\n\n2) Go to sn\\_occ\\_base\\_data\\_input\\_config - There may be records left with \"empty\" name\n\n\n3) Go to sn\\_occ\\_data\\_input\\_extension - There may be records left with \"empty\" name\n\n\n\n## Workaround\n\n\n\nIf you have not uninstalled the store app yet, stop all data inputs.\n\n\n1) Navigate to Health Log Analytics -> Data Input -> Data Inputs\n\n\n2) Mark all records.\n\n\n3) Open the list action menu and choose \"Stop Data Input\"\n\n\n4) Make sure all data inputs are in \"Not Active\" state (you may need to wait a few seconds).\n\n\n  \nIf you have uninstalled the store app:\n\n\n1) Go to sn\\_occ\\_data\\_input\\_extension\n\n\n2) Per each record, click it and run \"Stop\" ui action\n\n\n3) Delete the record\n\n\n4) If there are records left in sn\\_occ\\_base\\_data\\_input\\_config, delete them.\"\n\n",
        "QUERY": " 'Records are left with empty name while uninstalling a store app'",
        "GROUND_TRUTH": "When uninstalling the \"Health Log Analytics\" store application, it is possible that some records may not be deleted, leaving data inputs with empty names. This can be seen in sn_occ_base_data_input_config and sn_occ_data_input_extension. \n\nIf you have not yet uninstalled the store app, you can prevent this by stopping all data inputs. Navigate to Health Log Analytics -> Data Input -> Data Inputs, mark all records, open the list action menu and choose \"Stop Data Input\". Ensure all data inputs are in \"Not Active\" state, which may require waiting a few seconds.\n\nIf you have already uninstalled the store app, go to sn_occ_data_input_extension. For each record, click it and run the \"Stop\" UI action, then delete the record. If there are records left in sn_occ_base_data_input_config, delete them as well."
    },
    {
        "id": 29,
        "CONTEXT": "\"## \n\n## Description\n\nThis issues can occurs when a consumer instance create a new consumer set. Producer instance approve the consumer subscription. However, the consumer instance never gets the approval from the producer instance.\n\n\nChecking idr\\_replication\\_log on consumer, IDRMetadataConsumer job is not receiving any metadata records.\u00a0  \ni.e.\n\n\nsource = IDRMetadataConsumerJob - system  \nmessage = finished: IDRMetadataConsumerJob, IDRConsumerJob#metadata: 0,\u00a0\n\n\nThe reason for metadata consumer job not receiving any metadata is due to the fact that we have 7 days retention policy for metadata topics. \u00a0If for 7 days, an instance hasn't received any metadata message, kafka would lose their offset for their metadata topic. Now if after 7 days, they try to do a poll on kafka, since their metadata offset is null, poll won\u0092t return any messages.\u00a0\n\n## Steps to Reproduce\n\n 1) have two instances that have active sets but haven't create\/updates their sets for 7 days.  \n2) Create a new producer replication set on one instance  \n3) create the corresponding consumer set\u00a0  \n4) When producer sends approval, consumer will not receive it\n\n## Workaround\n\nThe following scripts will work with instances on Orlando or later.\n\n\nNavigate to the idr\\_replication\\_log table and add \"Created\" to list view.  \n  \n\n\n\n\u00a0\n\n\n  \n  \n\n\n\nUse the filter shown below to get the logs for metadata:  \n*Source* is \"IDRMetadataConsumerJob\"  \n*Message* does not contain \"IDRConsumerJob#metadata: 0\"  \nThen order by Created time (z-a)\n\n\n\u00a0\n\n\n  \n  \n\n\n\nIf 0 metadata value and 'created' less than 4 days old; no need to resolve  \nif non-zero metadata and 'created' less than 4 days old; no need to resolve  \n  \nif non-zero metadata and 'Created' more than 4 days old; run scripts below review the 'offset' and update accordingly.\n\n\n\u00a0\n\n\n#### Check the current offset:\n\n\n\n```\nvar target = Packages.com.glide.idr.cluster.ClusterInfoProvider.getInstanceId();\nvar topicAddress = Packages.com.glide.idr.commons.Constants.getReadTopicAddress(target);\nvar topicClass = new Packages.com.glide.mq.clients.api.Topic(topicAddress);\ntry {\n    var provider = function () { return new Packages.com.glide.idr.protocol.deserialization.IDRV0Deserializer()}; \u00a0 \u00a0   \n    var job = \"IDRMetadataConsumerJob\";  \n  \n\/* Note: 'getConsumer' function does take a different number of parameters based on the release version.\nPlease confirm the instance release version and change the code to use the correct format of the 'getConsumer' function call.*\/\n\n\n\/\/This is for pre-Rome\nvar consumer = new Packages.com.glide.mq.clients.api.MQClientManager.getConsumer(job, provider);\n\n\n\/\/This is for Rome \n\/\/var consumer = new Packages.com.glide.mq.clients.api.MQClientManager.getConsumer(job, provider, \"\", \"\");\n\n\n\/\/This is for San Diego \n\/\/var consumer = new Packages.com.glide.mq.clients.api.MQClientManager.getConsumer(job, null, provider, \"\", \"\");\n\n    consumer.subscribe([topicClass]);\n    var currentOffset = consumer.currentOffset(topicClass);\n    var endOffset = consumer.endOffset(topicClass);\n    gs.print(\"Current Offset: \" + currentOffset);\n    gs.print(\"End Offset: \"\u00a0 + endOffset.getRawOffset());\n} finally {\n\tconsumer.close();\n}\n\n```\n\n\u00a0\n\n\n**If \"*Current Offset*\" from the above script returns as NULL:**\n\n\nDisable the IDRMetadataConsumer job\n\n\n\u00a0\n\n\n\u00a0\n\n\n#### **Update offset**\n\n\n\n```\n\/\/NOTE: Do not alter the script as it does alter the offset value and can cause further issues if the incorrect value is set.   \n  \nvar target = Packages.com.glide.idr.cluster.ClusterInfoProvider.getInstanceId(); \nvar topicAddress = Packages.com.glide.idr.commons.Constants.getReadTopicAddress(target); \nvar topicClass = new Packages.com.glide.mq.clients.api.Topic(topicAddress); \ntry { \n    var provider = function () { return new Packages.com.glide.idr.protocol.deserialization.IDRV0Deserializer()};  \n    var job = \"IDRMetadataConsumerJob\";\n   \n   \/* Note: 'getConsumer' function does take a different number of parameters based on the release version.\nPlease confirm the instance release version and change the code to use the correct format of the 'getConsumer' function call.*\/\n\n\n\/\/This is for pre-Rome\nvar consumer = new Packages.com.glide.mq.clients.api.MQClientManager.getConsumer(job, provider);\n\n\n\/\/This is for Rome \n\/\/var consumer = new Packages.com.glide.mq.clients.api.MQClientManager.getConsumer(job, provider, \"\", \"\");\n\n\n\/\/This is for San Diego \n\/\/var consumer = new Packages.com.glide.mq.clients.api.MQClientManager.getConsumer(job, null, provider, \"\", \"\");\n   \n   consumer.subscribe([topicClass]); \n    var endOffset = consumer.endOffset(topicClass); \/\/leaving the print statement for debugging \n    gs.print(\"End Offset: \" + endOffset); consumer.subscribe([topicClass]);\n    consumer.poll();\n    if (null === endOffset || endOffset.getRawOffset() == 0) {  \n        var offset = new Packages.com.glide.mq.clients.api.Offset(topicClass, 0);   \n        consumer.seek(topicClass, offset);\n    } else { \n        var newOffset = new Packages.com.glide.mq.clients.api.Offset(topicClass, endOffset.getRawOffset() - 1);\n        consumer.seek(topicClass, newOffset);\n    }\n    consumer.poll(); \n    consumer.commit();\n} finally { \n    consumer.close();\n}\n```\n\n\u00a0\n\n\nThen Re-run the 1st script to validate the offset\n\n\nIf the \"Current Offset\" is no longer NULL re-activate the IDRMetadataConsumer job.\n\n  \n**Related Problem: PRB1433180**\"\n\n",
        "QUERY": " 'producer instance not approving a new consumer instance'",
        "GROUND_TRUTH": "The issue of a producer instance not approving a new consumer instance can occur when a consumer instance creates a new consumer set and the producer instance approves the consumer subscription, but the consumer instance never receives the approval from the producer instance. This is due to a 7-day retention policy for metadata topics. If an instance hasn't received any metadata message for 7 days, Kafka would lose their offset for their metadata topic. When they try to poll on Kafka after 7 days, since their metadata offset is null, the poll won\u2019t return any messages.\n\nTo resolve this issue, navigate to the idr_replication_log table and add \"Created\" to list view. Use the filter to get the logs for metadata where Source is \"IDRMetadataConsumerJob\" and Message does not contain \"IDRConsumerJob#metadata: 0\". Then order by Created time (z-a). If you find non-zero metadata and 'Created' more than 4 days old, run the provided scripts to review the 'offset' and update accordingly. If the \"Current Offset\" from the script returns as NULL, disable the IDRMetadataConsumerJob, update the offset using the provided script, and then re-activate the IDRMetadataConsumerJob."
    },
    {
        "id": 30,
        "CONTEXT": "\"## \n\n## Description\n\nThis is for the sys\\_properties \"glide.ais.ha.enable\\_auto\\_failover\" and \"glide.ais.event\\_queue\\_writable\" in relation to AI Search (AIS) node provisioning which can result in performance impact if certain conditions are met.\u00a0 This is due to a full cache flush being triggered when the AIS node is reprovisioned.\n\n\nFor context, with the use of AI Search (AIS), one AIS node is provided per customer which is shared across all instances (irrespective if it is a subproduction or production instance).\u00a0\n\n\nOn the AIS node, it contains dedicated AIS partitions for each instance. When a clone is initiated, when the capacity on the AIS node is not enough to provision a new AIS partition, it triggers a process called \u0093Drain and Upsize AIS node\u0094 after the clone is complete.\u00a0 This occurs when the source instance is a larger size than the target instance on a clone.\n\n\nWhen \u0093Drain and Upsize AIS node\u0094 occurs, we update the \"glide.ais.ha.enable\\_auto\\_failover\" and \"glide.ais.event\\_queue\\_writable\" sys\\_properties for all instances of a customer which has AIS partitions.\u00a0 This can even happen if a customer has a new instance and requests to provision AI search for a newly created instance.\n\n\nThe update in sys\\_properties after the \u0093Drain and Upsize AIS node\u0094 process as part of the AIS provisioning initiates a full cache flush upon update on all instances.\u00a0 This full cache flush can impact performance on both the source, target, and other instances owned by the same Company.\u00a0\n\n\nThe timing of the \u0093Drain and Upsize AIS node\u0094 is important to note since it occurs after the clone.\u00a0 The reason is because of the size of the clone, as some clones can take more than two days.\u00a0 For example, if a clone is initiated over the weekend, the process to reprovision the AIS node can occur during core business hours if a clone takes the entirety of a weekend.\u00a0\n\n\nIf a clone does take the entire weekend, the \u0093Drain and Upsize AIS node\u0094 can occur during regular business hours where a full cache flush can occur when the sys\\_properties are updated on all instances which results in impact on a production instance in their core hours.\n\n## Steps to Reproduce\n\n Issue cannot be reproduced on-demand.\u00a0\n\n## Workaround\n\nServiceNow will update the sys\\_properties ignore\\_cache= true for both \"glide.ais.ha.enable\\_auto\\_failover\" and \"glide.ais.event\\_queue\\_writable\" as a maintenance for identified instances that have AIS enabled.\n\n\n\n### FAQs\n\n\nQ: Can I reschedule or opt-out of this maintenance?\n\n\nA: Modifying this property is important to prevent future potential performance impact.\u00a0 We do not allow a reschedule, but if an opt-out exception is requested please let us know in the communication record opened.\u00a0 With the exception request, we require that the customer administrator run the script attached to this KB (PRB1578851 write audit script.txt) as soon as possible in scripts background under global scope on the instance mentioned in the communication record.\n\n\n----------\n\n\nQ: Will this maintenance have any service impact?\n\n\nA: Modifying the system properties will not have any service impact. This is a simple change to update both system properties. Node restart is NOT required during the maintenance.\n\n\nQ: What testing has been done before performing this maintenance on my instance?\n\n\nA: Testing has been done on internal instances for this maintenance.\n\n\nQ: After the clone, which instances will potentially be impacted?\n\n\nA: Any\/all of your instances can be affected by this issue, however production instances typically contain more live data where performance impact can be a bigger concern.\u00a0\n\n\nQ: How are these two properties used?\n\n\nA:\u00a0\n\n\n* \"glide.ais.ha.enable\\_auto\\_failover\" :\u00a0 This will control automatic failover to backup AIS service in case Primary AIS service goes down.\n* \"glide.ais.event\\_queue\\_writable\" : This is to control AIS events in the sysevents table.\n\n\nQ: How can I tell if the \u0093Drain and Upsize AIS node\u0094 will initiate?\n\n\nA: This occurs on clones from instances with AIS partitions, or provisioning AI search (doesn't mattern if it's on a source or target instance for a clone).\n\n\n  \n**Related Problem: PRB1578851**\"\n\n",
        "QUERY": " 'performance issue on production'",
        "GROUND_TRUTH": "The performance issue on production as described in KB12345 is related to the AI Search (AIS) node provisioning process. When a clone is initiated and the capacity on the AIS node is not enough to provision a new AIS partition, a process called \"Drain and Upsize AIS node\" is triggered. This process updates the sys_properties \"glide.ais.ha.enable_auto_failover\" and \"glide.ais.event_queue_writable\" for all instances of a customer which has AIS partitions. This update initiates a full cache flush on all instances, which can impact performance on both the source, target, and other instances owned by the same Company. The timing of this process is important as it occurs after the clone, and if a clone takes more than two days, the process can occur during core business hours, potentially impacting a production instance. The workaround provided by ServiceNow is to update the sys_properties ignore_cache= true for both \"glide.ais.ha.enable_auto_failover\" and \"glide.ais.event_queue_writable\" as a maintenance for identified instances that have AIS enabled."
    },
    {
        "id": 31,
        "CONTEXT": "\"## \n\n## Description\n\nUpon installing the agent client collector agent on Windows platform, the MSI installer logs the password used for the service account in plain text on the installer log.\n\n\nThis is detected as a security issue by some security scanners and while the impact is limited, it does expose a vulnerability on the target machine.\n\n## Steps to Reproduce\n\n 1. Install ACC agent on a test Windows machine.\n2. Read the Agent\\_Installation.log to find the plain text password for the local user account\n## Workaround\n\nThis problem has been fixed. If you are able to upgrade, review the Fixed In section to determine the latest version with a permanent fix your instance can be upgraded to.\n\n  \n**Related Problem: PRB1633872**\"\n\n",
        "QUERY": " 'Password exposed in the Agent installation logs'",
        "GROUND_TRUTH": "The issue of the password being exposed in the Agent installation logs occurs when the agent client collector agent is installed on a Windows platform. The MSI installer logs the password used for the service account in plain text on the installer log. This is detected as a security issue by some security scanners and exposes a vulnerability on the target machine. To reproduce this issue, you would install the ACC agent on a test Windows machine and then read the Agent_Installation.log to find the plain text password for the local user account. However, this problem has been fixed in the latest version of the software. Therefore, upgrading your instance to the latest version with a permanent fix would resolve this issue."
    },
    {
        "id": 32,
        "CONTEXT": "\"## \n\n### Details\n\nA table called Source [sys\\_object\\_source] stores information identifying the source of a discovery (by\u00a0ServiceNow\u00a0Discovery\u00a0or another product), the ID of that source, and the date\/time of the last scan. To view this information, configure a CI form and add the\u00a0Sources\u00a0related list. This table is populated automatically when the\u00a0[Discovery plugin](https:\/\/docs.servicenow.com\/bundle\/rome-it-operations-management\/page\/product\/discovery\/task\/t_ActivateTheDiscoveryPlugin.html)\u00a0is enabled.  \n  \nIt is a look up table which will get updated millions of times everyday and we should not create Business rules based on them. Creating Business Rules on it would create performance issues  \n  \n  \nNote : The last scan field of sys\\_object\\_source is similar to the most recently discovered field on the CIs, meaning an update only on the last scan\/most recent discovery field will not change the updated time field.\"\n\n",
        "QUERY": " 'Object Source Table Integrity Check'",
        "GROUND_TRUTH": "no answer found"
    },
    {
        "id": 33,
        "CONTEXT": "\"## \n\n## Description\n\n**MID Server upgrade fails** leaving MID Server Down, due to **Cisco Advanced Malware Protection (AMP)** for Endpoints preventing MID Server upgrade process replacing the Wrapper executable file.\n\n\nThe Upgrade starts, the MID Server launches the dist upgrade process and shuts itself down. The Dist-upgrade process failed with FileNotFoundException and stops due to the wrapper-windows-x86-64.exe file being locked\/blocked by Cisco AMP.\n\n\n**NOTE:** This PRB is specific to MID Server outages caused by Cisco AMP during upgrades, but this symptom is not always caused by Cisco AMP. The same symptom has also been reported on server not running Cisco AMP.\u00a0\n\n\nBefore applying the work-around we need to make sure that the root cause is Cisco AMP. To do this we need to verify the wrapper.log and also make sure that Cisco AMP is running.\u00a0\n\n\n  \n\n\n\n**Verifying the error message in wrapper.log**\n\n\nThe MID Server wrapper.log will show this at the end (assuming no manual attempt was made to start it since):\n\n\n\n```\n  \nMay 12, 2020 2:57:33 PM com.snc.dist.mid_upgrade.UpgradeMain run\nSEVERE: com.snc.dist.mid_upgrade.UpgradeException: java.io.FileNotFoundException: C:\\ServiceNowagent\bin\\wrapper-windows-x86-64.exe (Access is denied)\ncom.snc.dist.mid_upgrade.UpgradeException: java.io.FileNotFoundException: C:\\ServiceNowagent\bin\\wrapper-windows-x86-64.exe (Access is denied)\nat com.snc.dist.mid_upgrade.UpgradeMain.migrateToTarget(UpgradeMain.java:840)\nat com.snc.dist.mid_upgrade.UpgradeMain.run(UpgradeMain.java:313)\nat java.lang.Thread.run(Thread.java:748)\nCaused by: java.io.FileNotFoundException: C:\\ServiceNowagent\bin\\wrapper-windows-x86-64.exe (Access is denied)\nat java.io.FileOutputStream.open0(Native Method)\nat java.io.FileOutputStream.open(FileOutputStream.java:270)\nat java.io.FileOutputStream.(FileOutputStream.java:213)\nat java.io.FileOutputStream.(FileOutputStream.java:162)\nat org.apache.commons.io.FileUtils.doCopyFile(FileUtils.java:1142)\nat org.apache.commons.io.FileUtils.doCopyDirectory(FileUtils.java:1446)\nat org.apache.commons.io.FileUtils.doCopyDirectory(FileUtils.java:1444)\nat org.apache.commons.io.FileUtils.copyDirectory(FileUtils.java:1388)\nat org.apache.commons.io.FileUtils.copyDirectory(FileUtils.java:1317)\nat com.snc.dist.mid_upgrade.UpgradeMain.migrateToTarget(UpgradeMain.java:837)\n... 2 more\n\nMay 12, 2020 2:57:33 PM com.snc.dist.mid_upgrade.UpgradeMain appendMidLogs\nINFO: Flushing logs\n<< UPGRADE LOG END >>  \n  \n  \n**Verifying Cisco AMP is running**Open \"Task Manager\" and make sure that CiscoAMP is running  \n![](\/sys_attachment.do?sys_id=d0641f991b6ae850ed6c9979b04bcbbc)  \n  \n\n```\n## Steps to Reproduce\n\n 1. Install a MID Server on a Windows host running Cisco Advanced Malware Protection (AMP)\n2. Cause the MID Server to upgrade\n3. Some upgrades will fail to upgrade at the point that the old agent\bin\\wrapper-windows-x86-64.exe is deleted\n## Workaround\n\nTo resolve the issue, you need to add an exclusion set including the following exclusions to the policy applied for the MID Server host machine on Cisco AMP Console:\n\n\n* File Scan for wrapper-windows-x86-64.exe under agent\bin folder (with Apply to child processes marked)\n* File Scan for java.exe under agent\\jre\bin folder (with Apply to child processes marked)\n* Wildcard for the MID Server folder\n\n\nTo add the exclusions\n\n\n1. Make sure you have proper privilege to add an exclusion to Cisco AMP\n2. On the Cisco AMP console  \n\n\t1. Create a new Exclusion set\n\t2. Add a new \"File Scan\" exclusion to the exclusion set to exclude the wrapper-windows-x86-64.exe existing under agent\bin folder. The details of how you can add this exclusion is described in [KB0867749](\/kb_view.do?sysparm_article=KB0867749 \"KB0867749\") When you define this exclusion make sure  \n\t\n\t\t* You use the complete path for wrapper-windows-x86-64.exe under agent folder. For example for a MID Server with the agent folder path C:\\Midservers\\mid1agent you need to exclude \"C:\\Midservers\\mid1agent\bin\\wrapper-windows-x86-64.exe\"\n\t\t* You mark **\u0093Apply to child processes\u0094**when you define the exclusion\n\t3. Add a new \"File Scan\" exclusion to the exclusion set for java.exe. This File Scan exclusion is defined to exclude the java.exe existing under agent\\jre\bin folder. The details of how you can add this exclusion is described in [KB0867749](\/kb_view.do?sysparm_article=KB0867749 \"KB0867749\") When you define this exclusion make sure   \n\t\n\t\t* You use the complete path for java.exe under agent folder. For example for a MID Server with the agent folder path C:\\Midservers\\mid1agent you need to exclude \"C:\\Midservers\\mid1agent\\jre\bin\\java.exe\"\n\t\t* You mark **\u0093Apply to child processes\u0094**when you define the exclusion\n\t4. Add a wildcard exclusion for the MID Server folder. For example for the agent folder \"C:\\Midservers\\mid1agent\" you can add \"C\\Midservers\\*\". The details are available in [KB0867749](\/kb_view.do?sysparm_article=KB0867749 \"KB0867749\").\n\t5. Add the new exclusion set to the policy applied for the MID Server host machine.\n3. Make sure the policy is synchronized on the MID Server host machine before starting MID upgrade. If you are not familiar with this process refer to\u00a0 [KB0867749](\/kb_view.do?sysparm_article=KB0867749 \"KB0867749\").\n4. Cause the MID Server to upgrade.\n\n\n  \n\n\n  \n**Related Problem: PRB1408516**\"\n\n",
        "QUERY": " 'Mid server upgrade failure with Cisco AMP endpoints'",
        "GROUND_TRUTH": "The MID Server upgrade failure with Cisco AMP endpoints occurs when the Cisco Advanced Malware Protection (AMP) for Endpoints prevents the MID Server upgrade process from replacing the Wrapper executable file. This issue can be identified by a FileNotFoundException in the dist-upgrade process due to the wrapper-windows-x86-64.exe file being locked\/blocked by Cisco AMP.\n\nTo resolve this issue, you need to add an exclusion set in the policy applied for the MID Server host machine on the Cisco AMP Console. This exclusion set should include:\n\n1. File Scan for wrapper-windows-x86-64.exe under the agent folder (with 'Apply to child processes' marked)\n2. File Scan for java.exe under the agent\\jre folder (with 'Apply to child processes' marked)\n3. A wildcard for the MID Server folder\n\nAfter adding the exclusion set, ensure the policy is synchronized on the MID Server host machine before starting the MID upgrade."
    },
    {
        "id": 34,
        "CONTEXT": "\"## \n\n### Description\n\n\n  \n  \n\n# Description\n\n\n\n\n---\n\n\nLazy Writer is a mechanism (background thread) that we use in the platform to defer updates to the database so that user sessions can go on about their business (not unlike an ASYNC business rule).\n\u00a0\nEach application server node will have a glide.lazy.writer thread that handles these deferred updates.  \n  \nLazy\u00a0writer handles asynchronous writes to sysevent, audit, sys\\_user\\_presence. When the instance is processing a large number of user presences updates and sysevent updates,\u00a0lazy\u00a0writer can cause row lock contention as all the rows in a large batch are locked until the transaction is committed.\u00a0 User presence updates have direct impact on UI transactions, and end users\u00a0will start to experience slowness.\n  \nWhen the Lazy Writer queue is full then its writes become\u00a0Sync writes and\u00a0that\u00a0causes the lock contention  \n  \n\n# Procedure\n\n\n\n\n---\n\n\n1. Check threads.do and localhost logs  \n  \nCheck the \/threads.do output and see if multiple threads are in the following stack trace:\nat com.glide.db.DBCompositeAction.executeLazy(DBCompositeAction.java:291)\n  \n\nCheck the localhost\\_log.$(date +\"%Y-%m-%d\").txt and look for threads with the following error:\nFAILED TRYING TO EXECUTE ON CONNECTION 15: UPDATE sys\\_user\\_presence\n  \n  \n2018-04-03 07:36:28 (640) Presence-thread-3 58A5E0FCDB9517C8DAF72FEB0B961945 SEVERE \\*\\*\\* ERROR \\*\\*\\* FAILED TRYING TO EXECUTE ON CONNECTION 15: UPDATE sys\\_user\\_presence SET `ua\\_time` = '2018-04-03T14:35:36.978Z', `path` = '\/hrportal', `sys\\_updated\\_by` = '\\_\\_USERID\\_\\_', `sys\\_mod\\_count` = 29230, `sys\\_updated\\_on` = '2018-04-03 14:35:37', `user\\_agent` = 'Mozilla\/5.0 (Windows NT 10.0; WOW64; Trident\/7.0; rv:11.0) like Gecko', `status`= NULL WHERE sys\\_user\\_presence.`sys\\_id` = '2327c028db4cba00b9b178f9bf9619d5' \/\\* dell079, gs:58A5E0FCDB9517C8DAF72FEB0B961945, tx:35e34dfcdbd957c8daf72feb0b9619e2 \\*\/\u00a0  \n**Lock wait timeout exceeded; try restarting transaction**  \n**java.sql.SQLException: Lock wait timeout exceeded; try restarting transaction**  \nat org.mariadb.jdbc.internal.SQLExceptionMapper.get(SQLExceptionMapper.java:149)\u00a0  \nat org.mariadb.jdbc.internal.SQLExceptionMapper.throwException(SQLExceptionMapper.java:106)\u00a0  \nat org.mariadb.jdbc.MySQLStatement.executeQueryEpilog(MySQLStatement.java:268)\u00a0  \nat org.mariadb.jdbc.MySQLStatement.execute(MySQLStatement.java:296)\u00a0  \nat org.mariadb.jdbc.MySQLStatement.execute(MySQLStatement.java:387)\u00a0  \nat sun.reflect.GeneratedMethodAccessor22.invoke(Unknown Source)\u00a0  \nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\u00a0  \nat java.lang.reflect.Method.invoke(Method.java:498)\u00a0  \nat com.glide.db.StatementWrapper.invoke(StatementWrapper.java:40)\u00a0  \nat com.sun.proxy.$Proxy7.execute(Unknown Source)\u00a0  \nat com.glide.db.DBI.executeStatement0(DBI.java:921)\u00a0  \nat com.glide.db.DBI.executeStatement(DBI.java:877)\u00a0  \nat com.glide.db.DBI.executeStatement(DBI.java:850)\u00a0  \nat com.glide.db.DBAction.executeAsResultSet(DBAction.java:283)\u00a0  \nat com.glide.db.DBCompositeAction.executeAsResultSet(DBCompositeAction.java:139)\u00a0  \nat com.glide.db.DBCompositeAction.executeAsResultSet0(DBCompositeAction.java:92)\u00a0  \nat com.glide.db.DBAction.executeAndReturnTable(DBAction.java:247)\u00a0  \nat com.glide.db.DBAction.executeNormal(DBAction.java:236)\u00a0  \nat com.glide.db.DBAction.executeAndReturnException(DBAction.java:197)\u00a0  \nat com.glide.db.DBAction.execute(DBAction.java:136)\u00a0  \n**at com.glide.db.DBCompositeAction.executeLazy(DBCompositeAction.java:291)**  \n**at com.glide.db.DBCompositeAction.executeLazy(DBCompositeAction.java:275)**  \n**at com.glide.db.DBCompositeAction.executeLazy(DBCompositeAction.java:271)**  \nat com.glide.ui.ng.NGPresenceService.update(NGPresenceService.java:132)\u00a0  \nat com.glide.ui.ng.NGPresenceService.updatePresence(NGPresenceService.java:92)\u00a0  \nat sun.reflect.GeneratedMethodAccessor470.invoke(Unknown Source)\u00a0  \nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\u00a0  \nat java.lang.reflect.Method.invoke(Method.java:498)\u00a0  \nat com.glide.rest.handler.impl.ServiceHandlerImpl.invokeService(ServiceHandlerImpl.java:43)\u00a0  \nat com.glide.rest.processors.RESTAPIProcessor.process(RESTAPIProcessor.java:228)\u00a0  \nat com.glide.processors.AProcessor.runProcessor(AProcessor.java:415)\u00a0  \nat com.glide.processors.AProcessor.processTransaction(AProcessor.java:186)\u00a0  \nat com.glide.processors.ProcessorRegistry.process0(ProcessorRegistry.java:178)\u00a0  \nat com.glide.processors.ProcessorRegistry.process(ProcessorRegistry.java:167)\u00a0  \nat com.glide.ui.GlideServletTransaction.process(GlideServletTransaction.java:49)\u00a0  \nat com.glide.sys.Transaction.run(Transaction.java:1977)\u00a0  \nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\u00a0  \nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\u00a0  \nat java.lang.Thread.run(Thread.java:748)  \n  \n  \n1.1 Check the issue is not due to excessive sys\\_user\\_presence updates, ie: via the localhost on the application nodes:  \n  \ngrep \"UPDATE sys\\_user\\_presence\" localhost\\_log.$(date +\"%Y-%m-%d\").txt |awk '{ print $4 }' |sort |uniq -c  \n  \nsample output:  \n\n[app128153.sjc111:\/glide\/nodes]$ grep \"UPDATE sys\\_user\\_presence\" localhost\\_log.$(date +\"%Y-%m-%d\").txt |awk '{ print $4 }' |sort |uniq -c\n\u00a023936 glide.lazy.writer[glide]\n\u00a0 \u00a012299 Presence-thread-1\n\u00a0 \u00a012291 Presence-thread-2\n\u00a0 \u00a012289 Presence-thread-3\n\u00a0 \u00a012290 Presence-thread-4\n  \nWe would expect to\u00a0only see glide.lazy.writer updates.\u00a0   \nIn the above example, this shows excessive updates by Presence Threads\n2. Check INNOTOP \/ Process List and also observe the INSERT\/UPDATE\/DELETE lock contention\n3. Check the application node logs, and observe errors similar to;  \n  \n\n2018-04-03 07:58:18 (254) Presence-thread-3 CDE47834DB9113C84C931AAC0B961959 **WARNING \\*\\*\\* WARNING \\*\\*\\* Current lazy writer delay: 0:01:18.809**\n2018-04-03 07:58:19 (087) **glide.lazy.writer[glide] SYSTEM SEVERE \\*\\*\\* ERROR \\*\\*\\* Exception during batch statement commit to** glide:dell\\_1:emc:jdbc:mysql:\/\/db160020.iad106.service-now.com:3402\/, **falling back to single commits: Deadlock found when trying to get lock; try restarting transaction**\n2018-04-03 07:58:19 (092) glide.lazy.writer[glide] SYSTEM [0:00:01.394] Statement batcher: 1000\n2018-04-03 07:58:19 (389) glide.lazy.writer[glide] SYSTEM Time: 0:00:00.223 id: dell\\_1[glide.17] for: UPDATE sys\\_user\\_presence\u00a0 SET `ua\\_time` = '2018-04-03T14:56:27.335Z', `path` = '\/hrportal', `sys\\_updated\\_by` = ...\n  \n  \n4.1 Check SPLUNK (Visualization tab)  \nsourcetype=appnode\\_localhost\\_log instance=\\_\\_INSTANCE\\_\\_ \"lock\\_wait\" | timechart count span=1m  \n  \n![](sys_attachment.do?sys_id=c83f7426db0ab450e515c223059619fd)  \n  \n  \n  \n4.2 Check Big Data  \n![](sys_attachment.do?sys_id=cc3fb426db0ab450e515c22305961902)\n4. Once confirmed, review binlogs and application node logs to determine if single (small group) of sessions are the cause.\u00a0 Use logs to identify the table(s) being updated too frequently.\u00a0 \u00a0 \u00a0See\u00a0[KB0657202](https:\/\/support.servicenow.com\/kb_view.do?sysparm_article=KB0657202 \"KB0657202\")\n5. Work backwards to establish the root cause of the\u00a0update flood\u00a0- initiating transactions from that sessionID in the application node logs.\u00a0 Possible causes could be widgets on \/sp Service Portal pages generating excessive 'sys\\_user\\_presence' updates.  \n  \nComparitive numbers:  \nGE during peak times sees around 100 per second (no issue)  \nDell during outage event P1s saw 500 updates per second to sys\\_user\\_presence which was the cause of slow performance  \n  \n  \n6.\u00a0Upgrade to a fixed release.  \nPRB1267824 optimizes the LazyWriter to avoid using db transactions unnecessarily so that\u00a0it does not\u00a0hold row locks for an unnecessarily long period of time.\n\n\n\u00a0\n\n\n\u00a0\n\n\n# Applicable Versions\n\n\n\n\n---\n\n\nFixed in:\u00a0\n\n\nJakarta Patch 9  \nKingston Patch 5\n\n\n\u00a0\n\n\n# Additional Information\n\n\n\n\n---\n\n\nRelated properties:\n\n\nglide.db.lazy.writer.wait\\_threads\u00a0\nglide.db.lazy.writer.debug\u00a0\nglide.db.lazy.writer.use\\_transaction\u00a0\nglide.db.allow.lazy.payload  \n  \n  \n[KB0657202](https:\/\/support.servicenow.com\/kb_view.do?sysparm_article=KB0657202 \"KB0657202\") TSE Binlog Query Examples  \n\n#\"\n\n",
        "QUERY": " 'Lazy Writer Queue is long'",
        "GROUND_TRUTH": "The Lazy Writer is a background thread used in the platform to defer updates to the database, allowing user sessions to continue without interruption. Each application server node has a glide.lazy.writer thread that handles these deferred updates. However, when the Lazy Writer queue is full, its writes become synchronous, which can cause lock contention.\n\nIf you're experiencing a long Lazy Writer Queue, you can follow these steps to diagnose and address the issue:\n\n1. Check the \/threads.do output and localhost logs for multiple threads in the stack trace and for threads with errors related to sys_user_presence updates.\n2. Check INNOTOP \/ Process List for INSERT\/UPDATE\/DELETE lock contention.\n3. Check the application node logs for errors similar to \"Current lazy writer delay\" and \"Exception during batch statement commit to glide\".\n4. Check SPLUNK (Visualization tab) and Big Data for lock_wait counts.\n5. Review binlogs and application node logs to determine if a single session or small group of sessions are causing frequent updates. Work backwards to establish the root cause of the update flood.\n6. Upgrade to a fixed release. PRB1267824 optimizes the LazyWriter to avoid using db transactions unnecessarily, reducing the time it holds row locks."
    },
    {
        "id": 35,
        "CONTEXT": "\"## \n\n## Description\n\nWhen an ATF test suite that contains many child suites and\/or parameterized tests is rescheduled due to a mutually exclusive test running, the suite takes a long time to be rescheduled and will use a lot of the instance memory. In extreme cases, the instance may run out of memory and terminate the ATF test execution, leaving the test suite run incomplete.\n\n\nThis issue becomes worse with bigger test suite hierarchies and\/or mixing child test suites with parameterized tests; for example, a test suite containing many child suites, each of which has child suites of their own with multiple parameterized tests in them will use a significant amount of memory when it is rescheduled.\n\n## Steps to Reproduce\n\n 1) Create a large ATF test suite containing multiple child suites (which may have child suites of their own) and parameterized tests. This can be done in the \"Automated Test Framework (ATF) > Tests\" and \"Automated Test Framework (ATF) > Suites\" modules  \n  \nFor context, this issue was observed with the following setup:  \nOne root suite containing 10 child suites, each of which has a single child suite of their own  \nEvery suite mentioned above (including the root suite) contained 10 parameterized tests, each of which had 3 parameter sets  \nThe grand total number of tests is 21 suites \\* 10 tests per suite = 210 tests  \n  \n2) Run the test suite  \n3) At some point during the test suite run, force it to reschedule  \n  \nNOTE: One way to do this is to create a long running test (e.g. a test with a \"Run Server Side Script\" step that uses gs.sleep() to sleep for 10 minutes), then navigate to the Mutually Exclusive Tests related list on the test form, click \"Add Mutual Exclusion\" and choose one of the tests in the suite. Then run the test and the suite in parallel; the mutual exclusion rule on the test will make the suite reschedule when it tries to run the mutually exclusive test (at that point the long-running test can be cancelled to allow the suite to run)  \n  \n4) Wait ~10 seconds for the test suite to resume execution (at this point there's no need for it to reschedule again, so the test started in Step 3 can be completed)  \n  \nExpected: The suite run should resume normal execution after being rescheduled  \nObserved: The suite run takes a very long time (several minutes) to resume execution. While waiting for the test to resume execution, more and more memory is consumed; in extreme cases, the instance may run out of memory and kill the test run\n\n## Workaround\n\nAs mentioned in this description, this issue occurs when a test suite execution is rescheduled, and gets worse with large test suite hierarchies. This means there are two potential workarounds for this issue:\n\n\n1. Update the tests in the suite so they are less likely to be rescheduled. In most cases this means updating the tests so that they don't rely on updating pre-existing record data. Check the \"Mutually Exclusive Tests\" related list on the tests in the suite to see which tests have record conflicts with other tests. See below for some documentation on Parallel Testing and Mutually Exclusive Tests\n2. Break down large test suites hierarchies; in particular, removing child test suites from their parents (\"flattening\" the test suite) and running them separately will significantly reduce the impact of this issue\n\n\nParallel testing documentation:\n\n\n<https:\/\/docs.servicenow.com\/bundle\/rome-application-development\/page\/administer\/auto-test-framework\/concept\/parallel-testing.html>\n\n\n<https:\/\/docs.servicenow.com\/bundle\/rome-application-development\/page\/administer\/auto-test-framework\/concept\/mutual-exclusion-rule.html>\n\n  \n**Related Problem: PRB1539992**\"\n\n",
        "QUERY": " 'Large ATF test suite hierarchies takes a lot of time when rescheduled'",
        "GROUND_TRUTH": "When an Automated Test Framework (ATF) test suite that contains many child suites and\/or parameterized tests is rescheduled due to a mutually exclusive test running, it takes a long time to be rescheduled and uses a significant amount of the instance memory. In extreme cases, the instance may run out of memory and terminate the ATF test execution, leaving the test suite run incomplete. This issue becomes worse with larger test suite hierarchies and\/or mixing child test suites with parameterized tests. \n\nTo reproduce this issue, you would need to create a large ATF test suite containing multiple child suites and parameterized tests, run the test suite, and at some point during the test suite run, force it to reschedule. The suite run takes a very long time to resume execution after being rescheduled, and more and more memory is consumed during this time.\n\nThere are two potential workarounds for this issue. The first is to update the tests in the suite so they are less likely to be rescheduled. This usually means updating the tests so that they don't rely on updating pre-existing record data. The second workaround is to break down large test suite hierarchies, particularly removing child test suites from their parents and running them separately, which will significantly reduce the impact of this issue."
    },
    {
        "id": 36,
        "CONTEXT": "\"## \n\n## Description\n\nAWS pointed discovery failing for RDS provisioned through TFO\/TFE in case terraform template has aws provider version 5.0.0 and above. Root cause of this issue is terraform aws provider changed its id attribute value from 'The RDS instance ID' (v 4.67.0) to 'RDS DBI resource ID' (v 5.0.0+).\n\n\nFor pattern based target discovery we will see similar error:\n\n\n1. On Cloud User Portal\n\n\n*2023-10-04 00:09:31 : [RDStfo Interface] RDStfo.Provision.Discovery of resource type: aws\\_db\\_instance::db-KXHQQUITCPASMD5IWOZRHRKJXM - Error. {\"discoveryStatusId\":\"67e7a2ab472db510dd429f77746d43a3\"}*\n\n\n2. In order (sn\\_cmp\\_order) table\u00a0\n\n\n*Pattern targeted discovery failed for resource RDStfo.Provision.Discovery of resource type: aws\\_db\\_instance::db-KXHQQUITCPASMD5IWOZRHRKJXM. Please check discovery status DIS0010061 for more details. Pattern gracefully terminated.*\n\n\n3. In Discovery Status table\n\n\n*No RDS is found in the LDC (region) , Pattern name:\u00a0Amazon AWS Relational Database Service, To Check Pattern Log Press Here.*\n\n## Steps to Reproduce\n\n 1. Create a TFO\/TFE config provider make sure discovery is completed(VCS discovery is completed).  \n2. Add github credentials and list all the repositories discovery is completed.(Make sure rds related template is present).  \n3. Create a catalog using config installable for rds.  \n4. Now navaigate to User portal, order the catalog created in step3 .   \n  \nObservation: AWS pointed pattern discovery for resources getting failed with error message : 2023-09-20 22:34:37 : Pattern targeted discovery failed for resource AWSRDS2.Provision.Discovery of resource type: aws\\_db\\_instance::db-TUUIMPPBZDAHGBWBKWZ65KQ6QU. Please check discovery status DIS0034955 for more details. Pattern gracefully terminated. PFA.  \n  \n\n\n## Workaround\n\n1. Use fixed version of aws provider 4.67.0 in terraform template.\n\n\n**example:**\n\n\nterraform {  \n\u00a0 required\\_providers {  \n\u00a0 \u00a0 aws = {  \n\u00a0 \u00a0 \u00a0 source \u00a0= \"hashicorp\/aws\"  \n\u00a0 \u00a0 \u00a0 version = \"4.67.0\"  \n\u00a0 \u00a0 }  \n\u00a0 }  \n}\n\n  \n**Related Problem: PRB1699880**\"\n\n",
        "QUERY": " 'Kubernetes discovery failing with duplicate record error'",
        "GROUND_TRUTH": "no answer found"
    },
    {
        "id": 37,
        "CONTEXT": "\"## \n\n## Introduction\n\n\nCurrently (2023 and onward), customers who are migrating from on-premise to cloud do not have the ability to automatically migrate their KMF encrypted data. This is because on-premise instances use customer-generated keystores which ServiceNow does not have access to. \u00a0This document details guidance needed for on-premise to cloud migrations.\u00a0 Customer will need to be informed that they may need to manually re-enter Password2 data into their respective tables.\n\n\nNOTE: This document applies to Password2 data ONLY \u0096 if your customer also supports Column Level Encryption, please reach out to the Column Level Encryption Team for guidance.\n\n\n\u00a0\n\n\nStep 1: Analyze the KMF Password2 Splash Zone\n\n\n1. Log into your on-premise instance with your admin account\n2. Via the navigator, go to System Definition > Script - Background Script\n3. Execute the provided script in global scope to find all KMF Password2 encrypted data \u0096 see Appendix A\n4. Examine the output of the script (check the end of summary)\n\t* If there are only a handful of records, it will be more efficient to manually re-input the Password2 data on the cloud instance \u0096 continue to Step 2\n\t* If there is a large number for records, please create a case, and in turn ask the support engineer to create a case task to the KMF Team to discuss further options\n\n\n\u00a0\n\n\nStep 2: Password2 Data Re-Entry\n\n\n1. Move forward with your on-premise to cloud migration\n2. Once the migration is complete, on the cloud instance, verify that the instance is KMF healthy and operational \u0096 if it is not, open a case with support with the recommendation that a case task for the KMF Team is opened\n\n\n1. * To check KMF Health, via the navigator, go to Key Management > Health (Diagnostics)\n\t* Note, you need to have the appropriate KMF role (KMF Admin or KMF Cryptographic Manager) to view the KMF Health page \u0096 see [Key Management Framework roles](https:\/\/docs.servicenow.com\/csh?topicname=kmf-roles.html&version=latest) documentation for further details\n\n\n3. On the cloud instance, manually re-input the Password2 data \u0096 the instance will encrypt the data with cloud-based cryptographic keys\n\t* Use the output in Step 1 to locate the\u00a0 appropriate records for re-entry\n\t* If there are issues recalling what the Password2 data (plaintext value) was, you should be able to decrypt the KMF Password2 encrypted data on your **on-premise** instance \u0096 see Appendix B\n\t* Note, if you see an access denied error message, likely you\u0092ll need to adjust the module access policy that is blocking your decryption attempt \u0096 see [Module access policy overview](https:\/\/docs.servicenow.com\/csh?topicname=module_access_policy_overview.html&version=latest) and [Create a module access policy](https:\/\/docs.servicenow.com\/bundle\/vancouver-platform-security\/page\/administer\/key-management-framework\/task\/create-module-access-policy.html) documentation for more details\n\t* Note, you can delete Module Access Policies as needed once you're done with the decryption process\n\t* Note, if you run into problems decrypting, please open a case\n\t* Note, there could be cases where the record in question is no longer applicable for the cloud instance \u0096 in such cases, you can leave the record alone or delete the record (customer discretion)\n4. Once complete, we recommend that you test the changes that you made, depending on your use case\n\n\n\u00a0\n\n\n## Appendix A\n\n\n\n```\n\/**\n * Purpose: List all records with KMF-encrypted password2 fields\n * Read-only: YES\n * Scope: global\n *\/\nvar count = 0;\nvar password2Tables = getPassword2Tables();\nvar records = [];\n\/\/gs.info(\"DEBUG - password2Tables: \" + password2Tables);\n\nfor (var i = 0; i < password2Tables.length; i++) {\n    var tblCol = password2Tables[i];\n    var idxDot1 = tblCol.indexOf(\".\");\n    var idxDot2 = tblCol.lastIndexOf(\".\");\n    var tblName = tblCol.substring(0, idxDot1);\n    var colName = tblCol.substring(idxDot1 + 1, idxDot2);\n    var sys_id = tblCol.substring(idxDot2 + 1);\n    var varOrSysId = GlideStringUtil.isEligibleSysID(sys_id) ? sys_id : null;\n    gs.info(\"Examining records in \" + tblName + \" table (\" + colName + \" field) ...\");\n    validPassword2Fields(tblName, colName, varOrSysId);\n}\n\n\/*\n *\n * Returns all tables and columns that have password or password2 fields in the\n * following format tableName.fieldName.sysId or sys_properties.value.sysId\n *\/\nfunction getPassword2Tables() {\n    var res = [];\n    var tables = new GlideRecord('sys_dictionary');\n\n    tables.addQuery('internal_type', 'password2');\n    tables.orderBy('name');\n    tables.query();\n    while (tables.next()) {\n        var tblName = tables.getValue('name');\n        var colName = tables.getValue('element');\n        var sysId = tables.getValue('sys_id');\n        if (tblName.startsWith('var__m_')) {\n            \/\/ special case: it's pointing to a variable in sys_variable_value\n            tblName = 'sys_variable_value';\n            colName = 'value';\n        }\n        res.push(tblName + \".\" + colName + \".\" + sysId);\n    }\n\n    var props = new GlideRecord('sys_properties');\n    props.addQuery('type', 'IN', 'password' + \",\" + 'password2');\n    props.orderBy('name');\n    props.query();\n    while (props.next()) {\n        var sysId = props.getValue('sys_id');\n        res.push('sys_properties' + \".\" + 'value' + \".\" + sysId);\n    }\n\n    return res;\n}\n\nfunction validPassword2Fields(tableName, fieldName, varOrSysId) {\n    var gr = new GlideRecord(tableName);\n    if (!gr.isValid())\n        return;\n\n    gr.setWorkflow(false);\n    \/\/ gr.setRunEngines(false);\n    \/\/ special case: sys_variable_value table\n    if (tblName.equalsIgnoreCase('sys_variable_value'))\n        gr.addQuery('variable', varOrSysId);\n    \/\/ special case: sys_properties\n    if (tblName.equalsIgnoreCase('sys_properties'))\n        gr.addQuery('sys_id', varOrSysId);\n\n    gr.query();\n\n    if (gr.getRowCount() == 0)\n        gs.info(\"\t- no records in table\");\n    var sys_id;\n    var fieldVal;\n    var output;\n    while (gr.next()) {\n        fieldVal = gr.getValue(fieldName);\n        if (fieldVal != null && fieldVal.startsWith(\"???\")) {\n            sys_id = gr.getUniqueValue();\n            count = count + 1;\n            output = \"tableName: \" + tableName + \", sys_id: \" + sys_id + \", fieldName: \" + fieldName + \": \" + fieldVal;\n            records.push(\"[\" + count + \"] \" + output);\n            gs.info(\"\t- [\" + count + \"] !Found KMF Password2 Encrypted Value: \" + output);\n        } else {\n            gs.info(\"\t- data not KMF Password2 encrypted\");\n        }\n    }\n    gs.info(\"\t- done with this data set\n\")\n}\n\ngs.info(\"Summary:\");\ngs.info(\"\tTotal Number of Records with KMF Password2 Encrypted Data: \" + count);\nfor (var i = 0; i < records.length; i++) {\n    gs.info(\"\t - \" + records[i]);\n}\n\n```\n\n\u00a0\n\n\n## Appendix B\n\n\n\n```\n\/\/ Run this script on the scope the table belongs to. \n\/\/ Note this query will need to be adjusted if we are printing more than one \n\/\/ Password2 field associated with the table.\n\/\/ You can gather the appropriate table name and field element name from the\n\/\/ output from Appendix A script.\n\/\/ Note, if you see an acces denied error \u0096 you\u0092ll likely have to adjust the\n\/\/ policy that is blocking the decryption.\n\nvar g = new GlideRecord('<table name>');\n\/\/ If you are targeting a specific record, you can use g.get instead g.query.\n\/\/ g.get(<'sys_id_of_record'>);\ng.query();\n\nwhile(g.next()) {\n  var password2field_1 = g.getElement('<password2 element name>').getDecryptedValue(); \n  if (password2field_1.length != 0){\n      gs.info('sys_id: ' + g.getUniqueValue());\n      gs.info('password2field_1 value: ' + password2field_1);\n  }\n}\n\n```\"\n\n",
        "QUERY": " 'kmf on-prem to cloud migration'",
        "GROUND_TRUTH": "The migration of KMF encrypted data from on-premise to cloud involves two main steps: analyzing the KMF Password2 Splash Zone and re-entering Password2 data.\n\nIn the first step, you need to log into your on-premise instance with your admin account and navigate to System Definition > Script - Background Script. Here, you execute a provided script in global scope to find all KMF Password2 encrypted data. The output of the script will determine your next steps. If there are only a few records, it's more efficient to manually re-input the Password2 data on the cloud instance. If there are many records, you should create a case and ask the support engineer to create a case task to the KMF Team for further options.\n\nIn the second step, you proceed with your on-premise to cloud migration. Once the migration is complete, you need to verify that the instance is KMF healthy and operational on the cloud instance. If it's not, you should open a case with support and recommend that a case task for the KMF Team is opened. You can check KMF Health by navigating to Key Management > Health (Diagnostics). You then manually re-input the Password2 data on the cloud instance, which will encrypt the data with cloud-based cryptographic keys. If you encounter any issues, you should open a case.\n\nThe document also provides two appendices with scripts that can be used to list all records with KMF-encrypted password2 fields (Appendix A) and to decrypt the KMF Password2 encrypted data on your on-premise instance (Appendix B)."
    },
    {
        "id": 38,
        "CONTEXT": "\"## \n\n## Description\n\nIssue in Visual Task Board for release records\n\n## Steps to Reproduce\n\n * In the VTB when clicking on the info icon the description field is getting updated\n* Hop to nsd1 (Sandiego Version and Hop to empsoujanya(Rome Patch 7)\n* Issue reproducible both in the versions\n* Go to the incident table and create a record with a long description\n* Now open VTB and create a freeform board and data-driven board\n* Open the incident record created in the native UI through the filter\n* Click on the description field and click on the Info icon we will see a new entry has been created in the work notes\n* Note this only happens for the description field\n## Workaround\n\n<p>This problem has been fixed. If you are able to upgrade, review the <strong>Fixed In<\/strong> or <strong>Intended Fix Version<\/strong> fields to determine whether any versions have a planned or permanent fix.<\/p>\n\n  \n**Related Problem: PRB1576439**\"\n\n",
        "QUERY": " 'Journal entries duplication issue on the record'",
        "GROUND_TRUTH": "no answer found"
    },
    {
        "id": 39,
        "CONTEXT": "\"## \n\nSchedule a horizontal Discovery\n\n# Schedule a horizontal Discovery\n\n\nA Discovery schedule determines what\n horizontal Discovery searches for, when it runs, and which MID Servers are used. Create a Discovery\n schedule for your local environment or a schedule for discovering the resources in your\n cloud service account.\n\n\n\nEnsure that your Discovery schedule conforms to security best\n practices, such as limiting the range of discovery targets and using the most secure\n credentials.\n\n\nMake sure to [test your\n credentials](..\/product\/credentials\/task\/t_CreateCredential.dita\/t_CreateCredential.html) before you run a schedule. Bad\n credentials are a leading cause of failed discoveries.\n\n\nRoles required: admin, discovery\\_admin\n\n\n\n\nYou can use a Discovery schedule to launch horizontal Discovery,\n which uses probes, sensors, and pattern operations to scan your network for CIs. Use\n this procedure to create a schedule manually from the Discovery\n schedule form.\nService Mapping also provides a Discovery schedule for top-down Discovery. See [Schedule a top-down discovery by Service Mapping](..\/..\/service-mapping\/task\/t_CreateDiscoSchedForCITypes.html \"After Service Mapping discovers configuration items (CIs) belonging to your application service for the first time, it then rediscovers CIs to find changes and updates. Create or modify discovery schedules to control how often Service Mapping rediscovers services or CIs. For example, you may create custom discovery schedules to avoid redundant stress on the infrastructure.\") for more\n information.\n\n\nUse the Discovery schedule module in the\n Discovery application to:* Configure a schedule to discover resources in your cloud service account.\n* Configure a schedule to discover certificates from URL scans.\n* Configure device identification by IP address or other identifiers.\n* Determine if credentials are used in device probes.\n* Name the MID Server to use for a particular type of Discovery.\n* Create or disable a schedule that controls when the Discovery runs in\n your network.\n* Configure the use of multiple Shazzam probes for load balancing.\n* Configure the use of multiple MID Servers for load\n balancing.\n* Run a Discovery schedule manually.\n* Run Discovery on a single IP address.\n\n\n\nNote: To view the run-results of your schedules for both IP-based and Cloud Discovery, use the summaries on the [Discovery Home page](..\/concept\/discovery-home-page.html \"The Discovery Home page provides a summary of discoveries that were triggered by cloud and non-cloud schedules. You can view any errors that occurred during a Discovery and find remediation suggestions. Use the Home page view to examine the cloud resources discovered for the service accounts you selected in a cloud schedule.\"). The Home page publishes the\n details of any errors that might have occurred and displays possible actions to\n take to remediate problems.\n\n\n1. Navigate to All > Discovery > Discovery schedules to create a new record.\n2. Select the type of schedule to open:\n\n\n\t* New: Creates a new horizontal schedule for discovering\n\t components in your network.\n\t* Quick Discovery: Runs an horizontal Discovery on\n\t a single IP address without requiring a schedule.\n\t* Create a Cloud Discovery schedule: Creates a schedule, using\n\t the Discovery Manager wizard, for discovering resources in a cloud service account.\n3. Complete the Discovery schedule\n form, using the fields in the table.\n4. Right-click in the header of the record\n and select Save from the context\n menu.\n5. To create a range of IP addresses to discover, click\n Quick Ranges under Related\n Links.\n\nNote: To improve security, limit the range of discovery targets to exclude\n unnecessary networks and devices.\nFigure 1. Discovery schedule\n![Discovery schedule](..\/image\/DiscoverySchedule.png)\n\n\n\nTable 1. Discovery schedule Form\n| Field | Description |\n| --- | --- |\n| Name | Enter a unique, descriptive name for your  schedule. |\n| Discover | Select one of the following scan types: \t* Configuration items: Uses \t Discovery identifiers to match devices with CIs in \t the CMDB and \t update the CMDB appropriately. Perform a simple discovery by \t selecting a specific MID Server to \t scan for all protocols (SSH, WMI, and SNMP). Or, \t perform advanced discoveries with discovery \t behaviors. When you select a behavior, the \t MID Server field is not \t available. \t* IP addresses: Scans \t devices without the use of credentials. These \t scans discover all the active IP addresses in the \t specified range and create device history records, \t but do not update the CMDB. IP address \t scans also show multiple IP addresses that are \t running on a single device. Identify devices by \t class and by type, such as Windows \t computers and Cisco network gear. The Max \t range size Shazzam probe property \t determines the maximum number of IP addresses \t Shazzam scans. See [Configure Shazzam probe](t_ConfigureTheShazzamProbe.html \"When you run Discovery, the Shazzam probe finds your active network devices by scanning specified ports on specified IP address ranges. If the list of IP ranges being scanned is large, you can configure the Shazzam payload for JSON encoding to reduce its size.\") for details. \t* Networks: Discovers IP \t networks (routers and switches). Results from this \t search are used to populate the IP Network \t [cmdb\\_ci\\_ip\\_network] table in Discovery > IP Networks with a list of IP addresses and \t network masks. Network scans update routers and \t layer 3 switches in the CMDB. \t* Service: Discovers \t services for the Service Mapping \t application. See [Schedule a top-down discovery by Service Mapping](..\/..\/service-mapping\/task\/t_CreateDiscoSchedForCITypes.html \"After Service Mapping discovers configuration items (CIs) belonging to your application service for the first time, it then rediscovers CIs to find changes and updates. Create or modify discovery schedules to control how often Service Mapping rediscovers services or CIs. For example, you may create custom discovery schedules to avoid redundant stress on the infrastructure.\") for instructions. \t* Serverless: Finds CIs \t without needing to run Discovery on \t a host, or CIs on a proxy host that is already in \t the CMDB. See \t [Serverless Discovery](..\/concept\/serverless-discovery.html \"Discovery can find applications on host machines without the need to discover the host first. This type of Discovery is referred to as serverless Discovery.\") for more information. \t* Cloud application: \t Discovers only the cloud resources for the \t patterns that you specify. See [Exploring Cloud Discovery](..\/concept\/cloud-discovery-wizard.html \"Cloud Discovery enables IT departments of companies to collect detailed information about their cloud-based infrastructure. Cloud Discovery finds resources in major cloud service providers.\") \t for instructions. \t* Cloud resources: \t Discovers resources for one of the supported [cloud providers](..\/concept\/cloud-discovery-wizard.html \"Cloud Discovery enables IT departments of companies to collect detailed information about their cloud-based infrastructure. Cloud Discovery finds resources in major cloud service providers.\"). This option only appears \t when you [run \t Discovery](..\/concept\/discovery-manager.html \"Create schedules for discovering cloud resources based on the cloud discovery method you choose: service accounts or IP ranges.\") on a cloud service account. You \t cannot select it from a new Discovery schedule. \t* Certificates. Discovers \t certificates based on URLs. Selecting this option \t adds the Certificate Discovery Type field: URL \t Certificate Discovery. See [Run Certificate Discovery via individual URL scans](..\/concept\/run-cert-discovery.html#run-cert-inventory-mgmt-urls \"To discover certificates from URL scans, you need to manually add individual URLs and then set up a new certificate Discovery schedule.\") for \t more information. |\n| MID Server selection method | Select the method that Discovery uses  to select a MID Server: \t* Auto-Select MID Server: \t Allow Discovery to select the MID Server automatically based on the \t Discovery IP Ranges you configure. \t To find a matching MID Server, you \t must configure MID Servers to \t use: \t\t+ The Discovery application, or \t\t ALL applications. This setting authorizes the MID Server access from Discovery. \t\t+ The IP Range that includes the ranges you \t\t configure on the Discovery \t\t schedule. \t* Specific MID Cluster: Use \t a preconfigured cluster of MID Servers. Select the cluster. \t You are not required to specify one member of the \t cluster. The MID Server cannot be \t part of multiple clusters, such as one that \t supports load balancing and one that supports \t failover. You can add any cluster regardless of \t the application that the MID Servers are assigned to. When \t you select the cluster, the \t Discovery application is \t automatically added when it does not exist for the \t MID Servers in the \t cluster. \t* Specific MID Server: Use \t only one MID Server. If that MID Server is part of a cluster, only \t that MID Server is \t used. \t The cluster is not used. You can \t add any MID Server regardless of \t the application it is assigned to. The \t Discovery application is \t automatically added when it is not already \t assigned for the MID Server you \t select. You can assign a specific MID Server for all types of Discover \t scans except Service. \t* Use Behavior: [Use a behavior](create-disco-behavior.html \"Create a Discovery behavior to determine which probes Shazzam launches and which MID Server is used.\") when a single schedule \t requires the use of multiple MID Servers to perform any of the \t following activities: \t\t+ Scans requiring multiple Windows \t\t credentials. \t\t+ A schedule that must execute two or more \t\t particular protocols (SNMP, SSH, or WMI) using \t\t more than one MID Server. \t\t+ Load balancing for large discoveries where a \t\t single MID Server would be \t\t inadequate. \t\t+ Scanning multiple domains.Note: The Discovery schedule enforces  domain separation. The MID Servers that are available  for selection are limited to the same domain of  the user who is configuring the schedule.  See [MID Server selection sequence for Discovery schedules](t_CreateADiscoverySchedule.html#c-MIDServerSelectionSequence \"The Discovery application follows this sequence to find a MID Server.\") for additional information. |\n| MID Server | Select the MID Server to use for this  schedule. This field is available if MID Server  selection method is set to Specific  MID Server, or if you discover IP  addresses, networks, or web services.To verify that  the MID Server you selected is up and  validated, look at the [MID Server dashboard](..\/product\/mid-server\/concept\/c_MIDServerDashboard.dita\/c_MIDServerDashboard.html). |\n| MID Server Cluster | Select the MID Server cluster to use  for this schedule. This field is available if  MID Server selection method is set  to Specific MID Cluster. |\n| Behavior | Select a behavior configured for the MID Servers in your network.  This field is available only if MID  Server selection method is set to  Use Behavior. |\n| Active | Select the check box to enable this schedule. If you  clear the check box, the schedule is disabled, but you  can still run a Discovery manually from  this form, using the configured values. |\n| Location | Choose a location to assign to the CIs that the  schedule discovers. If this field is blank, then no  location is assigned. |\n| Max run time | Set a time limit for running this schedule. When the  configured time elapses, the remaining tasks for the Discovery are canceled, even if the scan  is not complete. Use this field to limit system load to  a desirable time window. If no value is entered in this  field, this schedule runs until complete. |\n| Run and related fields | Determines the run schedule of the Discovery. Configure the frequency in the  Run field and the other  fields that appear to specify an exact time. Note: The  run time always uses the system time zone. If you  add the optional Run as tz  field, it has no effect on the actual  runtime. |\n| Log state changes | Select this check box to create a log entry every  time the state changes during a Discovery, such as a device going from Active  to Classifying. View the Discovery states from the Discovery Devices related list on the Discovery Status form. The  Completed activity and  Current activity fields display the  states. |\n| Shazzam batch size | Enter the number of IP addresses that each Shazzam  probe can scan. Dividing the IP addresses into batches  improves performance by allowing classification for each  batch to begin after the batch completes. rather than  after all IP addresses have been scanned. The probes run  sequentially. For example, the value is set to 1000 and  a discovery scans 10,000 IP addresses using a single MID Server. It creates 10 Shazzam probes  with each probe scanning 1000 IP addresses. By default,  the batch size is 1000. A UI policy enforces a minimum  batch size of 256 because batch sizes below 256 IP  addresses do not benefit from clustering. The policy  converts any value below 256 to a value of zero.The  value for this field cannot exceed the value defined  in the maximum range size property for the Shazzam  probe. |\n| Shazzam cluster support | Select the check box to distribute Shazzam processing  among multiple MID Servers in a  cluster and improve performance. This setting works with  the Shazzam batch size. For example, a schedule is  created to scan 100,000 IP addresses, with 10 MID Servers assigned to do the  work. Each MID Server is assigned to scan  10,000 IP addresses. If the Shazzam batch size is set to  5,000 IP addresses per probe, the schedule runs two  Shazzam probes per MID Server (10,000 IP  addresses\/5,000 per batch). These probes are run in  sequence and not concurrently. |\n| Use SNMP Version | Use this field to designate the SNMP version to use  for this discovery. SNMP v3 is  the most secure option. Valid options are  v1\/v2c,  v3, or  All. |\n| URL Certificate Batch Size | Define the number of URLs to discover per batch  during Discovery. Leave the batch size as it is unless  recommended to change. |\n| Quick ranges | Define IP addresses and address ranges to scan by  entering IP addresses in multiple formats (network,  range, or list) in a single, comma-delimited string. For  more information, see [Create a Quick IP range for a Discovery schedule](..\/reference\/discovery-ip-address-configuration.html#t_CreateAQuickRange \"Quick ranges allow administrators to define IP addresses to scan in a single comma-delimited string without creating separate records.\"). |\n| Discover now | Use this link to immediately start this  Discovery. |\n| Related  lists | |\n| IP Ranges | This related list defines the ranges of IP addresses  to scan with this schedule. If you are using a simple CI  scan (no behaviors), use this related list to define the  IP addresses to discover. Note: To improve security,  limit the range of discovery targets to exclude  unnecessary networks and devices. |\n| Discovery Range Sets | This related list defines each range set in a  schedule to scan by one or more Shazzam probes. |\n| Discovery Status | This related list displays the results of current and  past schedule runs. |\n| Certificate URLs | This related list displays the URLs that are  discovered using this schedule. You can add or delete  URLs from this list. |\n6. Define run options for this discovery as covered in [Run options for discovery schedules](..\/..\/it-operations-management\/reference\/discovery-schedule-run-options.html \"Both horizontal and top-down discovery use these run options.\").\n\n\n\n\n## Run a Quick Discovery\n\n\nQuick Discovery, or DiscoverNow, allows an administrator to run a CI Configuration\n discovery on a single IP address without requiring a schedule.\n\n\n\nThe platform automatically selects the correct MID Server to use for the discovery if\n one is associated with the IP address selected. If no MID Server is configured for\n the network in which that address appears, you can select a MID Server. Use this\n feature to discover new devices in the network as soon as they are connected to the\n network, rather than waiting for a regularly scheduled discovery.\n\n\nTo configure the system to automatically determine which MID Server to use, set up\n the IP range capabilities for each MID Server in your system.\n\n\nYou can run DiscoverNow from a Discovery schedule form or\n from a script.\n\n\n\n1. Open Quick Discovery from one of these locations:\n\n\n\t* Navigate to Discovery > Discovery Schedules and click Quick Discovery in the\n\t header bar.\n\t* Navigate to Discovery > Home and click Discovery Quick Start\n\t under the Schedules tile.\nA dialog box appears asking for an IP address and the name of the MID\n Server to use. Only Up and\n Validated MID Servers are available.\n2. Enter the target IP address for a discovery in the Target\n IP field.\n\nNote: DiscoverNow does not currently support IP network\n discovery. Make sure\n that\n you enter a single IP address only and not an entire\n network, such as 10.105.37.0\/24.\n\nWhen a MID Server is assigned to the subnet containing the target IP address\n and currently in an operational status of Up, the\n name appears automatically in the MID Server field.\n If multiple MID servers are found, the system selects one for you. The value\n in the MID Server field can be overwritten if you\n want to select a different MID Server.Important: If the selected\n MID Server is part of a load balanced cluster and becomes unavailable\n for any reason, the instance does not assign another MID\n Server from that cluster to the quick Discovery. You must select another\n MID Server from the list of appropriate MID Servers.\n3. If no MID Server is defined for that network, select one from the list of\n available MID Servers.\n\nFigure 2. Quick Discovery Dialog\n![Quick discovery](..\/image\/QuickDiscoveryDialog.png)\n4. Click OK to run discovery.\n\nThe status record for that discovery appears. The\n Schedule column is empty because no schedule is\n associated with this discovery.Figure 3. Quick Discovery Status List\n![Quick discovery status list](..\/image\/QuickDiscoveryStatusList.png)\n\n\n\n\n\n## Run DiscoverNow from a script\n\n\nYou can run DiscoverNow from a script, such as a background job, a business rule, or\n web services.\n\n\nRole required: admin\n1. Create the following script:\n\n\n```\nvar d = new Discovery();\nvar statusID = d.discoveryFromIP(TARGET_IP, TARGET_MIDSERVER);\n```\n\nThe discoveryFromIP method takes two arguments:\n IP and MID Server. The\n IP argument is mandatory, but the MID\n Server argument is optional.\n2. To choose the MID Server, supply either the sys\\_id or name\n of the MID Server as the argument.\nIf you do not name a MID Server, the system attempts to find a valid one\n automatically. A valid MID Server has a status of Up and\n can discover the given IP address. If the system finds a valid MID Server and\n runs a Discovery, the\n discoveryFromIP method returns the\n sys\\_id of the Discovery status record.\n If no MID Server can discover this IP address, the method returns the value\n undefined.If you manually specify the\n TARGET\\_MIDSERVER, the system validates the given value and ensures that the\n MID Server table contains the specified MID Server record. If the validation\n passes, the discoveryFromIP method returns the sys\\_id of the discovery\n status record. If the validation fails, the method return the value\n undefined.\n\n\n\n\n\n## Validate discovery results\n\n\nValidate the results of your discovery by accessing the ECC queue, analyzing the XML payload, and checking the Discovery log.\n\n\nRole required: discovery\\_admin\nInitial discoveries often reveal unexpected results, such as previously unknown devices and processes or failed authentication. Results should also accurately identify known devices and update the CMDB appropriately. Become\n familiar with the network that is being discovered and the types of data returned for the different types of discoveries. Use the Discovery Log and the ECC Queue to monitor the Discovery process as data is returned from probes or pattern operations.\n1. To view the actual payload of a probe, click the XML icon in a record in the ECC Queue.\n\nFigure 4. ECC Queue\n![ECC Queue](..\/image\/DiscoveryECCQueueView.png)\n2. To view the actual payload of a probe, click the XML icon in a record in the ECC Queue.\n3. Use the Discovery Log form for a quick look at how the probes are doing. To display the Discovery Log, navigate to Discovery > Discovery Log.\n\nFigure 5. Discovery Log\n![The Discovery log](..\/image\/DiscoveryLog.png)\n\n\nThe Discovery Log provides this information:\n\n\n| Column | Information |\n| --- | --- |\n| Created | Displays the timestamp for the probe launched. Click this link to view the record for the probe launched in this list. |\n| Level | Displays the type of data returned by this probe. The possible levels are: \t* Debug \t* Error \t* Information \t* Warning |\n| Message | Message describing the action taken on the information returned by the probe. |\n| ECC queue input | Displays the ECC queue name associated with the log message. |\n| CI | The CI discovered. Click this link to display the record from the CMDB for this CI. |\n| Source | Displays the probe name that generated the log message. |\n| Device | Displays the IP address explored by the probe. Click this link to examine all the log entries for the action taken on this IP address by this Discovery. |\n\n\n\nNote: If you cancel an active discovery, note the following information:\n\t* Existing sensor jobs that have started processing are immediately terminated.\n\t* The existing sensor jobs that are in a Ready state, but have not started processing, are deleted from the system.\n4. View the [Discovery Home page](..\/concept\/discovery-home-page.html \"The Discovery Home page provides a summary of discoveries that were triggered by cloud and non-cloud schedules. You can view any errors that occurred during a Discovery and find remediation suggestions. Use the Home page view to examine the cloud resources discovered for the service accounts you selected in a cloud schedule.\") for details about all schedules, cloud resources (virtual machines), discovered devices, and related errors that might have occurred.\n\n[Error details](..\/concept\/discovery-home-page.html#view-ci-discovery-schedule-errors \"From the ServiceNow Home page, you can view the Discovery errors that occurred during a Discovery and get suggestions for resolving these errors. You can view the errors for all schedules or for a single schedule.\") include possible remediation steps.\n\n\n\n\n\n## MID Server selection sequence for Discovery schedules\n\n\nThe Discovery application follows\n this sequence to find a MID Server.\n\n\n### MID Server auto-selection\n\n\nDiscovery follows this sequence when you select Auto-Select MID Server\n for the MID Server selection method on the Discovery Schedule\n form.1. Discovery looks for a MID Server\n that also has an appropriate IP range configured.\n2. If no MID Servers meet these criteria, it looks for a MID Server that has the\n ALL application that also has an appropriate IP range\n configured.\n3. If more than one MID Servers meet the criteria, Discovery chooses the first MID\n Server with the status of Up. If more than one MID Servers are\n up, it randomly picks one.\n4. If none are up, it uses the default MID Server specified for the Discovery application, assuming it\n is up.\n5. If no default MID Server is specified, it uses the default MID Server specified for\n the ALL application, assuming it is up.\n6. If no default MID Server is specified, Discovery cycles through the\n previous steps and looks for MID Servers with the status of\n Paused or Upgrading. Note: When a MID\n Server is paused or upgrading, it does not actually process commands until it returns\n to the status of Up.\n\n\n\n\n### MID Server clusters\n\n\nThese steps are followed when you select Specific MID Cluster for the\n MID Server selection method on the Discovery\n form, and the cluster is a load balancing cluster:1. Discovery uses the first MID Server in the cluster that it finds\n with the status of Up.\n2. If more than one MID Servers are up, it randomly picks one. If it cannot find any MID\n Servers, it looks for MID Servers in the cluster with the status of\n Paused or Upgrading.\n\n\n\nThese steps are followed when the cluster is a failover cluster:1. Discovery uses the MID Server with the lowest\n Order value that also has the status of\n Up.\n2. If no MID Servers are found, it looks for MID Servers in the cluster with the status of\n Paused or Upgrading, choosing the one\n with the lowest Order value.\n\n\nNote: Discovery ignores the default MID Server for it and\n ALL applications when selecting a MID Server from the\n cluster.\n\n\n### Port scan (Shazzam) phase\n\n\nDuring the port scan phase, Discovery collects all the target IP\n addresses. It splits them equally between MID Servers matching the criteria (MID Servers are\n qualified to do the port scan). The Shazzam batch size, which you configured on the\n Discovery schedule, determines the number of IP addresses that each\n Shazzam probe can scan. This phase helps determine how much work each MID Server does during the\n port scan phase.\n\n\nFor example, you have 16,000 IP addresses to scan among three qualified MID Servers, and you\n use the default Shazzam batch size of 5000. Two of the MID Servers handle 5000 IP address scans\n (one Shazzam probe each). The other MID Server handles 6000 IP address scans by launching two\n Shazzam probes.\n\n\n\n\n\n**Related topics**  \n\n* [MID Server pause](product\/mid-server\/task\/t_PauseTheMIDServer.html#c_MIDServerPause)\n* [MID Server cluster configuration](product\/mid-server\/task\/t_ConfigureAMIDServerCluster.html#mid-server-clusters)\"\n\n",
        "QUERY": " 'Issue in detecting additional IP address during horizontal discovery'",
        "GROUND_TRUTH": "If you're having an issue detecting additional IP addresses during horizontal discovery, there could be several reasons for this. \n\nFirstly, ensure that the IP addresses you want to discover are included in your Discovery schedule. You can do this by navigating to All > Discovery > Discovery schedules to create a new record. In the 'Quick ranges' field, define the IP addresses and address ranges to scan. \n\nSecondly, check the MID Server selection. The Discovery application follows a sequence to find a MID Server. If the MID Server associated with the IP address range of the additional IP addresses is not up or does not have the correct capabilities configured, it may not be selected for the Discovery, causing the IP addresses to be missed.\n\nLastly, during the port scan phase, Discovery collects all the target IP addresses and splits them equally between MID Servers matching the criteria. If you have a large number of IP addresses to scan, ensure that you have enough MID Servers and that the Shazzam batch size is configured appropriately on the Discovery schedule.\n\nIf you've checked all these settings and are still having issues, you may need to validate your discovery results by accessing the ECC queue, analyzing the XML payload, and checking the Discovery log. This can help you identify any errors or issues that occurred during the Discovery process."
    },
    {
        "id": 40,
        "CONTEXT": "\"## \n\n**Note: The following steps to sign Powershell script is not a supported feature. These are the recommended steps to be followed by the customers. We don't have the ability to support customers apart from the following steps.**\n\n\n**Issue** \u00a0\n\n\n**1. Customer is NOT experiencing a failure but they want PowerShell scripts signed.** \n\n\n* Some customers like\u00a0 Defense Information Security Agency (DISA) Security Technical Implementation Guidelines (STIG) have policies that have this as a requirement\n* This customer wants IntegrationHub action\u0092s script signed, specifically Active Directory Spoke. They want us to **force and require signing**\u00a0of the scripts.\n\n\n**2. Customer experiences a failure because their Windows host PowerShell ExecutionPolicy is AllSigned or RemoteSigned, and ServiceNow PowerShell scripts are not signed**\n\n\n* This customer had errors when testing Orch activities and credentials. When the customer changed EP to \u0093Unrestricted\u0094 it worked.\n* Usually, scripts execute regardless of ExecutionPolicy on local MID but may fail per customer configuration.\n\n\n**Signing Powershell Scripts**\n\n\nIf a customer must sign their PowerShell scripts, use the following guidance\u00a0\n\n\n1. Powershell scripts can be found in table ecc\\_agent\\_script\\_file (*query: Parent = Powershell*)\n2. First, sign **PSScript.ps1**. Both IntegrationHub and Orchestration run scripts on the MID using this as a wrapper script\n3. We recommend signing other infrastructure Powershell scripts (*query:* *Parent = Powershell and Directory = false*)\n4. If remote targeting is used then you will need to sign **ExecuteRemote.ps1** as well. This script handles executing script on the target machine\n5. Now go ahead and sign all the other Powershell scripts  \n\n\t* For example - if you want to sign AD spoke Powershell scripts, you can find these scripts with query ***Parent = AdSpoke*** on table ecc\\_agent\\_script\\_file\n\n\n**Note: As of now, inline scripts cannot be signed. During runtime, we store the inline script in the temp file on the MID server and delete it after the execution of the Powershell step. If the customer needs to sign these, then they must be converted to script files.**\n\n\n\u00a0\n\n\n**Saving the signed scripts**\n\n\n1. Save the signed scripts as an attachment.\u00a0  \n\n\t* Select Use Attachment field and attach the script file to the\u00a0ecc\\_agent\\_script\\_file record\n\t* Note: The attachment name should be the same as the ecc\\_agent\\_script\\_file name field\n2. We are saving the script as an attachment to avoid OOB formatting when the script is saved inline\n3. A disadvantage of this approach is that we lose versioning of the ecc\\_agent\\_script\\_file records\n\n\n**Verify scripts are valid**\n\n\n1. Wait for the scripts to get downloaded to MID. It takes around 10 seconds\n2. Verify the content of the signed scripts on MID. The scripts can be found in ..***mid\\_folder\\_path\/agent\/scripts\/Powershell*** or look for log messages similar to  \n\u00a0 \u00a0 \u00a0 \u00a0*02\/09\/21 07:16:02 (822) FileSync:ecc\\_agent\\_script\\_file Already synchronized C:\\<mid\\_folder\\_path>agent\\scripts\\PowerShell\\PSScript.ps1*\n3. To validate the scripts on PowerShell ISE using the command: ***Get-AuthenticodeSignature \"<file path>\"***\n4. Run Powershell test to confirm if the signature works\n\n\n**Debugging issues with signed scripts**\n\n\n1. Don\u0092t update the Powershell script directly in Windows. We regularly sync the ecc\\_agent\\_script\\_file scripts with MID. This sync job will override the local scripts with the instance version\n2. If the scripts fail to execute due to auth failures verify the Powershell script still as signature and verify the signature is valid by using the command:***Get-AuthenticodeSignature\u00a0\"<file path>\"***\n3. If Powershell script contains the signature and fails to validate using the above command you will have to resign the script\"\n\n",
        "QUERY": " 'How to verify whether powershell scripts are valid?'",
        "GROUND_TRUTH": "To verify whether PowerShell scripts are valid, follow these steps:\n\n1. Wait for the scripts to get downloaded to MID. This process usually takes around 10 seconds.\n2. Verify the content of the signed scripts on MID. The scripts can be found in the directory: mid_folder_path\/agent\/scripts\/Powershell. Alternatively, you can look for log messages similar to \"02\/09\/21 07:16:02 (822) FileSync:ecc_agent_script_file Already synchronized C:\\<mid_folder_path>agent\\scripts\\PowerShell\\PSScript.ps1\".\n3. To validate the scripts on PowerShell ISE, use the command: Get-AuthenticodeSignature \"<file path>\".\n4. Run a PowerShell test to confirm if the signature works.\n\nIf you encounter issues with the signed scripts, do not update the PowerShell script directly in Windows as the sync job will override the local scripts with the instance version. If the scripts fail to execute due to authentication failures, verify the PowerShell script still has a signature and verify the signature is valid by using the command: Get-AuthenticodeSignature \"<file path>\". If the PowerShell script contains the signature and fails to validate using the above command, you will have to resign the script."
    },
    {
        "id": 41,
        "CONTEXT": "\"## \n\n**Installation** \n\n\n1. The following steps should be replicated by a **ServiceNow Admin** in any instance ServiceNow BCM is installed\n2. A ServiceNow BCM Technician or Partner will supply you with a folder which contains 5 Update Sets\n3. Navigate to System Definition: Plugins\n4. Search for Webkit HTML to PDF and make sure it is installed\n\n\n![](\/sys_attachment.do?sys_id=ece9b4cddb40f8d066e0a345ca961904)\n\n\n5. Navigate to Retrieved Update Sets and select the related link Import Update Set from XML\n6. Import the 5 Update sets\n7. Preview and commit the 5 update sets in the following order  \n\n\t1. BCM PDF Global (Note: this update set is in global)\n\t2. BCM PDF\n\t3. BCM Update PDF UI\n\t4. BCM Update PDF UI 2\n\t5. BCM PDF Image Update\n\n\n  \n\n\n\n**Testing**\n\n\n1. Navigate to All Plans in the Business Continuity application\n2. Open a plan record and select View Plan in the header of the record\n3. In the top right of the Business Continuity UI select Generate PDF\n\n\n![](\/sys_attachment.do?sys_id=60e9b4cddb40f8d066e0a345ca961902)\n\n\n4. A new tab will open, and a progress bar will appear\n5. When the progress bar is completed a pdf will be downloaded which can be opened in the bottom left of the screen\"\n\n",
        "QUERY": " 'how to test the generated pdf in a ServiceNow BCM?'",
        "GROUND_TRUTH": "To test the generated PDF in a ServiceNow BCM, follow these steps:\n\n1. Navigate to 'All Plans' in the Business Continuity application.\n2. Open a plan record and select 'View Plan' in the header of the record.\n3. In the top right of the Business Continuity UI, select 'Generate PDF'.\n4. A new tab will open, and a progress bar will appear.\n5. When the progress bar is completed, a PDF will be downloaded which can be opened in the bottom left of the screen."
    },
    {
        "id": 42,
        "CONTEXT": "\"## \n\n## Description\n\nAs per the below documentation, there should be a catalog item present with name \"Submit Idea\" under self-service > service catalog for users to submit the idea request but it is not available  \n<https:\/\/docs.servicenow.com\/bundle\/tokyo-it-business-management\/page\/product\/planning-and-policy\/task\/submit-idea-from-self-service.html>\n\n## Steps to Reproduce\n\n Login to OOB Instance (orsandiego)  \nNavigate to Self-Service > Service Catalog.  \nClick Can We Help You?  \nClick Submit Idea.  \n  \nExpected: There should a Submit Idea catalog item available  \n  \nActual: There is no catalog item with name \"Submit Idea\" is available\n\n## Workaround\n\nThis issue is under review. To receive notifications when more information is available, subscribe to this Known Error article by clicking the Subscribe button at the top right of the article. If you are able to upgrade, review the Fixed In or Intended Fix Version fields to determine whether any versions have a planned or permanent fix.\n\n  \n**Related Problem: PRB1629452**\"\n\n",
        "QUERY": " 'how to submit an idea'",
        "GROUND_TRUTH": "no answer found"
    },
    {
        "id": 43,
        "CONTEXT": "\"## \n\n### Instructions\n\nGo to: \n\n\n<https:\/\/support.servicenow.com\/>\n\n\n\u00a0\n\n\nIn the navigator type: 'Grant'\n\n\nClick on 'Grant\/Revoke read-only Impersonator'\n\n\nClick on 'Grant Role'\n\n\n![](\/sys_attachment.do?sys_id=e2e22d6edb74e510770be6be13961913 \"Screen Shot 2023-02-06 at 11.27.26 AM.png\")\n\n\n\u00a0\n\n\nOn top right corner select 'Log out'\n\n\nClick 'Sign-In'\n\n\nLog in to NowSupport \n\n\nOn top right corner select 'Impersonate User'\n\n\nType name of User\n\n\nOn top right corner select 'Switch to NOW Support Portal'\n\n\n![](\/sys_attachment.do?sys_id=eae22d6edb74e510770be6be13961916 \"Screen Shot 2023-02-06 at 11.35.08 AM.png\")\n\n\n\u00a0\n\n\n**How to stop impersonation**:\n\n\nOn top right corner select 'Switch to Advance'\n\n\nOn top right corner select 'End Impersonation'\n\n\n![](\/sys_attachment.do?sys_id=eee22d6edb74e510770be6be1396191b \"Screen Shot 2023-02-06 at 11.36.41 AM.png\")\n\n\n\u00a0\n\n\nSelect 'Grant\/Revoke read-only Impersonator'\n\n\nClick on 'Revoke Role'\n\n\n![](\/sys_attachment.do?sys_id=b6e22d6edb74e510770be6be1396192f \"Screen Shot 2023-02-06 at 11.38.17 AM.png\")\n\n\n\u00a0\n\n\nOn top right corner select 'Log out'\n\n\nClick 'Sign-In'\n\n\nLog in to NowSupport\"\n\n",
        "QUERY": " 'How to impersonate a customer in NowSupport (HI)'",
        "GROUND_TRUTH": "To impersonate a customer in NowSupport (HI), follow these steps:\n\n1. Go to the ServiceNow support site at https:\/\/support.servicenow.com\/.\n2. In the navigator, type 'Grant' and select 'Grant\/Revoke read-only Impersonator'.\n3. Click on 'Grant Role'.\n4. Log out from the top right corner and then click 'Sign-In'.\n5. Log in to NowSupport.\n6. On the top right corner, select 'Impersonate User' and type the name of the User you want to impersonate.\n7. On the top right corner, select 'Switch to NOW Support Portal'.\n\nTo stop impersonation:\n\n1. On the top right corner, select 'Switch to Advance'.\n2. On the top right corner, select 'End Impersonation'.\n3. Select 'Grant\/Revoke read-only Impersonator' and click on 'Revoke Role'.\n4. Log out from the top right corner and then click 'Sign-In'.\n5. Log in to NowSupport."
    },
    {
        "id": 44,
        "CONTEXT": "\"## \n\n## Description\n\nIn instances with large volume of label\\_entry records, semaphore exhaustion may occur when multiple MID servers restart at the same time - such as post instance upgrade.  \n  \nThe relevant Business Rule is \"ECC Queue - process config file\"  \n  \nThis Business Rule is used to process the \"SystemCommand\" type \"config.file\" records and includes functionality that uses the \"TagUtil\" script include.  \n  \nIt queries looking for the \"NO Overwrite\" tag - used to ensure an ecc\\_agent\\_config record is not overwritten.  \n  \nIt then looks via sys\\_id of this tag to find any tags with that sys\\_id against ecc\\_agent\\_config records.  \n  \nIf the tag table [label\\_entry] is large, and there is no \"NO Overwrite\" tag present on the instance, a lot of concurrent large transactions are made to this table resulting in performance issues and exhausted semaphores  \n  \nThis may result in flapping MID Status, or observed performance issues.\n\n\nFrom the MID Server, this may appear in the logs as HTTP 429, such as:\n\n\n\u00a0\n\n\n\n```\n2023-10-14T16:58:26.510+1100 ERROR (MIDServer) [RemoteGlideRecord:918] getRecords failed (Method failed: (https:\/\/<instance>.service-now.com\/ecc_agent.do?SOAP&displayvalue=all&redirectSupported=true)HTTP\/1.1 429 Too Many Requests with code: 429)\n2023-10-14T16:58:26.511+1100 WARN (MIDServer) [RetryExecutor:104] MIDRemoteGlideRecord.query failed, retrying in 10 seconds\n2023-10-14T16:58:36.557+1100 WARN (MIDServer) [HTTPClient:830] Method failed: (https:\/\/<instance>.service-now.com\/ecc_agent.do?SOAP&displayvalue=all&redirectSupported=true)HTTP\/1.1 429 Too Many Requests with code: 429\n2023-10-14T16:58:36.557+1100 ERROR (MIDServer) [RemoteGlideRecord:918] getRecords failed (Method failed: (https:\/\/<instance>.service-now.com\/ecc_agent.do?SOAP&displayvalue=all&redirectSupported=true)HTTP\/1.1 429 Too Many Requests with code: 429)\n2023-10-14T16:58:36.558+1100 WARN (MIDServer) [RetryExecutor:104] MIDRemoteGlideRecord.query failed, retrying in 15 seconds\n```\n## Steps to Reproduce\n\n Have large number of records on \"label\\_entry\" table.   \nHave no entries in the \"label\\_entry\" table for tag \"NO Overwrite\"  \nRestart multiple MID Servers at the same time (such as during an instance upgrade)\n\n## Workaround\n\nYou can replace the content of the business rule \"ECC Queue - process config file\" with the following content:\u00a0\n\n\n\n```\nprocessConfigFile();\n\nfunction processConfigFile() {\n    var midName = new String(current.source).split(\"\/\")[1];\n    var midID = getMidServerID(midName);\n    if (!midID)\n    \treturn;\n    \n    var newParams = getNewParams();\n    var curParams = getCurParams(midID);\n    \n    \/\/ Collect information for tagged records. Records with 'NO Overwrite' tag, will not be modified\n    var tagUtil = new TagUtil();\n    var tagSysId = tagUtil.getTagSysId('NO Overwrite');\n    var taggedRecords = tagSysId ? tagUtil.getTaggedRecords(tagSysId, 'ecc_agent_config') : [];\n    var taggedRecordsNumber = taggedRecords.length;\n    \n    \/\/ reconcile new and current...\n    for (var curName in curParams) {\n    \tcurValue = curParams[curName];\n    \tnewValue = newParams[curName];\n    \tif (newValue == null)\n    \t\tdeleteCur(midID, curName, taggedRecords, tagSysId);\n    \telse\n    \t\tupdateCur(midID, curName, newValue, taggedRecords, tagSysId);\n    \tdelete newParams[curName];\n    }\n    \n    for (var newName in newParams) {\n    \tnewValue = newParams[newName];\n    \tinsertCur(midID, newName, newValue);\n    }\n    \/\/ If for the current config.file ecc queue record we avoid updating any record\n    \/\/ MID Server needs to sync with the instance to retrieve the existing information\n    if (taggedRecords.length < taggedRecordsNumber) {\n    \tvar priority = 'Interactive';\n    \tvar eccParams = {\n    \t\t'source': 'updateConfig',\n    \t\t'topic': 'SystemCommand'\n    \t};\n    \tnew SNC.MidEccSender().sendMessageToSpecificMid(midName, priority, eccParams, {});\n    \n        \/\/ If we processed all tagged records, it is safe to remove the tag\n    \tif (taggedRecords.length == 0 && tagSysId) {\n    \t\ttagUtil.deleteTag(tagSysId);\n    \t}\n    }\n    \n    current.state = 'processed';\n}\n\nfunction deleteCur(midID, name, taggedRecords, tagSysId) {\n    var tagUtil = new TagUtil();\n    var gr = new GlideRecord('ecc_agent_config');\n    gr.addQuery('ecc_agent', midID);\n    gr.addQuery('param_name', name);\n    gr.query();\n    gr.setWorkflow(false);\n    while (gr.next()) {\n        var recIndex = taggedRecords.indexOf(gr.sys_id + '');\n    \tif (recIndex > -1) {\n    \t\ttagUtil.deleteTagEntry(tagSysId, taggedRecords[recIndex], 'ecc_agent_config');\n    \t\ttaggedRecords.splice (recIndex, 1);\n    \n    \t} else {\n    \t\tgr.deleteRecord();\n    \t}\n    }\n}\n\nfunction updateCur(midID, name, value, taggedRecords, tagSysId) {\n \tvar tagUtil = new TagUtil();\n    var gr = new GlideRecord('ecc_agent_config');\n    gr.addQuery('ecc_agent', midID);\n    gr.addQuery('param_name', name);\n    gr.query();\n    if (!gr.next())\n    \treturn;\n        var recIndex = taggedRecords.indexOf(gr.sys_id + '');\n    \tif (recIndex > -1) {\n    \t\ttagUtil.deleteTagEntry(tagSysId, taggedRecords[recIndex], 'ecc_agent_config');\n    \t\ttaggedRecords.splice (recIndex, 1);\n    \t} else {\n    \t\tif (gr.value != value) {\n    \t\t\tgr.value = value;\n    \t\t\tgr.setWorkflow(false);\n    \t\t\tgr.update();\n    \t\t}\n    \t}\n    }\n    \nfunction insertCur(midID, name, value) {\n    var gr = new GlideRecord('ecc_agent_config');\n    gr.initialize();\n    gr.ecc_agent = midID;\n    gr.param_name = name;\n    gr.value = value;\n    gr.setWorkflow(false);\n    gr.insert();\n}\n \nfunction getNewParams() {\n \tvar result = {};\n \tvar XMLUtil = GlideXMLUtil;\n \tvar doc = XMLUtil.parse(current.payload);\n \tvar params = XMLUtil.selectNodes(doc.getDocumentElement(), \"\/\/parameter\");\n \tfor (var i = 0; i < params.getLength(); i++) {\n \t\tvar param = params.item(i);\n \t\tvar name = param.getAttribute('name');\n \t\tvar value = param.getAttribute('value');\n \t\tresult[name] = value;\n \t}\n \treturn result;\n}\n \nfunction getCurParams(midID) {\n\tvar result = {};\n\tvar gr = new GlideRecord('ecc_agent_config');\n\tgr.addQuery('ecc_agent', midID);\n\tgr.query();\n\twhile (gr.next())\n \t\tresult['' + gr.param_name] = '' + gr.value;\n \treturn result;\n}\n\nfunction getMidServerID(midName) {\n \tvar gr = new GlideRecord('ecc_agent');\n\tgr.addQuery('name', midName);\n\tgr.query();\n    return gr.next() ? gr.sys_id : null;\n}\n```\n\n\u00a0\n\n\nComparing the original to the new content, there are 2 replaced lines:\n\n\nFor the first replaced line - the original:\n\n\n\n```\n    var taggedRecords = tagUtil.getTaggedRecords(tagSysId, 'ecc_agent_config');\n\n```\n\nHas been replaced with:\n\n\n\n```\n    var taggedRecords = tagSysId ? tagUtil.getTaggedRecords(tagSysId, 'ecc_agent_config') : [];\n\n```\n\n\u00a0\n\n\n\u00a0\n\n\nAnd for the second replaced line - the original:\n\n\n\n```\n    \tif (taggedRecords.length == 0) {\n\n```\n\nHas been replaced with:\n\n\n\n```\n    \tif (taggedRecords.length == 0 && tagSysId) {\n\n```\n  \n**Related Problem: PRB1706762**\"\n\n",
        "QUERY": " 'how to handle external payload in ecc queue'",
        "GROUND_TRUTH": "no answer found"
    },
    {
        "id": 45,
        "CONTEXT": "\"## \n\n### Description\n\nRead Replicas | Troubleshooting, statistics and configuring secondary databases\n\n\n  \n  \n\n# Overview\n\n\n\n\n---\n\n\nFor a ServiceNow deployment, a potential performance bottleneck is traffic to the database. To alleviate this bottleneck and improve performance, you can set up secondary replica databases and configure database connection pools for accessing them. In this configuration, the primary database still performs all write operations, and the secondary databases can run queries that are less sensitive to replication lag. Thus, you off-load traffic from the main database, leaving it with more resources to complete time-critical operations.\n\n\n\u00a0\n\n\n# Secondary database categories and pools\n\n\n\n\n---\n\n\nTo view the configuration of secondary database categories and pools:\n\n\n1. Hop in to the instance, and go to **System Maintenance**:  \n  \n![Preview Image](\/sys_attachment.do?sys_id=95ee34e2db0ab450e515c22305961967)\n\n\n**Secondary Database Pools:** shows the lag, replicas, and current state.  \n  \n![](\/sys_attachment.do?sys_id=19ee34e2db0ab450e515c2230596196e)\n\n\n  \n  \n\n**Secondary Database Categories**: shows per category replication lag thresholds and which read replicas are configured (categories are hard set).  \n  \n![](\/sys_attachment.do?sys_id=e5ee34e2db0ab450e515c22305961986)\n\n\n\u00a0\n\n\n# Reviewing replica statistics\n\n\n\n\n---\n\n\nTo review replicas statistics:\n\n\n1. Hop in to the instance, and go to **System diagnostics > table iostats > personal list** (add **pool name** field).\n2. Sort by selects, filter out **glide,** and check if the SELECTs are incrementing against the replicas.\n\n\n![](\/sys_attachment.do?sys_id=2dee34e2db0ab450e515c22305961998)\n\n\n\n\n\u00a0\n### Release or Environment\n\nAll\n\n### Resolution\n\n\n# Troubleshooting read replicas\n\n\n\n\n---\n\n\nTo troubleshoot read replicas:\n\n\n1. Log in to the primary database server and review a MySQL show processlist to confirm if their are any active **select** queries being run. This should not be the case if read replicas are active.\n2. Confirm if the MySQL read replicas are lagging behind the master. If the seconds behind master exceeds what is defined in the category (see Secondary Database Categories above), this read replica stops receiving traffic.\n3. Review the replica query statistics per \"Reviewing replica statistics\" and ensure the counters are incrementing.\n4. Use the MySQL client on each of the replicas and perform the\u00a0 show processlist.\n\n\nIf there are very long running UPDATE, DELETE, INSERT or OPTIMIZE statements being run on the master, this could cause the read replicas to lag behind. If they lag behind longer than the counters set within **Secondary Database Categories**, this removes the replica from the pool and causes the related READ queries to be diverted to the master.\n5. If you see no queries on the read-replicas: ensure the table 'sys\\_db\\_pool' has up to date\/consistent configuration i.e the (replica\\_for field) is pointing to the correct replicas: (i.e has it been recently reseeded and configuration missed? )  \n  \nmysql>use <database\\_name>;  \nmysql> select \\* from sys\\_db\\_pool\n\n\n**Previous issues:**  \n  \nPost upgrade, an optimize table is run on ts\\_c% tables. If these tables are very large, they could cause significant lag on each of the replicas. If there is lag on the read replicas that exceeds what is set within the\u00a0**Secondary Database Categories**, traffic is diverted away from the read replicas. This then causes the load on the primary database to increase exponentially.\n\n\nOne option is to kill the optimize queries to provide relief to the customer, but you should contact Customer Support and SysAdmin before proceeding.\n\n\nRead Replica reseeds have been executed, without the sys\\_db\\_pool being updated, (5.) above.\n\n\n\u00a0\n\n\n# Configuring secondary databases\n\n\n\n\n---\n\n\nYou can configure secondary databases to perform these types of queries:\n\n\n* Reports\n* Homepages\n* Auto-complete\n* Text search\n* Lists\n* Embedded lists\n* Related lists\n* Logs\n* ODBC driver\n* CMDB relationships\n\n\n\u00a0\n\n\n## Setting up secondary databases\n\n\nTo use secondary database pools, set up secondary database servers and enable replication of the primary database. Ensure that your hardware configuration meets these requirements:\n\n\n* Database replication uses a supported replication technology, such as Tungsten Replicator, MySQL binary logging, Shareplex for Oracle, or Oracle Dataguard. For an example, see the Deploy MySQL Replication section.\n* The secondary database servers are in read-only mode. The secondary database pools feature does not support multiple primary databases.\n* All secondary database names are the same as the primary database name.\n* The primary and secondary database server hard drives have sufficient disk space to accommodate replication logs.\n* [Recommended] The primary and secondary database servers are located in the same datacenter. Having the secondary databases in the same datacenter improves performance and security.\n\n\n\n\n|  | **Note:**  Secondary database pools require that the primary database is always available. ServiceNow recommends enabling high availability and disaster recovery to ensure that a primary database is always available. |\n| --- | --- |\n\n\n## Configuring database pools\n\n\nWhen you have completed the physical set up of secondary databases with replication, you are ready to set up ServiceNow to route queries to the secondary databases:\n\n\n* Activate the plugin.\n* Define a pool record for each connection pool.\n* Route categories of queries to specific connection pools.\n\n\n## Activating the plugin\n\n\nTo install this feature, activate the Secondary Database Pools plugin. See [ServiceNow instructions for activating a plugin](https:\/\/docs.servicenow.com\/csh?topicname=t_ActivateAPlugin.html&version=latest \"ServiceNow instructions for activating a plugin\").\n\n\n\u00a0\n\n\n## Defining database pools\n\n\nTo define a pool record for each connection pool on every secondary database:\n\n\n1. Navigate to **System Maintenance > Secondary Database Pools**.\n2. Define the pool by completing the below form:  \n\n\t* **Field:** Description\n\t* Enter a name for the pool record.\n\t* **Database Name:** Enter the database name. **Note:** All secondary database names must be the same as the primary database name.\n\t* **Type:** Select the database management system on the secondary database (MySQL, Oracle, or SQLServer).\n\t* **Active:** Select the check box to make the connection pool available.\n\t* **URL:** Enter a valid JDBC connection URL for the secondary database. For example:?jdbc:mysql:\/\/172.16.241.133\/\n\t* **User:** Enter the user name for the database service account.\n\t* **Password:** Enter the password for the database service account.\n\t* **Min connections:** Define the minimum number of concurrent database connections.\n\t* **Max connections:** Define the maximum number of concurrent database connections. **Note:** Consider using the same minimum and maximum connection settings as you use for your primary database, but do not exceed the connection limit of your database management system. Factor in the number of application servers as well as the number of pools associated with each secondary database. For example, you define two connection pools for a secondary database and set the maximum connections for each pool to 20. If you have two application servers, then you have configured a maximum of 80 total connections for the database (20 to each pool for each application server).\n3. Click **Submit**.\n4. Open the record and click **Test Database Connectivity** to verify that the database connection is defined correctly. If necessary, modify the pool record or database setup to establish connectivity. This example shows an issue with database settings for the maximum allowed packet size.\n\n\n## Routing queries to pools\n\n\nNote: If there are any expensive OOB queries related to TextSeach, Report, PA, etc., then please log a PRB to Dev-Persistence. They can redirect the PRB to the teams to tag their code to reroute the query to be issued. Setting the query category from the call sites that issues the query is a much safer way than doing it by query pattern because it is hard for a person to understand where a query pattern is used.\n\n\nNote: In general, tables in sys\\_metadata family(it includes any table that is extended from sys\\_metadata) should not be routed to Read-Replica. If there is a good reason to re-route these queries, then please validate with Dev team(Dev-Persistence) before re-routing the query. PRB1352069 fix will try to exclusion list additional tables to the above list so that people don't accidentally route a wrong table. exclusion listing the tables doesn't prevent a new query pattern or new tables that show up in the slow query module.\n\n\nCAVEAT: It has been requested by development that we open an Case task with the Dev - Persistence team to validate whether we can reroute a query to the read replicas, as we have seen some unexpected results (such as PRB1174630: Infinite loop querying sys\\_db\\_category). \u00a0If Dev does agree to reroute the query, then we should test a restart of a standby node after implementing the reroute to verify that nodes will not fail to restart after implementing the query category. \u00a0If the node does not restart, then backout the query category routing.\n\n\nUse categories to route types of queries to one or more specific secondary database connection pools. The categories below are available:\n\n\n\u0095 Reports (includes scheduled reports and exports)  \n\u0095 Homepages  \n\u0095 Auto-complete  \n\u0095 Text search  \n\u0095 Lists (includes exports)  \n\u0095 Embedded lists  \n\u0095 Related lists  \n\u0095 Logs  \n\u0095 ODBC driver  \n\u0095 CMDB relationships\n\n\nTo configure categories settings to route queries to pools:\n\n\n1. Navigate to **System Maintenance > Secondary Database Categories**.\n2. Select the category of queries to route to secondary databases (for example, auto-complete).\n3. Configure the category settings by completing the form:  \n\n\t* **Field:** Description\n\t* **Name:** This identifies the type of queries you are routing to secondary databases (auto-complete, embedded\\_list, homepage, list, log, odbc, related\\_list, reporting, or text search).   \n\t(*Note:* Do not change this field value or create a new category.)\n\t* **Pool selection:** When multiple connection pools are configured for a category, select the method for determining where to route a query.  \n\t\n\t\t+ **Round-Robin:** Randomly select but evenly distribute queries among connection pools.\n\t\t+ **Priority:** Select a connection pool in a predictable order (determined by the **Order** field. See step 4). Uses the lowest order pool that is available.  \n\t\t*(Note:* When all secondary database pools are unavailable, queries are always routed to the primary database.)\n\t* **Active:** Select the check box to route queries to secondary database pools. Query routing is disabled for all categories by default.\n\t* **Latency sensitivity:** Select the method for determining whether replication is too far behind to use a secondary database pool:  \n\t\n\t\t+ **None:** Disregard replication lag and always route this category of queries to an available pool.\n\t\t+ **Threshold:** Only route queries to a pool when replication lag is less than the replication lag threshold.\n\t\t+ **Threshold & users updates:** Only route queries to a pool when the replication lag is less than the threshold and less than the time elapsed since the last update on the queried table during the current session. For example, consider enabling this setting for lists so that users always see their own updates when they return to list view.\n\t* **R****eplication lag threshold:** Define how far behind the replication can be before the system stops routing queries to a secondary database pool. For example, you may set a larger threshold for homepages than for reference auto-completion. When replication falls further behind than the threshold, queries in this category are routed to the the primary database until replication catches up to within the threshold.\n4. Add pools in the Member Secondary Database Pools embedded list:  \n\n\t* **Field:** Description\n\t* **Secondary Database Pool:** Enter the pool name (Name field on the pool record).\n\t* **Order:** If you configured the priority pool selection, specify the priority for the connection pool. The system routes queries to the lowest ordered pool that is available.\n5. Click **Update**.\n\n\n## Setting the query category based on hash\n\n\nAs maint, you can route certain queries to a secondary database category by adding a record in the sys\\_query\\_category table. This is accessible through the \"System Maintenance > Query Categories (Rerouting)\". Open the module, click \"New\". The mapping is based on the \"Example\" query, so copy\/paste it exactly from the applicable sys\\_query\\_pattern record.\n\n\n![](sys_attachment.do?sys_id=7495fd961b11b8107a5933f2cd4bcb0a)\n\n\nThere is also a UI Action named \"Set Query Category\" on the \"Slow Queries\" (sys\\_query\\_pattern) table that will achieve the same effect:\n\n\n![](sys_attachment.do?sys_id=3495fd961b11b8107a5933f2cd4bcb0d)\n\n\n## Setting the query category in custom scripts\n\n\nTo route queries in custom scripts to secondary database pools, you must manually set the query category by using the setCategory method in GlideRecord. For example, you can set the category to homepage for custom homepage widgets and UI pages:\n\n\n<g2:evaluate var=\"jvar\\_inc\">?var inc = new GlideRecord('incident');?inc.addActiveQuery();?inc.addQuery('priority',1);?inc.setCategory('homepage'); \/\/set the category to 'homepage'?inc.query();?<\/g2:evaluate>\n\n\nThe setCategory and getCategory methods are available in GlideRecord for working with query categories. setCategory(String) sets the category, which determines how the query is routed to a secondary database pool.  \n  \n\n\n\n**Parameters**\n\u00a0\nString: The category name. Use one of the standard query categories (for example, homepage).  \n  \n\n**Returns**\n\n\nvoid  \n  \n\n**getCategory()**\n\n\nDetermines whether a category is set for a query  \n  \n\n**Parameters**\n\n\nNone  \n  \n\n**Returns**\n\n\nString: The category name, if one is set  \n  \n  \n\n## Setting the query category with a URL parameter\n\n\nYou can use the URL parameter below to route a query to secondary database connection pool:\n\n\nsysparm\\_query\\_category=<category name>\n\n\nFor example, export data from sys\\_audit with this URL below. The query is routed according to the list category settings.:   \n  \nhttps:\/\/instance.service-now.com\/sys\\_audit.do?XML&sysparm\\_query=tablename=sys\\_ui\\_policy&sysparm\\_query\\_category=list  \n  \nThe URL parameter takes precedence over the default query category. For example, a URL parameter routes log modules according to the log category settings instead of the list category.  \n  \n  \n\n\n\n## Setting the query category\u00a0in a REST request\n\n\nYou can use the query parameter\u00a0below to route a query to secondary database connection pool:\n\n\nsysparm\\_read\\_replica\\_category=<category name>\n\n\nQuerying data from read replicas is supported by the [Table API](https:\/\/docs.servicenow.com\/?context=CSHelp:REST-Table-API \"Table API\") and [Attachment API](https:\/\/docs.servicenow.com\/?context=CSHelp:REST-Attachment-API \"Attachment API\"). See\u00a0KB0596723 for additional information about REST read-replica support.\n\n\n## Monitoring and debugging secondary database pools\n\n\nSeveral diagnostic tools below are available for monitoring and debugging the performance of secondary database pools.\n\n\n**Replication performance graphs**\n\n\nThis monitors replication with the Replication Lag system performance graph, which tracks the replication latency for the connection pool over time.\n\n\n* View the ServiceNow Performance homepage or add performance metrics to your homepage\n* Select Replication and then select the desired connection pool\n\n\n**Note:** The Replication Throughput graph is not relevant to secondary database pools. It only applies to replication for high availability deployments.  \n  \n\n\n\n**Log messages**\n\n\nYou can view secondary database pool log messages by turning on session-level debugging or viewing the slow query log (for any query that takes over 100 ms). The debugging logs monitor:\n\n\n* Queries: shows which queries are routed to secondary databases\n* Category routing: shows where types of queries are routed and why\n\n\nTo view query log messages:\n\n\n1. Navigate to **System Diagnostics > Session Debug > Debug SQL**. The format for a log message is: <database name>[<pool>.<connection ID>] <message text>  \n  \nFor example, this log message shows that a query to the Locations table was routed to the glideclone secondary database connection pool.\n\n\nTo view category routing information as well as query log messages:\n\n\n1. Navigate to **System Diagnostics > Session Debug > Debug SQL (Detailed)**.  \n  \nFor example, this log message shows that a list query was routed to the primary database because replication lag on the secondary database exceeded the acceptable latency settings.\n\n\n\u00a0\n\n\n**Table Iostats**\n\n\nThe Table Iostats module provides statistics on the queries that a pool processes. To view:\n\n\n1. Navigate to **System Diagnostics > Table Iostats**.\n2. Personalize the list to add the **Pool Name** field.\n3. Filter the list by the pool name.\n\n\n**Viewing stats**\n\n\nTo view basic availability and status information for database connection pools:\n\n\n1. Navigate to **System Diagnostics > Stats**. Lag indicates how far behind the replication is in milliseconds.\n\n\n**XMLstats**\n\n\nComplete system diagnostic information is available in XMLstats.do.\n\n\n* To view all available information, navigate to the URL: *<base instance URL>\/xmlstats.do*\n* To view only the IO statistics by pool, navigate to the URL: *<base instance URL>\/xmlstats.do?include=iostats*\n\n\n**Recycling pools**\n\n\nIn most cases, secondary database pools recycle automatically when necessary (for example, after a configuration change). When you are debugging an issue, you may want to manually recycle a database pool. To recycle:\n\n\n1. Navigate to **System Maintenance > Secondary Database Pools**.\n2. Open the pool you want to recycle.\n3. Click **Recycle Pool**.? A new pool is initialized and replaces the existing pool. Pools recycle gracefully, that is, current transactions are allowed to complete, and new transactions are routed to the new pool.\n\n\n\u00a0\n\n\n# SQL statements of interest\n\n\n\n\n---\n\n\n**Does the DB have replicas\u00a0 enabled?**  \nselect name, value, description, sys\\_updated\\_on from sys\\_properties where name ='glide.secondary\\_db\\_pools.enable';  \n  \n  \n**To show the 'replication lag' threshold for each of the categories:**  \nselect active, name, max\\_behindness, sys\\_updated\\_on from sys\\_db\\_category\"\n\n",
        "QUERY": " 'how to get the secondary db'",
        "GROUND_TRUTH": "To get the secondary database in a ServiceNow environment, you need to follow these steps:\n\n1. Set up secondary database servers and enable replication of the primary database. Ensure that your hardware configuration meets the requirements mentioned in the article.\n\n2. Activate the Secondary Database Pools plugin. \n\n3. Define a pool record for each connection pool on every secondary database. Navigate to System Maintenance > Secondary Database Pools and define the pool by completing the form.\n\n4. Route categories of queries to specific connection pools. Navigate to System Maintenance > Secondary Database Categories and select the category of queries to route to secondary databases. Configure the category settings by completing the form.\n\n5. You can also set the query category based on hash, in custom scripts, with a URL parameter, or in a REST request.\n\n6. Monitor and debug secondary database pools using the diagnostic tools mentioned in the article.\n\nRemember, secondary database pools require that the primary database is always available. ServiceNow recommends enabling high availability and disaster recovery to ensure that a primary database is always available."
    },
    {
        "id": 46,
        "CONTEXT": "\"## \n\n### Description\n\nWhen selecting an app for download on the Store, this will generate an encrypted format for a specific instance. Encryption is accomplished by using a PKCS12 key pair certificate. This process can be managed from a computer with internet access, which can then download the encrypted application and generate a certificate which will allow to finalize the app installation on the self-hosted instance.\n\n\nThis process will need to be done for each single instance. The same certificate can be used on all instances under the same customer account, but it will need to be uploaded once on each instance (i.e. prod, sub-prod, dev, test).\n\n\nA store application file has an expiry date upon download. Administrators will have two weeks time to install it, after which they will need to download it again.\n\n\nCertificates are valid for one year, after which they will be marked as invalid, and the administrator will need to generate a new one and remove the old one.\n\n\n### How to generate a new certificate?\n\n\nAfter requesting an application in the store for their on-premise instance, the customer will be requested to upload an existing certificate or generate a new certificate.\u00a0\n\n\nIf they do not have a valid existing certificate, they can proceed as follows:\n\n\n**NOTE:** ***Role required: admin***\n\n\n1. Go to System Definition > Certificates\n2. Click Generate a new certificate\n3. Enter a password to generate a new one  \n  \n![](\/sys_attachment.do?sys_id=6ddeaea8479d3910b6d8aa25126d4364)\n4. The certificate is generated\n5. Click on Download certificate to save it for future use\n6. Click on Download application\n\n\n![](\/sys_attachment.do?sys_id=65deaea8479d3910b6d8aa25126d4360)\n\n\n\u00a0\n\n\n### Uploading a certificate to the instance\n\n\n1. Go to System Definition> Certificates\n2. Click New\n3. Add Name\n4. Add Type: select PKCS12\n5. Add Key store password: enter the same password chosen when generating the certificate\n6. Click the attachment icon to upload the certificate file (.p12) previously downloaded\n7. Click on the link to Validate Stores\/Certificates\n8. A banner confirming the valid certificate will display\n9. Right click on top grey header and select 'Save'\n\n\n![](\/sys_attachment.do?sys_id=11deaea8479d3910b6d8aa25126d435d)\n\n\n\u00a0\n\n### Additional Information\n\n#### Common issues related to certificates\n\n\n**Reasons for receiving an error for invalid certificate**\n\n\n1. If a certificate was never uploaded in the instance, but the customer proceeds with uploading the application directly, an error ' No valid certificate found to process application upload' displays.\n\n\n2. There is a mismatch between the certificate that was uploaded when downloading the app in the Store and the certificate that was already uploaded in the instance on a previous occasion. When uploading the application in the instance, the error displays.\u00a0\n\n\n? Ensure only one valid certificate with the same Type is uploaded in the instance. This should match the one uploaded when requesting the app in the store.\u00a0\n\n\n3. The certificate record was not created correctly in the instance or the .p12 file was not added as an attachment.\n\n\n? Customer can go to System Definition> Certificates and validate all active records by clicking on **Validate Store\/Certificate**. If result is invalid, the reason will be highlighted. The customer can just fix the record or, in case of obsolete records, deactivate them in favor of new ones, by unflagging the **Active** checkbox.\n\n\n![](\/sys_attachment.do?sys_id=21deaea8479d3910b6d8aa25126d4378)\n\n\n4. The incorrect type of certificate was selected in the certificate form when uploading the certificate on the instance.\n\n\n? The selected Type should be PKCS12 Key Store\n\n\n5. The instance ID displayed in the selected instance when requesting an app in the Store does not match the current instance ID in the instance stats.\n\n\n?Customer will need to ensure the instance ID for their instance matches the one in their stats. If not, they can update their instance information with [Manage On-Premise Instance - Now Support Service Catalog](https:\/\/support.servicenow.com\/kb?id=kb_article_view&sysparm_article=KB0551693 \"Manage On-Premise Instance - Now Support Service Catalog\")\n\n\n6. The App Store Certificate got generated using the latest Java version but the On-Premise customer instance is using an older Java version.\n\n\nIn this case, these are the possible 3 workarounds (The certificate must be in PKCS12 format. The store app will not accept a PEM certificate.)\n\n\n1. Update the customer instance to at least JDK 1.8.0\\_342\/1.8.0\\_345 or 11.0.16. Ref. [OpenJDK 11 Installation for Red Hat Systems (RHEL\/CentOS) - Self-hosted](https:\/\/support.servicenow.com\/kb?id=kb_article_view&sysparm_article=KB0963329 \"OpenJDK 11 Installation for Red Hat Systems (RHEL\/CentOS) - Self-hosted\")\n2. Alternatively customer can try converting the pkcs12 certificate to the legacy\/weaker format, using a newer JDK with the **'keystore.pkcs12.legacy'** option\n3. Customer can use the below example to convert the certificate using **keytool** and then upload this newly generated certificate to the instance before doing the application upload:\n\n\nkeytool -importkeystore -srckeystore certname.p12 -srcstorepass **changeit** -srcstoretype PKCS12 -deststoretype PKCS12 -deststorepass **changeit** -destkeystore convertedcertname.p12 -J-Dkeystore.pkcs12.legacy\n\n\n*(where 'changeit' must be replaced with the password set by the customer when generating the certificate).*\"\n\n",
        "QUERY": " 'how to generate and upload a certificate for on-prem store app'",
        "GROUND_TRUTH": "To generate and upload a certificate for an on-prem store app, follow these steps:\n\nTo generate a new certificate:\n1. Go to System Definition > Certificates.\n2. Click on 'Generate a new certificate'.\n3. Enter a password to generate a new one.\n4. The certificate is generated.\n5. Click on 'Download certificate' to save it for future use.\n6. Click on 'Download application'.\n\nTo upload a certificate to the instance:\n1. Go to System Definition > Certificates.\n2. Click on 'New'.\n3. Add a Name.\n4. Add Type: select PKCS12.\n5. Add Key store password: enter the same password chosen when generating the certificate.\n6. Click the attachment icon to upload the certificate file (.p12) previously downloaded.\n7. Click on the link to 'Validate Stores\/Certificates'.\n8. A banner confirming the valid certificate will display.\n9. Right-click on the top grey header and select 'Save'.\n\nPlease note that the role required for these actions is 'admin'."
    },
    {
        "id": 47,
        "CONTEXT": "\"## \n\n## Description\n\nIssue Definition: Bug in OOB Script (Schedule Job: Auto Close Case) - HRSD  \ngrContext.query(); is not being used, after adding queries.  \n  \nObserved in: Orlando Patch 4, Orlando Patch 7  \n  \n\n\n## Steps to Reproduce\n\n Steps to Reproduce:  \n1. On any OOB instance, open the below OOB scheduled script:  \n\/sysauto\\_script.do?sys\\_id=8d5f5dfb53032300c128ddeeff7b12bd&sysparm\\_record\\_target=sysauto&sysparm\\_record\\_row=1&sysparm\\_record\\_rows=6&sysparm\\_record\\_list=nameCONTAINScase%5EORDERBYname\n\n\n  \n2. In the script block, after line 56 :  \n\"grContext.query();\" is missing  \n  \n\n\n## Workaround\n\n1. Make changes to the auto close case: ( the scheduled job with sys\\_id\u00a0 - 8d5f5dfb53032300c128ddeeff7b12bd) in line number 53 - 56\n\n\n`if(grCase.hr_service.le_type) {`  \n`if(new sn_hr_core.hr_CaseUtils()._getActivitySetContextCases(caseId))`  \n`return;`  \n`}`\n\n\n2. Make changes to the script include\u00a0hr\\_CaseUtils\u00a0 (with sys\\_id 24c782869f202200d9011977677fcf89) as this :  \nadd a method -\n\n\n`_getActivitySetContextCases: function(caseId){  \n var grContext = new GlideRecord('sn_hr_le_activity_set_context');  \n grContext.addQuery('hr_case',caseId);  \n grContext.addQuery('state','awaiting_trigger');  \n grContext.query();  \n return grContext.hasNext();  \n },`  \n3. Run the following background script:\u00a0 `new sn_hr_core.hr_CaseUtils()._getActivitySetContextCases('123');`  \nThis should generate an RCA. Make sure there are two RCAs - one for the script include, and one for the scheduled script.\n\n  \n**Related Problem: PRB1442036**\"\n\n",
        "QUERY": " 'How to generate a RCA in OOB script?'",
        "GROUND_TRUTH": "To generate a RCA in OOB script, follow these steps:\n\n1. Open the OOB scheduled script on any OOB instance using the following link: \/sysauto_script.do?sys_id=8d5f5dfb53032300c128ddeeff7b12bd&sysparm_record_target=sysauto&sysparm_record_row=1&sysparm_record_rows=6&sysparm_record_list=nameCONTAINScase%5EORDERBYname\n\n2. In the script block, after line 56, you will notice that \"grContext.query();\" is missing.\n\n3. Make changes to the auto close case (the scheduled job with sys_id - 8d5f5dfb53032300c128ddeeff7b12bd) in line number 53 - 56 as follows:\n\n```\nif(grCase.hr_service.le_type) {\nif(new sn_hr_core.hr_CaseUtils()._getActivitySetContextCases(caseId))\nreturn;\n}\n```\n\n4. Make changes to the script include hr_CaseUtils (with sys_id 24c782869f202200d9011977677fcf89) by adding a method:\n\n```\n_getActivitySetContextCases: function(caseId){\n var grContext = new GlideRecord('sn_hr_le_activity_set_context');\n grContext.addQuery('hr_case',caseId);\n grContext.addQuery('state','awaiting_trigger');\n grContext.query();\n return grContext.hasNext();\n },\n```\n\n5. Run the following background script: new sn_hr_core.hr_CaseUtils()._getActivitySetContextCases('123');\n\nThis should generate an RCA. Make sure there are two RCAs - one for the script include, and one for the scheduled script."
    },
    {
        "id": 48,
        "CONTEXT": "\"## \n\nKey Management Framework roles\n\n# Key Management Framework roles\n\n\nThe Key Management Framework (KMF) introduces specific roles for cryptographic module\n and key management-related configurations.\n\n\n\nImportant:\nTo assign the KMF admin role, you must have the admin, security\\_admin, and sn\\_kmf\\_admin\n roles.\n\n\nUse the KMF admin role to assign other KMF roles.\n\n\n\n\n![KMF roles](..\/image\/kmf-roles.png)\n\n\n\n\n\n| Role | Role Name | Description |\n| --- | --- | --- |\n| KMF admin | sn\\_kmf.admin | Only users with the? KMF admin? role? can? assign? KMF roles, and in addition  can perform all capabilities of the KMF cryptographic manager. |\n| KMF cryptographic manager | sn\\_kmf.cryptographic\\_manager | Assigned to a user by a KMF admin. KMF cryptographic managers can perform  create, read, and update operations on cryptographic modules (association of keys  to cryptographic usage and algorithm configurations) and module access policies.  Also, KMF cryptographic managers can perform key management (generate, rotate,  revoke) and life cycle operations. |\n| KMF auditor | sn\\_kmf.cryptographic\\_auditor | Assigned to a user by a KMF admin. KMF cryptographic auditors can view  cryptographic module information, key metadata, and life cycle-related details, as  well as module access policy information. |\n| KMF integrator | sn\\_kmf.cryptographic\\_integrator | Assigned to a user by a KMF admin. KMF integrators can access and utilize  (integrate) cryptographic modules in third party applications. |\n\n\n\n\n\n\n\n## Assign KMF roles\n\n\nAdministrators with the security\\_admin role can assign KMF admins, who in turn can assign other KMF roles.\n\n\n\nRole required: admin and security\\_admin\n\n\nYou must elevate to the security\\_admin role before assigning the KMF admin role. For instructions, see\n [Elevate to a privileged\n role](..\/..\/security\/task\/t_ElevateToAPrivilegedRole.html \"The base system admin can elevate to a privileged role to have access to the features of High Security Settings.\")\n\n\n\n1. Elevate to the security admin role.\n2. Navigate to User Administration > Users and select the user you want to be the KMF admin.\n3. Verify that the user already has the admin and security\\_admin roles. If not,\n click Edit under the Roles related list and add\n admin and security\n \\_admin.\n4. Navigate to System Security > Key Management Administration.\n5. Select the user you want to be KMF\n admin in the Available Users column and move them to the\n Selected User(s) column.\n\n![KMF admin role](..\/..\/encryption\/image\/kmf-admin-select-user.png)\n6. Click Save.\n7. Navigate to User Administration > Users and select the user you just gave the KMF admin role to.\nThat user now has the sn\\_kmf.admin role in the Roles related list. That\n user can now assign other KMF roles.![KMF admin role](..\/..\/encryption\/image\/sn-kmf-admin-role.png)\n\n\n\nIf you have the KMF admin role, follow these steps for assigning other KMF roles:\n\n\n1. Navigate to User Administration > Users and select the user you want to have another KMF role, such\n as KMF Cryptographic Manager.\n2. In the Roles related list, click Edit and select the\n KMF roles you want to assign the users. All KMF roles start with\n `sn_kmf`.![Assigning other KMF roles](..\/..\/encryption\/image\/kmf-roles-assignments.png)\"\n\n",
        "QUERY": " 'how to add Kmf roles to user'",
        "GROUND_TRUTH": "To add KMF roles to a user, follow these steps:\n\nIf you are an administrator with the security_admin role, you can assign KMF admins. Before assigning the KMF admin role, you must elevate to the security_admin role. \n\n1. Elevate to the security admin role.\n2. Navigate to User Administration > Users and select the user you want to be the KMF admin.\n3. Verify that the user already has the admin and security_admin roles. If not, click Edit under the Roles related list and add admin and security_admin.\n4. Navigate to System Security > Key Management Administration.\n5. Select the user you want to be KMF admin in the Available Users column and move them to the Selected User(s) column.\n6. Click Save.\n7. Navigate to User Administration > Users and select the user you just gave the KMF admin role to. That user now has the sn_kmf.admin role in the Roles related list. That user can now assign other KMF roles.\n\nIf you have the KMF admin role, you can assign other KMF roles:\n\n1. Navigate to User Administration > Users and select the user you want to have another KMF role, such as KMF Cryptographic Manager.\n2. In the Roles related list, click Edit and select the KMF roles you want to assign the users. All KMF roles start with `sn_kmf`."
    },
    {
        "id": 49,
        "CONTEXT": "\"## \n\nUnclassed hardware\n# Unclassed hardware\n\n\nIf an asset does not find a match in the Configuration Management Database (CMDB)\n by CI lookup rules, the asset is termed as an unclassed hardware.\n\n\nWhen data is imported from ServiceNow built integrations, Vulnerability Response automatically uses host data to search for matches in the CMDB. The CI lookup rules are used to identify\n configuration items (CIs) and add them to the vulnerable item record to aid in\n remediation. If the assets are not found in the CMDB, the Identification and Reconciliation engine (IRE) runs its own identification rules\n defined on the hardware class and all its children and creates a CI under an Unclassed\n Hardware class. When Discovery finds this asset, instead of reclassifying\n the existing unclassed hardware, it creates a CI. This is because the identification\n rule relies on the name of the hardware CI.\n\n\nIf the Identification and Reconciliation engine (IRE) is\n activated, the reclassify option from discovered items is not\n supported.\n\n\nThe following extracts show the IRE payload for creating CI in Unclassed Hardware via Vulnerability Response and Discovery respectively.\n \n```\n\"className\": \"cmdb_ci_unclassed_hardware\",\n\n\"values\": {\n        \"name\": \"asset01.company.com\",\/\/from NetBios,\n        \"mac_address\": \"1a:5e:2c:0e:42:f3\",\n        \"fqdn\": \"asset01.company.com\",\n        \"ip_address\": \"133.10.3.123\"\n    }\n\n\"settings\": {\n        \"updateWithoutSwitch\": true\n    }\n\n```\n\n\n```\n\"className\": \"cmdb_ci_cmdb_ci_win_server\"\n\n\"values\": {\n        \"name\": \"asset01\",\n        \"mac_address\": \"1a:5e:2c:0e:42:f3\",\n        \"fqdn\": \"asset01.company.com\",\n        \"ip_address\": \"133.10.3.123\",\n        \"os_domain\": \"company\"\n    }\n\n\"settings\": {\n        \"updateWithoutSwitch\": false\n    }\n\n```\n\nIn this case, the existing Unclassed Hardware CI is not reclassified to Windows server,\n and instead a new CI under Windows server is created. The reason being the\n identification rule relies on the name of the Hardware CI. If you notice the payloads,\n the name in the payload from Vulnerability Response is the same as either\n FQDN\/NETBIOS, but it is just the host name for the payload from Discovery.\"\n\n",
        "QUERY": " 'How does records on unclassed hardware affect CMDB?'",
        "GROUND_TRUTH": "Unclassed hardware records affect the CMDB in several ways. When an asset does not find a match in the CMDB by CI lookup rules, it is termed as an unclassed hardware. When data is imported from ServiceNow built integrations, Vulnerability Response automatically uses host data to search for matches in the CMDB. If the assets are not found in the CMDB, the Identification and Reconciliation engine (IRE) runs its own identification rules defined on the hardware class and all its children and creates a CI under an Unclassed Hardware class. \n\nHowever, when Discovery finds this asset, instead of reclassifying the existing unclassed hardware, it creates a new CI. This is because the identification rule relies on the name of the hardware CI. If the IRE is activated, the reclassify option from discovered items is not supported. This means that the existing Unclassed Hardware CI is not reclassified to Windows server, and instead a new CI under Windows server is created. This process can lead to the creation of multiple CIs for the same asset in the CMDB, which can affect the accuracy and efficiency of the CMDB."
    },
    {
        "id": 50,
        "CONTEXT": "\"## \n\nContainer Vulnerability Response\n# Container Vulnerability Response\n\n\nThe ServiceNow\u00ae\nContainer Vulnerability Response application imports container vulnerable items (CVITs) and according to the rules enables you to remediate\n container vulnerabilities. Vulnerability data is pulled from internal and external sources, such as the National Vulnerability Database (NVD) or third-party integrations.\n\n\n\nStarting with version 18.0 of Vulnerability Response, you can monitor and remediate CVITs in Vulnerability Manager Workspace and IT Remediation Workspace respectively. For more information, see [Explore the Vulnerability Manager Workspace](..\/..\/vr-vulnerability-manager-workspace\/concept\/exploring-vuln-mgr-workspace.html \"As a vulnerability manager and analyst, get a unified view of all the vulnerability types in the Vulnerability Manager Workspace. You can see host, application, and container vulnerabilities and configuration issues, create watch topics across these vulnerabilities and misconfigurations, and create remediation tasks.\") and [Explore the IT Remediation Workspace](..\/..\/vulnerability-response\/task\/vr-ws-itro-wkspce.html \"The IT Remediation Workspace is intended for IT remediation owners and IT groups. It is composed of home and list views as well as data visualizations that you can click that let you see the remediation tasks you've been assigned and how many records assigned to you have solutions.\").\n\n\n\n## Request apps on the Store\n\n\nVisit the [ServiceNow Store](https:\/\/store.servicenow.com\/sn_appstore_store.do#!\/store\/home) website to view all the available apps\n and for information about submitting requests to the store. For cumulative release notes\n information for all released apps, see the [ServiceNow Store version history release\n notes](https:\/\/docs.servicenow.com\/bundle\/store-release-notes\/page\/release-notes\/store\/sn-store-release-notes.html).\n\n\n\n\n\nTable 1. Container Vulnerability Response\n| [Explore  Learn about Container Vulnerability Response concepts and features.](..\/..\/secops-integration-vr\/prisma\/concept\/exploring-cvr.html \"The Container Vulnerability Response application imports container vulnerable items (CVITs). According to the rules, the feature enables you to remediate the container vulnerabilities. Container Vulnerability Response is available through a separate subscription.\") | [Configure  Configure Container Vulnerability Response.](..\/concept\/configure-cvr.html \"Before you run Container Vulnerability Response in your Now Platform instance, you must configure it.\") | [Integrate  Extend Container Vulnerability Response by integrating with other applications.](..\/concept\/integrate-cvr.html \"Extend the capabilities of Container Vulnerability Response by integrating with other applications.\") |\n| --- | --- | --- |\n| [Remediate  Learn how to remediate container vulnerabilities.](..\/concept\/remediate-cvr.html \"Monitoring remediation is a process that begins with reviewing status and ends with closing container vulnerable items (CVITs). Container Vulnerability Response offers tools and procedures to make that process more productive and efficient.\") | [Analytics & Reporting  View Container Vulnerability Response analytics](..\/..\/..\/use\/application-content-packs\/concept\/cvr-content-pack.html \"This Platform Analytics Solution contains prepackaged Platform Analytics content for use with other Now Platform products. This Platform Analytics Solution provides an exposure on how vulnerabilities are managed through the various charts.\") | [Reference  Get details about Container Vulnerability Response components like fields, tables and properties.](..\/concept\/cvr-reference.html \"Reference topics containing information about tables, roles, and properties installed with the Container Vulnerability Response.\") |\n\n\n\n## Benefits\n\n\nThe Container Vulnerability Response application provides the following benefits:* Integrates with third-party container security products, like Prisma Cloud Compute from Palo Alto Networks.\n* Imports vulnerability data for the images that are deployed to runtime, and enriches the vulnerability data with runtime contextual information (hosts, Kubernetes clusters, services, and namespaces).\n* Provides a list of the references created from vulnerabilities to the relevant Kubernetes entities in the Configuration Management Database (CMDB) using ServiceNow Kubernetes Discovery.\n* Offers a comprehensive reporting dashboard, providing insights into the vulnerability and remediation trends.\n\n\n\n\n## Key features\n\n\nThe Container Vulnerability Response application manages vulnerabilities detected in the containers. It provides the following features:* Point to source Docker Image from CVITs instead of running containers.\n* Configure granularity of CVITs to track at image, Kubernetes cluster, namespace, or service level.\n* Track new image versions to identify fixed vulnerabilities. Any vulnerabilities reported in older versions are automatically resolved in ServiceNow when new image versions are deployed at runtime.\n* Track CVITs in Base images separately from Application images to enable independent remediation.\n* Raise exception requests or false positive requests, which can be reviewed through a multi-level approver process.\n* Define exception rules to defer CVITs automatically.\n\n\n\n\n## Use cases\n\n\nContainers are a great way of deploying and scaling applications on multiple environments with less overhead and increased portability. However, vulnerabilities in container images can pose risk to the underlying host and\n ultimately to the infrastructure where containers have been launched from these images. While there are many container security products that scan container images for vulnerabilities, there are a few considerations and problems\n associated with remediation of the vulnerabilities. Container Vulnerability Response can help solve various problems or use cases involved in remediating container image vulnerabilities. Explore ways to take advantage of the Container Vulnerability Response module:Runtime contextVulnerabilities in container images can be discovered by scanning the image in the following stages of the application life cycle.* Stage 1: When images are being built in the CI\/CD pipeline.\n* Stage 2: When images are published to the registry\n* Stage 3: When images are deployed to runtime.\n\n\nWhile it\u0092s important to identify vulnerabilities as early as possible in stage 1 and stage 2, performing a scan on those images that are deployed to a runtime environment is equally important. It offers the\n following benefits:* Identifying any new common vulnerabilities and exposures (CVEs) that got published.\n* Providing accurate visibility into the risk posture of applications deployed.\n* Prioritizing of vulnerabilities that must be resolved. The runtime context in terms of the application services or business services impacted due to a vulnerability can help with prioritization.\n\n\n\nContainer Vulnerability Response integrates with container security products such as Prisma Cloud Compute from Palo Alto Networks to pull the vulnerability data for those images that are deployed to runtime and enriches the vulnerability data with the runtime contextual information such as hosts, Kubernetes clusters,\n services, and namespaces where these container images are deployed. Customers using the ServiceNow Kubernetes discovery can see the references created from vulnerabilities to the relevant Kubernetes entities in their Configuration Management Database (CMDB). In addition to enriching the metadata, ServiceNow also offers a comprehensive reporting dashboard to provide insights into the vulnerability and remediation trends.![](..\/image\/runtime-context.png)![Runtime context](..\/image\/runtime-context.png)\n\n\nIdentify ownershipPre-requisites\n* Kubernetes metadata and references: For Container Vulnerability Response to populate Kubernetes metadata (namespace, cluster, and so on) and references to Configuration Management Database (CMDB) entries, you must implement the Kubernetes discovery from Information Technology Operations Management (ITOM). Kubernetes discovery populates Docker Image, the running Docker Containers, Pods, Kubernetes Clusters, and so on, in the CMDB. Container Vulnerability Response identifies the Docker Image in CMDB based on image ID, and then identifies the related Kubernetes entities and populates the references to those entities from vulnerable items.\n* Cloud metadata and Docker Image labels: Container Vulnerability Response also populates Docker Image labels, cloud account IDs, regions where an image is deployed. This data is maintained in \u0093Discovered Container Image\u0094 record associated with\n the vulnerable item. There are no pre-requisites for this data to be populated. Container Vulnerability Response uses the data returned by container security products (for example, Palo Alto Prisma Cloud Compute) to populate these entries.\n\n\n\n\nOwnership for patching a vulnerability in a container image can vary from organization to organization. This information may be captured in different places. A few organizations use Docker Image labels as a way of\n identifying the application teams who own a certain container image while other organizations may use the Kubernetes namespace or the cloud account where a container image is deployed to identify the right owner.\n\n\nContainer Vulnerability Response provides the necessary data model elements to be able to capture and use the Docker Image labels, Kubernetes cluster\/service\/namespace metadata, or the cloud account metadata\n (cloud account id, region, provider, and so on) in the \u0091Assignment rules\u0092 module and automatically assign vulnerabilities to the right application team based on the metadata of the image or the runtime\n environment.\n\n\n![ownership identification](..\/image\/ownership-identification.png)\n\n\nTrack vulnerabilities in the base imagesPre-requisitesFor \u0091Base Image\u0092 property to be populated in Container Vulnerability Response, base images must be configured explicitly in the Vulnerability Response Integration with Palo Alto Networks Prisma Cloud Compute console. For more information on how to configure base images in Prisma Cloud, see https:\/\/docs.paloaltonetworks.com\/prisma\/prisma-cloud\/prisma-cloud-admin-\n compute\/vulnerability\\_management\/base\\_images.\n\n\nContainer Vulnerability Response enables for the creation of separate vulnerability records for a base layer so that they can be assigned to a different team.\n\n\nTrack vulnerabilities identified in a base OS\n image such as Alpine from the vulnerabilities detected in other layers of the container image. Many organizations have dedicated teams who are responsible for patching base OS images and making them available for all\n the application teams.![base image](..\/image\/base-image.png)\n\n\nDefine granularity for vulnerable itemsPre-requisitesConfigure the granularity of CVITs by navigating to All > Container Vulnerability Response > Administration > Configure VI Granularity.\n\n\n![VI granularity](..\/image\/vi-granularity.png)\n\n\n\nBy default, a vulnerable item is created for every unique combination of CVE and the Docker Image version (reference + tag). However, a few Docker Images may be deployed in more than one Kubernetes namespace and each\n namespace could be owned by different business units or teams. Each team may follow their own cadence for rolling out new versions of container images to fix vulnerabilities. To accommodate this scenario, Container Vulnerability Response enables you to define granularity for vulnerable items: Whether one vulnerable item should be created for each Kubernetes namespace\/cluster\/service even for every unique\n combination of Docker Image version and vulnerability.\n\n\n![VI granularity](..\/image\/vi-granularity-usecase.png)\n\n\nIn the example, you can see two vulnerable items created for the same Docker Image and CVE combination. One for the Kubernetes namespace \u0091k8s-finservco-loans\u0092 and the other one for\n \u0091k8s-finservco-wealthandinsurance\u0092.\n\n\nIdentify impacted services using tag-based service identificationPre-requisites\n* Identify various services in your application and define the tags\/ key-value pairs that represent those services.\n* Deploy Docker Images and Kubernetes pods with those tags or labels.\n* Deploy ITOM Kubernetes Discovery Define 'Tag-based Services' with the right tags or labels.\n* Deploy ITOM Kubernetes Discovery\n* Define 'Tag-based Services' with the right tags or key-value pairs.\n* Import vulnerability data into ServiceNow using Container Vulnerability Response\n\n\n\n\nRisk calculation for vulnerable items can be done by looking at the CI (Docker Image) and the criticality of associated services in CMDB. However, to make it easier to identify impacted services, Container Vulnerability Response offers tag-based service identification.\n\n\nWhen Kubernetes pods or entities are published with key-values or labels matching the key-values defined in any \u0091Tag-based Service\u0092 in ServiceNow, Container Vulnerability Response automatically establishes the relationship between a Docker Image, and the impacted application service and uses the criticality of that application service in risk\n calculation.\n\n\nThe flow is as follows:1. Container Vulnerability Response imports vulnerability data from container security product.\n2. For each container vulnerable item, Container Vulnerability Response populates the Docker Image from CMDB that has the vulnerability.\n3. Container Vulnerability Response then looks at the Docker Image labels, or the labels\/key-values published with Kubernetes pods where this image is deployed. This image is available in CMDB if you have the Kubernetes discovery running.\n4. Container Vulnerability Response then searches for \u0091Tag-based Services\u0092 in CMDB with the same matching key-value pairs defined.![Risk calculation](..\/image\/risk-calculation1.png)![Risk calculation](..\/image\/risk-calculation2.png)\n5. Container Vulnerability Response uses the \u0091Business criticality\u0092 of the matched \u0091Tag-based Service\u0092 (if any found) to calculate the risk of a container vulnerable item.![Risk calculation](..\/image\/risk-calculation3.png)\n\n\n![Risk calculation](..\/image\/risk-calculation4.png)\n\n\n\nTracking VulnerabilitiesSetting remediation targetsServiceNow enables vulnerability managers to define \u0091Remediation target rules\u0092 to be able to define service level agreements (SLAs) for fixing vulnerabilities found in container images.\n Remediation target date can be defined based on a condition\/criterion on image metadata or vulnerability information. Remediation owners receive email communication on the vulnerabilities that are approaching the due date.![Set remediation target](..\/image\/set-remediation-target.png)\n\n\nIdentifying fixed vulnerabilitiesUnlike host vulnerabilities that can be fixed by applying a patch on a host, container vulnerabilities can\u0092t be patched. New versions of container images must\n be created and deployed to replace the older versions. The new versions have a different identifier (image ID) compared to the previous versions that makes it challenging to keep track which vulnerabilities have been\n fixed already.\n\n\nServiceNow identifies vulnerabilities that were reported in the previous versions of the container images but resolved in the latest version of the image and automatically moves those\n vulnerabilities to \u0091Closed\/Fixed\u0092 state so that security teams always have accurate visibility into the latest risk posture.\n\n\nManage exceptions\nApplication teams or remediation owners for the vulnerabilities might need the ability to request for an exception due to the following reasons.\n\n\n* A mitigation control is already in place\n* Risk accepted\n* Awaiting maintenance window to push the fix.\n\n\nServiceNow enables security admins to define multiple levels of approvers for exception requests. You can also define auto exception rules that can be used to defer automatically\n vulnerabilities matching a given condition.![Exceptions](..\/image\/exceptions-usecase.png)\n\n\n![Exceptions](..\/image\/exceptions-usecase2.png)\n\n\n\n\n\n## What's new\n\n\nTo learn more about what's new and what's changed in Vancouver, see the Vancouver release notes.\n\n\n\n## Get started\n\n\n* For an overview about Security Operations in your Now Platform instance, see [Understanding Security Operations](..\/..\/security-operations\/concept\/understanding-secops.html \"Security Operations brings incident data from your security tools into a structured response engine that uses intelligent workflows, automation, and a deep connection with IT to prioritize and resolve threats based on the impact they pose to your organization.\").\n* For information about all the Security Operations applications available for download from the ServiceNow Store, see [Security Operations and the ServiceNow Store](..\/..\/planning-and-policy\/concept\/secops-and-store.html \"Starting with Madrid, all Security Operations applications and supported integrations are available for download from the ServiceNow Store. This allows you to obtain new and updated features more rapidly. Before you can use any Security Operations applications, you must verify that you have entitlement to them (that is, you have valid licenses to use them), download them from the ServiceNow Store, and activate them.\").\"\n\n",
        "QUERY": " 'How are the vulnerabilities detected in CMDB?'",
        "GROUND_TRUTH": "The Container Vulnerability Response application in ServiceNow detects vulnerabilities in the Configuration Management Database (CMDB) by integrating with third-party container security products, such as Prisma Cloud Compute from Palo Alto Networks. It imports vulnerability data for the images that are deployed to runtime and enriches this data with runtime contextual information such as hosts, Kubernetes clusters, services, and namespaces. The application provides a list of references created from vulnerabilities to the relevant Kubernetes entities in the CMDB using ServiceNow Kubernetes Discovery. Additionally, the application offers a comprehensive reporting dashboard, providing insights into vulnerability and remediation trends."
    },
    {
        "id": 51,
        "CONTEXT": "\"## \n\n### Description\n\n**CMDB Health Dashboard**\n\n\nLooking for an alternative to filtering by CI Status Field.   \nThe Duplicate health metric report is\u00a0showing duplicates from CI's with Status as\u00a0Retired.   \nWould like to filter out any CI's that do not have a Status of Retired from appearing in the **CMDB Health Duplicate** report\n\n### Resolution\n\nThere is a feature called **\"identification inclusion rules**\", With this, it can narrow the scope of CIs that are included in the identification process. The same identification rules are automatically used by the CMDB Health duplicate metric also. For example, this scenario user can include CIs whose state is not \"Retired\", so Retired CI's are not considered during the duplicate CI identification process and during metrics as well.\n\n\nPlease keep in mind that as these rules are considered during duplicate identification process, duplicate CI's may get created even if there is a Retired CI in the system.\n\n\nHere is the doc link which I am referring above <https:\/\/docs.servicenow.com\/csh?topicname=create-id-inclusion-rule.html&version=latest>\n\n\n**Steps to be taken :**\n\n\n1. Add the Identification inclusion rule:\n\n```\n\/nav_to.do?uri=cmdb_ie_active_config.do?sys_id=-1\n```\n2. Remove the records that are created in **cmdb\\_health\\_result and reconcile\\_duplicate\\_task table.**   \n**Note:** Please use table cleaner to clean the records <https:\/\/docs.servicenow.com\/csh?topicname=c_DataManagement.html&version=latest>.Refer to table cleaner sections\n3. Make the CI **discovery\\_source** to \"\" or anything else other than \"Duplicate\" and \"unknown\" and the Identification engine should run against the identification rule and filter out the retired CIs from the health result table.\n4. Re-run the **Health Job** to re-populate the data\n5. Review the results once the job is completed\n### Additional Information\n\nTo hide the CIs with retired status from other Health Metric, please refer to docs below  \n  \n<https:\/\/docs.servicenow.com\/csh?topicname=create-health-inclusion-rule.html&version=latest>\"\n\n",
        "QUERY": " 'Health inclusion rules for duplicate metric not working as expected for non-operational\/retired CIs'",
        "GROUND_TRUTH": "According to KB12345, if the health inclusion rules for duplicate metric are not working as expected for non-operational\/retired CIs, you can use the \"identification inclusion rules\" feature to filter out these CIs. Here are the steps:\n\n1. Add the Identification inclusion rule by navigating to the following URL: `\/nav_to.do?uri=cmdb_ie_active_config.do?sys_id=-1`\n2. Remove the records that are created in the `cmdb_health_result` and `reconcile_duplicate_task` table. Use the table cleaner to clean the records. Refer to the table cleaner sections in the provided documentation link.\n3. Change the CI `discovery_source` to \"\" or anything else other than \"Duplicate\" and \"unknown\". The Identification engine should then run against the identification rule and filter out the retired CIs from the health result table.\n4. Re-run the Health Job to re-populate the data.\n5. Review the results once the job is completed.\n\nPlease note that as these rules are considered during the duplicate identification process, duplicate CIs may get created even if there is a Retired CI in the system. \n\nFor hiding the CIs with retired status from other Health Metric, refer to the provided documentation link."
    },
    {
        "id": 52,
        "CONTEXT": "\"## \n\nNow Assist Q&A Genius Results\n[Administer the Now Platform](..\/..\/..\/administer\/general\/concept\/intro-now-platform-landing.html \"As a platform administrator, you have the power of the Now Platform at your fingertips. The Now Platform is an application platform as a service that automates business processes across the enterprise.\") > [Configure Now Platform Core Features](..\/..\/..\/administer\/general\/concept\/config-now-platform-core-features.html \"The Now Platform provides for a multitude of customization options to your applications. Customize your UI, handle user and data administration, and localize your instance for time zones, currencies, and more.\") > [Search administration](..\/..\/..\/administer\/search-administration\/reference\/search-administration.html \"The Now Platform includes multiple search engines and interfaces to help users easily and quickly find the information they need.\") > [AI Search](..\/..\/..\/administer\/ai-search\/concept\/overview-ais.html \"The ServiceNow AI Search application provides a consumer-grade search engine for ServiceNow Service Portal, ServiceNow Now Mobile, and ServiceNow Virtual Agent . Intelligent query features enable users to quickly find the answers they need.\") > [Extending AI Search with ServiceNow Store applications and integrations](..\/..\/..\/administer\/ai-search\/concept\/extending-ais-store-apps.html \"Applications and integrations from the ServiceNow Store extend AI Search functionality and provide insight into search usage and behavior.\") > [Now Assist in AI Search](..\/..\/..\/administer\/ai-search\/reference\/now-assist-ais.html \"The Now Assist in AI Search ServiceNow Store application combines the power of search with the Now LLM generative AI model to answer questions in user searches with actionable AI-generated summaries of relevant knowledge articles. By constraining the context passed to the Now LLM, Now Assist in AI Search can increase the likelihood that generated responses are grounded in indexed content.\") > \n# Now Assist Q&A Genius Results\n\n\nThe Now Assist Q&A Genius Result configuration uses Now LLM to display concise, actionable answers from knowledge article results in Service Portal, Virtual Agent, Employee Center, and global searches.\n\n\n## Overview\n\n\nNow Assist Q&A Genius Results display the top search results that were extracted from the HTML fields of the records on the Knowledge [kb\\_knowledge] table and the tables\n that extend it. Each Now Assist Q&A Genius Result answer card shows an answer snippet that is extracted from a knowledge article by Now LLM. For reference, the answer card also includes a link you can select to view the source knowledge article.\n\n\nThe following example shows a Now Assist Q&A Genius Result answer card containing a summary of a knowledge article. Select the answer card's View article action link to view the\n full knowledge article.Figure 1. Sample Now Assist Q&A answer card\n![Now Assist Q&A answer card showing generated answer banner, knowledge article summary, source knowledge article title, View article action link, information link, and thumbs-up and thumbs-down feedback links.](..\/image\/genius-result-q-a-nas.png)\n\nNote: Because the Now Assist Q&A Genius Result answer is automatically generated, it's a good idea to review it for accuracy. You can provide feedback on the answer by selecting the thumbs-up icon (![Thumbs-up icon](..\/image\/genius-result-feedback-positive.png)) if the summary is accurate, or the thumbs-down icon (![Thumbs-down icon](..\/image\/genius-result-feedback-negative.png)) if it's not. Your feedback helps ServiceNow improve future versions of this Genius Result configuration.\n\nThe Now Assist Q&A Genius Result configuration replaces the original Q&A Genius Result configuration from the base system. The base system's Q&A Genius Result\n configuration extracts answers from knowledge articles using internal routines instead of using the Now LLM. To learn more about the base system's Q&A Genius Result configuration, see [Default Genius Result configurations](default-genius-result-configs-ais.html \"Pre-configured AI Search Genius Result configurations display actionable top results for Catalog Item, user, and Knowledge search queries.\").\n\n\n\n## Enabling Now Assist Q&A Genius Results\n\n\nTo use Now Assist Q&A Q&A Genius Results in AI Search applications, you can link the Now Assist Q&A Genius Result configuration to your search profiles for those applications. For details on this procedure, see [Link a Genius Result configuration to a search profile](..\/task\/link-gr-search-profile-ais.html \"Link a Genius Result configuration to one or more search profiles.\").Note: When you link the Now Assist Q&A Genius Result configuration to a search profile, be sure to unlink the default Q&A Genius Result configuration from that search profile.\n\nTo use Now Assist Q&A Genius Results with global searches in the \nAI Search for Next Experience application, see [Enable Now Assist Q&A Genius Results in global search](..\/task\/enable-now-assist-qna-global-srch.html \"Add the Now Assist Q&A Genius Result configuration to global searches in the AI Search for Next Experience application.\").\n\n\n\n## Limitations\n\n\nNow Assist Q&A Genius Results only support English-language searches.\n\n\nKnowledge articles that are boosted or promoted by result improvement rules are more likely to appear as Now Assist Q&A Genius Results, but aren't guaranteed to appear.Note: The knowledge search property settings don't affect Now Assist Q&A Genius Results. For more information on these settings, see [Knowledge search properties](..\/product\/knowledge-management\/reference\/r_KnowledgeProperties.dita\/r_KnowledgeProperties.html).\n\n\n## Answer snippet creation for Now Assist Q&A Genius Results\n\n\nAI Search uses the Now LLM to create a Now Assist Q&A answer snippet from the knowledge article record's text field. Now LLM automatically determines which elements of a knowledge article's text to include in the Now Assist Q&A answer snippet. You can't configure the criteria for this behavior.\n\n\nThe Now LLM summarizes and abstracts content from the knowledge article's text. The answer snippet displayed on the Now Assist Q&A Genius Result answer card may not exist word for word in the original article.\n\n\nBoth AI Search content retrieval and the Now LLM are continually improving, so Now Assist Q&A results for specific queries may vary over time. Because results from the Now LLM are non-deterministic, you should expect a higher answer variability compared to the base system's Q&A Genius Results.\n\n\n\n## Interaction with other search features\n\n\nThe following table describes the interactions between Now Assist Q&A Genius Results and other search features.\n\n\n\n\n\n| Feature | Interaction with Now Assist Q&A Genius Results |\n| --- | --- |\n| [Result improvement rules](result-improvement-rules-ais.html \"Define rules with configurable trigger conditions to boost, block, or promote search results for specific searches. You can also boost search results for documents matching elements of the user context, such as a user's country, OS, or device type.\") with block actions | When computing Now Assist Q&A Genius Result answers for a search query, AI Search ignores any blocked search results. |\n| [Stop words](stop-words-ais.html \"Stop words remove search query terms that don't produce meaningful results. Improve search precision and relevancy by configuring stop word dictionaries and defining stop words.\") | When computing Now Assist Q&A Genius Result answers for a search query, AI Search ignores any stop words in the query. |\n| [Synonyms](synonyms-ais.html \"Synonyms expand search queries to include additional terms with equivalent meaning or usage. Improve search recall by configuring synonym dictionaries and defining synonyms.\") | When computing Now Assist Q&A Genius Result answers for a search query, AI Search expands any synonyms in the query. |\n| [Typo handling](typo-handling-ais.html \"AI Search replaces misspelled search query terms with auto-correction terms derived from indexed content. Configure typo handling dictionary settings to control derivation of auto-correction terms.\") | When computing Now Assist Q&A Genius Result answers for a search query, AI Search corrects misspelled terms in the query. |\n\n\n\n\n\n\n\n\n**Parent Topic:** [Configuring Now Assist in AI Search](..\/..\/..\/administer\/ai-search\/concept\/configuring-now-assist-ais.html \"As an administrator or search administrator, you can configure settings to control the behavior of Now Assist in AI Search.\")\"\n\n",
        "QUERY": " 'Genius result not displayed when using AI search'",
        "GROUND_TRUTH": "The Genius result might not be displayed when using AI Search due to several reasons. Firstly, the Now Assist Q&A Genius Result configuration must be linked to your search profiles for AI Search applications. If it's not linked, the Genius results will not be displayed. Also, the default Q&A Genius Result configuration must be unlinked from that search profile. \n\nSecondly, the Now Assist Q&A Genius Results only support English-language searches. If the search is conducted in a different language, the Genius results may not appear. \n\nThirdly, the knowledge articles that are boosted or promoted by result improvement rules are more likely to appear as Now Assist Q&A Genius Results, but aren't guaranteed to appear. \n\nLastly, the interaction with other search features such as Result improvement rules with block actions, Stop words, Synonyms, and Typo handling could also affect the display of Genius results. For instance, when computing Now Assist Q&A Genius Result answers for a search query, AI Search ignores any blocked search results and stop words in the query. It also expands any synonyms in the query and corrects misspelled terms in the query."
    },
    {
        "id": 53,
        "CONTEXT": "\"## \n\n## Description\n\nThe flow status of the action shows success but there is an error message says Authentication failure with the user ...  \n  \n? The handle error step itself has a success state in spite of the errors. That is to say, the window\\_error\\_code is empty which as design makes the action assume it was successful  \n  \n? In general, many systems use the error message as informational and only when combines with error code it is identified really an 'error'. Flow designer ad spoke cannot assume it is an error as there is no error code\n\n## Steps to Reproduce\n\n 1. Configure the Reset AD user password with the AD Spoke installed  \n  \n2. Use the Reset AD user password action  \n  \n3. Put wrong credentials  \n  \n4. The system will say in the step two Authentication failure with the user ... but the status of query AD action is success  \n  \n5. More info in screenshoot\n\n\n![](sys_attachment.do?sys_id=56ef07451b166c10ed6c9979b04bcbf6)\n\n## Workaround\n\nWith admin privileges, import the attached XML because if you import the xml first and then update the plugin in case you have to do it for any other requirement, it will override the changes of the imported file.\n\n  \n**Related Problem: PRB1475449**\"\n\n",
        "QUERY": " 'Flow status is incorrect when there is an Authentication failure during AD password reset'",
        "GROUND_TRUTH": "The issue of incorrect flow status during an Authentication failure during AD password reset occurs because the handle error step itself has a success state despite the errors. This is because the window_error_code is empty, which by design makes the action assume it was successful. In many systems, the error message is used as informational and only when combined with an error code is it identified as an 'error'. The Flow designer and AD spoke cannot assume it is an error as there is no error code. \n\nTo reproduce this issue, you can follow these steps:\n1. Configure the Reset AD user password with the AD Spoke installed.\n2. Use the Reset AD user password action.\n3. Input wrong credentials.\n4. The system will say in the step two Authentication failure with the user, but the status of query AD action is success.\n\nA workaround for this issue is to import the attached XML with admin privileges. However, you should import the XML first and then update the plugin in case you have to do it for any other requirement, as it will override the changes of the imported file."
    },
    {
        "id": 54,
        "CONTEXT": "\"## \n\n## Description\n\nSLA not getting attached to the sn\\_hr\\_core\\_task if the Short Description field is used as start condition on the SLA.\n\n\nAlso, when doing the repair for the affected HR task & HR case where the SLA is not attached, getting below error message.  \nSLARepair.recreateByFilter: No matching Tasks found\n\n## Steps to Reproduce\n\n Go to an HR Case and click on Repair\n\n\nError: SLARepair.recreateByFilter: No matching Tasks found\n\n\n\u00a0\n\n## Workaround\n\n1. Navigate to \"Application Restricted Caller Access\"  \n2. Sort by \"Status\"  \n3. Update the relevant records with source \"Script Include:SLARepair\" & \"Script Include:SLAConditionBase\" from status \"Requested\" to \"Allowed\"\n\n  \n**Related Problem: PRB1481877**\"\n\n",
        "QUERY": " 'Error: SLARepair.recreateByFilter: No matching Tasks found'",
        "GROUND_TRUTH": "The error \"SLARepair.recreateByFilter: No matching Tasks found\" occurs when you try to repair an HR Case where the SLA is not attached. This issue arises when the Short Description field is used as a start condition on the SLA and the SLA is not getting attached to the sn_hr_core_task. To resolve this issue, you need to follow the workaround provided:\n\n1. Navigate to \"Application Restricted Caller Access\".\n2. Sort by \"Status\".\n3. Update the relevant records with source \"Script Include:SLARepair\" & \"Script Include:SLAConditionBase\" from status \"Requested\" to \"Allowed\".\n\nThis workaround allows the SLARepair and SLAConditionBase scripts to run, which should resolve the error."
    },
    {
        "id": 55,
        "CONTEXT": "\"## \n\n## Description\n\n\nA schedule can have a child schedule, and that child schedule's entries are included as part of the overall schedule. For example, the base system instance has a \"Holidays\" child schedule; however, the \"grandchild\" schedules (a child of a child schedule) are not taken into account.\n## Steps to Reproduce\n\n\n\n\u00a0\n\n\n1. Ensure that a SLA is attached to an incident to ensure proper functionality of the demo instance:\n\t1. Create a new incident with impact of 1 and urgency of 3 (this results in a priority of 3). Click **Save** and check the task SLA at the bottom.\n\t2. Adjust the related list to display the field **Planned end time**.\n\t3. Note the planned end time for later comparison.\n2. Modify the SLA and add the schedule:\n\t1. Go to **Schedules** and create a new schedule (for example, NEW test schedule).\n\t2. Add a schedule entry with the following parameters: (not listed fields stay with their default value)   \n\t**Name:** Workday 8-17   \n\t**Show as:** Busy  \n\t**When:** Use the given dates but change first time to **08:00:00** and the second one to **17:00:00**.  \n\t**Repeats:** \"Every Weekday (Mon-Fri)\"\n\t3. Click **Submit**.\n\t4. Go to **SLA Definitions** and open **Priority 3 resolution (5 day)**.\n\t5. Adjust the field **Schedule** to use the just-created schedule (NEW test schedule) and update the record.\n3. Test the first adjustments:\n\t1. Create a new incident with a priority of 3 (see 1), save it. Check the related list task SLA at the bottom of the form.\n\t2. Compare the planned end time of this incident to the first one and note the difference.\n\t3. Note that this time it reflects the applied schedule of weekdays 8-17.\n4. Add a holiday to the schedule and check with a new incident:\n\t1. Go to **Schedules**. Open the NEW test schedule record and add another schedule entry.   \n\tParameters for this entry: (not listed fields stay with their default value)   \n\t**Name:** holiday  \n\t**Type:** Excluded   \n\t**Show as:** Free  \n\t**When:** Check the box for **All day**. Select a date which is 2-3 days in the future and is a weekday (Mon-Fri).\n\t2. Click **Submit** to save the new entry.\n\t3. Create a new incident with a priority of 3 (see step 1). Save it and check the planned end time of the active SLA within the related list Task SLA at the bottom of the form.\n\t4. Compare with the previous ticket, and note that it is reflecting the holiday as the date now one day in the future.\n5. Remove the holiday and recreate as the child schedule:\n\t1. Go to **Schedules**. Open **NEW test schedule** and delete the schedule entry **holiday**.\n\t2. Switch to the related list **Child Schedules** and click **New**.\n\t3. Name this **NEW child schedule** and save the record (not submit in order to stay on the form).\n\t4. Create a new entry within the related list **Scheduled entries**: (not listed fields stay with their default value)   \n\t- Name: **holiday**  \n\t- Type **Excluded**  \n\t- Show as: **Free**  \n\t- When: Check the box for **All day**. Select a date which is 2-3 days in the future and is a weekday (Mon-Fri).\n\t5. Click **Submit** to save the new entry.\n\t6. Create a new incident with a priority of 3 (see step 1). Save it and check the planned end time of the active SLA within the related list Task SLA at the bottom of the form.\n\t7. Compare with the previous ticket and note that it is also reflecting the holiday and the date is the same.\n6. Remove the holiday from child schedule and recreate in next recursion level:\n\t1. Go to **Schedules**. Open **NEW child schedule** and delete the schedule entry **holiday**.\n\t2. Switch to the related list **Child Schedules** and click **New**.\n\t3. Name it **NEW childs' child schedule** and save the record (not submit in order to stay on the form).\n\t4. Create a new entry within the related list Scheduled entries: (not listed fields stay with their default value)   \n\t**Name:** holiday  \n\t**Type:** Excluded   \n\t**Show as:** Free   \n\t**When:** Check the box for **All day**. Select a date which is 2-3 days in the future and is a weekday (Mon-Fri).\n\t5. Hit **Submit** to save the new entry.\n\t6. Create a new incident with a priority of 3 (see step\u00a0 1). Save it and check the planned end time of the active SLA within the related list Task SLA at the bottom of the form.\n\t7. Compare with the previous ticket and note that it is now one day earlier than the previous two tests.\n\t8. Note that the holiday, even though it is configured properly, is not reflected by the SLA calculation.\n\n\n\n## Workaround\n\n\n\nThere is currently no workaround for this issue. As per Using Schedules in product documentation:\u00a0\n\n\n\"Note: Schedules only include schedule entries from a parent and its direct child schedules. Schedule entries from a child schedule of another child schedule are not included in a parent schedule. For example, if schedules B and C both have schedule A as their parent schedule, then the schedule entries for both schedules B and C are included in schedule A. However, if the parent schedule of schedule C is schedule B, the schedule entries for schedule C are excluded from schedule A.\"\n\n\nThis means, the system is behaving as expected. That is, duration is always calculated based on the parent and child and not to arbitrary levels.\n\n\n\n\u00a0\n\n\n  \n**Related Problem: PRB581690**\"\n\n",
        "QUERY": " 'Entries from grandchild schedules are not considered'",
        "GROUND_TRUTH": "According to KB12345, a schedule can have a child schedule, and the entries of that child schedule are included as part of the overall schedule. However, entries from \"grandchild\" schedules (a child of a child schedule) are not taken into account. This is demonstrated in the steps to reproduce the issue, where a holiday is added to a grandchild schedule and it is not reflected in the SLA calculation. The system is designed this way, as per the product documentation, which states that schedules only include schedule entries from a parent and its direct child schedules, but not from a child schedule of another child schedule. Therefore, the system is behaving as expected and there is currently no workaround for this issue."
    },
    {
        "id": 56,
        "CONTEXT": "\"## \n\n## Description\n\nScheduled job \"SAM - Subscription Maintenance\" can't run properly in SAM Foundation so that it fails to update entitlement status\n\n## Steps to Reproduce\n\n Go to the scheduled job \"SAM - Subscription Maintenance\". Click \"Execute Now\".  \n  \nGo to System logs and observe that there is warning as below:  \n------  \norg.mozilla.javascript.EcmaError: \"SAMPReserveEntitlementUtil\" is not defined.  \nCaused by error in <refname> at line 10  \n  \n7: }  \n8:  \n9: function sampUpdateStateStatus() {  \n==> 10: SAMPReserveEntitlementUtil.processExpiredReserveEntitlements();  \n11:  \n12: var today = gs.daysAgo(0);  \n13: var grEnt = new GlideRecord('alm\\_license');  \n-------\n\n## Workaround\n\nComment out the line below under the function sampUpdateStateStatus() of the \"SAM - Subscription Maintenance\" Scheduled Script Execution:\n\n\nSAMPReserveEntitlementUtil.processExpiredReserveEntitlements();\n\n  \n**Related Problem: PRB1475316**\"\n\n",
        "QUERY": " 'EcmaError while running SAM subscription maintenance job'",
        "GROUND_TRUTH": "The 'EcmaError while running SAM subscription maintenance job' occurs when the scheduled job \"SAM - Subscription Maintenance\" fails to run properly in SAM Foundation, resulting in an inability to update the entitlement status. The error message in the system logs indicates that \"SAMPReserveEntitlementUtil\" is not defined. This error is caused by a line in the function sampUpdateStateStatus() of the \"SAM - Subscription Maintenance\" Scheduled Script Execution. The workaround for this issue is to comment out the line \"SAMPReserveEntitlementUtil.processExpiredReserveEntitlements();\" under the function sampUpdateStateStatus(). This should resolve the 'EcmaError' issue."
    },
    {
        "id": 57,
        "CONTEXT": "\"## \n\n## Description\n\nAfter impersonating from the admin to a user role, an attempt to export an attachment that has encryption text as xls\/pdf for incident or any other table results in a successful download. However, when opening the downloaded file, the error \"Requested attachment does not exist\" occurs.  \n  \n\n\n## Steps to Reproduce\n\n 1. Log in to an instance as an admin.\n2. Activate the Encryption plugins.\n3. Create an Encryption context and assign to the admin role.\n\n\nFor more information, see the product documentation topic [Encryption support](https:\/\/docs.servicenow.com\/csh?topicname=c_EncryptionSupport.html&version=latest).\n4. Go to the incident table configuration and make a file (for example, Description) an Encrypted Text field.\n5. Impersonate a user who has an access to a table list such as the incident table.\n6. Go to incident-list.do.\n7. Right-click in the banner, then choose **Export > Excel**.\n8. After the download is complete, click **Download** to download the file.\n9. Check the sys\\_attachment table.\n\n\nNote that the attachment exists (the attachment is against the sys\\_poll table).\n\n\n\u00a0\n\n\n\u00a0\n\n## Workaround\n\n\nYou can choose from various workarounds:\n\n\n* Don't use impersonation when exporting data with encrypted-text column. Instead, log in as the user.\n* Set the **glide.encryption.export\\_encrypted\\_data.allowed** property to false\n* Create a special decrypt\\_attachment role that includes the Encrypted Export Attachment context, and add that role to the impersonating user.\n\n\n**Note** \u0096 In order to be able to see and add that context, disable the \"Hide system contexts\" business rule on sys\\_encryption\\_context first.\n  \n**Related Problem: PRB1268370**\"\n\n",
        "QUERY": " 'Downloaded attachment redirects to page \"Requested attachment does not exist\" after impersonating to a user for the attachment which has encryption context'",
        "GROUND_TRUTH": "The issue of the downloaded attachment redirecting to the page \"Requested attachment does not exist\" after impersonating to a user for the attachment which has encryption context is due to the encryption of the attachment. When you impersonate from an admin to a user role and attempt to export an attachment that has encryption text as xls\/pdf for incident or any other table, the download is successful but the error occurs when opening the downloaded file. \n\nTo resolve this issue, you can choose from various workarounds:\n\n1. Don't use impersonation when exporting data with encrypted-text column. Instead, log in as the user.\n2. Set the glide.encryption.export_encrypted_data.allowed property to false.\n3. Create a special decrypt_attachment role that includes the Encrypted Export Attachment context, and add that role to the impersonating user. \n\nPlease note that in order to be able to see and add that context, you need to disable the \"Hide system contexts\" business rule on sys_encryption_context first."
    },
    {
        "id": 58,
        "CONTEXT": "\"## \n\n### Description\n\n\n  \n  \n\n# Description\n\n\n\n\n---\n\n\nLazy Writer is a mechanism (background thread) that we use in the platform to defer updates to the database so that user sessions can go on about their business (not unlike an ASYNC business rule).\n\u00a0\nEach application server node will have a glide.lazy.writer thread that handles these deferred updates.  \n  \nLazy\u00a0writer handles asynchronous writes to sysevent, audit, sys\\_user\\_presence. When the instance is processing a large number of user presences updates and sysevent updates,\u00a0lazy\u00a0writer can cause row lock contention as all the rows in a large batch are locked until the transaction is committed.\u00a0 User presence updates have direct impact on UI transactions, and end users\u00a0will start to experience slowness.\n  \nWhen the Lazy Writer queue is full then its writes become\u00a0Sync writes and\u00a0that\u00a0causes the lock contention  \n  \n\n# Procedure\n\n\n\n\n---\n\n\n1. Check threads.do and localhost logs  \n  \nCheck the \/threads.do output and see if multiple threads are in the following stack trace:\nat com.glide.db.DBCompositeAction.executeLazy(DBCompositeAction.java:291)\n  \n\nCheck the localhost\\_log.$(date +\"%Y-%m-%d\").txt and look for threads with the following error:\nFAILED TRYING TO EXECUTE ON CONNECTION 15: UPDATE sys\\_user\\_presence\n  \n  \n2018-04-03 07:36:28 (640) Presence-thread-3 58A5E0FCDB9517C8DAF72FEB0B961945 SEVERE \\*\\*\\* ERROR \\*\\*\\* FAILED TRYING TO EXECUTE ON CONNECTION 15: UPDATE sys\\_user\\_presence SET `ua\\_time` = '2018-04-03T14:35:36.978Z', `path` = '\/hrportal', `sys\\_updated\\_by` = '\\_\\_USERID\\_\\_', `sys\\_mod\\_count` = 29230, `sys\\_updated\\_on` = '2018-04-03 14:35:37', `user\\_agent` = 'Mozilla\/5.0 (Windows NT 10.0; WOW64; Trident\/7.0; rv:11.0) like Gecko', `status`= NULL WHERE sys\\_user\\_presence.`sys\\_id` = '2327c028db4cba00b9b178f9bf9619d5' \/\\* dell079, gs:58A5E0FCDB9517C8DAF72FEB0B961945, tx:35e34dfcdbd957c8daf72feb0b9619e2 \\*\/\u00a0  \n**Lock wait timeout exceeded; try restarting transaction**  \n**java.sql.SQLException: Lock wait timeout exceeded; try restarting transaction**  \nat org.mariadb.jdbc.internal.SQLExceptionMapper.get(SQLExceptionMapper.java:149)\u00a0  \nat org.mariadb.jdbc.internal.SQLExceptionMapper.throwException(SQLExceptionMapper.java:106)\u00a0  \nat org.mariadb.jdbc.MySQLStatement.executeQueryEpilog(MySQLStatement.java:268)\u00a0  \nat org.mariadb.jdbc.MySQLStatement.execute(MySQLStatement.java:296)\u00a0  \nat org.mariadb.jdbc.MySQLStatement.execute(MySQLStatement.java:387)\u00a0  \nat sun.reflect.GeneratedMethodAccessor22.invoke(Unknown Source)\u00a0  \nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\u00a0  \nat java.lang.reflect.Method.invoke(Method.java:498)\u00a0  \nat com.glide.db.StatementWrapper.invoke(StatementWrapper.java:40)\u00a0  \nat com.sun.proxy.$Proxy7.execute(Unknown Source)\u00a0  \nat com.glide.db.DBI.executeStatement0(DBI.java:921)\u00a0  \nat com.glide.db.DBI.executeStatement(DBI.java:877)\u00a0  \nat com.glide.db.DBI.executeStatement(DBI.java:850)\u00a0  \nat com.glide.db.DBAction.executeAsResultSet(DBAction.java:283)\u00a0  \nat com.glide.db.DBCompositeAction.executeAsResultSet(DBCompositeAction.java:139)\u00a0  \nat com.glide.db.DBCompositeAction.executeAsResultSet0(DBCompositeAction.java:92)\u00a0  \nat com.glide.db.DBAction.executeAndReturnTable(DBAction.java:247)\u00a0  \nat com.glide.db.DBAction.executeNormal(DBAction.java:236)\u00a0  \nat com.glide.db.DBAction.executeAndReturnException(DBAction.java:197)\u00a0  \nat com.glide.db.DBAction.execute(DBAction.java:136)\u00a0  \n**at com.glide.db.DBCompositeAction.executeLazy(DBCompositeAction.java:291)**  \n**at com.glide.db.DBCompositeAction.executeLazy(DBCompositeAction.java:275)**  \n**at com.glide.db.DBCompositeAction.executeLazy(DBCompositeAction.java:271)**  \nat com.glide.ui.ng.NGPresenceService.update(NGPresenceService.java:132)\u00a0  \nat com.glide.ui.ng.NGPresenceService.updatePresence(NGPresenceService.java:92)\u00a0  \nat sun.reflect.GeneratedMethodAccessor470.invoke(Unknown Source)\u00a0  \nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\u00a0  \nat java.lang.reflect.Method.invoke(Method.java:498)\u00a0  \nat com.glide.rest.handler.impl.ServiceHandlerImpl.invokeService(ServiceHandlerImpl.java:43)\u00a0  \nat com.glide.rest.processors.RESTAPIProcessor.process(RESTAPIProcessor.java:228)\u00a0  \nat com.glide.processors.AProcessor.runProcessor(AProcessor.java:415)\u00a0  \nat com.glide.processors.AProcessor.processTransaction(AProcessor.java:186)\u00a0  \nat com.glide.processors.ProcessorRegistry.process0(ProcessorRegistry.java:178)\u00a0  \nat com.glide.processors.ProcessorRegistry.process(ProcessorRegistry.java:167)\u00a0  \nat com.glide.ui.GlideServletTransaction.process(GlideServletTransaction.java:49)\u00a0  \nat com.glide.sys.Transaction.run(Transaction.java:1977)\u00a0  \nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\u00a0  \nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\u00a0  \nat java.lang.Thread.run(Thread.java:748)  \n  \n  \n1.1 Check the issue is not due to excessive sys\\_user\\_presence updates, ie: via the localhost on the application nodes:  \n  \ngrep \"UPDATE sys\\_user\\_presence\" localhost\\_log.$(date +\"%Y-%m-%d\").txt |awk '{ print $4 }' |sort |uniq -c  \n  \nsample output:  \n\n[app128153.sjc111:\/glide\/nodes]$ grep \"UPDATE sys\\_user\\_presence\" localhost\\_log.$(date +\"%Y-%m-%d\").txt |awk '{ print $4 }' |sort |uniq -c\n\u00a023936 glide.lazy.writer[glide]\n\u00a0 \u00a012299 Presence-thread-1\n\u00a0 \u00a012291 Presence-thread-2\n\u00a0 \u00a012289 Presence-thread-3\n\u00a0 \u00a012290 Presence-thread-4\n  \nWe would expect to\u00a0only see glide.lazy.writer updates.\u00a0   \nIn the above example, this shows excessive updates by Presence Threads\n2. Check INNOTOP \/ Process List and also observe the INSERT\/UPDATE\/DELETE lock contention\n3. Check the application node logs, and observe errors similar to;  \n  \n\n2018-04-03 07:58:18 (254) Presence-thread-3 CDE47834DB9113C84C931AAC0B961959 **WARNING \\*\\*\\* WARNING \\*\\*\\* Current lazy writer delay: 0:01:18.809**\n2018-04-03 07:58:19 (087) **glide.lazy.writer[glide] SYSTEM SEVERE \\*\\*\\* ERROR \\*\\*\\* Exception during batch statement commit to** glide:dell\\_1:emc:jdbc:mysql:\/\/db160020.iad106.service-now.com:3402\/, **falling back to single commits: Deadlock found when trying to get lock; try restarting transaction**\n2018-04-03 07:58:19 (092) glide.lazy.writer[glide] SYSTEM [0:00:01.394] Statement batcher: 1000\n2018-04-03 07:58:19 (389) glide.lazy.writer[glide] SYSTEM Time: 0:00:00.223 id: dell\\_1[glide.17] for: UPDATE sys\\_user\\_presence\u00a0 SET `ua\\_time` = '2018-04-03T14:56:27.335Z', `path` = '\/hrportal', `sys\\_updated\\_by` = ...\n  \n  \n4.1 Check SPLUNK (Visualization tab)  \nsourcetype=appnode\\_localhost\\_log instance=\\_\\_INSTANCE\\_\\_ \"lock\\_wait\" | timechart count span=1m  \n  \n![](sys_attachment.do?sys_id=c83f7426db0ab450e515c223059619fd)  \n  \n  \n  \n4.2 Check Big Data  \n![](sys_attachment.do?sys_id=cc3fb426db0ab450e515c22305961902)\n4. Once confirmed, review binlogs and application node logs to determine if single (small group) of sessions are the cause.\u00a0 Use logs to identify the table(s) being updated too frequently.\u00a0 \u00a0 \u00a0See\u00a0[KB0657202](https:\/\/support.servicenow.com\/kb_view.do?sysparm_article=KB0657202 \"KB0657202\")\n5. Work backwards to establish the root cause of the\u00a0update flood\u00a0- initiating transactions from that sessionID in the application node logs.\u00a0 Possible causes could be widgets on \/sp Service Portal pages generating excessive 'sys\\_user\\_presence' updates.  \n  \nComparitive numbers:  \nGE during peak times sees around 100 per second (no issue)  \nDell during outage event P1s saw 500 updates per second to sys\\_user\\_presence which was the cause of slow performance  \n  \n  \n6.\u00a0Upgrade to a fixed release.  \nPRB1267824 optimizes the LazyWriter to avoid using db transactions unnecessarily so that\u00a0it does not\u00a0hold row locks for an unnecessarily long period of time.\n\n\n\u00a0\n\n\n\u00a0\n\n\n# Applicable Versions\n\n\n\n\n---\n\n\nFixed in:\u00a0\n\n\nJakarta Patch 9  \nKingston Patch 5\n\n\n\u00a0\n\n\n# Additional Information\n\n\n\n\n---\n\n\nRelated properties:\n\n\nglide.db.lazy.writer.wait\\_threads\u00a0\nglide.db.lazy.writer.debug\u00a0\nglide.db.lazy.writer.use\\_transaction\u00a0\nglide.db.allow.lazy.payload  \n  \n  \n[KB0657202](https:\/\/support.servicenow.com\/kb_view.do?sysparm_article=KB0657202 \"KB0657202\") TSE Binlog Query Examples  \n\n#\"\n\n",
        "QUERY": " 'Database Connection Issue - Gateway shard connection failure'",
        "GROUND_TRUTH": "no answer found"
    },
    {
        "id": 59,
        "CONTEXT": "\"## \n\nVancouver security and notable fixes\n[Vancouver release notes](..\/..\/release-notes\/family-release-notes.html \"The ServiceNow Vancouver release includes new products and applications, as well as additional features and fixes for existing products. Read the release notes to learn about the release, prepare for your upgrade, and upgrade your instance.\") > [Learn about the Vancouver release](..\/..\/release-notes\/concept\/rn-learn-landing-page.html \"The Vancouver release includes new features and improvements built on the Now Platform.\") > \n# Vancouver security and notable fixes\n\n\nThe Vancouver release contains important problem fixes.\n\n\n\nVancouver was released on August 3, 2023.\n* Build date: 07-26-2023\\_1029\n* Build tag: glide-vancouver-07-06-2023\\_\\_patch0-07-18-2023\n\n\n\n\n\nImportant: For more information about how to upgrade an instance, see [ServiceNow upgrades](..\/upgrades\/reference\/upgrade.html \"The upgrade process moves your instance to a new ServiceNow release version. Understand the difference between upgrading and patching, release definitions, rollback and backup options, and how to test your non-production and production instance upgrades.\").\n\nFor more information about the release cycle, see the [ServiceNow Release Cycle](https:\/\/support.servicenow.com\/kb_view.do?sysparm_article=KB0547244).\n\n\nNote: This version is now available for use within all regulated market environments. For more information about services available in isolated environments, see [KB0743854](https:\/\/support.servicenow.com\/kb?id=kb_article_view&sysparm_article=KB0743854&_ga=2.238511747.200430442.1684856845-2052949275.1611611591).\n\n\n\nFor a downloadable, sortable version of the fixed problems in this release, click [here](https:\/\/downloads.docs.servicenow.com\/enus\/vancouver\/rn\/patches\/PRBs-V00.00.xlsx).\n\n\n\n## Security-related fixes\n\n\nVancouver includes fixes for security-related problems that affected certain ServiceNow\u00ae applications and the Now Platform\u00ae. We recommend that customers upgrade to this release for the most secure and up-to-date features. For more details on security problems fixed in Vancouver, refer to [KB1430552](https:\/\/support.servicenow.com\/kb?id=kb_article_view&sysparm_article=KB1430552).\n\n\n\n## Notable fixes\n\n\n\n\n| Problem | Short description | Description | Steps to reproduce |\n| --- | --- | --- | --- |\n| Activity Stream PRB1616546 [KB1213047](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB1213047) | Activity audit relation entries aren't captured when a record is deleted | It generates an error: 'Activity.Rules \\*\\*\\* RULE FAILED with exception: table=sys\\_audit\\_relation \\*\\*\\* GlideRecord.setTableName - empty table name'. | Refer to the listed KB article for details. |\n| Activity Stream PRB1632844 [KB1219727](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB1219727) | Comments\/worknotes don't clear in the journal box after they're posted on Workspace | This issue is intermittent. | 1. Log in to any Tokyo instance. 2. Open Agent Workspace. 3. Ensure that glide.ui.journal.use\\_html is set to true. 4. Open an existing record or create a record of incident\/case. 5. Change a field on the form. 6. Add a multiline text on the work notes. 7. Click Post.  Notice that comments\/worknotes don't clear in the journal box after they're posted on Workspace. |\n| Activity Stream PRB1638410 | 'Need to pass' in isJournal prop to HTML compose fields prevent controls from uploading attachments |  | 1. Ensure the glide.ui.journal.use\\_html system property is set to true. 2. Navigate to a record on the workspace. 3. Open the 'Network' tab in the developer console. 4. Copy and paste or drag an image into the rich text editor for work notes or comments. |\n| Activity Stream PRB1640708 | Activity Stream's 'Expand all' doesn't have a logical cap on the server-side, consuming resources | The instance struggles to process and return a result. |  |\n| Activity Stream PRB1648393 [KB1298601](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB1298601) | When the Activity Stream API is v1 and a 'sent' email with a recipient and an event creator resolve to the same username, the usernames fail to resolve | If a document record has an email and a journal, attachment, or field change event from the same user, all usernames fail to resolve in the document's activity stream. The name is either their user ID, their email, or  possibly their sys\\_id. | Refer to the listed KB article for details. |\n| Activity Stream PRB1650645 | A field change translation from English to Japanese is incorrect in Agent Workspace | The Japanese translation for field changes in 'Activities' isn't correct in Agent Workspace. | 1. Enable the Japanese plugin. 2. Navigate to Agent Workspace. 3. Navigate to List > Incident > Open.. 4. Open any record. 5. Check the activity. 6. Observe that there's a field change in English. 7. Switch the language to Japanese. 8. Check the activity. 9. Observe that there's a field change in Japanese.  Observe that the Japanese translation for field changes in 'Activities' isn't correct. |\n| Activity Stream PRB1656090 | Junk and undeliverable emails are shown in the activity stream in Agent Workspace | This issue is observed when email begins in 'Received' and takes 10+ seconds to be marked as 'Received-ignored'. | 1. Create an email from an incident. 2. In a different tab, navigate to the sys\\_email table and change the email is step #1 type to 'Received'. The email should be in the incident in step #1's activity stream. 3. Navigate back to the sys\\_email tab and change the email's type to 'Received-ignored'.  The email should disappear from the incident in step #1's activity stream. |\n| Activity Stream PRB1656837 | Making glide.ui16.live\\_ forms.enabled=false displays text on the same line | In Utah, making glide.ui16.live\\_forms .enabled=false displays text entered into Activity fields inline. | 1. Navigate to any form. 2. Type the following text to work notes and comments: \t* 111 \t* 222 \t* 333 3. Ensure that the text is entered on 3 separate lines and displays accordingly. 4. Click Save. 5. Make glide.ui16.live\\_forms.enabled = false. 6. Open the same form. 7. Type the same message to any Activity field. 8. Click Save.  Expected behavior: The text is displayed as follows:111222333 Actual behavior: The text displays on the same line. |\n| Agent Chat PRB1574449 | A slow query from the 'User's Tasks' relationship on the 'Interaction' table causes the '\/interaction.do' form to load slowly | The 'Interaction' form loads slowly due to the 'User's Task' related list. | 1. Navigate to 'interaction.list' or navigate to an 'interaction' list view within Agent Workspace. 2. Open an 'interaction' record. 3. Navigate to the 'User's Tasks' tab.  Expected behavior: It shouldn't take long to load the related lists for the 'interaction' record (these can be executed in 0.005 seconds each with Union Replacement). Actual behavior: It can take a long time to load the related lists for the 'interaction' record (sometimes 15 seconds for all related queries). |\n| Agent Chat PRB1660613 [KB1315854](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB1315854) | In Agent Workspace, the chat text-area doesn't expand when there's added text | In Utah, agents in Agent Workspace notice that the text area no longer expands when more lines of text are added. | Refer to the listed KB article for details. |\n| Agent Workspace for Incident Management PRB1623898 [KB1288300](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB1288300) | Incident Timeline is not loading for an ITIL user in Agent Workspace |  | Refer to the listed KB article for details. |\n| Agent Workspace PRB1511618 [KB0992711](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB0992711) | The 'Audit History' related list is blank if 'History Set' doesn't yet exist when a record is viewed in Agent Workspace | The 'Audit History' related list on a task record is blank until the browser is refreshed in Agent Workspace or the record is opened at least once. | Refer to the listed KB article for details. |\n| Agent Workspace PRB1617662 [KB1202275](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB1202275) | Clicking multiple list items in quick succession on the Workspace causes random list rendering | Clicking on several lists in next workspace starts session flicking between multiple list views by itself. |  |\n| Agent Workspace PRB1630542 [KB1262203](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB1262203) | A 'Workspace' list doesn't load when workspace.list. columnWidths.[sys\\_id]'s user preference has the value 'auto' | A 'Workspace' list fails to load when the sys\\_user\\_preference for workspace.list.columnWidths <sys\\_id\\_of\\_sys\\_ux\\_list> contains 'auto'. | 1. View a list in a configurable workspace. 2. Resize one of the columns with the drag handle in the list header. 3. Find the user preference that was just created\/modified, named workspace.list.columnWidths. <sys\\_id\\_of\\_sys\\_ux\\_list>. 4. Change the user preference's value to 'auto' (without quotes). 5. Log out. 6. Log back in. 7. View the same list.  Expected behavior: The list should load. Actual behavior: The list doesn't load. The preference value is apparently invalid and not handled gracefully. |\n| Agent Workspace PRB1647878 [KB1309211](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB1309211) | An HTML tag displays in an Agent Workspace composition field when the user applies a template | In Agent Workspace, when composing a field using a template (with HTML tags in the template), the HTML tags aren't rendered and display in the field. | Refer to the listed KB article for details. |\n| AI Search for Virtual Agent PRB1648246 [KB1307893](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB1307893) | AI search results don't display in Virtual Agent because the sys\\_prop glide.cs.runtime. user\\_input\\_max\\_length is set to 10240 | 10240 is too small. |  |\n| AI Search PRB1598675 | When searching with a term that matches the attachment in a catalog item, unexpected results are displayed | Genius results don't work as expected. | 1. Open any San Diego instance. 2. Create a catalog item with an attachment. 3. Navigate to Service Portal. 4. Try to search using the same terms as used in the attachment name.  Incorrect results are displayed (the catalog item is displayed with unexpected data). |\n| AI Search PRB1626677 [KB1226869](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB1226869) | The user can't select the search configuration in the search header's list | The user should able to select the search configuration in the search header's list, which is related to the current workspace. | Refer to the listed KB article for details. |\n| AI Search PRB1632710 | For the input\/active mode, the text entered is white over the white background and completely invisible |  | 1. Turn on the dark theme on an instance with Next Experience Global Search. 2. Start typing inside the search combobox on the homepage.  On Next Experience global search with dark theme on, the search Combobox input\/active mode text is white over the white background and completely invisible. |\n| AI Search PRB1657749 [KB1338972](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB1338972) | In Utah, the user can't search phone numbers, including special characters, in Agent Workspace | When the user tries to search for phone numbers in Agent Workspace's global search that have special characters (for example, (123)-456-7890), the search bar freezes. | Refer to the listed KB article for details. |\n| Application Manager PRB1620791 | Unable to install the application because hidden dependencies are blocking installation |  |  |\n| Application Navigator PRB1628759 | Multiple records are being created to the Knowledge Use (kb\\_use) table when navigating through the page | The Next Experience is generating duplicate events. | 1. Log in to an instance. 2. Open Knowledge Article from Kb\\_View page. 3. Open any Published Knowledge Article. 4. After opening the article, scroll the article twice. 5. Navigate to kb\\_use table and apply a filter with Article Number. |\n| Approvals PRB1664166 | 'Ask for Approval' does not update the approver's approval state to 'no longer required' immediately after approval |  |  |\n| Asset Management PRB1472520 | The 'Assign from Stock' business rule is updating fields on cmdb\\_ci | The 'Assign from Stock' business rule on requested item (RITM \/ sc\\_req\\_item) records is modifying fields on the linked configuration item (cmdb\\_ci) record. A workflow for the catalog item (CI) may take a variable value  and populate the CI field to record that the CI is affected by the RITM. The correct data in the CI's fields is replaced by incorrect values, causing data loss. |  |\n| Change Management PRB1654202 [KB1292390](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB1292390) | The 'Change Model: Resuables' client script switches a change request's state to 'New' rather than a valid state | The issue occurs when legacy-type based changes don't follow the state's flow. | Refer to the listed KB article for details. |\n| Change Management PRB1660588 | When Next Experience is turned off and a user upgrades to Utah, there's an issue adding affected CIs and clearing a page | In the 'Change Request' form, when users add affected CI and click 'Select All' in the UI page, it displays an empty screen. Issue only occurs in Utah, when displaying 50+ records, and when Next Experience is turned  off. | 1. Log in to an instance. 2. Turn off Next Experience. 3. Navigate to any change request. 4. Scroll down to the bottom of the screen to the related list tab 'Affected CIs'. 5. Click Add. 6. Ensure 'Show' has rows per page = 20. 7. Scroll to the bottom of the list. 8. Click the Select All checkbox. 9. Change the 'Show' rows per page = 50. 10. Scroll to the bottom of the list. 11. Click the Select All checkbox.  Expected behavior: When 'Show' = 50, clicking the 'Select All' checkbox should highlight all records in the list. Actual behavior: When 'Show' = 50, clicking the 'Select All' checkbox displays a blank page. |\n| Change Management PRB1661151 | Approvals' states (approve\/reject buttons) aren't updated automatically in CAB Workbench | This is caused by the approval responder not being registered correctly when moving to the next item in the agenda. | 1. Log in to an instance. 2. Impersonate a CAB meeting manager. 3. Start a CAB meeting. 4. Navigate to sys\\_rw\\_action.list. 5. Notice that there are records for a sysapproval\\_approver record with the name 'cab.responder'. 6. Check the responder key value. 7. In CAB workbench, navigate to the next agenda item. 8. In sys\\_rw\\_action list, duplicate the tab. 9. Compare the new responder record with the previous responder.  Expected behavior: The new responder should have a different responder key. Actual behavior: The new responder has the same responder key (which can potentially cause issues). |\n| Configuration Management Database (CMDB) PRB1558521 | The job 'CMDB Health Dashboard - Relationship Score Calculation' has a slow query | The query doesn't have any conditions nor defined range, and performs a full table scan. This causes a load on the CPU. | 1. Clone an instance. 2. Run the 'CMDB Health Dashboard - Relationship Score Calculation' job. |\n| Configuration Management Database (CMDB) PRB1591705 [KB1169983](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB1169983) | When users are using query builder and querying for more items, the Save and Run buttons at the top disappear | When users access query builder and query for a decent number of items, the page shifts down. TheSave and Run query buttons are hidden | Refer to the listed KB article for details. |\n| Configuration Management Database (CMDB) PRB1611377 [KB1182003](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB1182003) | The 'CMDB Baseline creation' job causes an 'app node out of memory (OOM)' error when a configuration item (CI) has a large number of records referencing it | The referencing records might be tasks or other table records with a reference field to a CI. When the baseline creation gets to the CI with many relations, it causes the app node to have an OOM error and restart. It  usually re-runs the same job again with every restart. The symptom is poor performance for any users logged into that app node. The restart may cause any other transactions running at the time to stop. The baseline being  created doesn't finish. Due to re-running many times, there's duplicate cmdb\\_baseline\\_entry records for the same CI sys\\_ids. | 1. Create a lot of task records all referencing the same CI. 2. Create a CMDB baseline for that CI. \t* Saving the cmdb\\_baseline record triggers the 'SNC Create Baseline' business rule, which creates a 'ASYNC: Script Job' scheduled job using SNC.CMDBUtil.baselineSchedule for the new record. 3. Observe the memory usage. 4. Take a heap dump.  There's a large amount of memory used for the related task data. |\n| Configuration Management Database (CMDB) PRB1615879 [KB1218266](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB1218266) | CMDB\\_CI Index, added as part of PRB1519942, isn't added to the CMDB partition tables on upgrades | On upgrading to San Diego or later, the index CMDB (install\\_status, sys\\_created\\_on) isn't added to the CMDB partition tables. This results in a UI and GlideRecord.query errors. This could prevent a user from accessing  some CMDB and child table lists. This could also prevent some GlideRecord.query() methods from returning the expected results. | Refer to the listed KB article for details. |\n| Configuration Management Database (CMDB) PRB1627051 [KB1211804](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB1211804) | An encoded query isn't properly displayed on a condition builder when editing a policy | When using the CMDB data manager policies, a condition builder isn't displaying the selected class name. Filters aren't saved even after it shows that the record was saved successfully. | Refer to the listed KB article for details. |\n| Configuration Management Database (CMDB) PRB1629419 [KB1220355](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB1220355) | Not all the sys\\_class\\_names for all records are validated | Not validating the sys\\_class\\_names records causes errors in Health Job processing. | Refer to the listed KB article for details. |\n| Core Platform PRB1381876 | If the user uses 'trend on' or 'after', there's a getGlideObject() application scope error when notifications are triggered | There's an error: 'Function getGlideObject is not allowed in scope sn\\_customerservice'. |  |\n| Core Platform PRB1401625 [KB0966877](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB0966877) | CSRF token rotation isn't happening during log in flows |  | Refer to the listed KB article for details. |\n| Core Platform PRB1617296 | Next Experience UI record not found | In the incident table before login displays 'record not found', even after logging in. |  |\n| Core Platform PRB1635023 | Exception in ExpiringCache.cleanup | The cache cleanup sometimes generates an exception (ExpiringCache.java:120). This can happen randomly and to different threads that need to access files on the file system. |  |\n| Core Platform PRB1635585 [KB1221999](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB1221999) | StaticContentServer caches the existence and non-existence of snc\\_node\\_disable.html | After an instance migration, some instances don't come online when nodes are re-pointed to a new database. |  |\n| Core Platform PRB1646526 | Unable to run SCCM load tests after upgrading to Utah | A CO transformation fails with an IllegalArgumentException, InvalidPathException, or Concurrent ModificationException when running concurrent transformer instances with segmented path caching logic enabled. |  |\n| Core Platform PRB1663472 [KB1323602](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB1323602) | If a record is deleted before a cache flush can be processed, an null point exception (NPE) is thrown and it prevents further processing of sys\\_cache\\_flush records | When processing the cache flush messages, any errors should be handled gracefully and the transaction should be processed. However, an exception is thrown and the transaction doesn't complete. This manifests as the  users are unable to log in, because their transactions are trying to process the flush messages. On exception, they are unable to proceed. | Refer to the listed KB article for details. |\n| Currency Administration PRB1317349 [KB0866499](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB0866499) | In Service Portal list, price and currency values only show in the user's currency | Non-USD dollar currencies (for example, Euro) will show up as the equivalent USD currency on the Service Portal list. |  |\n| Currency Administration PRB1458430 | When updating a record, the reference amount updates based on the latest currency rate, even if the input\/session amount has not changed | For instances configured with a multi-currency setup with a reference currency set to EUR, the reference amount for a currency field is updated based on the latest currency rate whenever a record is being updated. This  occurs even if the update did not change the input currency amount or currency. For non-system currency (System Currency is EUR) when rate card is updated, this effects the existing reference currency instances and may add  up over time, resulting in costs higher or lower than the intended amount. | 1. Create a currency field u\\_test\\_currency on the incident table. 2. Set the property glide.system.locale to nl.NL. Single currency mode is set to false by default. 3. In UI16, create an incident record or update an existing one and set the currency field value to 100 JPY (or GBP or CHF). 4. Identify the entry created in fx\\_currency\\_instance. Observe the reference amount value (in EUR). 5. Navigate to the fx\\_rate table and create a rate for the input currency used in step 3 or update value for the most recent rate. 6. Open the incident record opened at step 3 and update the short description. 7. Navigate back to fx\\_currency\\_instance and observe that th ereference amount has been updated based on latest currency rate for ALL.  Expected behavior: Once the currency is entered in the system for the record, the reference amount should not be updated to match the latest rate after the rate change in the system (if the field with currency was not  touched by the user). Actual behavior: The reference amount for the currency field is updated to match the latest rate after the rate change in the system, even though nothing was updated for that field in the Form. |\n| Database Persistence - Data Access PRB1326267 [KB0745443](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB0745443) | Database views with 'left joins' fail to retrieve data | When the left-join coercion is active on an instance, the query fails to retrieve data on database views with left-join. | Refer to the listed KB article for details. |\n| Database Persistence - Data Scale PRB1662075 | There is contention on sys\\_db\\_table\\_checkpoint when a transaction is open for long time | A sys\\_db\\_table\\_checkpoint record update creates a global lock per table, which causes lock contention on high churning tables. |  |\n| Database Persistence PRB1524031 | The GlideAggregate setGroupByFollowRef() isn't working |  |  |\n| Database Persistence PRB1581528 [KB1274851](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB1274851) | Queries still run on the database after the client has disconnected | There's a 'Read timed out' exception. | Refer to the listed KB article for details. |\n| Declarative Actions PRB1671096 | Avatars appear on the left side of the screen and are stacked one on top of another | Icons appear on the left side, next to the short description, and also one on top of another. Icons should show on the right side and they should appear horizontally. |  |\n| Discovery PRB1552385 [KB1124532](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB1124532) | Default behavior should be changed for SNMP Discovery to use 'all' SNMP versions |  | Refer to the listed KB article for details. |\n| Discovery PRB1595177 [KB1182085](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB1182085) | Discovery patterns don't use credential affinities |  | Refer to the listed KB article for details. |\n| Discovery PRB1595328 [KB1156526](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB1156526) | Invalid table selected on the classification record and scoping issue when updating due to the table value being in a different scope than the record | The dictionary is missing the allow\\_public=true for the table field. | Refer to the listed KB article for details. |\n| Discovery PRB1637111 [KB1264221](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB1264221) | The Visibility Content plugin version is downgraded after upgrading | A Visibility Content plugin version higher than 6.0.0 is reverted to an earlier version after upgrading from Tokyo to Utah. | Refer to the listed KB article for details. |\n| Discovery PRB1665907 [KB1369852](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB1369852) | GetMIDInfo (CloudServiceAccountInfoUtil) hangs in an infinite loop until it exhausts node memory | When a GetMIDInfo scripted SOAP service responds to GetCloudService AccountInfo requests from a MID Server, and if any of the cloud\\_service\\_account\\_view records are missing a sa\\_account\\_id value, it hangs in a loop. It  allocates memory until the instance application node goes out-of-memory and restarts. Cloud\\_service\\_ account\\_view is a database view that takes its sa\\_account\\_id field from the account\\_id field of table cmdb\\_ci\\_cloud\\_  service\\_account. That field is mandatory in the dictionary, but it is still possible to have an empty value. This will also prevent Cloud Discovery from working (like AWS), as the cloud service accounts won't be synched to  the MID Server. | Refer to the listed KB article for details. |\n| Document Services PRB1615472 | Export to PDF and Configure > Form Layout can be very slow if any table in the hierarchy has many references to it (as sys\\_dictionary records) | For example, when exporting a PDF from a 'cmdb\\_ci\\_win\\_server' record with five sections, the user queries sys\\_dictionary with 'active=true^referenceIN cmdb\\_ci\\_win\\_server, cmdb\\_ci\\_server, cmdb\\_ci\\_computer,  cmdb\\_ci\\_hardware, cmdb\\_ci, cmdb' five times. If the query returns ~15k results each time due to many references to cmdb\\_ci tables, and the user repeats that query five times, the time adds up quickly. |  |\n| Email Notifications PRB1660631 [KB1307652](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB1307652) | The SMTP sender job is stuck for over 60 minutes | Outgoing emails are stuck in the SMTP process for more than 60 minutes. | Refer to the listed KB article for details. |\n| Encryption PRB1547401 [KB1208278](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB1208278) | Gateway database pool can't be instantiated at node startup | It succeeds after SecondaryDatabaseSweeperJob runs. | Refer to the listed KB article for details. |\n| Event Management PRB1628223 [KB1212632](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB1212632) | Clone Excludes\/Preservers are missing for Event Management MID Server extension contexts (ecc\\_agent\\_ext\\_context\\_event and eif\\_listener\\_context) |  | Refer to the listed KB article for details. |\n| Field Administration PRB1360299 [KB0951799](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB0951799) | The Image field doesn't clear the sys\\_id of the attached image when the user clicks 'Delete' | A field holds the sys\\_id of the sys\\_attachment record, even if it has already been deleted from the instance. | Refer to the listed KB article for details. |\n| Field Administration PRB1559416 | The g\\_form.clearValue() for date field does not work in the UI Policy on that same field | The value is not saved but it still visually persists in the input. | 1. Log in to an instance. 2. Navigate to an incident record and ensure there is a test date field that is visible in both the default view and the Agent Workspace view. 3. Create a UI policy with conditions where the test date field is set to a date before today and enter the following script in the script section:function onCondition() { g\\_form.clearValue('u\\_testfield');}. 4. Test the UI policy by entering any past time in the test field on an incident record in both the platform and the Agent Workspace.  Expected behavior:The value of the test field clears in both the platform and the Agent Workspace, it is set to a value that matches the condition in the UI policy. Actual behavior: The value of the test field clears in the platform it is set to a value that matches the condition in the UI policy, but doesn't clear the field in Agent Workspace. |\n| Field Administration PRB1613998 | In Agent Workspace, the Search on Translated Text reference field only works in ENU | If a user exposes a reference field on a form in Agent Workspace that references a Translated Text field on the destination table, record searching only works for ENU. Specifically, only when  entering text into it to identify a potential record to select. | 1. In Tokyo, install a French language plugin. 2. Create a reference field on an incident form with these details: \t1. Table: Incident [incident] \t2. Type: Reference \t3. Column Label: Announcement \t4. Column name: announcement \t5. Reference Specification: Reference = Announcement \t6. Use reference qualifier: Simple 3. Expose the Announcement field on the 'Workspace' and 'Default view' views for an incident form. 4. With the language set to ENU, navigate to an open incident record in the ITSM backend. 5. Click the Announcement field. \t* The ENU translations of announcement records are visible. 6. Begin typing in the field to filter the records in the list. 7. Verify the list of potential records are filtered as expected. 8. Open the same incident record in Agent Workspace and repeat steps 5 and 6. 9. Verify the list of potential records are filtered as expected. 10. Change the language to French and repeat steps 5-7.  In Agent Workspace, the list is blank and users receive a 'R\u00e9sultats introuvables' message. |\n| Field Administration PRB1627737 | In HR Agent Workspace, the description field does not show up when the user switches to a different tab before the tab opens fully or when multiple tabs open |  | 1. Check that the HR case has the field 'rich\\_description' of type 'HTML'. 2. Create an onLoad client script to set the 'rich\\_description' field to read-only. 3. Open Agent Workspace for HR Case Management. 4. Navigate to the lists of HR cases. 5....",
        "QUERY": " 'currency changes to USD in a read-only currency field though the locale is set to a different currency'",
        "GROUND_TRUTH": "no answer found"
    },
    {
        "id": 60,
        "CONTEXT": "\"## \n\nCreate a many-to-many relationship\n[Building applications](..\/..\/..\/build\/custom-application\/concept\/build-applications.html \"Learn how to become an application developer using Now Platform low-code tools. Start with what you know and use a library of reusable components and published applications to modernize your legacy processes.\") > [Planning your application](..\/..\/..\/build\/custom-application\/concept\/planning-applications.html \"The application development process starts with planning. Consider how the application will work, who will use it, and how it improves user experience.\") > \n# Create a many-to-many relationship\n\n\nMany-to-many relationships allow a list to point to a list of entries, rather than to\n single field.\n\n\n\nFor example, if a knowledge base article points to a list of related configuration\n items, it uses a related list. Not all lists can be used as related lists, however.\n For a list to be related to another list, a many-to-many relationship that refers to\n both tables must exist.\n\n\nThe Many to Many Definitions `[sys_m2m]` table allows administrators to create custom many-to-many relationships.\n\n\nMany-to-Many tables are not considered custom tables and don't count toward your custom table allotment.\n\n\n\n1. In the navigation filter, enter sys\\_m2m.list.\n2. Click New.\n3. In the From table field, specify a parent table.\n4. In the To table field, specify a child table.\nThe Many-to-Many form automatically populates the other fields with suggested values.\n5. **Optional:** \nEdit other field values, if appropriate. Many-to-Many table names cannot exceed\n 30 characters.\n\n\n\n\n* **[Reference default many-to-many relationships](..\/..\/..\/administer\/table-administration\/reference\/r_RefDefaultManyToManyRels.html)**  \n\nSome many-to-many relationships are defined by default.\n\n\n\n**Parent Topic:** [Table administration](..\/..\/..\/administer\/table-administration\/concept\/c_TableAdministration.html \"A table is a collection of records in the database. Each record corresponds to a row in a table, and each field on a record corresponds to a column on that table.\")\n\n**Related reference**  \n\n* [Reference default many-to-many relationships](..\/reference\/r_RefDefaultManyToManyRels.html \"Some many-to-many relationships are defined by default.\")\"\n\n",
        "QUERY": " 'Creating a Many-to-Many Relationship'",
        "GROUND_TRUTH": "To create a many-to-many relationship, you need to use the Many to Many Definitions `[sys_m2m]` table. This table allows administrators to create custom many-to-many relationships. Here are the steps to follow:\n\n1. In the navigation filter, enter sys\\_m2m.list.\n2. Click New.\n3. In the From table field, specify a parent table.\n4. In the To table field, specify a child table.\n\nThe Many-to-Many form will automatically populate the other fields with suggested values. Optionally, you can edit other field values if appropriate. However, keep in mind that Many-to-Many table names cannot exceed 30 characters.\n\nMany-to-many relationships allow a list to point to a list of entries, rather than to a single field. For example, if a knowledge base article points to a list of related configuration items, it uses a related list. For a list to be related to another list, a many-to-many relationship that refers to both tables must exist."
    },
    {
        "id": 61,
        "CONTEXT": "\"## \n\n### Description\n\n**CMDB Health Dashboard**\n\n\nLooking for an alternative to filtering by CI Status Field.   \nThe Duplicate health metric report is\u00a0showing duplicates from CI's with Status as\u00a0Retired.   \nWould like to filter out any CI's that do not have a Status of Retired from appearing in the **CMDB Health Duplicate** report\n\n### Resolution\n\nThere is a feature called **\"identification inclusion rules**\", With this, it can narrow the scope of CIs that are included in the identification process. The same identification rules are automatically used by the CMDB Health duplicate metric also. For example, this scenario user can include CIs whose state is not \"Retired\", so Retired CI's are not considered during the duplicate CI identification process and during metrics as well.\n\n\nPlease keep in mind that as these rules are considered during duplicate identification process, duplicate CI's may get created even if there is a Retired CI in the system.\n\n\nHere is the doc link which I am referring above <https:\/\/docs.servicenow.com\/csh?topicname=create-id-inclusion-rule.html&version=latest>\n\n\n**Steps to be taken :**\n\n\n1. Add the Identification inclusion rule:\n\n```\n\/nav_to.do?uri=cmdb_ie_active_config.do?sys_id=-1\n```\n2. Remove the records that are created in **cmdb\\_health\\_result and reconcile\\_duplicate\\_task table.**   \n**Note:** Please use table cleaner to clean the records <https:\/\/docs.servicenow.com\/csh?topicname=c_DataManagement.html&version=latest>.Refer to table cleaner sections\n3. Make the CI **discovery\\_source** to \"\" or anything else other than \"Duplicate\" and \"unknown\" and the Identification engine should run against the identification rule and filter out the retired CIs from the health result table.\n4. Re-run the **Health Job** to re-populate the data\n5. Review the results once the job is completed\n### Additional Information\n\nTo hide the CIs with retired status from other Health Metric, please refer to docs below  \n  \n<https:\/\/docs.servicenow.com\/csh?topicname=create-health-inclusion-rule.html&version=latest>\"\n\n",
        "QUERY": " 'Could not retrieve AIS Partition Health Status'",
        "GROUND_TRUTH": "no answer found"
    },
    {
        "id": 62,
        "CONTEXT": "\"## \n\n## Description\n\nSyntax highlighting not honoured with KBs with Code snippets in Content view of a Knowledge article, both in platform and service portal view\n\n## Steps to Reproduce\n\n 1. Add 'codesample' in the property: glide.ui.html.editor.v4.toolbar.valid.buttons to make it one of the valid button  \n2. Add 'codesample' in the property glide.ui.html.editor.v4.toolbar.line1 to add in the toolbar  \n3. Open a kb\\_knowledge record  \n4. In the HTML field 'Article body', navigate to the toolbars of TinyMCE. Notice a new button is available as {;}. Click on this  \n5. It will open an editor. Select any language. I chose javascript, and wrote below code:  \nvar test ='testing';  \nconsole.log(test);  \n6. Save the record. Note that var and log keywords are showing in color  \n7. Open the record in SP with the below URL  \nsp?id=kb\\_article&sys\\_id=<SYS\\_ID OF THE KB>  \n  \nOR   \nOpen in platforrm  \n\/kb\\_view?sys\\_id=<SYS\\_ID OF THE KB>  \n  \n8. Note that the codes are not showing color (no highlight for syntax)  \n  \nrefer the screenshots\n\n## Workaround\n\nThis is expected behavior and by design in all currently supported releases.\n\n  \n**Related Problem: PRB1434350**\"\n\n",
        "QUERY": " 'Code not highlighted for syntax in KBs with code snippet'",
        "GROUND_TRUTH": "The issue of code not being highlighted for syntax in KBs with code snippets is a known issue and is by design in all currently supported releases. This issue can be reproduced by adding 'codesample' in the properties glide.ui.html.editor.v4.toolbar.valid.buttons and glide.ui.html.editor.v4.toolbar.line1. After adding a code snippet in a kb_knowledge record and saving it, the keywords in the code will show in color. However, when the record is opened in the service portal or platform view, the code does not show color highlighting for syntax. This is the expected behavior and there is no workaround for this issue as of now."
    },
    {
        "id": 63,
        "CONTEXT": "\"## \n\n## Description\n\n\n\nOn an instance that has the Data Archiving plugin installed and in use for the CMDB tables, the CMDB Identification and Reconcilliation engine can throws a huge number of errors like this in the syslog table:\u00a0\n\u00a0\nCompactRelation: failed to get details of CI bc4fe864135b9bc0574c75276144b09f: no thrown error | syslog | 000009c213579700f6a7f107d144b0b3 | system | 2018-07-15 21:36:31 |\u00a0 | 2 | com.glide.ui.ServletErrorListener | service\\_cache\\_mgr\u00a0\n\u00a0\nAnd the app node localhost log:\n\u00a0\n2018-07-26 10:35:54 (605) Default-thread-3 AAD39C29136797C04DD2F027D144B0B4 #10927362 \/ngbsmprocessor.do Parameters -------------------------\nactionType=loadBasic\nmapScriptID=\nserviceMode=false\ncmd=get\nid=8f59d4dce19f71c424893534899bf84e\ncacheKill=1532626554567\n2018-07-26 10:35:55 (694) Default-thread-3 AAD39C29136797C04DD2F027D144B0B4 SEVERE \\*\\*\\* ERROR \\*\\*\\* service\\_cache\\_mgr : CompactRelation: failed to get details of CI 0008b8644fe49ec06433ab99f110c7cf\n2018-07-26 10:35:55 (696) Default-thread-3 AAD39C29136797C04DD2F027D144B0B4 SEVERE \\*\\*\\* ERROR \\*\\*\\* service\\_cache\\_mgr : CompactRelation: failed to get details of CI 001ae60913d2ba006203b2d96144b0ad\n2018-07-26 10:35:55 (698) Default-thread-3 AAD39C29136797C04DD2F027D144B0B4 SEVERE \\*\\*\\* ERROR \\*\\*\\* service\\_cache\\_mgr : CompactRelation: failed to get details of CI 009d6cd81337fac0b23ffea2e144b057\n2018-07-26 10:35:55 (700) Default-thread-3 AAD39C29136797C04DD2F027D144B0B4 SEVERE \\*\\*\\* ERROR \\*\\*\\* service\\_cache\\_mgr : CompactRelation: failed to get details of CI 00fc04514f126a806433ab99f110c70d\n.......\n\u00a0\nThis code runs when the Dependency View ngbsmprocessor needs to collect details of all related CIs.\n\u00a0\nThis causes 2 main problems:\n* ServiceCacheManager.getInboundRelations and .getOutboundRelations won't work properly, even though the data is there in the instance.\n* On an instance using the CMDB and discovery sources heavily, this can add such a large number of entries to syslog tables to cause disk space and performance issues.\n\n\n\u00a0\nUnlike the CI Form, which realizes the CI is archived and redirects to the archived record,\u00a0 this code does not check to see if a CI record is archived and instead just throws an error.\n\u00a0\nNeither the Archiving nor CMDB plugins have any feature for 'cascade archiving' related records such as cmdb\\_rel\\_ci when a CI is archived that would prevent a relationship record remaining after a CI is archived.\n\n## Steps to Reproduce\n\n\n\n\u00a0\n\n\nOn a clean Kingston instance with demo data:\n1\/ Install 'Data Archiving' plugin\n2\/ For the purposes of this test, create an archive rule for the Linux Server table to archive any\u00a0 CIs with condition: name starts with lnux, which will cover 'lnux100' and 'lnux101' CIs\n3\/ Activae the rule, then manually run the 'Archive' scheduled job to avoid waiting\n4\/ Open the form for the \"Client Services\" business service CI. This has relationships with those server\n5\/ Open the Dependency view (BSM Map) from the button ion the CI relations section (which you'll notice doesn't show the archived Linux servers)\n6\/ Check the syslog entries, and you will see a pair of errors, one for each linux server\n\u00a0\n2018-07-27 18:18:28 Error service\\_cache\\_mgr : CompactRelation: failed to get details of CI 53958ff0c0a801640171ec76aa0c8f86: no thrown error com.glide.ui.ServletErrorListener\n2018-07-27 18:18:28 Error service\\_cache\\_mgr : CompactRelation: failed to get details of CI 539747cac0a801640163e60735fbbf6e: no thrown error com.glide.ui.ServletErrorListener\u00a0\n\n## Workaround\n\n\n\nFor Data Archiving there is an option to archive related records and whenever a rule is created for CMDB CI archival, the corresponding relations should also be archived.\n\n\nHowever,\u00a0cmdb\\_rel\\_ci (CMDB relationship) table has two references to cmdb\\_ci table (parent and child). When creating a \"Archive Related Records\" rule, you can only select 1 of those fields, and therefore **this workaround is only half a workaround, but is better than nothing**.\u00a0\n\n\n1. In your CMDB related archive rules, add a new Archive Related Records entry\n2. Select \"Relationships\" in drop down, and it will pre-select \"parent\" element and there is no way\u00a0to add the \"child\" element as well.\n\n\n![](sys_attachment.do?sys_id=d7852c62db02b450e515c223059619bb)\n\n\n\u00a0\n\n\n\n  \n**Related Problem: PRB1296280**\"\n\n",
        "QUERY": " 'CMDB Identification and Reconciliation engine throwing CompactRelation: failed to get details of CI error'",
        "GROUND_TRUTH": "The issue of the CMDB Identification and Reconciliation engine throwing a CompactRelation error occurs on an instance that has the Data Archiving plugin installed and in use for the CMDB tables. The error is thrown when the Dependency View ngbsmprocessor needs to collect details of all related CIs. This can cause two main problems: ServiceCacheManager.getInboundRelations and .getOutboundRelations won't work properly, and on an instance using the CMDB and discovery sources heavily, this can add a large number of entries to syslog tables causing disk space and performance issues.\n\nThe error occurs because the code does not check to see if a CI record is archived and instead just throws an error. Neither the Archiving nor CMDB plugins have any feature for 'cascade archiving' related records such as cmdb_rel_ci when a CI is archived that would prevent a relationship record remaining after a CI is archived.\n\nA workaround for this issue is to archive related records whenever a rule is created for CMDB CI archival, the corresponding relations should also be archived. However, this is only a partial workaround as you can only select one of the fields (parent or child) when creating an \"Archive Related Records\" rule."
    },
    {
        "id": 64,
        "CONTEXT": "\"## \n\n## Description\n\nCMDB Baseline creation job causing app node out of memory when a CI has a huge number of tasks referencing it.  \nThe referencing records might be Tasks, or other table records with a reference field to Configuration Item.  \nWhen the baseline creation gets to the CI with loads of relations, it will cause the app node to go OOM and restart, and usually re-run the same job again, and again every restart.\n\n\nThe symptom is very poor performance for any users logged into that app node, and then the restart may cause any other transactions running at the time.  \nThe new baseline being created will not finish. Due to re-running many times there will be duplicate cmdb\\_baseline\\_entry records for the same CI sys\\_ids.\n\n## Steps to Reproduce\n\n 1. Create a lot of task records, all referencing the same Configuration Item. e.g. 1 million\n2. Create a new CMDB baseline for that CI. You could add just that CI sys\\_id in the baseline filter before saving.\n3. Saving the cmdb\\_baseline record triggers the \"SNC Create Baseline\" business rule, which creates a \"ASYNC: Script Job\" scheduled job using SNC.CMDBUtil.baselineSchedule for the new record.\n4. watch memory usage.\n5. take a heap dump, and you will see huge amount of memory used for the related task data.\n\n\nOne example seen was for 3 million tasks referring to a single application CI. That caused >1.8GB memory to be used by the job. \u00a0In another, lots of \"conflict\" table records were referencing the same CI.\n\n\nThe job name is just \"ASYNC: Script Job\", but can be identified as CMDB Baseline creation from the job context script \"new SncBaselineCMDB\" and \".create()\"\n\n\n2022-10-07 03:37:08 (997) worker.7 worker.7 txid=70694383db92 \\*\\*\\* Start Background transaction - system, user: system  \n2022-10-07 03:37:09 (004) worker.7 worker.7 txid=70694383db92 Starting: ASYNC: Script Job.<id>, Trigger Type: Once, Priority: 100, Upgrade Safe: false, Repeat:  \n2022-10-07 03:37:09 (004) worker.7 worker.7 txid=70694383db92 Name: ASYNC: Script Job  \nJob Context:  \n#Fri Sep 30 18:30:00 PDT 2022  \nScript:  \n**var base = new SncBaselineCMDB(\"<baseline name>\"); base.create();base.notify(\"<sys\\_id>\");**\n\n\nStack trace a the time of high memory usage will look something like:\n\n\nglide.scheduler.worker.7  \nat java.util.Arrays.copyOfRange([BII)[B (Arrays.java:4030)  \nat java.lang.StringCoding.decodeUTF8([BIIZ)Ljava\/lang\/StringCoding$Result; (StringCoding.java:723)  \nat java.lang.StringCoding.decode(Ljava\/nio\/charset\/Charset;[BII)Ljava\/lang\/StringCoding$Result; (StringCoding.java:257)  \nat java.lang.String.<init>([BIILjava\/nio\/charset\/Charset;)V (String.java:507)  \nat org.mariadb.jdbc.internal.com.read.resultset.rowprotocol.TextRowProtocol.getInternalString(Lorg\/mariadb\/jdbc\/internal\/com\/read\/resultset\/ColumnInformation;Ljava\/util\/Calendar;Ljava\/util\/TimeZone;)Ljava\/lang\/String; (TextRowProtocol.java:250)  \nat org.mariadb.jdbc.internal.com.read.resultset.SelectResultSet.getString(I)Ljava\/lang\/String; (SelectResultSet.java:1010)  \nat com.glide.db.meta.StorageUtils.getString(Ljava\/sql\/ResultSetMetaData;ILjava\/sql\/ResultSet;)Ljava\/lang\/String; (StorageUtils.java:147)  \nat com.glide.db.meta.StorageUtils.getObject(Ljava\/sql\/ResultSetMetaData;ILjava\/sql\/ResultSet;)Ljava\/lang\/Object; (StorageUtils.java:58)  \nat com.glide.db.meta.RowStorage.<init>(Lcom\/glide\/db\/PositionMap;Ljava\/sql\/ResultSet;)V (RowStorage.java:40)  \nat com.glide.db.meta.RowFactory.store(Ljava\/sql\/ResultSet;Lcom\/glide\/db\/PositionMap;)Lcom\/glide\/db\/meta\/IRow; (RowFactory.java:62)  \nat com.glide.data.access.internal.CachedTable.query()V (CachedTable.java:105)  \nat com.snc.cmdb.**BaselineCMDB.getRelatedRecords**(Lcom\/glide\/script\/GlideRecord;Ljava\/lang\/String;)Lcom\/glide\/data\/access\/ITable; (**BaselineCMDB.java:347**)  \nat com.snc.cmdb.**BaselineCMDB.getRelation**(Ljava\/util\/HashMap;Ljava\/lang\/String;Lcom\/glide\/script\/GlideRecord;Z)V (BaselineCMDB.java:581)  \nat com.snc.cmdb.BaselineCMDB.getRelations(Lcom\/glide\/script\/GlideRecord;Z)Ljava\/util\/HashMap; (BaselineCMDB.java:565)  \nat com.snc.cmdb.BaselineCMDB.addRelations(Lcom\/glide\/script\/GlideRecord;)V (BaselineCMDB.java:279)  \nat com.snc.cmdb.BaselineCMDB.setBaseLine(Lcom\/glide\/script\/GlideRecord;)V (BaselineCMDB.java:265)  \nat com.snc.cmdb.BaselineCMDB.processCIs()V (BaselineCMDB.java:190)  \nat com.snc.cmdb.BaselineCMDB.create()V (BaselineCMDB.java:152)  \n...\n\n\nAt the point of the OOM restart, the app node wrapper log will show something like:\n\n\nINFO | jvm 24 | 2022\/10\/07 00:40:06.603 | #  \nINFO | jvm 24 | 2022\/10\/07 00:40:06.604 | # java.lang.OutOfMemoryError: Java heap space  \nINFO | jvm 24 | 2022\/10\/07 00:40:06.604 | # -XX:OnOutOfMemoryError=\"..\/scripts\/kill\\_jvm\\_only.sh\"  \nINFO | jvm 24 | 2022\/10\/07 00:40:06.604 | # Executing \/bin\/sh -c \"..\/scripts\/kill\\_jvm\\_only.sh\"...  \nERROR | wrapper | 2022\/10\/07 00:40:51.457 | JVM exited unexpectedly.\n\n## Workaround\n\nThis problem is currently under review. You can contact [ServiceNow Technical Support](https:\/\/www.servicenow.com\/support\/contact-support.html) or subscribe to this Known Error article by selecting **Subscribe** in the options at the top of this article to be notified when more information becomes available.\n\n\nTo avoid this, try to identify the CI that has loads of relationships, and exclude that CI from the filter of the cmdb\\_baseline record.\n\n\nFrom a heap dump, it is possible to identify the table and key field values from the RowStorage objects, within the huge arrays of these objects that take up most of the memory. If you can find an example of one of those records, you can see which CI it links to.\n\n\nThe cmdb\\_baseline record may have been inserted manually from a form, but it is also common to have a scheduled job which does the cmdb\\_baseline insert periodically to create new baselines.\u00a0 Search in the script field of the sysauto\\_script table for \"cmdb\\_baseline\" to find it. Deactivating this job, and killing any existing \"ASYNC: Script Job\" transactions that are running \"com.snc.cmdb.BaselineCMDB.getRelatedRecords\" in the stack trace will provide relief.\n\n  \n**Related Problem: PRB1611377**\"\n\n",
        "QUERY": " 'cmdb baseline creation job causing app node out of memory when a ci has a huge number of records referencing it'",
        "GROUND_TRUTH": "The CMDB baseline creation job can cause the app node to run out of memory when a Configuration Item (CI) has a large number of records referencing it. This happens when the baseline creation reaches a CI with a large number of relations, leading to poor performance for any users logged into that app node. The app node may restart and re-run the same job repeatedly, leading to duplicate cmdb_baseline_entry records for the same CI sys_ids. \n\nTo reproduce this issue, you can create a large number of task records all referencing the same CI, then create a new CMDB baseline for that CI. Saving the cmdb_baseline record triggers the \"SNC Create Baseline\" business rule, which creates a \"ASYNC: Script Job\" scheduled job using SNC.CMDBUtil.baselineSchedule for the new record. Monitoring the memory usage and taking a heap dump will show a large amount of memory used for the related task data.\n\nAs a workaround, you can identify the CI that has a large number of relationships and exclude that CI from the filter of the cmdb_baseline record. You can also deactivate the job that inserts the cmdb_baseline record and kill any existing \"ASYNC: Script Job\" transactions that are running \"com.snc.cmdb.BaselineCMDB.getRelatedRecords\" in the stack trace to provide relief."
    },
    {
        "id": 65,
        "CONTEXT": "\"## \n\n## Description\n\nTraditionally MID server selection is based on:  \n> Supported Applications  \n> IP Range  \n> Capabilities  \n  \nWe have a Supported Application option that's called \"Cloud Management\".  \nHowever Cloud Provisioning does not support this, but only respects \"Capabilities\".  \n  \n\n\n## Steps to Reproduce\n\n > Configure a single MID server on an instance, with Supported Applications \"Discovery\" only, and Capabilities \"All\"  \n> Launch a stack from Cloud User Portal  \n> Check ecc\\_queue table, the MID server is used to run APIProxyProbe  \n> Provision works successfully even though the MID server doesn't have Supported Application \"Cloud Management\"\n\n## Workaround\n\nImport and commit attached update set.\n\n  \n**Related Problem: PRB1641341**\"\n\n",
        "QUERY": " 'Cloud provisioning works though the MID server doesn't have Cloud management'",
        "GROUND_TRUTH": "According to KB12345, the MID server selection is traditionally based on Supported Applications, IP Range, and Capabilities. However, Cloud Provisioning does not support the \"Cloud Management\" Supported Application, but only respects \"Capabilities\". This means that even if a MID server doesn't have the Supported Application \"Cloud Management\", it can still successfully provision because it respects the \"Capabilities\". This is demonstrated in the steps to reproduce the issue, where a single MID server is configured with Supported Applications \"Discovery\" only, and Capabilities \"All\". When a stack is launched from the Cloud User Portal, the MID server is used to run APIProxyProbe and the provision works successfully, despite the lack of \"Cloud Management\" support."
    },
    {
        "id": 66,
        "CONTEXT": "\"## \n\n## Description\n\nWhen we use list filter for any field that is using auto numbering (e.g. = javascript:global.getNextObjNumberPadded()); and then click New, we prefill the field's value with list filter's value.\n\n## Steps to Reproduce\n\n Pre-req;   \n- Create a field (i.e. u\\_testfield) on any table (i.e. incident)  \n- Set the 'Use dynamic default' to true  \n- Set the 'Dynamic default value' to 'Get Next Padded Number'  \n- Create a record on the table (incident) and fill the created field (u\\_testfield). i.e. ASD001  \n  \n1. Go to the list view of the table (incident.list)  \n2. Filter the list according the created field. (\/incident\\_list.do?sysparm\\_query=u\\_testfield%3DASD001)  \nor select 'Show matching' on the field's value.  \n3. Click New  \n  \nActual behavior:  \nObserve that the field (u\\_testfield) is prefilled with the same number\/string (ASD001) you selected.  \nExpected behavior:  \nThe field should use the next sequence unique number\/string instead of the prefilled value\n\n## Workaround\n\nAdd the following attribute in the sys\\_dictionary record of the required field. \"ignore\\_filter\\_on\\_new=true\"\n\n  \n**Related Problem: PRB1537504**\"\n\n",
        "QUERY": " 'Auto numbering field is not working properly when combined with pre-filled value from List filter'",
        "GROUND_TRUTH": "The issue of the auto-numbering field not working properly when combined with a pre-filled value from a list filter is due to the system prefilling the field's value with the list filter's value. This happens when you use a list filter for any field that uses auto numbering and then click 'New'. The expected behavior is that the field should use the next sequence unique number\/string instead of the prefilled value. To resolve this issue, you can add the attribute \"ignore_filter_on_new=true\" in the sys_dictionary record of the required field. This workaround will ensure that the field uses the next sequence unique number\/string instead of the prefilled value."
    },
    {
        "id": 67,
        "CONTEXT": "\"## \n\n## Description\n\nPost clone, application nodes may not have the correct scheduled job configuration.\n\n\nSpecifically jobs with parent record set to 'Active Nodes' fail to be created per application node preventing them from running\n\n\nThis will impact several areas of the platform including event processing\n\n## Steps to Reproduce\n\n Review a recently cloned instance for which application nodes have not restarted post clone\n\n## Workaround\n\nIf an instance has been impacted then no jobs set to 'Active Nodes' will run. You have two options to fix this issue:\n\n\n1. Restart the instance - This can be requested from ServiceNow by raising a new Case via the NowSupport Portal\n2. Manually trigger the creation of the jobs. This is a straight forward process that can be actioned as follows:\n\n\n1. 1. Navigate to sys\\_trigger.list\n\t2. Filter for all jobs where 'System ID' = 'Active Nodes'\n\t3. For each record perform the following:  \n\t\n\t\t1. Open the record\n\t\t2. Update the 'Next Action' time and increase by 1 second\n\t\t3. Save the record\n\n\nThis will generate all the child records for these jobs and restart processing of events and other functions\n\n  \n**Related Problem: PRB1576973**\"\n\n",
        "QUERY": " 'Application nodes are not restarting post the clone'",
        "GROUND_TRUTH": "According to KB12345, after cloning, application nodes may not have the correct scheduled job configuration. Specifically, jobs with the parent record set to 'Active Nodes' fail to be created per application node, preventing them from running. This impacts several areas of the platform, including event processing. If an instance has been impacted, no jobs set to 'Active Nodes' will run. There are two options to fix this issue:\n\n1. Restart the instance - This can be requested from ServiceNow by raising a new Case via the NowSupport Portal.\n2. Manually trigger the creation of the jobs. This process can be actioned as follows:\n   - Navigate to sys_trigger.list\n   - Filter for all jobs where 'System ID' = 'Active Nodes'\n   - For each record perform the following:  \n     - Open the record\n     - Update the 'Next Action' time and increase by 1 second\n     - Save the record\n\nThis will generate all the child records for these jobs and restart processing of events and other functions."
    },
    {
        "id": 68,
        "CONTEXT": "\"## \n\n## Description\n\nWhen adding any incident to the backlog from the triage board, the story is created as expected. However, the 'Agile Story' field is blank in respective story.\n\n\nThe 'Agile Story' field is populated correctly when the story is created from:\n\n\n* **Agile Board > Create story**\n* or\n* rm\\_story table (rm\\_list.do > New)\n## Steps to Reproduce\n\n On an affected instance:\n\n\n1. Create an Agile Board.\n2. Create a Triage Board on incidents in that Agile board.\n3. Add any Incident to Backlog.\n4. Observe that New story created for that incident.\n5. Observe that the new story's Agile field is empty.\n## Workaround\n\nThis is working as expected for the following reason:\n\n\nFor Task derivatives like incidents and problems, when they get added to the backlog, we populate the agile story field in the incident or problems table, but for wrapper story which gets created for the incident which was added in the backlog we only set the original task field pointing to the incident, the agile story is not populated for wrapper story. This is as per design.\n\n  \n**Related Problem: PRB1457284**\"\n\n",
        "QUERY": " 'Agile story field is empty in the new story created'",
        "GROUND_TRUTH": "When an incident is added to the backlog from the triage board, a new story is created as expected. However, the 'Agile Story' field in the new story is left blank. This is because for task derivatives like incidents and problems, when they are added to the backlog, the agile story field is populated in the incident or problems table, but not for the wrapper story that gets created for the incident added to the backlog. In this case, only the original task field pointing to the incident is set, the agile story is not populated for the wrapper story. This is as per the design and is not a bug or error."
    },
    {
        "id": 69,
        "CONTEXT": "\"## \n\n## Description\n\nIn a sn\\_customerservice\\_case record, if additional comments are set from templates, the value has to be posted using POST button and saving the form does not always save the additional comment value.\n\n## Steps to Reproduce\n\n 1. Install the Customer Service - com.sn\\_customerservice plugin.\n2. Go to the following URL and open any existing case.\n\n\nnav\\_to.do?uri=sn\\_customerservice\\_case\\_list.do\n3. Right-click the form header and choose View > Case.\n4. Toggle the template bar in the context menu of the form and create a new template with the following values:\n\n\n\t* Short description: test\n\t* Additional Comments: some random text\n5. Apply this template on the sn\\_customerservice\\_case form and click Save.\n\n\nAlthough you can see its value in Additional comments, when the form is saved, the value is not always shown in the activities list. (Try clicking POST to save the value.)\n## Workaround\n\nThis issue is under review. To receive notifications when more information is available, subscribe to this Known Error article by clicking the **Subscribe** button at the top right of the article. If you are able to upgrade, review the **Fixed In** field to determine whether any versions have a permanent fix.\n\n  \n**Related Problem: PRB1206918**\"\n\n",
        "QUERY": " 'Additional comments are not saved in sn_customerservice_case record'",
        "GROUND_TRUTH": "According to the KB12345 article, in a sn_customerservice_case record, if additional comments are set from templates, the value has to be posted using the POST button. Simply saving the form does not always save the additional comment value. This issue can be reproduced by installing the Customer Service - com.sn_customerservice plugin, opening any existing case, creating a new template with a short description and additional comments, and applying this template on the sn_customerservice_case form. After clicking Save, the value in Additional comments is not always shown in the activities list. The issue is currently under review. To receive notifications when more information is available, you can subscribe to this Known Error article."
    },
    {
        "id": 70,
        "CONTEXT": "\"## It is not possible to archive all cmdb_rel_ci record when CI records are Archived  leading to Compac\n\n## Description\n\nOn an instance that has the Data Archiving plugin installed and in use for the CMDB tables  the CMDB Identification and Reconcilliation engine can throws a huge number of errors like this in the syslog table:\nCompactRelation: failed to get details of CI bc4fe864135b9bc0574c75276144b09f: no thrown error \\| syslog \\| 000009c213579700f6a7f107d144b0b3 \\| system \\| 2018-07-15 21:36:31 \\| \\| 2 \\| com.glide.ui.ServletErrorListener \\| service_cache_mgr\nAnd the app node localhost log:\n2018-07-26 10:35:54 (605) Default-thread-3 AAD39C29136797C04DD2F027D144B0B4 #10927362 \/ngbsmprocessor.do Parameters -------------------------\nactionType=loadBasic\nmapScriptID=\nserviceMode=false\ncmd=get\nid=8f59d4dce19f71c424893534899bf84e\ncacheKill=1532626554567\n2018-07-26 10:35:55 (694) Default-thread-3 AAD39C29136797C04DD2F027D144B0B4 SEVERE \\*\\*\\* ERROR \\*\\*\\* service_cache_mgr : CompactRelation: failed to get details of CI 0008b8644fe49ec06433ab99f110c7cf\n2018-07-26 10:35:55 (696) Default-thread-3 AAD39C29136797C04DD2F027D144B0B4 SEVERE \\*\\*\\* ERROR \\*\\*\\* service_cache_mgr : CompactRelation: failed to get details of CI 001ae60913d2ba006203b2d96144b0ad\n2018-07-26 10:35:55 (698) Default-thread-3 AAD39C29136797C04DD2F027D144B0B4 SEVERE \\*\\*\\* ERROR \\*\\*\\* service_cache_mgr : CompactRelation: failed to get details of CI 009d6cd81337fac0b23ffea2e144b057\n2018-07-26 10:35:55 (700) Default-thread-3 AAD39C29136797C04DD2F027D144B0B4 SEVERE \\*\\*\\* ERROR \\*\\*\\* service_cache_mgr : CompactRelation: failed to get details of CI 00fc04514f126a806433ab99f110c70d\n.......\nThis code runs when the Dependency View ngbsmprocessor needs to collect details of all related CIs.\nThis causes 2 main problems:\n\n* ServiceCacheManager.getInboundRelations and .getOutboundRelations won't work properly  even though the data is there in the instance.\n* On an instance using the CMDB and discovery sources heavily  this can add such a large number of entries to syslog tables to cause disk space and performance issues.\n\nUnlike the CI Form  which realizes the CI is archived and redirects to the archived record  this code does not check to see if a CI record is archived and instead just throws an error.\nNeither the Archiving nor CMDB plugins have any feature for 'cascade archiving' related records such as cmdb_rel_ci when a CI is archived that would prevent a relationship record remaining after a CI is archived.\n\n## Steps to Reproduce\n\nOn a clean Kingston instance with demo data:\n1\/ Install 'Data Archiving' plugin\n2\/ For the purposes of this test  create an archive rule for the Linux Server table to archive any CIs with condition: name starts with lnux  which will cover 'lnux100' and 'lnux101' CIs\n3\/ Activae the rule  then manually run the 'Archive' scheduled job to avoid waiting\n4\/ Open the form for the \"Client Services\" business service CI. This has relationships with those server\n5\/ Open the Dependency view (BSM Map) from the button ion the CI relations section (which you'll notice doesn't show the archived Linux servers)\n6\/ Check the syslog entries  and you will see a pair of errors  one for each linux server\n2018-07-27 18:18:28 Error service_cache_mgr : CompactRelation: failed to get details of CI 53958ff0c0a801640171ec76aa0c8f86: no thrown error com.glide.ui.ServletErrorListener\n2018-07-27 18:18:28 Error service_cache_mgr : CompactRelation: failed to get details of CI 539747cac0a801640163e60735fbbf6e: no thrown error com.glide.ui.ServletErrorListener\n\n## Workaround\n\nFor Data Archiving there is an option to archive related records and whenever a rule is created for CMDB CI archival  the corresponding relations should also be archived.\n\nHowever  cmdb_rel_ci (CMDB relationship) table has two references to cmdb_ci table (parent and child). When creating a \"Archive Related Records\" rule  you can only select 1 of those fields  and therefore **this workaround is only half a workaround  but is better than nothing**.\n\n1. In your CMDB related archive rules  add a new Archive Related Records entry\n2. Select \"Relationships\" in drop down  and it will pre-select \"parent\" element and there is no way to add the \"child\" element as well.\n\n**Related Problem: PRB1296280**\n\n\n\n## Vancouver security and notable fixes\n\n The job 'CMDB Health Dashboard - Relationship Score Calculation' has a slow query                                                                                                                               | The query doesn't have any conditions nor defined range  and performs a full table scan. This causes a load on the CPU.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | 1. Clone an instance. 2. Run the 'CMDB Health Dashboard - Relationship Score Calculation' job. {#vancouver-security-notables__ol_f3m_zd1_2yb}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n| Configuration Management Database (CMDB) PRB1591705 [KB1169983](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB1169983) | When users are using query builder and querying for more items  the Save and Run buttons at the top disappear                                                                                                   | When users access query builder and query for a decent number of items  the page shifts down. TheSave and Run query buttons are hidden                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | Refer to the listed KB article for details.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n| Configuration Management Database (CMDB) PRB1611377 [KB1182003](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB1182003) | The 'CMDB Baseline creation' job causes an 'app node out of memory (OOM)' error when a configuration item (CI) has a large number of records referencing it                                                     | The referencing records might be tasks or other table records with a reference field to a CI. When the baseline creation gets to the CI with many relations  it causes the app node to have an OOM error and restart. It usually re-runs the same job again with every restart. The symptom is poor performance for any users logged into that app node. The restart may cause any other transactions running at the time to stop. The baseline being created doesn't finish. Due to re-running many times  there's duplicate cmdb_baseline_entry records for the same CI sys_ids.                                                                                                                  | 1. Create a lot of task records all referencing the same CI. 2. Create a CMDB baseline for that CI. * Saving the cmdb_baseline record triggers the 'SNC Create Baseline' business rule  which creates a 'ASYNC: Script Job' scheduled job using SNC.CMDBUtil.baselineSchedule for the new record. {#vancouver-security-notables__ul_w3m_zd1_2yb} 3. Observe the memory usage. 4. Take a heap dump. {#vancouver-security-notables__ol_v3m_zd1_2yb} There's a large amount of memory used for the related task data.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n| Configuration Management Database (CMDB) PRB1615879 [KB1218266](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB1218266) | CMDB_CI Index  added as part of PRB1519942  isn't added to the CMDB partition tables on upgrades                                                                                                                | On upgrading to San Diego or later  the index CMDB (install_status  sys_created_on) isn't added to the CMDB partition tables. This results in a UI and GlideRecord.query errors. This could prevent a user from accessing some CMDB and child table lists. This could also prevent some GlideRecord.query() methods from returning the expected results.                                                                                                                                                                                                                                                                                                                                            | Refer to the listed KB article for details.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n| Configuration Management Database (CMDB) PRB1627051 [KB1211804](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB1211804) | An encoded query isn't properly displayed on a condition builder when editing a policy                                                                                                                          | When using the CMDB data manager policies  a condition builder isn't displaying the selected class name. Filters aren't saved even after it shows that the record was saved successfully.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | Refer to the listed KB article for details.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n| Configuration Management Database (CMDB) PRB1629419 [KB1220355](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB1220355) | Not all the sys_class_names for all records are validated                                                                                                                                                       | Not validating the sys_class_names records causes errors in Health Job processing.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | Refer to the listed KB article for details.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n| Core Platform PRB1381876                                                                                                         | If the user uses 'trend on' or 'after'  there's a getGlideObject() application scope error when notifications are triggered                                                                                     | There's an error: 'Function getGlideObject is not allowed in scope sn_customerservice'.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n| Core Platform PRB1401625 [KB0966877](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB0966877)                            | CSRF token rotation isn't happening during log in flows                                                                                                                                                         |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | Refer to the listed KB article for details.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n| Core Platform PRB1617296                                                                                                         | Next Experience UI record not found                                                                                                                                                                             | In the incident table before login displays 'record not found'  even after logging in.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n| Core Platform PRB1635023                                                                                                         | Exception in ExpiringCache.cleanup                                                                                                                                                                              | The cache cleanup sometimes generates an exception (ExpiringCache.java:120). This can happen randomly and to different threads that need to access files on the file system.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n| Core Platform PRB1635585 [KB1221999](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB1221999)                            | StaticContentServer caches the existence and non-existence of snc_node_disable.html                                                                                                                             | After an instance migration  some instances don't come online when nodes are re-pointed to a new database.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   ...",
        "QUERY": " 'CMDB Identification and Reconciliation engine throwing CompactRelation: failed to get details of CI error'",
        "GROUND_TRUTH": "The issue of the CMDB Identification and Reconciliation engine throwing a CompactRelation error when it fails to get details of a CI occurs when the Data Archiving plugin is installed and in use for the CMDB tables. This issue arises when the Dependency View ngbsmprocessor needs to collect details of all related CIs. The main problems caused by this issue are that ServiceCacheManager.getInboundRelations and .getOutboundRelations won't work properly even though the data is there in the instance, and it can add a large number of entries to syslog tables causing disk space and performance issues. \n\nThe issue occurs because the code does not check to see if a CI record is archived and instead just throws an error. Neither the Archiving nor CMDB plugins have any feature for 'cascade archiving' related records such as cmdb_rel_ci when a CI is archived that would prevent a relationship record remaining after a CI is archived.\n\nA workaround for this issue is to archive related records whenever a rule is created for CMDB CI archival, the corresponding relations should also be archived. However, this workaround is only half a workaround as you can only select 1 of the fields (either parent or child) when creating an \"Archive Related Records\" rule."
    },
    {
        "id": 71,
        "CONTEXT": "\"## 6.0\n\n## Description\n\nServiceNow has identified a defect that can result in Clone failures of instances configured with gateway shards. A Module Access Policy included in San Diego for gateway and secondary database pool plugins caused a regression in our Cloud Automation Infrastructure.\n\nCustomers can face this defect on gateway shard instances if at least one of the Source or Target instances of the Clone are on San Diego.\n\n## Steps to Reproduce\n\n1. Ensure the Source instance to be used for Cloning is configured with gateway shards.\n2. Upgrade Target instance to be used for Cloning to San Diego\n3. Initiate a shard over shard Clone from the source instance to the target instance on San Diego.\n\n## Workaround\n\nServiceNow fixed this issue in San Diego Patch 2  and we recommend updating your instances to this version to prevent any impact caused by this defect. If either source or target version is on San Diego and on a version before San Diego Patch 2  below workaround can be applied. The below steps need to be performed on the Target instance of the Clone that is already on San Diego. They need to be performed on the Source instance as well if it is also upgraded to San Diego.\n\n### **Steps to apply the workaround on the instance(s) on San Diego:**\n\n#### **Part 1**\n\n* Login to the Instance as an 'admin' user.\n* Run the below script from '**Scripts - Background'.**This is will deactivate the Module Access Policy included in San Diego that's configured to reject Script Access of password.\n\nvar gr = new GlideRecord(\u0093sys_kmf_crypto_caller_policy\u0094);\ngr.get(\u0093fcf89b91eb4330100fcee5b26b5228c8\u0094);\ngr.setValue(\u0093active\u0094  \u00930\u0094);\ngr.update();\n\n#### **Part 2**\n\n* Create a new Module Access Policy as below. This requires user to have sn_kmf.cryptographic_manager or sn_kmf.admin role\n* Navigate to Key Management \\> Module Access Policies \\> All\n* Click on 'New' to create a new Module Access Policy with below fields:  \n  * Policy name - Allow Maint to access gateway module\n  * Crypto Module - com_snc_da_gateway_glideencrypter (See Note Below if this Crypto Module is not found)\n  * Type = Role\n  * Target Role = maint\n  * Result = Track\n  * Active Flag selected\n* Click 'Submit' and save the new Module Access Policy\n\n### **Data Preserver on source instance to protect the workaround**\n\nData preserver should be defined in the source instance to protect the above created 'Module Access Policy' on the target instance from being overwritten during a clone. Data Preservers are defined in the source instance but preserve records in the target instance.\n\n1. Login to the Source Instance as an 'admin' user.\n2. On the source instance  navigate to **System Clone** \\> **Preserve Data**.\n3. Click on 'Column Options' next to any Column Name or Right-Click on any Column Name and click on 'Import XML' menu from the drop-down.\n4. Select the attached XML (clone_data_preserver_28c9ba1d1b6641105edf32ebdc4bcb44.xml) and Upload.\n5. 'sys_kmf_crypto_caller_policy' Clone Data Preserver will be imported. This preserver is used to preserve the modifications made to the Module Access Policies mentioned in Part 1 and Part 2 above.\n6. Verify that the Clone Data Preserver imported is configured for Module Access Policy (sys_kmf_crypto_caller_policy) table.\n7. Verify that 2 conditions are configured with Or Condition as below:  \n   * Policy name is 'Allow Maint to access gateway module'\n   * Sys ID is 'fcf89b91eb4330100fcee5b26b5228c8'\n\nIf the data preserver is not configured correctly on the source instance  the workaround implemented on a target instance can be lost after a clone has been completed successfully. Therefore it is suggested to check if the workaround is present or not always before requesting for a new shard-over-shard clone on to a target instance on San Diego.\n\nBelow script can be run from '**Scripts - Background'**as an 'admin' user:\n\nvar gr = new GlideRecord(\u0093sys_kmf_crypto_caller_policy\u0094);\n\ngr.get(\u0093fcf89b91eb4330100fcee5b26b5228c8\u0094);\n\ngs.print(\"OOB MAP Status is \" + gr.active + '');\nvar queryStr = \u0093policy_name=Allow Maint to access gateway module\u0094;\ngr.initialize();\ngr.addEncodedQuery(queryStr);\ngr.query();\nif (gr.next())\n{\ngs.print(\"Custom Created MAP Status is \" + gr.active + '');\ngs.print(gr.getDisplayValue(\u0091type\u0092));\ngs.print(gr.getDisplayValue(\u0091target_role\u0092));\ngs.print(gr.getDisplayValue(\u0091crypto_module\u0092));\ngs.print(gr.sys_id);\n}\nOutput returned should be like below to confirm that out of the box included Module Access Policy (MAP) is not active and customer created MAP is active and configured so that with 'maint' role can access the gateway crypto module\n\n\\*\\*\\* Script: OOB MAP Status is false\n\\*\\*\\* Script: Custom Created MAP Status is true\n\\*\\*\\* Script: Role\n\\*\\*\\* Script: maint\n\\*\\*\\* Script: com_snc_da_gateway_glideencrypter\n\\*\\*\\* Script: \\<SYS ID\\>\n\n\n\nClone failure for gateway shard instance on San Diego\n\n\n\n## Work Instruction | How to perform H-AHA transfers\/failovers: appstoreprod and DATACENTER instances\n\nTarget Colo is iad3\n\nApp Node Hosts for chrismutzelcatmovetest1 are:\napp32046.iad3.service-now.com\napp32048.iad3.service-now.com\napp84047.sjc4.service-now.com\napp84048.sjc4.service-now.com\n\nSource Colo is sjc4\nCheckpoint: 2013-02-12_16:22:51\n\nChecking slave lag on db36087.iad3.service-now.com...\n\nLag is 0 secs... moving on.\n\nRepointing Instance URL to iad3...\nCheckpoint: 2013-02-12_16:22:51\n\nShutdown chrismutzelcatmovetest1 Nodes  Fast Fail File Management and Update 'glide.db.properties' for db36087.iad3.service-now.com...\nRemoving maintenance file to target nodes on app32046.iad3.service-now.com\nRemoving maintenance file to target nodes on app32048.iad3.service-now.com\nAdding maintenance file to source nodes on app84047.sjc4.service-now.com\nAdding maintenance file to source nodes on app84048.sjc4.service-now.com\nCheckpoint: 2013-02-12_16:22:52\n\nSetting db35092.sjc4.service-now.com to Read-Only...\nKilling any long running SELECTs on db35092.sjc4.service-now.com\n\nSetting db36087.iad3.service-now.com to Read-Write...\n\nSet schedulers in sys_cluster_state for active colo: iad3\n\nValidate shutdown and updates have completed on App nodes...\nCheckpoint: 2013-02-12_16:23:07\n\nGateway not set  skipping Mongo...\nCheckpoint: 2013-02-12_16:23:07\nCheckpoint: 2013-02-12_16:23:09\n\nStarting chrismutzelcatmovetest1 App Nodes...\nStarting app nodes on app32046.iad3.service-now.com\nStarting app nodes on app32048.iad3.service-now.com\nStarting app nodes on app84047.sjc4.service-now.com\nStarting app nodes on app84048.sjc4.service-now.com\nCheckpoint: 2013-02-12_16:23:09\n\nValidate nodes startup has completed...\n\nAll done... please validate!\n\nEnd time: 2013-02-12_16:23:14\nScript done  file is \/dev\/null\n\n# Collecting crash info\n\nIf instance was unresponsive due to XFS crash or other HW failure we need to collect informations from failed DB node\n\n## Before reboot\n\n### Disable RR\n\nOnce the customer is back up  disable the RR nodes which are in same datacenter as failed DB node\n\n* \"hop\" into the UI and navigate to \"System Maintenace --\\> Secondary Database Pools\" on the left nav\n* Set the \"Active\" field to FALSE for all the lines that have a lhr1(ams3) host in the URL field\n\nAnyone with \"MAINT\" role should be able to manage the RR changes (steps 2 \\& 6)  including CS if they are available to help (as another set of hands.)\n\n### General Logs\n\n* examine \/var\/log\/messages and capture any related messages for the time period around the failure\n\n### FusionIO\n\n* run the command\n\nfio-bugreport\nThis will collect a bunch of info for the system and stats from the FusionIO cards\n\n### XFS\n\n* stop all services and processes which are using \/glide volume\n* umount \/glide volume and run\n\nxfs_repair -n \/dev\/vg-fusionio\/glide-lv\nThis command will not actually repair anything  but will print out information about what XFS thinks is in a bad state. This may take some work  since the mount is in a bad state. SE should try what ever they can to facilitate this before rebooting. If this can be accomplished successfully  then a reboot (and possibly a full power off\/on) can be performed.\n\n## After reboot\n\n### Repair XFS\n\n* run from \"single user mode\" or with stopped services(MySQL  cloudrunner  PatchAgent) and umounted \/glide another pass of:\n\nxfs_repair -n \/dev\/vg-fusionio\/glide-lv\n\n* mount \/glide and restart previously stopped services\n\n### Enable RR\n\n* verify replication restarts and catches up on failed node and then re-enable the RR nodes that were disabled (Set the \"Active\" field to TRUE in UI)\n\n\"\n\n",
        "QUERY": " 'Database Connection Issue - Gateway shard connection failure'",
        "GROUND_TRUTH": "The issue you're experiencing with the 'Database Connection Issue - Gateway shard connection failure' could be due to a defect identified by ServiceNow. This defect can result in Clone failures of instances configured with gateway shards. This issue is particularly prevalent if at least one of the Source or Target instances of the Clone are on San Diego.\n\nServiceNow has provided a workaround for this issue. They recommend updating your instances to San Diego Patch 2 to prevent any impact caused by this defect. If either source or target version is on San Diego and on a version before San Diego Patch 2, you can apply the workaround steps provided in the article. These steps involve deactivating the Module Access Policy included in San Diego that's configured to reject Script Access of password and creating a new Module Access Policy. \n\nAdditionally, a data preserver should be defined in the source instance to protect the above created 'Module Access Policy' on the target instance from being overwritten during a clone. \n\nFinally, you can confirm that the workaround has been implemented correctly by running a script provided in the article. The output should confirm that the out of the box included Module Access Policy (MAP) is not active and the customer created MAP is active and configured so that with 'maint' role can access the gateway crypto module."
    },
    {
        "id": 72,
        "CONTEXT": "\"## Changing Dictionary Attributes does not affect Glide variables when rendering in the UI until restar\n\n## Description\n\nWhen you change a sys_dictionary's Attributes values and then attempt to render a page that loads a Glide variable that is affected by the Attributes on that Dictionary record  the value you set in Attributes is not present. cache.do and hard browser cache refresh have no effect. Only after restarting the node do the changes begin to render properly on the Glide variable.\n\n## Steps to Reproduce\n\nGo to \/sys_dictionary_list.do and search for the entry with the following values:\n\n* table = sys_user\n* type = collection\n* Attributes contains \"ref_auto_completer=AJAXTableCompleter\"\n\nChange the Reference auto completer attribute from AJAXTableCompleter to \"MY_NEW_BETTER_VALUE\" and click **Update**.\n\nConfirm that sys_schema_attribute_m2m now contains a record where value = \"MY_NEW_BETTER_VALUE\".\n\nRender the associated Glide variable.\n\n1. Navigate to **Automated Test Framework \\> Tests**   start a new test record and click **Save**.\n2. On the saved Test form  click **Add Test Step**.\n3. Select **Server \\> Impersonate** and click the **Next** button.\n\nInspect the User field input element and look at the DOM.\n\nInstead of the data-completer value having changed to MY_NEW_BETTER_VALUE  it remained AJAXTableCompleter.\n\n## Workaround\n\nFor an immediate fix  you can flush the cache by running **gs.invalidateCache()**. However  this can cause a performance degradation of the node and should be done only if you need urgent relief.\n\n**Related Problem: PRB1034023**\n\n\n\n## Work Instruction | How to perform H-AHA transfers\/failovers: appstoreprod and DATACENTER instances\n\n\\`label\\`  var_dictionary0.\\`model_id\\`  sys_dictionary0.\\`staged\\`  sys_dictionary0.\\`reference_type\\`  var_dictionary0.\\`help\\`  sys_metadata0.\\`sys_package\\`  sys_dictionary0.\\`formula\\`  sys_dictionary0.\\`attributes\\`  sys_dictionary0.\\`choice\\`  sys_dictionary0.\\`table_reference\\`  sys_dictionary0.\\`reference_qual\\`  sys_metadata0.\\`sys_customer_update\\`  sys_dictionary0.\\`text_index\\`  sys_dictionary0.\\`function_definition\\` FROM ((var_dictionary var_dictionary0 INNER JOIN sys_metadata sys_metadata0 ON var_dictionary0.\\`sys_id\\` = sys_metadata0.\\`sys_id\\` ) INNER JOIN sys_dictionary sys_dictionary0 ON var_dictionary0.\\`sys_id\\` = sys_dictionary0.\\`sys_id\\` ) WHERE sys_dictionary0.\\`name\\` = 'var__m_3961a1da0a0a0b5c00ecd84822f70d85' AND sys_dictionary0.\\`element\\` IS NOT NULL;\n\nSELECT sys_variable_value0.\\`sys_id\\` FROM sys_variable_value sys_variable_value0 WHERE sys_variable_value0.\\`variable\\` IN ('493cba6a0a6a803f07df24b118473551'   '493d2c1d0a6a803f3217a75738ea3289'   'c35e50810a0a0ba96248a709b450fc32') AND sys_variable_value0.\\`document\\` = 'wf_activity' AND sys_variable_value0.\\`document_key\\` = 'd9be58383c54b510869ff2105a07b5d6';\n\nDo not proceed with the change if there is a significant degradation in response time. Review the results with the internal team first.\n\n# Change Plan Instructions\n\n## Request and obtain Firefighter Access\n\n## **SRE-DevOps must obtain Firefighter access before performing the global pause.**\n\n## See**[KB0864106 - Work Instruction \\| How to Escalate to Admin Using FireFighter](.\/kb?id=kb_article_view&sysparm_article=KB0864106)** for instructions.\n\n## Run pre-validation on standby DB\n\njmp db169042.bwi101.service-now.com\n\n# Check replication is caught up\n\nsnow mysql-info datacenterinfra_3401\n\n## Run pre-validation on standby DB\n\nSRE to run AHA Validation\n#=========================================#\nOpen Instance CI - [https:\/\/datacenter.service-now.com\/cmdb_ci_service_now.do?sysparm_query=GOTOname=datacenterinfra](https:\/\/datacenter.service-now.com\/cmdb_ci_service_now.do?sysparm_query=GOTOname=datacenterinfra)\nAHA Transfer\/Failover -\\> Validate Now\nConfirm 54 out of 54 tests passed\n\n## Activate Global Pause\n\nSRE-DevOps performs the global pause and gives the go signal:\n\n#### Pre-pause validation request:\n\nPlease open the following URL (replacing the instance name) and you will notice 100's of ecc_queue records. [https:\/\/datacenter.service-now.com\/ecc_queue_list.do?sysparm_query=sys_created_onRELATIVEGT%40minute%40ago%401\\&sysparm_view=](https:\/\/datacenter.service-now.com\/ecc_queue_list.do?sysparm_query=sys_created_onRELATIVEGT%40minute%40ago%401&sysparm_view=)\n\n1. open u_automation_group_pause_request_list.do\n2. Fill in the following details.\n   * Change ticket.\n   * Leave Resume At field empty\n   * Pause All to be checked.\n3. Click **Submit**.\n4. Copy the Automation Number request into worknotes.\n\n#### Post pause validation request:\n\nPlease open the following URL (replacing the instance name) and you will notice 10's of ecc_queue records.\n[https:\/\/datacenter.service-now.com\/ecc_queue_list.do?sysparm_query=sys_created_onRELATIVEGT%40minute%40ago%401\\&sysparm_view=](https:\/\/datacenter.service-now.com\/ecc_queue_list.do?sysparm_query=sys_created_onRELATIVEGT%40minute%40ago%401&sysparm_view=)\n\n## Run the fail-live script\n\nTDO performs the next steps.\n\n## Parameters\n\nList of possible script options:\n\nops01.xxxx:\/xxxxx\/xxxxx\/xxxxxx\n$ \/home\/failover\/FAIL-LIVE.sh\n\nUsage:\n.\/FAIL-LIVE.sh \\<options\\>\n\n*Required* :\n-i = Instance Name\n-V = Standby VIP\n\n*Options* :\n-s = Source DB Host (FQDN of current primary DBI)\n-S = Source DB Port\n-t = Target DB Host (FQDN of DB host to AHA\/Failover)\n-T = Target DB Port (Port of DB host to AHA\/Failover )\n-c = Catalog Name (example: ge_1 or barclays_2)\n-n = Do NOT perform any DNS updates - this will be manual\n-f = Force Failover (Emergency - not controlled. Normally used if Source DB is hard down.)\n-d = Dry-Run (test env and validate connectivity)\n-x = Debug mode (for the shell execution)\n-D = Domain name\n-h = Help\n-v = Script Version\n\n## Procedure\n\nLog in to the appropriate ops01 from the bastion hosts and switch to the user 'failover'.\n\n<bssh|jmp> ops01.\nams0|lhr0|iad0|sjc0\n.service-now.com\npbrun su - failover\nThis will take you straight to the directory where the failover script is located in \/home\/failover\/.\n\nThe script will prompt for two things:\n\n* **username** : This is the user that will be used to ssh to servers *NOT* the failover user that you switched to. The user must have pbul access to run elevated privileges on the removed systems.\n* **password**: This is the LDAP password for the user indicated above. This password is used to ssh to the remote systems.\n\nFor auditing  the script **will not run** as a relative path (eg. .\/FAIL-LIVE.sh). Always run it using the full path (e.g. \/home\/failover\/FAIL-LIVE.sh -i\\<instance name\\>)\n\n##### Example commands:\n\n|                                               **Reason for transfer\/failover**                                               |                                                       **Command**                                                       |\n|------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------|\n| The instance is up and responsive (e.g. scheduled transfer).                                                                 | $ \/home\/failover\/FAIL-LIVE.sh -i \\<instance\\> -V \\<vip of new site\\>                                                    |\n| The instance is down from the application point of view but the source database is fine (e.g. failover when issues with F5). | $ \/home\/failover\/FAIL-LIVE.sh -i \\<instance\\> -V \\<vip of new site\\> -s \\<fqdn source DB server\\> -S \\<source DB port\\> |\n| The instance is down from the database point of view (e.g. failover when issues with source DB).                             | $ \/home\/failover\/FAIL-LIVE.sh -i \\<instance\\> -f -t \\<fqdn target DB server\\> -T \\<target DB port\\>                     |\n**Note:** A 30-second replication lag is ok in AHA. The FAIL-LIVE.sh script uses the same policy as per [KB0863409](.\/kb?id=kb_article_view&sysparm_article=KB0863409 \"KB0863409\").\n\n##### Sample Output\n\nbash-4.2$ \/home\/failover\/FAIL-LIVE.sh -i empkamiscaray3 -V 103.23.65.119 -d\nscreen is terminating\n************** DRY RUN MODE **************\nEnter your username: keith.amiscaray\nEnter your ssh password:\nPlease confirm you password:\nCollecting instance data. \u00a0Please wait....\nChecking the app server app130165.syd101.service-now.com if accessible ...\nChecking the app server app131014.syd101.service-now.com if accessible ...\n\n## Vancouver security and notable fixes\n\n A Visibility Content plugin version higher than 6.0.0 is reverted to an earlier version after upgrading from Tokyo to Utah.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | Refer to the listed KB article for details.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n| Discovery PRB1665907 [KB1369852](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB1369852)                                | GetMIDInfo (CloudServiceAccountInfoUtil) hangs in an infinite loop until it exhausts node memory                                                                                                                | When a GetMIDInfo scripted SOAP service responds to GetCloudService AccountInfo requests from a MID Server  and if any of the cloud_service_account_view records are missing a sa_account_id value  it hangs in a loop. It allocates memory until the instance application node goes out-of-memory and restarts. Cloud_service_ account_view is a database view that takes its sa_account_id field from the account_id field of table cmdb_ci_cloud_ service_account. That field is mandatory in the dictionary  but it is still possible to have an empty value. This will also prevent Cloud Discovery from working (like AWS)  as the cloud service accounts won't be synched to the MID Server. | Refer to the listed KB article for details.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n| Document Services PRB1615472                                                                                                     | Export to PDF and Configure \\> Form Layout can be very slow if any table in the hierarchy has many references to it (as sys_dictionary records)                                                                 | For example  when exporting a PDF from a 'cmdb_ci_win_server' record with five sections  the user queries sys_dictionary with 'active=true\\^referenceIN cmdb_ci_win_server  cmdb_ci_server  cmdb_ci_computer  cmdb_ci_hardware  cmdb_ci  cmdb' five times. If the query returns \\~15k results each time due to many references to cmdb_ci tables  and the user repeats that query five times  the time adds up quickly.                                                                                                                                                                                                                                                                             |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n| Email Notifications PRB1660631 [KB1307652](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB1307652)                      | The SMTP sender job is stuck for over 60 minutes                                                                                                                                                                | Outgoing emails are stuck in the SMTP process for more than 60 minutes.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | Refer to the listed KB article for details.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n| Encryption PRB1547401 [KB1208278](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB1208278)                               | Gateway database pool can't be instantiated at node startup                                                                                                                                                     | It succeeds after SecondaryDatabaseSweeperJob runs.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | Refer to the listed KB article for details.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n| Event Management PRB1628223 [KB1212632](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB1212632)                         | Clone Excludes\/Preservers are missing for Event Management MID Server extension contexts (ecc_agent_ext_context_event and eif_listener_context)                                                                 |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | Refer to the listed KB article for details.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n| Field Administration PRB1360299 [KB0951799](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB0951799)                     | The Image field doesn't clear the sys_id of the attached image when the user clicks 'Delete'                                                                                                                    | A field holds the sys_id of the sys_attachment record  even if it has already been deleted from the instance.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | Refer to the listed KB article for details.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n| Field Administration PRB1559416                                                                                                  | The g_form.clearValue() for date field does not work in the UI Policy on that same field                                                                                                                        | The value is not saved but it still visually persists in the input.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | 1. Log in to an instance. 2. Navigate to an incident record and ensure there is a test date field that is visible in both the default view and the Agent Workspace view. 3. Create a UI policy with conditions where the test date field is set to a date before today and enter the following script in the script section:function onCondition() { g_form.clearValue('u_testfield');}. 4. Test the UI policy by entering any past time in the test field on an incident record in both the platform and the Agent Workspace. {#vancouver-security-notables__ol_tlm_zd1_2yb} Expected behavior:The value of the test field clears in both the platform and the Agent Workspace  it is set to a value that matches the condition in the UI policy. Actual behavior: The value of the test field clears in the platform it is set to a value that matches the condition in the UI policy  but doesn't clear the field in Agent Workspace.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n| Field Administration PRB1613998                                                                                                  | In Agent Workspace  the Search on Translated Text reference field only works in ENU                                                                                                                             | If a user exposes a reference field on a form in Agent Workspace that references a Translated Text field on the destination table  record searching only works for ENU. Specifically  only when entering text into it to identify a potential record to select.                                                                                                                                                                                                                                                                                                                                                                                                                                     | 1. In Tokyo  install a French language plugin. 2. Create a reference field on an incident form with these details: 1. Table: Incident \\[incident\\] 2. Type: Reference 3. Column Label: Announcement 4. Column name: announcement 5. Reference Specification: Reference = Announcement 6. Use reference qualifier: Simple {#vancouver-security-notables__ol_d4m_zd1_2yb} 3. Expose the Announcement field on the 'Workspace' and 'Default view' views for an incident form. 4. With the language set to ENU  navigate to an open incident record in the ITSM backend. 5. Click the Announcement field. \"\n\n",
        "QUERY": " 'Value set in attributes lost while changing the sys_dictionary's attribute values'",
        "GROUND_TRUTH": "According to the KB12345 article, when you change a sys_dictionary's Attributes values and then attempt to render a page that loads a Glide variable that is affected by the Attributes on that Dictionary record, the value you set in Attributes is not present. This issue persists even after cache.do and hard browser cache refresh. The changes only begin to render properly on the Glide variable after restarting the node.\n\nTo reproduce this issue, you can follow the steps provided in the article. After changing the Reference auto completer attribute from AJAXTableCompleter to \"MY_NEW_BETTER_VALUE\" and confirming that sys_schema_attribute_m2m now contains a record where value = \"MY_NEW_BETTER_VALUE\", you can render the associated Glide variable. However, instead of the data-completer value having changed to MY_NEW_BETTER_VALUE, it remains AJAXTableCompleter.\n\nThe immediate workaround for this issue is to flush the cache by running gs.invalidateCache(). However, this can cause a performance degradation of the node and should be done only if you need urgent relief."
    },
    {
        "id": 73,
        "CONTEXT": "\"## 6.0\n\n## Description\n\nServiceNow has identified a defect that can result in Clone failures of instances configured with gateway shards. A Module Access Policy included in San Diego for gateway and secondary database pool plugins caused a regression in our Cloud Automation Infrastructure.\n\nCustomers can face this defect on gateway shard instances if at least one of the Source or Target instances of the Clone are on San Diego.\n\n## Steps to Reproduce\n\n1. Ensure the Source instance to be used for Cloning is configured with gateway shards.\n2. Upgrade Target instance to be used for Cloning to San Diego\n3. Initiate a shard over shard Clone from the source instance to the target instance on San Diego.\n\n## Workaround\n\nServiceNow fixed this issue in San Diego Patch 2  and we recommend updating your instances to this version to prevent any impact caused by this defect. If either source or target version is on San Diego and on a version before San Diego Patch 2  below workaround can be applied. The below steps need to be performed on the Target instance of the Clone that is already on San Diego. They need to be performed on the Source instance as well if it is also upgraded to San Diego.\n\n### **Steps to apply the workaround on the instance(s) on San Diego:**\n\n#### **Part 1**\n\n* Login to the Instance as an 'admin' user.\n* Run the below script from '**Scripts - Background'.**This is will deactivate the Module Access Policy included in San Diego that's configured to reject Script Access of password.\n\nvar gr = new GlideRecord(\u0093sys_kmf_crypto_caller_policy\u0094);\ngr.get(\u0093fcf89b91eb4330100fcee5b26b5228c8\u0094);\ngr.setValue(\u0093active\u0094  \u00930\u0094);\ngr.update();\n\n#### **Part 2**\n\n* Create a new Module Access Policy as below. This requires user to have sn_kmf.cryptographic_manager or sn_kmf.admin role\n* Navigate to Key Management \\> Module Access Policies \\> All\n* Click on 'New' to create a new Module Access Policy with below fields:  \n  * Policy name - Allow Maint to access gateway module\n  * Crypto Module - com_snc_da_gateway_glideencrypter (See Note Below if this Crypto Module is not found)\n  * Type = Role\n  * Target Role = maint\n  * Result = Track\n  * Active Flag selected\n* Click 'Submit' and save the new Module Access Policy\n\n### **Data Preserver on source instance to protect the workaround**\n\nData preserver should be defined in the source instance to protect the above created 'Module Access Policy' on the target instance from being overwritten during a clone. Data Preservers are defined in the source instance but preserve records in the target instance.\n\n1. Login to the Source Instance as an 'admin' user.\n2. On the source instance  navigate to **System Clone** \\> **Preserve Data**.\n3. Click on 'Column Options' next to any Column Name or Right-Click on any Column Name and click on 'Import XML' menu from the drop-down.\n4. Select the attached XML (clone_data_preserver_28c9ba1d1b6641105edf32ebdc4bcb44.xml) and Upload.\n5. 'sys_kmf_crypto_caller_policy' Clone Data Preserver will be imported. This preserver is used to preserve the modifications made to the Module Access Policies mentioned in Part 1 and Part 2 above.\n6. Verify that the Clone Data Preserver imported is configured for Module Access Policy (sys_kmf_crypto_caller_policy) table.\n7. Verify that 2 conditions are configured with Or Condition as below:  \n   * Policy name is 'Allow Maint to access gateway module'\n   * Sys ID is 'fcf89b91eb4330100fcee5b26b5228c8'\n\nIf the data preserver is not configured correctly on the source instance  the workaround implemented on a target instance can be lost after a clone has been completed successfully. Therefore it is suggested to check if the workaround is present or not always before requesting for a new shard-over-shard clone on to a target instance on San Diego.\n\nBelow script can be run from '**Scripts - Background'**as an 'admin' user:\n\nvar gr = new GlideRecord(\u0093sys_kmf_crypto_caller_policy\u0094);\n\ngr.get(\u0093fcf89b91eb4330100fcee5b26b5228c8\u0094);\n\ngs.print(\"OOB MAP Status is \" + gr.active + '');\nvar queryStr = \u0093policy_name=Allow Maint to access gateway module\u0094;\ngr.initialize();\ngr.addEncodedQuery(queryStr);\ngr.query();\nif (gr.next())\n{\ngs.print(\"Custom Created MAP Status is \" + gr.active + '');\ngs.print(gr.getDisplayValue(\u0091type\u0092));\ngs.print(gr.getDisplayValue(\u0091target_role\u0092));\ngs.print(gr.getDisplayValue(\u0091crypto_module\u0092));\ngs.print(gr.sys_id);\n}\nOutput returned should be like below to confirm that out of the box included Module Access Policy (MAP) is not active and customer created MAP is active and configured so that with 'maint' role can access the gateway crypto module\n\n\\*\\*\\* Script: OOB MAP Status is false\n\\*\\*\\* Script: Custom Created MAP Status is true\n\\*\\*\\* Script: Role\n\\*\\*\\* Script: maint\n\\*\\*\\* Script: com_snc_da_gateway_glideencrypter\n\\*\\*\\* Script: \\<SYS ID\\>\n\n\n\nClone failure for gateway shard instance on San Diego\n\n\n\n## Work Instruction | How to perform H-AHA transfers\/failovers: appstoreprod and DATACENTER instances\n\nTarget Colo is iad3\n\nApp Node Hosts for chrismutzelcatmovetest1 are:\napp32046.iad3.service-now.com\napp32048.iad3.service-now.com\napp84047.sjc4.service-now.com\napp84048.sjc4.service-now.com\n\nSource Colo is sjc4\nCheckpoint: 2013-02-12_16:22:51\n\nChecking slave lag on db36087.iad3.service-now.com...\n\nLag is 0 secs... moving on.\n\nRepointing Instance URL to iad3...\nCheckpoint: 2013-02-12_16:22:51\n\nShutdown chrismutzelcatmovetest1 Nodes  Fast Fail File Management and Update 'glide.db.properties' for db36087.iad3.service-now.com...\nRemoving maintenance file to target nodes on app32046.iad3.service-now.com\nRemoving maintenance file to target nodes on app32048.iad3.service-now.com\nAdding maintenance file to source nodes on app84047.sjc4.service-now.com\nAdding maintenance file to source nodes on app84048.sjc4.service-now.com\nCheckpoint: 2013-02-12_16:22:52\n\nSetting db35092.sjc4.service-now.com to Read-Only...\nKilling any long running SELECTs on db35092.sjc4.service-now.com\n\nSetting db36087.iad3.service-now.com to Read-Write...\n\nSet schedulers in sys_cluster_state for active colo: iad3\n\nValidate shutdown and updates have completed on App nodes...\nCheckpoint: 2013-02-12_16:23:07\n\nGateway not set  skipping Mongo...\nCheckpoint: 2013-02-12_16:23:07\nCheckpoint: 2013-02-12_16:23:09\n\nStarting chrismutzelcatmovetest1 App Nodes...\nStarting app nodes on app32046.iad3.service-now.com\nStarting app nodes on app32048.iad3.service-now.com\nStarting app nodes on app84047.sjc4.service-now.com\nStarting app nodes on app84048.sjc4.service-now.com\nCheckpoint: 2013-02-12_16:23:09\n\nValidate nodes startup has completed...\n\nAll done... please validate!\n\nEnd time: 2013-02-12_16:23:14\nScript done  file is \/dev\/null\n\n# Collecting crash info\n\nIf instance was unresponsive due to XFS crash or other HW failure we need to collect informations from failed DB node\n\n## Before reboot\n\n### Disable RR\n\nOnce the customer is back up  disable the RR nodes which are in same datacenter as failed DB node\n\n* \"hop\" into the UI and navigate to \"System Maintenace --\\> Secondary Database Pools\" on the left nav\n* Set the \"Active\" field to FALSE for all the lines that have a lhr1(ams3) host in the URL field\n\nAnyone with \"MAINT\" role should be able to manage the RR changes (steps 2 \\& 6)  including CS if they are available to help (as another set of hands.)\n\n### General Logs\n\n* examine \/var\/log\/messages and capture any related messages for the time period around the failure\n\n### FusionIO\n\n* run the command\n\nfio-bugreport\nThis will collect a bunch of info for the system and stats from the FusionIO cards\n\n### XFS\n\n* stop all services and processes which are using \/glide volume\n* umount \/glide volume and run\n\nxfs_repair -n \/dev\/vg-fusionio\/glide-lv\nThis command will not actually repair anything  but will print out information about what XFS thinks is in a bad state. This may take some work  since the mount is in a bad state. SE should try what ever they can to facilitate this before rebooting. If this can be accomplished successfully  then a reboot (and possibly a full power off\/on) can be performed.\n\n## After reboot\n\n### Repair XFS\n\n* run from \"single user mode\" or with stopped services(MySQL  cloudrunner  PatchAgent) and umounted \/glide another pass of:\n\nxfs_repair -n \/dev\/vg-fusionio\/glide-lv\n\n* mount \/glide and restart previously stopped services\n\n### Enable RR\n\n* verify replication restarts and catches up on failed node and then re-enable the RR nodes that were disabled (Set the \"Active\" field to TRUE in UI)\n\n\"\n\n",
        "QUERY": " 'Database Connection Issue - Gateway shard connection failure'",
        "GROUND_TRUTH": "The issue you're experiencing with the 'Database Connection Issue - Gateway shard connection failure' could be due to a defect identified by ServiceNow. This defect can result in Clone failures of instances configured with gateway shards. This issue is particularly prevalent if at least one of the Source or Target instances of the Clone are on San Diego.\n\nTo resolve this issue, ServiceNow recommends updating your instances to San Diego Patch 2. If this is not possible, a workaround can be applied. This involves deactivating the Module Access Policy included in San Diego that's configured to reject Script Access of password and creating a new Module Access Policy. \n\nAdditionally, a data preserver should be defined in the source instance to protect the above created 'Module Access Policy' on the target instance from being overwritten during a clone. \n\nFinally, you can confirm that the workaround is correctly implemented by running a script from 'Scripts - Background' as an 'admin' user. The output should confirm that the out of the box included Module Access Policy (MAP) is not active and the customer created MAP is active and configured so that with 'maint' role can access the gateway crypto module."
    },
    {
        "id": 74,
        "CONTEXT": "\"## Orphaned User Roles\n\nA User Role record with an \"empty\" user can be displayed in the User Roles table.\n\nThis is most likely caused by deleting a user that had the orphaned role.\n\nYou can open the User Role record. Show the XML and get the sys_id for the User. You can then check the \"Audit Deleted Records\" and filter for Table name =sys_user and Payload contains the sys_id you are looking for. You can then see what User the Role belonged to by checking the \"Display value\".\n\n\n\n## Internal FAQ & Case Handling Instructions for KB1553688\n\nServiceNow is providing a script designed to help our customers conduct an assessment of their instances with potentially misconfigured access control lists (ACLs). This is outlined in [https:\/\/support.servicenow.com\/kb?id=kb_article_view\\&sysparm_article=KB1561609](https:\/\/support.servicenow.com\/kb?id=kb_article_view&sysparm_article=KB1561609)\n\nThis Security Assessment Script simulates guest user access (unauthenticated users)  enabling customers to have a better understanding of data that may have been displayed to unauthenticated users prior to maintenance that ServiceNow recently performed.\n\n**Is there a plan to change the public access on widgets for all customer instances?**\nFor Support only. Do not copy-paste to customers.\nThis will be reviewed however ServiceNow is not performing this action as part of the current maintenance plan. For now this should be done by the customer where possible. If they need us to restrict public access on widgets they don't have access to do themselves  then have a CS SME create and perform the change as needed through a change request.\n\n**My team has updated 'X' amount of widgets but we don't have access to modify the remaining widgets as it requires \"Maint\" access. Can ServiceNow modify the remaining 'Y' amount of widgets?**\nFor Support only. Do not copy-paste to customers.\nYes. The case should be assigned moved from CS -- Mass Outage to a CS SME to gain approval from the customer and create and perform the change as needed.\n\n**Questions asking for RCA -- How long has ServiceNow Known about this misconfiguration? Why was this not addressed previously through patching? Etc.**\nPlease see KB1553688 and KB1555339 for information regarding this misconfiguration issue. ServiceNow is continuing to investigate this issue and will provide relevant information to customers as appropriate.\n\n**What is ServiceNow's response plan?**\nServiceNow is aware of the recent publications describing a potential misconfiguration issue that could result in unintended access and is actively investigating the reports that we have observed in various online resources.\nOut of an abundance of caution  ServiceNow has taken steps to try to mitigate this issue across all customer instances.\n\n* On October 18  ServiceNow performed proactive maintenance on customer instances to update ACLs that were configured empty -- ones that contained no role  no condition  and no script -- to require that a user be explicitly logged in to access the underlying data.\n* On October 20  ServiceNow applied additional proactive maintenance on customer instances to update ACLs with a similar configuration that may have been evaluated as empty.\n* On October 25 and 28  ServiceNow applied additional proactive maintenance on customer instances to update specific OOB widgets and further secure the sys_user table.  \nAdditional Information:\n* [KB1581507](https:\/\/support.servicenow.com\/kb?id=kb_article_view&sysparm_article=KB1581507) (Customer-Facing - WIP) Provides a short summary of the previously applied maintenance updates\n* [KB1555339](https:\/\/support.servicenow.com\/kb?id=kb_article_view&sysparm_article=KB1555339) (Customer Facing - Published) Details the proactive maintenance that ServiceNow has applied to enhance the security of customer instances by date. It also includes FAQs and recommended actions.  \n**There are some ACLs in my instance that have no role  no condition  and no script after the maintenance was performed. Was the maintenance complete?**  \nThe maintenance that ServiceNow applied on 10\/18 and 10\/20 did not adjust non-record ACLs. Additionally  the maintenance did not adjust inactive ACLs.\n\n**After the Maintenance - Customers experience issues with a report that uses a Function Field and may see the error - \"Access to this content denied based on report_view field ACLs\"**\n\nPlease note that PRB1675406 is not a PRB  but designed behaviour. Customers may see this issue because the write audit changed existing read ACLs.\n\nIn short  to read from a function field a user has to pass both:\n\n- read access to the function field.  \n- read access to all of the contributing fields used in the function.\n\nTo report on a function field a user has to pass all:\n\n- report_view access to the function field.   \n- report_view access to each of the contributing fields.   \n- role-only read ACL.   \n- role-only read read ACL for all contributing fields.\n\nUse the wording below when responding to the case after validating it matches the criteria.\n\n\"ServiceNow applied proactive maintenance to improve the security of your instance by updating some ACLs meeting specific criteria  as summarized in KB1555339. Following this update  some users may encounter access issues when attempting to access reports using function fields that are associated to a table for which one of the ACLs that had been updated by the maintenance is present.\n\nIf you encounter this scenario  please consider creating an appropriate ACL to expressly grant access to the intended users. Please refer to the documentation for more details: [https:\/\/docs.servicenow.com\/csh?topicname=acl-function-fields.html\\&version=latest](https:\/\/docs.servicenow.com\/csh?topicname=acl-function-fields.html&version=latest)\"\n\n\n\n## Utah Patch 7a: Known Errors\n\n \\[High chart upgrade 8.2.2\\] Heatmap and geomap visualization are broken                                                                                                                                                 |\n| [KB0870896](\/kb_view.do?sysparm_article=KB0870896) | PRB1459399 | Agent Workspace                        | Process Automation Designer (PAD) - Condition Builder fields are not populated in Modify Condition Widget in PAD Application                                                                                             |\n| [KB0966593](\/kb_view.do?sysparm_article=KB0966593) | PRB1477967 | UI Builder                             | Client Scripts do not save when using UIB in Firefox                                                                                                                                                                     |\n| [KB0966596](\/kb_view.do?sysparm_article=KB0966596) | PRB1485591 | Lists                                  | Simple - List component is not available in the Dashboard Builder                                                                                                                                                        |\n| [KB0966598](\/kb_view.do?sysparm_article=KB0966598) | PRB1492041 | Mobile Platform                        | Mobile Task button actions on WOT form page do not refresh the form once the Task has been accepted\/rejected                                                                                                             |\n| [KB0993213](\/kb_view.do?sysparm_article=KB0993213) | PRB1492353 | Platform Licensing                     | Inconsistencies with auto-calculation of per-user subscription allocation (System Entitlement)                                                                                                                           |\n| [KB1123826](\/kb_view.do?sysparm_article=KB1123826) | PRB1501129 | Software Asset Management Professional | \\[Accessibility\\] - License Usage - Focus is missing after clicking Run Reconciliation button from pop-up                                                                                                                |\n| [KB0966640](\/kb_view.do?sysparm_article=KB0966640) | PRB1507712 | Agent Workspace                        | The calendar picker is picking invalid dates for today  yesterday  and all calendar left menu options.                                                                                                                   |\n| [KB1000205](\/kb_view.do?sysparm_article=KB1000205) | PRB1513194 | Flow Designer                          | \\[Revisited\\] The output from the 'Get Catalog Variables for List Collector' type changed from string to GRProxy                                                                                                         |\n| [KB1080549](\/kb_view.do?sysparm_article=KB1080549) | PRB1539109 | Next Experience Unified Navigation     | POL_QE_RP1 - Next Experience does not allow user to login on Platform-only instance after installing 'Unified Navigation Admin Configuration' plugins and enabling Next Experience                                       |\n| [KB1005139](\/kb_view.do?sysparm_article=KB1005139) | PRB1539267 | PDF Generation                         | Korean letters are getting garbled when the PDF file is generated from a report                                                                                                                                          |\n| [KB1005406](\/kb_view.do?sysparm_article=KB1005406) | PRB1541411 | Lists                                  | \\[Workspace WHC\\] Highlighted \/ selected \/ hover pages are indistinguishable in list pagination                                                                                                                          |\n| [KB1080252](\/kb_view.do?sysparm_article=KB1080252) | PRB1555503 | Next Experience Unified Navigation     | Associating an empty core style record to the Next Experience theme displays a white screen                                                                                                                              |\n| [KB1120291](\/kb_view.do?sysparm_article=KB1120291) | PRB1557801 | UX Framework                           | The UXF does not trim strings for 'now-color' in UxFrameworkColorGenerator.java prior to parsing as an integer  which causes Next Experience pages to crash                                                              |\n| [KB1513421](\/kb_view.do?sysparm_article=KB1513421) | PRB1565976 | Key Management Framework               | Scheduled job fails daily because a Business Rule on sys_certificate deletes existing certificate attachments on update                                                                                                  |\n| [KB1117259](\/kb_view.do?sysparm_article=KB1117259) | PRB1569004 | IT Operations Management               | In Optimization and Health  old jobs are not being removed  if the plugin is installed after installation                                                                                                                |\n| [KB1224507](\/kb_view.do?sysparm_article=KB1224507) | PRB1569268 | Live Feed                              | Live feed notifications become active after upgrading to San Diego                                                                                                                                                       |\n| [KB1166040](\/kb_view.do?sysparm_article=KB1166040) | PRB1572894 | Enterprise Portal                      | EVAM data source bypasses Query Business Rules on Project Workspace                                                                                                                                                      |\n| [KB1274851](\/kb_view.do?sysparm_article=KB1274851) | PRB1581528 | Persistence                            | Queries still run on the database after the client has disconnected - \"Read timed out\" exception                                                                                                                         |\n| [KB1157817](\/kb_view.do?sysparm_article=KB1157817) | PRB1597897 | Discovery                              | Azure tag changes still insert a new cmdb_key_value with the same key  rather than updating the existing cmdb_key_value                                                                                                  |\n| [KB1198693](\/kb_view.do?sysparm_article=KB1198693) | PRB1601193 | Authentication                         | \\[Tokyo Partner Testing\\] Cannot register a new user account on Login page (Polaris view)                                                                                                                                |\n| [KB1182326](\/kb_view.do?sysparm_article=KB1182326) | PRB1606990 | Service Portal                         | Fix self-closing tag issues for Service Portal widgets                                                                                                                                                                   |\n| [KB1221835](\/kb_view.do?sysparm_article=KB1221835) | PRB1607765 | Graph API                              | \"graphql_schema_admin\" role missing from \"write_role\" field on \"glide.graphql.introspection_enabled\" sys_properties record.                                                                                              |\n| [KB1443045](\/kb_view.do?sysparm_article=KB1443045) | PRB1631282 |                                        | The plugin com.snc.platform_document_management creates snc_internal role with a wrong sys_id                                                                                                                            |\n| [KB1504068](\/kb_view.do?sysparm_article=KB1504068) | PRB1642990 |                                        | Unable to create new case with French Language                                                                                                                                                                           |\n| [KB1292152](\/kb_view.do?sysparm_article=KB1292152) | PRB1648000 |                                        | \\[RPS\\] Anti virus scan prevents syncing of attachments using Remote Process Sync                                                                                                                                        |\n| [KB1309227](\/kb_view.do?sysparm_article=KB1309227) | PRB1658441 |                                        | System Notifications are not translatable with respective i18 language                                                                                                                                                   |\n| [KB1309212](\/kb_view.do?sysparm_article=KB1309212) | PRB1659016 |                                        | Variable summarizer in the standard ticket tab does not honour UI policies\/display submitted variable incorrectly                                                                                                        |\n| [KB1560292](\/kb_view.do?sysparm_article=KB1560292) | PRB1659842 |                                        | Unexpected behaviour for System Archive Related Records when the tables are in hierarchy of the table structure                                                                                                          |\n| [KB1340471](\/kb_view.do?sysparm_article=KB1340471) | PRB1661584 |                                        | Non-admin Users are not triggering notifications for following records                                                                                                                                                   |\n| [KB1518969](\/kb_view.do?sysparm_article=KB1518969) | PRB1662906 |                                        | Only 1 input field coming for JDBC step on clicking Test Data button                                                                                                                                                     |\n| [KB1335068](\/kb_view.do?sysparm_article=KB1335068) | PRB1668655 |                                        | Initialize function of client-callable Script Include is evaluated when the Script Include is shown on the Form                                                                                                          |\n| [KB1510175](\/kb_view.do?sysparm_article=KB1510175) | PRB1670529 |                                        |\"\n\n",
        "QUERY": " 'Why is a user role record with an empty user gets displayed'",
        "GROUND_TRUTH": "A User Role record with an \"empty\" user can be displayed in the User Roles table due to the deletion of a user that had the orphaned role. This is most likely the cause of the orphaned role. To identify the user the role belonged to, you can open the User Role record, show the XML and get the sys_id for the User. Then, check the \"Audit Deleted Records\" and filter for Table name =sys_user and Payload contains the sys_id you are looking for."
    },
    {
        "id": 75,
        "CONTEXT": "\"## MS Teams installation with error code 9\n\n**Identify installation error**\n\nAfter activating Conversational Integration with Microsoft Teams users will get an option to install MS teams from messaging app integration page. During installation when the user clicks on install and if bellow error page is displayed with error code = 9  this is because the user on MS Team tenant does not have one of the required mentioned roles.\n\nPlease follow the [doc](https:\/\/docs.servicenow.com\/bundle\/quebec-now-intelligence\/page\/administer\/virtual-agent\/concept\/teams-conv-integration.html \"doc\") to know more about Conversational Integration with Microsoft Teams plugin\n\nHere is a [link](https:\/\/store.servicenow.com\/sn_appstore_store.do#!\/store\/application\/8be385e4776110105d7b3882a910610e\/2.0.0?sl=sh \"link\") for store app plugin for Conversational Integration with Microsoft Teams plugin\n\n**Required Roles**\n\nServicenow: virtual_agent_admin and external_app_install_admin or admin\n\nMicrosoft Teams administrator (one of the following): Global Administrator  Application Administrator  or Cloud Application Administrator\n\n**How to resolve the above error**\n\nHere is a [Microsoft document](https:\/\/docs.microsoft.com\/en-us\/microsoft-365\/admin\/add-users\/assign-admin-roles?view=o365-worldwide \"Microsoft document\") that will help to add required roles to users. Once the user is assigned one of the above roles  they should be able to install\/override MS Teams configuration.\n\n\n\n## ServiceNow for Microsoft 365 Integrations Issues and workaround\n\n#### Issue: Manifest generation fails and show error\n\nManually delete both OAuth Registry record (go to table oauth_entity.list) named 'Azure AD -- {TenantId}' and 'Azure AD -- sso - {TenantName}' along with corresponding OIDC provider configuration (go to table oidc_provider_configuration.list) record named 'Azure AD -- {TenantId}' or delete records OIDC provider configuration record with metadata URL\n[https:\/\/sts.windows.net\/{tenantID}\/.well-known\/openid-configuration](https:\/\/sts.windows.net\/%7btenantID%7d\/.well-known\/openid-configuration)\nOR\n[https:\/\/login.microsoftonline.com\/{tenantId}\/v2.0\/.well-known\/openid-configuration](https:\/\/login.microsoftonline.com\/a279ea8c-dc41-4e91-8646-3b0fc8ffe81c\/v2.0\/.well-known\/openid-configuration)\n\nIf both not present delete whichever present.\n\nAfter deleting above records please regenerate manifest this issue will disappear\n\n#### Issue: Employee Center not loading in Teams Error shows \"resource disabled\"\n\nThere is couple of reason for this as given below:\n\nOne reason could be configuration issue on Microsoft Azure Portal.\nPossible Resolution:\nLogin to [https:\/\/portal.azure.com](https:\/\/portal.azure.com\/)\nPlease check created microsoft azure application's 'Application ID URI'. Validate client-id correct it if wrong  Validate Instance URL it should be ServiceNow instance URL without http. E.g. api:\/\/\\<instance-url-without-http\\>\/\\<client-id\\>.\n\nAnother reason could be the clientId in manifest might be incorrect it should same as created Microsoft Azure Portal application's clientId. For more information please check in the doc [https:\/\/docs.servicenow.com\/bundle\/vancouver-employee-service-](https:\/\/docs.servicenow.com\/bundle\/vancouver-employee-service-management\/page\/product\/sn-teams\/task\/authenticate-users-your-hub-ms-teams.html)[management\/page\/product\/sn-teams\/task\/authenticate-users-your-hub-ms-teams.html](https:\/\/docs.servicenow.com\/bundle\/vancouver-employee-service-management\/page\/product\/sn-teams\/task\/authenticate-users-your-hub-ms-teams.html)\n\n#### Issue: Employee Center not loading in Teams - Error while authorising user\n\nThere are couple of reason for this issue or possible solution. But the root is customer is not able to get session of ServiceNow instance when trying to access Employee Center from Microsoft Teams. Below given some possible solution for this issue please validate all of them in customer ServiceNow instance and correct it.\n\nValidate loggedIn user's emailId in Microsoft Teams and ServiceNow instance sys_user table it should match.\nPlease check oidc_provider_configuration table's 'Azure AD -- {TenantId}' record user claim and User field point to the right values. OOTB user field value is 'Email'. User claim value can be one of the below.\n\nFor Self-configured apps configuration approach OOTB record user claim value is 'upn'.\nFor Pre-published configuration approach OOTB record user claim value is 'preferred_username'.\n\nIf primary email field is not sys_user.email then modify oidc_provider_configuration tables 'Azure AD -- {TenantId}' record user field to pick a field from sys_user table to match the value of 'upn' or 'preffered_username' of token.\n\nUser Field - this is picked from sys_user table. The value of this column and upn mentioned in User Claim field should be matching.\n\nIf customer has Self configured apps configuration Please check created azure app on azure portal if customer chose single tenant option in supported account types option.\n**Note:** Azure AD -- {TenantId}' is OOTB name customer may have changed the name of record please pick record with OIDC Meta data URL either it will be [https:\/\/sts.windows.net\/{tenantID}\/.well-known\/openid-configuration\/](https:\/\/sts.windows.net\/%7btenantID%7d\/.well-known\/openid-configuration\/) or [https:\/\/login.microsoftonline.com\/v2\/{tenantID}\/.well-known\/openid-configuration\/](https:\/\/login.microsoftonline.com\/v2\/%7btenantID%7d\/.well-known\/openid-configuration\/) based on whether customer chosen self- configured or pre-published configuration approach.\n\n#### Issue: Is it possible to remove or re-order Chat Tab  Static Tab in Microsoft Teams custom app (Or ServiceNow for Teams App or ServiceNow for Microsoft 365 app)?\n\nYes it's possible please go through this KB article for detailed explanation\n[https:\/\/support.servicenow.com\/kb?id=kb_article_view\\&sys_kb_id=c1ba32478352a1101824ad50ceaad384](https:\/\/support.servicenow.com\/kb?id=kb_article_view&sys_kb_id=c1ba32478352a1101824ad50ceaad384)\n\n#### Issue: Employee Center not loading in Teams\/Outlook\/M365 apps refused connection error\n\nPlease check value of records in sys_response_header table. Records which belongs to 'Azure Active Directory Mapping' application or 'Microsoft Integrations - Core' application e.g. ms_login UI page  proxy_login UI page and Service Portal(OOTB or Custom Portal) must include \\*.teams.microsoft.com \\*.office.com \\*.office365.com \\*.microsoft365.com [https:\/\/teamsproxy.service-now.com](https:\/\/teamsproxy.service-now.com) values. If there is a record in Global application which appliesTo all pages and it's active flag is true it should also contain above values.\n\n#### Issue: ServiceNow for Microsoft Teams integration is not working -- cross scope privilege issue\n\nAccessing employee center in teams throws error cross scope policy access.\n\nThis is mostly because of customer has customisation which was creating cross -scope privilege access. Please approve created cross scope privilege record. After approving it should load Employee Center Teams.\n\n#### Issue: How to load different portals in different domain or \"sn_now_teams.portal.suffix\" in order to route users to different portals using Microsoft Teams integration possible in domain separation\n\n\"sn_now_teams.portal.suffix\" system property is domain separated customers can configure different portal suffix url in different domain on this property. When user access Employee Center or Custom Portal in Teams  based on user domain in ServiceNow corresponding portal loads in Microsoft Teams.\n\n#### Issue: ServiceNow Teams App not visible in iOS device or ServiceNow Teams App portal loading issue in iOS device\n\nThis was the issue from Microsoft. In lower version schema of manifest Custom app was not loading in iOS device with 1.16 version of manifest\nMicrosoft fixed this issue. \n\n## An outdated authentication is being deprecated for ServiceNow for Teams (Request based chat)\n\nAn outdated authentication process for **Request based chat** (MS Teams chat) is being deprecated as it has reached its end-of-life and will no longer supported. Alternative \"Install Azure Apps\" based setup  is easy on-click installation and does not requires any manual configurations  it uses more secure ms teams chat authentication process.\n\nHow to identify if outdated authentication setup is being used for **Request based chat** integration?\n\nGo system properties table and check if following properties are populated\n\n* **sn_now_teams.app_client_id**\n* **sn_now_teams.tenant_id**\n\nGo to \"ServiceNow for Microsoft 365\" -\\> \"Install Azure Apps\" module. Check if tenant is not connected and **Request based chat** app are not installed.\n\nIf these properties are populated and **Request based chat**is not installed from \"Install Azure Apps\" module then instance is having outdated setup.\n\nHow to move away from outdated setup?\n\nNeed to install **Request based chat** application from \"ServiceNow for Microsoft 365\" -\\> \"Install Azure Apps\" module. By installing this application system will stop using old authentication process.\n\nPlease refer below documentation for setup process for **Request based chat**application.\n\n[https:\/\/docs.servicenow.com\/bundle\/vancouver-employee-service-management\/page\/product\/sn-teams\/concept\/setup-sn-ms-teams.html](https:\/\/docs.servicenow.com\/bundle\/vancouver-employee-service-management\/page\/product\/sn-teams\/concept\/setup-sn-ms-teams.html)\n\nDo we need reinstall or upgrade store application to complete this setup?\n\nNo.\n\n\"Install Azure Apps\" modules is already available on old versions of these store applications (IT Service Management integration with Microsoft Teams or HR Service Delivery Integration with Microsoft Teams).\n\nSo there is no need for reinstallation or upgradation of there applications. Only setup need to redone by admin  if at all instance is still using an outdated setup.\n\nWill end user see any difference?\n\nNo.\n\nEnd user will not see any difference in functionality. As this is only backend process change.\n\n\"\n\n",
        "QUERY": " 'Why Global Administrator is required for MS team integration'",
        "GROUND_TRUTH": "The Global Administrator role is required for MS Teams integration because it is one of the roles that has the necessary permissions to install and configure the integration. When a user attempts to install MS Teams from the messaging app integration page and encounters error code 9, it indicates that the user does not have one of the required roles on the MS Teams tenant. The required roles for Microsoft Teams are Global Administrator, Application Administrator, or Cloud Application Administrator. Therefore, having the Global Administrator role is crucial for the successful installation and configuration of the MS Teams integration."
    },
    {
        "id": 76,
        "CONTEXT": "\"## Delete demands\n\n[Strategic Portfolio Management](..\/..\/..\/product\/it-business-management\/reference\/r_ITBusinessManagement.html \"Align work with business goals to deliver products and services in a way that supports your strategic priorities. ServiceNow Strategic Portfolio Management (SPM  formerly IT Business Management  enables you to use digital transformation to drive more customer value faster. Plan  deliver  and track value across different methodologies.\") \\> [Project Portfolio Management](..\/..\/..\/product\/project-portfolio-suite\/concept\/c_ProjectPortfolioSuite.html \"Project Portfolio Management provides a simplified  team-oriented approach to Project Portfolio Management and IT development by combining several individual applications.\") \\> [Demand Management](..\/..\/..\/product\/planning-and-policy\/concept\/c_DemandManagement.html \"The Demand Management application consists of tools for capturing  centralizing  and assessing strategic and operational demands. It also provides a single location for managing all the demand information.\") \\>\n\n# Delete demands {#ariaid-title1}\n\nDemands can be deleted only while in the Pending state.\nRole required: it_demand_manager or it_demand_user\nWhen you delete a demand  all data related to the demand  such as risks  demand tasks  requirements  and decisions are deleted  but the stakeholders are not deleted from the Stakeholder Register \\[dmn_stakeholders_register\\] table.\n\nIf a project is already created from a demand  its reference is removed from the project along with the data related to the demand  however  the project is not deleted from the database.\n\nTo delete a demand:\n\n1. Navigate to All \\> Demand \\> Demands \\> All.\n2. Do:\n   * Click the demand to open the demand form and then click Delete.\n   * Select the check box next to the demand and then select Delete from the Actions choice list.\n{#t_DeletingDemands__choices_snq_kmm_2r}  \n**Parent Topic:** [Editing demands](..\/..\/..\/product\/planning-and-policy\/concept\/editing-demands.html \"Users with the demand manager role can view and modify demands using the Demand Management application.\")\n\n\n\n[Strategic Portfolio Management](..\/..\/..\/product\/it-business-management\/reference\/r_ITBusinessManagement.html \"Align work with business objectives to deliver products and services in a way that supports your strategic priorities. ServiceNow Strategic Portfolio Management (SPM  formerly ServiceNow IT Business Management  enables you to make informed choices and use digital transformation to drive more customer value faster. Plan  deliver  and track value across different methodologies.\") \\> [Project Portfolio Management](..\/..\/..\/product\/project-portfolio-suite\/concept\/c_ProjectPortfolioSuite.html \"Project Portfolio Management provides a simplified  team-oriented approach to Project Portfolio Management and IT development by combining several individual applications.\") \\> [Demand Management](..\/..\/..\/product\/planning-and-policy\/concept\/c_DemandManagement.html \"The Demand Management application consists of tools for capturing  centralizing  and assessing strategic and operational demands. It also provides a single location for managing all the demand information.\") \\>\n\n# Delete demands {#ariaid-title1}\n\nDemands can be deleted only while in the Pending state.\nRole required: it_demand_manager or it_demand_user\nWhen you delete a demand  all data related to the demand  such as risks  demand tasks  requirements  and decisions are deleted  but the stakeholders are not deleted from the Stakeholder Register \\[dmn_stakeholders_register\\] table.\n\nIf a project is already created from a demand  its reference is removed from the project along with the data related to the demand  however  the project is not deleted from the database.\n\nTo delete a demand:\n\n1. Navigate to Demand \\> Demands \\> All.\n2. Do:\n   * Click the demand to open the demand form and then click Delete.\n   * Select the check box next to the demand and then select Delete from the Actions choice list.\n{#t_DeletingDemands__choices_snq_kmm_2r}  \n**Parent Topic:** [Use Demand Management](..\/..\/..\/product\/planning-and-policy\/reference\/r_UsingDemandManagement.html \"Users with the demand manager role can create  view  and modify demands using the Demand Management application.\")  \n**Related concepts**   \n\n* [Assess demands](..\/..\/..\/product\/planning-and-policy\/concept\/c_AssessingDemands.html \"The Demand Management application comes with two demand visualization tools that can aid decision makers with demand assessment.\")\n* [Demand tasks](..\/..\/..\/product\/planning-and-policy\/concept\/demand-task.html \"A demand task is a unit of work  created within a demand  to break down initial planning activities before converting the demand into a project  change  enhancement  or defect.\")\n* [Actual cost and effort calculation for a demand and demand task](..\/..\/..\/product\/planning-and-policy\/concept\/actual-cost-effort-calculation-demand.html \"The actual cost and effort are realized cost incurred and time spent for the work performed on a demand and demand task during a specific time period. Actual cost and effort are calculated based on the approved time cards and hourly rate for the resources and vary based on how the hourly rate for the resource is derived.\")\n* [Add details to demands](..\/..\/..\/product\/planning-and-policy\/concept\/c_EnhancingDemands.html \"The demand manager typically works with a business relationship manager to identify stakeholders and elicit requirements  risks  and other important information.\")\n* [RIDAC (Risk  Issue  Decision  Action  and Request Changes) record entries for a demand](..\/..\/..\/product\/planning-and-policy\/concept\/ridac-entries-for-demand.html \"RIDAC is an acronym for Risk  Issue  Decision  Action  and Request Changes records. Create a risk record for your demand that you can convert to other records during the demand life cycle to track issues and to avoid having to manually copy relevant details in related records.\")\n* [Composite fields](..\/..\/..\/product\/project-management\/concept\/c_CompositeFields.html \"A composite field combines information from two fields in a table to form a single field.\")  \n**Related tasks**   \n* [Create a demand](..\/..\/..\/product\/planning-and-policy\/task\/t_CreatingDemands.html \"Create demands to capture your strategic and operational demands.\")\n* [View demands](..\/..\/..\/product\/planning-and-policy\/task\/t_ViewDemands.html \"You can view existing demands at any time.\")\n* [Reset a demand to Draft state](..\/..\/..\/product\/planning-and-policy\/task\/reset-demand-to-draft-state.html \"A demand can be moved back to Draft state  if required.\")\n* [Move and resize a demand](..\/..\/..\/product\/planning-and-policy\/task\/t_MoveAndResizeADemand.html \"As the demand manager  you can move and resize bubbles in the bubble chart.\")  \n**Related reference**   \n* [Stage fields](..\/..\/..\/product\/planning-and-policy\/reference\/r_StageFields.html \"The Stage field on the Ideas list displays the current state of an idea as it moves through the demand life cycle. The current state includes from an idea to a demand and then to the resulting project  enhancement  change  or defect.\")\n\n\n\n[Strategic Portfolio Management](..\/..\/..\/product\/it-business-management\/reference\/r_ITBusinessManagement.html \"Align work with business goals to deliver products and services in a way that supports your strategic priorities. ServiceNow Strategic Portfolio Management (SPM)  formerly IT Business Management  enables you to use digital transformation to drive more customer value faster. Plan  deliver  and track value across different methodologies.\") \\> [Project Portfolio Management](..\/..\/..\/product\/project-portfolio-suite\/concept\/c_ProjectPortfolioSuite.html \"Project Portfolio Management provides a simplified  team-oriented approach to Project Portfolio Management and IT development by combining several individual applications.\") \\> [Demand Management](..\/..\/..\/product\/planning-and-policy\/concept\/c_DemandManagement.html \"The Demand Management application consists of tools for capturing  centralizing  and assessing strategic and operational demands. It also provides a single location for managing all the demand information.\") \\>\n\n# Delete demands {#ariaid-title1}\n\nDemands can be deleted only while in the Pending state.\nRole required: it_demand_manager or it_demand_user\nWhen you delete a demand  all data related to the demand  such as risks  demand tasks  requirements  and decisions are deleted  but the stakeholders are not deleted from the Stakeholder Register \\[dmn_stakeholders_register\\] table.\n\nIf a project is already created from a demand  its reference is removed from the project along with the data related to the demand  however  the project is not deleted from the database.\n\nTo delete a demand:\n\n1. Navigate to Demand \\> Demands \\> All.\n2. Do:\n   * Click the demand to open the demand form and then click Delete.\n   * Select the check box next to the demand and then select Delete from the Actions choice list.\n{#t_DeletingDemands__choices_snq_kmm_2r}  \n**Parent Topic:** [Use Demand Management](..\/..\/..\/product\/planning-and-policy\/reference\/r_UsingDemandManagement.html \"Users with the demand manager role can create  view  and modify demands using the Demand Management application.\")  \n**Related concepts**   \n\n* [Assess demands](..\/..\/..\/product\/planning-and-policy\/concept\/c_AssessingDemands.html \"The Demand Management application comes with two demand visualization tools that can aid decision makers with demand assessment.\")\n* [Demand tasks](..\/..\/..\/product\/planning-and-policy\/concept\/demand-task.html \"A demand task is a unit of work  created within a demand  to break down initial planning activities before converting the demand into a project  change  enhancement  or defect.\")\n* [Actual cost and effort calculation for a demand and demand task](..\/..\/..\/product\/planning-and-policy\/concept\/actual-cost-effort-calculation-demand.html \"The actual cost and effort are realized cost incurred and time spent for the work performed on a demand and demand task during a specific time period. Actual cost and effort are calculated based on the approved time cards and hourly rate for the resources and vary based on how the hourly rate for the resource is derived.\")\n* [Add details to demands](..\/..\/..\/product\/planning-and-policy\/concept\/c_EnhancingDemands.html \"The demand manager typically works with a business relationship manager to identify stakeholders and elicit requirements  risks  and other important information.\")\n* [RIDAC (Risk  Issue  Decision  Action  and Request Changes) record entries for a demand](..\/..\/..\/product\/planning-and-policy\/concept\/ridac-entries-for-demand.html \"RIDAC is an acronym for Risk  Issue  Decision  Action  and Request Changes records. Create a risk record for your demand that you can convert to other records during the demand life cycle to track issues and to avoid having to manually copy relevant details in related records.\")\n* [Composite fields](..\/..\/..\/product\/project-management\/concept\/c_CompositeFields.html \"A composite field combines information from two fields in a table to form a single field.\")  \n**Related tasks**   \n* [Create a demand](..\/..\/..\/product\/planning-and-policy\/task\/t_CreatingDemands.html \"Create demands to capture your strategic and operational demands.\")\n* [View demands](..\/..\/..\/product\/planning-and-policy\/task\/t_ViewDemands.html \"You can view existing demands at any time.\")\n* [Reset a demand to Draft state](..\/..\/..\/product\/planning-and-policy\/task\/reset-demand-to-draft-state.html \"A demand can be moved back to Draft state  if required.\")\n* [Move and resize a demand](..\/..\/..\/product\/planning-and-policy\/task\/t_MoveAndResizeADemand.html \"As the demand manager  you can move and resize bubbles in the bubble chart.\")  \n**Related reference**   \n* [Stage fields](..\/..\/..\/product\/planning-and-policy\/reference\/r_StageFields.html \"The Stage field on the Ideas list displays the current state of an idea as it moves through the demand life cycle. The current state includes from an idea to a demand and then to the resulting project  enhancement  change  or defect.\")\n\n\"\n\n",
        "QUERY": " 'when project is deleted from a demand all information is deleted'",
        "GROUND_TRUTH": "According to the information in KB12345, when a demand is deleted, all data related to the demand such as risks, demand tasks, requirements, and decisions are deleted. However, if a project has already been created from a demand, its reference is removed from the project along with the data related to the demand, but the project itself is not deleted from the database. Therefore, not all information is deleted when a project is removed from a demand."
    },
    {
        "id": 77,
        "CONTEXT": "\"## Validate discovery results\n\nIf no default MID Server is specified  it uses the default MID Server specified for the ALL application  assuming it is up.\n6. If no default MID Server is specified  Discovery cycles through the previous steps and looks for MID Servers with the status of Paused or Upgrading.  \n   Note: When a MID Server is paused or upgrading  it does not actually process commands until it returns to the status of Up.\n{#c-MIDServerSelectionSequence__ol_sxc_bm3_tx}  \n\n### MID Server clusters\n\nThese steps are followed when you select Specific MID Cluster for the MID Server selection method on the Discovery form  and the cluster is a load balancing cluster:\n\n1. Discovery uses the first MID Server in the cluster that it finds with the status of Up.\n2. If more than one MID Servers are up  it randomly picks one. If it cannot find any MID Servers  it looks for MID Servers in the cluster with the status of Paused or Upgrading.\n{#c-MIDServerSelectionSequence__ol_jhj_wk3_tx}  \nThese steps are followed when the cluster is a failover cluster:\n1. Discovery uses the MID Server with the lowest Order value that also has the status of Up.\n2. If no MID Servers are found  it looks for MID Servers in the cluster with the status of Paused or Upgrading  choosing the one with the lowest Order value.\n\n{#c-MIDServerSelectionSequence__ol_g2m_1l3_tx}\nNote: Discovery ignores the default MID Server for it and ALL applications when selecting a MID Server from the cluster.\n\n### Port scan (Shazzam) phase\n\nDuring the port scan phase  Discovery collects all the target IP addresses. It splits them equally between MID Servers matching the criteria (MID Servers are qualified to do the port scan). The Shazzam batch size  which you configured on the Discovery schedule  determines the number of IP addresses that each Shazzam probe can scan. This phase helps determine how much work each MID Server does during the port scan phase.\n\nFor example  you have 16 000 IP addresses to scan among three qualified MID Servers  and you use the default Shazzam batch size of 5000. Two of the MID Servers handle 5000 IP address scans (one Shazzam probe each). The other MID Server handles 6000 IP address scans by launching two Shazzam probes.\n**Related topics**\n\n* [MID Server pause](product\/mid-server\/task\/t_PauseTheMIDServer.html#c_MIDServerPause)\n* [MID Server cluster configuration](product\/mid-server\/task\/t_ConfigureAMIDServerCluster.html#mid-server-clusters)\n\n\n\n## Run a Quick Discovery\n\nIf no default MID Server is specified  it uses the default MID Server specified for the ALL application  assuming it is up.\n6. If no default MID Server is specified  Discovery cycles through the previous steps and looks for MID Servers with the status of Paused or Upgrading.  \n   Note: When a MID Server is paused or upgrading  it does not actually process commands until it returns to the status of Up.\n{#c-MIDServerSelectionSequence__ol_sxc_bm3_tx}  \n\n### MID Server clusters\n\nThese steps are followed when you select Specific MID Cluster for the MID Server selection method on the Discovery form  and the cluster is a load balancing cluster:\n\n1. Discovery uses the first MID Server in the cluster that it finds with the status of Up.\n2. If more than one MID Servers are up  it randomly picks one. If it cannot find any MID Servers  it looks for MID Servers in the cluster with the status of Paused or Upgrading.\n{#c-MIDServerSelectionSequence__ol_jhj_wk3_tx}  \nThese steps are followed when the cluster is a failover cluster:\n1. Discovery uses the MID Server with the lowest Order value that also has the status of Up.\n2. If no MID Servers are found  it looks for MID Servers in the cluster with the status of Paused or Upgrading  choosing the one with the lowest Order value.\n\n{#c-MIDServerSelectionSequence__ol_g2m_1l3_tx}\nNote: Discovery ignores the default MID Server for it and ALL applications when selecting a MID Server from the cluster.\n\n### Port scan (Shazzam) phase\n\nDuring the port scan phase  Discovery collects all the target IP addresses. It splits them equally between MID Servers matching the criteria (MID Servers are qualified to do the port scan). The Shazzam batch size  which you configured on the Discovery schedule  determines the number of IP addresses that each Shazzam probe can scan. This phase helps determine how much work each MID Server does during the port scan phase.\n\nFor example  you have 16 000 IP addresses to scan among three qualified MID Servers  and you use the default Shazzam batch size of 5000. Two of the MID Servers handle 5000 IP address scans (one Shazzam probe each). The other MID Server handles 6000 IP address scans by launching two Shazzam probes.\n**Related topics**\n\n* [MID Server pause](product\/mid-server\/task\/t_PauseTheMIDServer.html#c_MIDServerPause)\n* [MID Server cluster configuration](product\/mid-server\/task\/t_ConfigureAMIDServerCluster.html#mid-server-clusters)\n\n\n\nIf no MID Servers meet these criteria  it looks for a MID Server that has the ALL application that also has an appropriate IP range configured.\n3. If more than one MID Server meet the criteria  Discovery chooses the first MID Server with the status of Up. If more than one MID Server is up  it randomly picks one.\n4. If none are up  it uses the default MID Server specified for the Discovery application  assuming it is up.\n5. If no default MID Server is specified  it uses the default MID Server specified for the ALL application  assuming it is up.\n6. If no default MID Server is specified  Discovery cycles through the previous steps and looks for MID Servers with the status of Paused or Upgrading.  \n   Note: When a MID Server is paused or upgrading  it does not actually process commands until it returns to the status of Up.\n{#c-MIDServerSelectionSequence__ol_sxc_bm3_tx}  \n\n### MID Server clusters\n\nThese steps are followed when you select Specific MID Cluster for the MID Server selection method on the Discovery form  and the cluster is a load balancing cluster:\n\n1. Discovery uses the first MID Server in the cluster that it finds with the status of Up.\n2. If more than one MID Server is up  it randomly picks one. If it cannot find any MID Servers  it looks for MID Servers in the cluster with the status of Paused or Upgrading.\n{#c-MIDServerSelectionSequence__ol_jhj_wk3_tx}  \nThese steps are followed when the cluster is a failover cluster:\n1. Discovery uses the MID Server with the lowest Order value that also has the status of Up.\n2. If no MID Servers are found  it looks for MID Servers in the cluster with the status of Paused or Upgrading  choosing the one with the lowest Order value.\n\n{#c-MIDServerSelectionSequence__ol_g2m_1l3_tx}\nNote: Discovery ignores the default MID Server for it and ALL applications when selecting a MID Server from the cluster.\n\n### Port scan (Shazzam) phase\n\nDuring the port scan phase  Discovery collects all the target IP addresses. It splits them equally between MID Servers matching the criteria (MID Servers are qualified to do the port scan). The Shazzam batch size  which you configured on the Discovery schedule  determines the number of IP addresses that each Shazzam probe can scan. This phase helps determine how much work each MID Server does during the port scan phase.\n\nFor example  you have 16 000 IP addresses to scan among three qualified MID Servers  and you use the default Shazzam batch size of 5000. Two of the MID Servers handle 5000 IP address scans (one Shazzam probe each). The other MID Server handles 6000 IP address scans by launching two Shazzam probes.\nNote: Shazzam can only process IP lists containing a total of 5000 IP addresses (both IPv4 and IPv6 addresses) if the list contains any IPv6 address. For schedules containing IP lists with IPv6 that go beyond this limit  Discovery gets cancelled with an error message.\n**Related topics**\n\n* [MID Server pause](product\/mid-server\/task\/t_PauseTheMIDServer.html#c_MIDServerPause)\n* [MID Server cluster configuration](product\/mid-server\/task\/t_ConfigureAMIDServerCluster.html#mid-server-clusters)\n\n\"\n\n",
        "QUERY": " 'What scanning does midserver perform'",
        "GROUND_TRUTH": "The MID Server performs a scanning process known as the Port scan (Shazzam) phase. During this phase, Discovery collects all the target IP addresses and splits them equally between MID Servers that match the criteria (MID Servers are qualified to do the port scan). The Shazzam batch size, which is configured on the Discovery schedule, determines the number of IP addresses that each Shazzam probe can scan. This phase helps determine how much work each MID Server does during the port scan phase. For example, if you have 16,000 IP addresses to scan among three qualified MID Servers and you use the default Shazzam batch size of 5000, two of the MID Servers handle 5000 IP address scans (one Shazzam probe each). The other MID Server handles 6000 IP address scans by launching two Shazzam probes. Note that Shazzam can only process IP lists containing a total of 5000 IP addresses (both IPv4 and IPv6 addresses) if the list contains any IPv6 address. For schedules containing IP lists with IPv6 that go beyond this limit, Discovery gets cancelled with an error message."
    },
    {
        "id": 78,
        "CONTEXT": "\"## Create a risk event task\n\n[Governance  Risk  and Compliance](..\/..\/..\/product\/grc-common\/reference\/r_WhatIsGRC.html \"Respond to business risks in real time. Connect security and IT with an integrated risk program offering continuous monitoring  prioritization  and automation.\") \\> [Using Risk Management](..\/..\/..\/product\/grc-risk\/concept\/using-risk-mgmt.html \"You can use the features and capabilities of the Risk Management application perform various activities such as creating issues  reporting risk events  performing risk assessments and so on.\") \\>\n\n# Create a risk event task {#ariaid-title1}\n\nA risk event might require associated tasks. Unless these tasks are created and eventually closed  the risk event cannot be closed.\nRole required: sn_risk.manager\n\n1. Navigate to All \\> Risk Events \\> All Events.\n2. Select the record for which a task must be created.\n3. Select the Tasks related list and then select New.\n4. On the form  fill the fields.  \n   {#d117488e102}\n   |        Field        |                                              Description                                              |\n   |---------------------|-------------------------------------------------------------------------------------------------------|\n   | Assigned to         | Person that the task is assigned to.                                                                  |\n   | Priority            | Level of importance given to a task  which indicates how readily a task or assignment can be delayed. |\n   | Short description   | Brief description of the task.                                                                        |\n   | Description         | Detailed description of the task.                                                                     |\n   | Work notes          | Any relevant information for your reference.                                                          |\n   | Additional comments | Any additional information that might be necessary.                                                   |\n   Table 1. Risk event task form   {#create-a-risk-event-task__table_cjy_rj3_f3b}\n5. Select Submit.\n\nReview the associated issues of a risk event or create a new issue during approval. For more information  see [Manually create GRC issues](..\/..\/grc-common\/task\/t_CreateAnIssue.html \"As a GRC user  you can manually create issues to document policy  risk  or audit observations  or to accept any GRC problems. You can also identify the source of the issue to help analyze and classify the issues.\").\n**Parent Topic:** [Use Risk Events](..\/..\/..\/product\/grc-risk\/concept\/use-risk-events.html \"Report risk events and monitor their workflow to prevent losses in your organization.\")\n\n\n\n[Governance  Risk  and Compliance](..\/..\/..\/product\/grc-common\/reference\/r_WhatIsGRC.html \"Respond to business risks in real time. Connect security and IT with an integrated risk program offering continuous monitoring  prioritization  and automation.\") \\> [Using Risk Management](..\/..\/..\/product\/grc-risk\/concept\/using-risk-mgmt.html \"You can use the features and capabilities of the Risk Management application perform various activities such as creating issues  reporting risk events  performing risk assessments and so on.\") \\>\n\n# Create a risk event task {#ariaid-title1}\n\nA risk event might require associated tasks. Unless these tasks are created and eventually closed  the risk event cannot be closed.\nRole required: sn_risk.manager\n\n1. Navigate to All \\> Risk Events \\> All Events.\n2. Select the record for which a task must be created.\n3. Select the Tasks related list and then select New.\n4. On the form  fill the fields.  \n   {#d116863e102}\n   |        Field        |                                              Description                                              |\n   |---------------------|-------------------------------------------------------------------------------------------------------|\n   | Assigned to         | Person that the task is assigned to.                                                                  |\n   | Priority            | Level of importance given to a task  which indicates how readily a task or assignment can be delayed. |\n   | Short description   | Brief description of the task.                                                                        |\n   | Description         | Detailed description of the task.                                                                     |\n   | Work notes          | Any relevant information for your reference.                                                          |\n   | Additional comments | Any additional information that might be necessary.                                                   |\n   Table 1. Risk event task form   {#create-a-risk-event-task__table_cjy_rj3_f3b}\n5. Select Submit.\n\nReview the associated issues of a risk event or create a new issue during approval. For more information  see [Manually create GRC issues](..\/..\/grc-common\/task\/t_CreateAnIssue.html \"As a GRC user  you can manually create issues to document policy  risk  or audit observations  or to accept any GRC problems. You can also identify the source of the issue to help analyze and classify the issues.\").\n**Parent Topic:** [Use Risk Events](..\/..\/..\/product\/grc-risk\/concept\/use-risk-events.html \"Report risk events and monitor their workflow to prevent losses in your organization.\")\n\n\n\n[Governance  Risk  and Compliance](..\/..\/..\/product\/grc-common\/reference\/r_WhatIsGRC.html \"Respond to business risks in real time. Connect security and IT with an integrated risk program offering continuous monitoring  prioritization  and automation.\") \\> [Using Risk Management](..\/..\/..\/product\/grc-risk\/concept\/using-risk-mgmt.html \"You can use the features and capabilities of the Risk Management application perform various activities such as creating issues  reporting risk events  performing risk assessments and so on.\") \\>\n\n# Create a risk event task {#ariaid-title1}\n\nA risk event might require associated tasks. Unless these tasks are created and eventually closed  the risk event cannot be closed.\nRole required: sn_risk.manager\n\n1. Navigate to All \\> Risk Events \\> All Events.\n2. Select the record for which a task must be created.\n3. Select the Tasks related list and then select New.\n4. On the form  fill the fields.  \n   {#d264128e106}\n   |        Field        |                                              Description                                              |\n   |---------------------|-------------------------------------------------------------------------------------------------------|\n   | Assigned to         | Person that the task is assigned to.                                                                  |\n   | Priority            | Level of importance given to a task  which indicates how readily a task or assignment can be delayed. |\n   | Short description   | Brief description of the task.                                                                        |\n   | Description         | Detailed description of the task.                                                                     |\n   | Work notes          | Any relevant information for your reference.                                                          |\n   | Additional comments | Any additional information that might be necessary.                                                   |\n   Table 1. Risk event task form   {#create-a-risk-event-task__table_cjy_rj3_f3b}\n5. Select Submit.\n\nReview the associated issues of a risk event or create a new issue during approval. For more information  see [Manually create GRC issues](..\/..\/grc-common\/task\/t_CreateAnIssue.html \"As a GRC user  you can manually create issues to document policy  risk  or audit observations  or to accept any GRC problems. You can also identify the source of the issue to help analyze and classify the issues.\").\n**Parent Topic:** [Use Risk Events](..\/..\/..\/product\/grc-risk\/concept\/use-risk-events.html \"Report risk events and monitor their workflow to prevent losses in your organization.\")\n\n\"\n\n",
        "QUERY": " 'what role can be assigned a risk event task'",
        "GROUND_TRUTH": "According to the article KB12345, the role that can be assigned a risk event task is 'sn_risk.manager'. This role is required to create and manage risk event tasks."
    },
    {
        "id": 79,
        "CONTEXT": "\"## Enable Dynamic Translation for an unsupported language\n\n[Now Platform capabilities](..\/..\/..\/administer\/general\/reference\/r_ServiceNowPlatform.html \"Digitize your workflows and run them at scale by using the core capabilities of the Now Platform. When you activate and configure these features  you can create custom business logic  manage and secure data  and build alternate user interfaces.\") \\> [Conversational Interfaces](..\/..\/..\/administer\/conversational-interfaces\/concept\/ci-landing-page.html \"Enable your users to engage with live agents  virtual agents  or both using ServiceNow Conversational Interfaces. These interfaces provide tools for creating and managing optimal conversational experiences for your users  in the chat channels that they prefer.\") \\> [Virtual Agent](..\/..\/..\/administer\/virtual-agent\/concept\/virtual-agent-landing-page.html \"Unlock your enterprise productivity with ServiceNow Virtual Agent. Provide your employees and customers with a friendly messaging interface  featuring pre-built conversations powered by artificial intelligence.\") \\> [Building and deploying Virtual Agent](..\/..\/..\/administer\/virtual-agent\/concept\/using-virtual-agent.html \"Convert the time your agents and technicians spend handling low-impact user requests into an intelligently managed interaction. Enable Virtual Agent with NLU to understand the intent of what people are looking for and provide them with more relevant answers.\") \\> [Localization options for Virtual Agent](..\/..\/..\/administer\/virtual-agent\/concept\/multi-language-options-va.html \"The ServiceNow platform provides several methods for serving your international Virtual Agent users  regardless of their language and locale. Professionally localize your conversations  use dynamic machine translation  or a combination of the two.\") \\>\n\n# Enable Dynamic Translation for an unsupported language {#ariaid-title1}\n\nThe ServiceNow\u00ae platform supports 22 languages  but you can enable dynamic machine translation for additional languages in Virtual Agent.\nInstall and configure the plugins for Dynamic Translation  as described in [Prerequisites for using Dynamic Translation in Virtual Agent](..\/concept\/prereq-using-dt-va.html \"To enable dynamic machine translation  install ServiceNow language plugins and Dynamic Translation plugins. Then configure Dynamic Translation for your instance.\").\n\nRole required: virtual_agent_admin or admin\nYou can configure Dynamic Translation for any installed language plugin on the ServiceNow\u00ae platform. Once installed  the language is available for configuration in the Multi language support card in Virtual Agent Chat Settings. But if you want to support a language that doesn't have a plugin  you can add it to the Languages \\[sys_language\\] table. Once added  you can enable dynamic machine translation for that language.\n\nFor more information about multi-language support options and their tradeoffs  see [Localization options for Virtual Agent](..\/concept\/multi-language-options-va.html \"The ServiceNow platform provides several methods for serving your international Virtual Agent users  regardless of their language and locale. Professionally localize your conversations  use dynamic machine translation  or a combination of the two.\"). For more information about custom localizations  see [Custom\ntranslations](..\/administer\/localization\/concept\/translating-applications.dita\/translating-applications.html).\n\n1. Navigate to All  and then enter sys_language.list in the filter.\n2. On the Languages page  click New.\n3. On the form  fill in the fields.  \n   {#d1141256e156}\n   |     Field      |                                                                                                                              Description                                                                                                                               |\n   |----------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n   | Name           | Name and locale of the language  if applicable. For example  Spanish (Mexican).                                                                                                                                                                                        |\n   | Active         | Check box indicating that the language is enabled on the instance.                                                                                                                                                                                                     |\n   | ID             | The [BCP 47](http:\/\/www.iana.org\/assignments\/language-subtag-registry\/language-subtag-registry) code for the language. For example  es-MX.                                                                                                                             |\n   | Text Direction | Direction of text for the language. Options are Left-to-Right or Right-to-Left.                                                                                                                                                                                        |\n   | Fallback       | Language to be used if a translation is not available. The language plugin must be installed on the instance and activated for NLU  if you are using NLU discovery. For example  if the language is Mexican Spanish  you can specify Spanish as the fallback language. |\n   {#enable-dt-unsupported-language__table_asg_jg4_mtb}\n4. Click Submit.\n5. **Optional:** For a better user experience  translate some of the Virtual Agent system messages into the new language.\n   1. Navigate to All  and then enter sys_ui_message.list in the filter.\n   2. Translate the following Keys:  \n      * Are you sure you want to end the current conversation?\n      * Click here to start a new conversation\n      * Close contact options\n      * \\[language name\\] (for example  English or German)\n      * Disable audio notifications\n      * End conversation\n      * I have detected you are typing in {0}. Would you like to continue the conversation in {0}?\n      * Just now\n      * No\n      * No Chat Agents Currently Available\n      * Please pick an option.\n      * Please type your request\n      * Please type your response here\n      * Send\n      * Start a new conversation\n      * Start typing to filter the list of topics below...\n      * Support options\n      * Yes\n      {#enable-dt-unsupported-language__ul_mpx_kl4_mtb}\nNote: If you specify a supported fallback language for the language you're adding  you do not need to translate Keys in the Messages table. \n\nFor example  if you add Mexican Spanish as a language  you can specify Spanish as the fallback language. The Spanish language plugin already contains translations of these messages. For more information  see [Specify a fallback language for locale-specific languages and NLU prediction](specify-fallback-language-nlu-prediction.html \"Provide language locale support by specifying a fallback language for Virtual Agent to use for topics  keywords  and NLU prediction. For example  the ServiceNow platform doesn't support Mexican Spanish (mx-es)  but it does support Spanish (es)  which you can designate as the fallback language for a better user experience.\").      {#enable-dt-unsupported-language__substeps_s44_c2h_15b}\n6. When the languages have been added  [enable Dynamic Translation for\nthem in Virtual Agent settings](enable-dynamic-lang-detection.html \"Enable Dynamic Translation for one or more languages in Chat Settings for Virtual Agent.\").  \n**Parent Topic:** [Using language detection and dynamic machine translation in Virtual Agent](..\/..\/..\/administer\/virtual-agent\/concept\/dynamic-lang-detection-translation.html \"Enable a combination of language detection and machine translation for Virtual Agent to improve the chat experience for diverse users. You can designate fallback languages for locales or dynamically translate languages that are not professionally localized or are not supported in NLU discovery.\")\n\n\n\n## Password Change form doesn't show up when the User language is 'French'  but works fine when set to \n\n## Description\n\nPassword Change form doesn't show up when the User language is 'French'  but works fine when set to either 'English' or 'Spanish'\n\n## Steps to Reproduce\n\n1. OOTB Instance: emprkrishnan00  \n2. Installed Password Reset Plugin  \n3. Make sure 'Password change' is checked in the 'Default Self Service'  \nhttps:\/\/emprkrishnan00.service-now.com\/pwd_process.do?sys_id=c6b0c20667100200a5a0f3b457415ad5  \n4. Installed the below plugins:  \nI18N: Internationalization plugin (com.glide.i18n)  \nI18N: French Translations plugin (com.snc.i18n.french)  \nhttps:\/\/docs.servicenow.com\/bundle\/vancouver-platform-administration\/page\/administer\/localization\/task\/t_ActivateALanguage.html  \n5. Impersonate 'abel.tuter' with 'ADMIN' role  \n6. Set the Language as 'English' and navigate to the below URL in the new tab. We will notice the Password change will be populated. (Screenshots attached: Screenshot 1.png)  \nhttps:\/\/emprkrishnan00.service-now.com\/$pwd_change.do  \n7. Now change the Language to 'French' and navigate to the below URL in the new tab. We will notice the Password change will not be populated. (Screenshots attached: Screenshot 2.png)  \nhttps:\/\/emprkrishnan00.service-now.com\/$pwd_change.do\n\n## Workaround\n\nThis problem is currently under review and targeted to be fixed in a future release. Subscribe to this Known Error article to receive notifications when more information will be available.\n\n**Related Problem: PRB1709661**\n\n\"\n\n",
        "QUERY": " 'what plugins are enabled when spanish plugin is enabled'",
        "GROUND_TRUTH": "no answer found"
    },
    {
        "id": 80,
        "CONTEXT": "\"## HLA System Properties\n\nThe purpose of this document is to list and provide an explanation for the HLA System Properties (**sn_occ_system_settings)**\n\nThe default (out of the box) settings are generally sufficient  however  in some cases customers may require changes and\/or optimisations to the core HLA System settings for some of the following reasons.\n\n* Enhance system performance (reduce bottlenecks)\n* Reduce noise\n* Adjust behaviour\n* Suit customer's data\n\nThe Properties below are grouped by subject  they are not in the order that they appear in the System Properties Table in HLA.\n\n#### **AGGREGATOR**\n\nMiddle of the Data Ingestion pipeline. Responsible for grouping and storing of metrics.\n\n**aggregator.bloom_filter_factor**\nThe coefficient which we should multiply by the Bloom Filter size\n\n**aggregator.concurrency_override**\nIf specified  the value will override the initial automatic allocation of resources to the aggregator.\n\n**aggregator.gauge.aggregation_type**\nControls whether gauge metrics should be tested using the average or median (default). Accepted values are: Average or Median\n\n**aggregator.metrics_bloom_filter_fpp**\nExpected false positive probability for the Bloom Filter. Used for monitoring purposes.\n\n**aggregator.min_non_null_values_for_stats**\nMinimum number of non null values in a series to calculate stats for (moving average  stdev...).\n\n**aggregator.number_of_expected_metrics**\nApproximation for how many unique metrics the aggregator should handle.\n\n**aggregator.queue_size**\nNumber of metrics that can be buffered in the aggregator before it starts blocking the processing pipe.\n\n**aggregator.resolution_seconds**\nThe resolution of the time series. i.e each data point represents the aggregation of data over this period of seconds.\n\n**aggregator.settle_seconds**\nHow many seconds should pass without receiving data until the window is considered settled. Once a window is settled  the detective can start running its algorithms.\n\n**aggregator.window_max_quantity_in_period_hours**\nCircuit-Breaker: Max active time-span of a metric  in hours. Events with metrics that arrive with timestamps spanning a wider time-span will not be aggregated.\n\n**aggregator.window_size_seconds**\nValue *must* be multiples of 60. How many seconds are considered a window. Windows are the time frame for the detection tasks to be exeucted on.\n\n**aggregator.window_size_seconds_custom**\nValue *must* be multiples of 60. How many seconds are considered a window for a CustomMetric. Windows are the time frame for the detection tasks to be exeucted on.\n\n**aggregator.workload_level**\nWork load level in which the Aggregator is considered stressed  options are: LOW  MEDIUM  HIGH.\n\n#### **ALERTS**\n\n**alerts.annotation_property**\nPart of the alert settings that deals with checking annotations for correlations.\n\n**alerts.is_anomaly_baseline_reference_decrease_disabled**\nIndicates if 'anomaly_baseline_reference_decrease' alert is disabled\n\n**alerts.max_alert_age_hours**\nAnomaly Detection won't apply to events older than this setting. This allows the system to identify and discard alerts that are considered 'too old'. If you are streaming real-time data and still see detection windows being dropped for age  this might indicate: 1) delay in the processing pipeline (for example: a specific Data Input was stopped for a couple of hours  then started again)  2) incorrect extraction of the timestamp field (for example wrong timezone: the timestamp being sent is supposed to be read as Easter Standard Time  but is being read as UTC since there is no indicator in the timestamp). If you are streaming historic data  this setting MUST be increased to include the dates of the historical data. (for example: if today is Jan 2021  and the historical data is being streamed from Jan 2020  please make sure the time here is AT LEAST 8760 (hours) i.e. 365 days \\* 24 hours\/day) {Also note that an additional setting should also be increased: broker.events.max_age_hours}\n\n**alerts.recent_events.max_size_bytes**\nThe maximum size (bytes) allowed for recent events\n\n**alerts.recent_events_for_timeless_gauge_period_seconds**\nsame as alerts.recent_events_period_seconds but for timeless-gauge detections\n\n**alerts.recent_events_period_seconds**\nTime period to look-back from the point of the anomaly to fetch relevant events - which will be used for RCA. (recommendation is not to exceed around 24 hours in seconds)\n\n#### **ALERTS CREATOR**\n\nThe last part of the pipeline before alerts are created and populated in the incident list.\n\n**alerts_creator.queue_size**\nNumber of detections that can be buffered in the alerts creator before it starts blocking the processing pipe.\n\n#### **ARCA** (Automatic Root Cause Analyses):\n\nThis refers to the properties extracted via Source Types that are categorized as \"ARC_only\".\n\n**arca.entities_analyzer.max_days_lookback**\nTo build the \"meaningful entities\" section of the RCA report  the AI engine goes back up to this number of days to analyze relevant events. (recommendation is not to exceed 2 days)\n\n**arca.entities_analyzer.max_entity_occurrences**\nFor each entity presented in the root-cause section  the AI engine adds events surrounding the detection time. This setting controls the number of such events that will be added.\n\n**arca.highlights_analyzer.majority_vote**\nIn the highlights analyzer  min number of past matching events from the same host\/day\/hour to qualify as a highlight\n\n**arca.highlights_analyzer.max_days_lookback**\n\n\nTo build the \"highlights\" section of the RCA report  the AI engine goes back up to this number of days to analyze relevant events.\n\n**arca.highlights_analyzer.number_of_highlights**\nNumber of highlights to be presented in the \"highlights\" section of the RCA\n\n**arca.mf_analyzer.number_of_changes**\nMax number of changes to show in the \"significant changes\" section in the RCA\n\n#### **BROKER**\n\nThis is the start of the Data Ingestion Pipeline. It is responsible for data integration and digestion  including parsing of the logs.\n\n**broker.concurrency_override**\nIf specified  the value will override the initial automatic allocation of resources to the event broker.\n\n**broker.events.max_age_hours**\nEvents older than this number of hours will be dropped\n\n**broker.headerdetection.vmware**\nlist of vmware apps used by header detection to detect as vmware\n\n**broker.header_detection.detect_beaver**\nWhen on  the AI engine will attempt to detect and parse beaver headers. Default is ON\n\n**broker.header_detection.detect_syslog5424**\nWhen on  the AI engine will attempt to detect and parse Syslog5424 headers. Default is ON\n\n**broker.queue_size**\nNumber of events that can be buffered in the event broker before it starts blocking the processing pipe.\n\n**broker.workload_level**\nWork load level in which the Event Broker considered stressed  options are: LOW  MEDIUM  HIGH.\n\n#### **CLOTHO**\n\n**clotho.batch_size**\nBulk size for persisting data points to Clotho\n\n**clotho.duration_days**\nClotho duration time per days\n\n**clotho.sampling_interval_minutes**\nClotho Sampling Interval per minute\n\n#### **DATA INPUTS**\n\nResponsible for the fetching or receiving of logs from different mediums.  \n\n**data_inputs.abstract_queue_size**\nQueue size of all data input.\n\n**data_inputs.examples_refresh_interval**\nInterval  in minutes  for updating the data-input examples in the database\n\n**data_inputs.max_length_bytes_per_stream**\nMax size (in bytes) of a single request that can be handled by any data input\n\n**data_inputs.preprocess.examples.buffer.size**\nSize of buffer for preprocess examples\n\n**data_input_mapping.max_examples**\nDefine the maximum number of samples to show on the Data Input Mapping screen  up to 500\n\n#### **DETECTIVE**\n\nTowards the end of the Data Ingestion Pipeline. Responsible for spotting 'regular' and 'anomalous' behavior in the data. Running multiple anomaly detection algorithms.\n\n**detective.alive_period_seconds_for_signal_dead**\nThe minimum period  in seconds  that a signal has to be alive before \"dropping dead\"  for a signal-dead alert to be fired. Additionally  if there was another \"dead signal\" with similar duration in this period  then the current one will be disqualified.\n\n**detective.allowed_future_time_minutes**\nAcceptable futuristic detection period. If a detection was created with a source time further down the future  its handling will be delayed.\n\n**detective.amplitude_coefficient**\nThis setting effects the overall sensitivity of the anomaly detection engine. The higher the number  the less alerts you will see.\n\n**detective.anomaly_detection.enabled**\nWhen set to false  the AI engine will not attempt anomaly detection.\n\n**detective.concurrency_override**\nIf specified  the value will override the initial automatic allocation of resources to the detective.\n\n**detective.detection_task_delay_seconds**\nDelay in seconds before starting a detection task after the corresponding window has settled.\n\n**detective.few_elements**\nA deep setting that effects the tolerance of weaker detection techniques\n\n**detective.global.mute_disabled**\nWhen set to true  the mute or disable feedback will apply on a specific metric across ALL Application-services\n\n**detective.max_moments_in_memory.derivative**\nThis setting effects the tolerance of the derivative algorithm\n\n**detective.max_moments_in_memory.signal_alive**\nHow many \"similar-in-amplitude\" bursts should the signal-alive detector allow in the preceding period. This setting is in effect when raising an alert\n\n**detective.max_moments_in_memory.signal_dead**\nHow many \"dead periods\" should the signal-dead detector allow. This setting is in effect when raising an alert\n\n**detective.memory_in_days**\nThe memory  in days  of the different anomaly detection models (baseline  derivative and others)\n\n**detective.min_events_per_window**\nThe min number of events per window for a detection to be triggered.\n\n**detective.of_custom_alert_concurrency_override**\nIf specified  the value will override the initial automatic allocation of resources to the customAlertDetective.\n\n**detective.points_in_timeless_trend**\nThe number of samples to consider when testing for trend shifts in disperse metrics\n\n**detective.queue_over_capacity_percent**\nWill create a system notification when the detective queue is greater than value% capacity for over 5 minutes.\n\n**detective.queue_size**\nNumber of detection tasks that can be buffered in the detective before it starts blocking the processing pipe.\n\n**detective.resolution.signal_dead**\nThe number of seconds a metric's signal must be consecutively \"dead\" (no data  graph showing zero) for a signal-dead detection to be triggered for this metric. This setting can be configured per source.\n\n**detective.sigma_coefficient**\nThe coefficient for the sigma-based anomaly detection\n\n**detective.workload_level**\nWork load level in which the Detective considered stressed  options are: LOW  MEDIUM  HIGH.\n\n#### **ELASTICSEARCH**\n\n**elasticsearch.bulk_actions**\nNumber of entities in one bulk request  using as threshold for elastic bulk operation (together with the request size).\n\n**elasticsearch.bulk_concurrent_requests**\nConcurrency of the Bulk write-requests to Elastic\n\n**elasticsearch.bulk_size_mb**\nSize of the bulk request in MB  using as threshold for elastic bulk operation (together with the request entities number threshold).\n\n**elasticsearch.client.connect_timeout_millis**\n\n\nConfigures the timeout in milliseconds until a connection is established to elasticsearch.\n\n**elasticsearch.client.io_threads**\nConfigures the number of I\/O dispatch threads to be used by the elasticsearch client\n\n**elasticsearch.client.socket_timeout_millis**\nConfigures the socket timeout in milliseconds to elasticsearch  which is the timeout for waiting for data or  put differently  a maximum period inactivity between two consecutive data packets.\n\n**elasticsearch.concurrency**\nHow many threads should index to elasticsearch\n\n**elasticsearch.flush_interval_seconds**\nBulk flush interval for indexing. Bulk will execute sooner if either bulk_actions or bulk_size_mb has been reached.\n\n**elasticsearch.mapping_keyword_properties**\nBy default  all string properties are indexed as 'keyword' (except message  rawMessage  stacktrace  and the additional_string_properties which are indexed as 'text') which allows aggregation but no partial searches. Any field with the 'property' prefix (e.g. 'property.UUID'  or 'property.srcIp') can also be indexed as 'keyword'. Note that a change will only apply to newly created indices. Please also note that when inserting the value you are not adding the prefix 'property'.\n\n**elasticsearch.minimal_indexing**\nWhen true  properties classified as invalid will not be indexed\n\n**elasticsearch.queue_size**\nBulk size for indexing.\n\n**elasticsearch.subsampling_ratio**\nUse this to only index some of the events to Elastic. 1 -\\> index 1 out of 1 events. 2 -\\> index 1 out of 2 events. N -\\> index one out of N events.\n\n#### **EVENTS**\n\n**events.keyword_extraction_non_patterned**\nKeyword extraction from the non-patterned message  when there is no pattern (i.e. message label is not assigned)\n\n**events.max_minutes_in_future**\nEvents that are further in the future than this will be dropped. Note: If you see event being dropped due to future timestamp double check that your timestamps are in the correct timezone.\n\n#### **GLIDE**\n\n**glide.datainput.max_errors_percentage_before_publish** Define the max % of errors in a data input before publishing a notification.\n\n**glide.table.change_detection.interval_seconds**\nInterval  in seconds  for getting tables that changed in glide\n\n**grpc.port**\nDefine glide port\n\n**health_log_analytics.use_case_export.enabled**Enables\/disables the migration of data inputs and source types between instances.\n\n#### **INCIDENTS**\n\n**incidents.alerts.dilute_target**\nWhen the number of alerts in an incident is too high (see incident.alerts.max_count)  alerts are diluted (removed) until this number is reached.\n\n**incidents.alerts.max_count**\nMaximum number of alerts in incident  to start the dilution process of excess alerts.\n\n**incidents.alert_interval_seconds**\nTime-span to consider two alerts as related if correlating by occurrence time\n\n**incidents.application_correlation**\nShould the alert application be taken into account when correlating alerts\n\n**incidents.component_correlation**\nShould the alert service be taken into account when correlating alerts\n\n**incidents.cooldown_period_minutes**\nMinutes to wait after the creation of an incident before sending a notification about it.\n\n**incidents.detection_time_correlation**\nPlease note that time frame of the correlation window is defined using: incidents.alert_interval_seconds setting\n\n**incidents.detection_type_correlation**\nShould the alert detection type (anomaly  signal-dead  baseline etc.) be taken into account when correlating alerts\n\n**incidents.entities**\nList of entities that will be used for correlation if two alerts shared the same value. This setting should be managed from the correlations setting pages (global or individual per-source)\n\n**incidents.host_correlation**\nShould the alert host be taken into account when correlating alerts\n\n**incidents.min_correlation_score_for_aggregating**\nSensitivity level for the correlation engine. The higher the number is  the more alerts will need to have in common in order to be correlated\n\n**incidents.pattern_text_correlation**\nShould the alert pattern-text be taken into account when correlating alerts\n\n**incidents.period_seconds**\nDefines the time-frame for the Alerts Smart-Correlations logic (an alert might only correlate with alerts created in the preceding T hours)\n\n#### **KEYWORDS**\n\n**dictionaries.resource.directory**\nDirectory name in which the dictionaries used in keyword's message extraction process are\n\n**keywords.message.extractor**\nWhen set to false  the AI engine will not attempt to automatically extract the message from Keyword-based alerts\n\n**keywords.message.max.length**\nMessages over the maximal specified length will not be extracted\n\n**keywords.message.stop.elsa.message**\nWhen set to true  Elsa will not attempt to automatically extract the message labels\n\n#### **LICENSING**\n\n**licensing.flush_interval_seconds**\nTime interval after which nodes will flushed to the glide table\n\n**licensing.max_map_size**\nMaximum number of nodes stored in memory before flushing to glide table\n\n**licensing.monitoring.interval_seconds**\nTime interval after which licensing monitoring service wakes up to check for new nodes\n\n#### **LOGSOURCEINFO (CMDB)\\***\n\n**logsourceinfo.flush_interval_seconds**\nTime interval  in seconds  for collecting log source host data and forwarding it to the Log-based CI candidate table. (Default value = 3600)\n\n**logsourceinfo.max_map_size**\nMaximum number of data nodes to be stored before the data is forwarded to the Log-based CI candidates table. (Default value = 1000)\n\n**logsourceinfo.monitoring.interval_seconds**\nTime interval  in seconds  for scanning log events to discover host-related data. (Default value = 60)\n\n\\**NEW system properties available starting from HLA December Store release*\n\n#### **METRICATOR**\n\nMiddle of the Data ingestion pipeline. Responsible for storing and measuring unique metrics.\n\n**metricator.cache_eviction_factor**\nNumber of raw metrics to evict from the cache when eviction is needed\n\n**metricator.cache_size**\"\n\n",
        "QUERY": " 'what is the HLA system property to discard old anomaly detection alerts'",
        "GROUND_TRUTH": "The HLA system property to discard old anomaly detection alerts is \"alerts.max_alert_age_hours\". This property determines the maximum age of alerts that the Anomaly Detection system will apply to. Events older than this setting are identified and discarded as they are considered 'too old'. The setting is used to manage the system's ability to handle real-time data and historic data. If you are streaming real-time data and still see detection windows being dropped for age, this might indicate a delay in the processing pipeline or incorrect extraction of the timestamp field. If you are streaming historic data, this setting must be increased to include the dates of the historical data."
    },
    {
        "id": 81,
        "CONTEXT": "\"## Learning release notes\n\n[Vancouver release notes](..\/..\/release-notes\/family-release-notes.html \"The ServiceNow Vancouver release includes new products and applications  as well as additional features and fixes for existing products. Read the release notes to learn about the release  prepare for your upgrade  and upgrade your instance.\") \\> [Learn about the Vancouver release](..\/..\/release-notes\/concept\/rn-learn-landing-page.html \"The Vancouver release includes new features and improvements built on the Now Platform.\") \\> [Release notes for upgrading from Utah](..\/..\/release-notes\/concept\/rn-n-1-landing-page.html \"When you upgrade from the Utah release  understand the fixes in each release version  notable changes for the user interface  browser support  plugin updates  and each feature's upgrade and migration tasks if applicable.\") \\> [Features and changes by product](..\/..\/release-notes\/new-features-changes.html \"Review the new features and changes in this release by product.\") \\> [Employee Service Management release notes](..\/..\/release-notes\/employee-service-management\/employee-service-management-rn-landing.html \"Employee Service Management has new and updated features in the Vancouver release\") \\>\n\n# Learning release notes {#ariaid-title1}\n\nThe ServiceNow\u00ae Learning application is a holistic learning portal that brings learning content from various learning management systems into the ServiceNow\u00ae portal experience enabling users to find  discover  and manage their learning courses in one place. Learning is a new application in the Vancouver release.\n\n## Learning highlights for the Vancouver release {#hr-lxp-rn__section_gqc_1qf_cvb}\n\n[Learning integration with the Employee Growth Plan and Development application](..\/product\/employee-service-management\/concept\/egd-landing-page.dita\/egd-landing-page.html):   Use Learning to achieve your next career milestone. Review your learnings  development plans  track your achievements  and save your learnings for easy sharing and referencing.\n\n[Learning challenges](..\/product\/human-resources\/concept\/learning-challenge.dita\/learning-challenge.html):   Keep yourself motivated with learning challenges. Set learning challenges to prioritize your career growth.\n\nManagerview:   Train your team members and help them grow in all aspects of their career. Assign learning courses to your team members from the Assigned to your team tab on My Learning.\n\n[Learning integration with Skills Intelligence](..\/product\/human-resources\/concept\/skills-intelligence-lxp.dita\/skills-intelligence-lxp.html):   Fetch skills-driven learning search results. View course recommendations and suggested courses based on your skills.\n\nSee [HR Service Delivery release notes](hr-service-delivery-landing-rn.html \"The ServiceNow HR Service Delivery application improves the employee service experience by automating HR interactions and providing a single platform for all HR services. HR Service Delivery was enhanced and updated in the Vancouver release.\") for more information.{#hr-lxp-rn__hr-lxp-rn-highlights-2}\nImportant: Learning is available in the ServiceNow Store. For details  see the \"Activation information\" section of these release notes.\n\n## Learning features {#hr-lxp-rn__section_jch_1qf_cvb}\n\n[My\nLearning](..\/product\/human-resources\/concept\/overview-lxp.dita\/overview-lxp.html):   Use the new and modern My Learning experience portal as a centralized space to manage all learnings. Content is curated and joined into paths by Learning admins for ease of learner consumption.\n\n[Activity](..\/product\/human-resources\/concept\/activity-lxp.dita\/activity-lxp.html):   View all your content activity in one place. See your assigned  saved  shared  and completed courses.\n\n[Achievements](..\/product\/human-resources\/concept\/achievements-lxp.dita\/achievements-lxp.html):   Use achievements to track your progress and show off all your learning achievements with badges.\n\n[Sharing](..\/product\/human-resources\/concept\/create-personal-collection.dita\/create-personal-collection.html):   Create personal collections to save content into customized folders for easy reference and sharing.\n\n[Career Assessment](..\/product\/human-resources\/concept\/exam-engine.dita\/exam-engine.html):   Use Career Assessment to accelerate your career with efficient learning and assessments. Exam administrators can create exams and quizzes to assess or test a learner's knowledge about a subject.\n\nLicensing\n\n:\n\n## Activation information {#hr-lxp-rn__section_mvm_1qf_cvb}\n\nInstall Learning by requesting it from the ServiceNow Store.{#hr-lxp-rn__hr-lxp-rn-activation}\n\nVisit the [ServiceNow Store](https:\/\/store.servicenow.com\/sn_appstore_store.do#!\/store\/home) website to view all the available apps and for information about submitting requests to the store. For cumulative release notes information for all released apps  see the [ServiceNow Store version history release\nnotes](https:\/\/docs.servicenow.com\/bundle\/store-release-notes\/page\/release-notes\/store\/sn-store-release-notes.html).\n\n## Related ServiceNow applications and features {#hr-lxp-rn__section_z1f_bqf_cvb}\n\n[Learning Core](..\/product\/human-resources\/concept\/learning-core.dita\/learning-core.html):   ServiceNow\u00aeLearning Core provides the base set of tables  content  learning bases  roles  and access configuration that are used in the Learning application and other coaching applications. Learning Core can be activated only as a dependent plugin.\n\n[Career Assessment](..\/product\/human-resources\/concept\/exam-engine.dita\/exam-engine.html):   ServiceNow\u00aeCareer Assessment works as an exam engine that provides the ability to create exams and quizzes that help test a learner's knowledge of a subject  enabling effective career advancement. This capability isleveraged by Learning.\n\n[Skills Intelligence](..\/product\/skills-intelligence\/concept\/skills-intelligence.dita\/skills-intelligence.html):   The ServiceNow\u00aeSkills Intelligence application is an accessible and dynamic artificial intelligence (AI) platform that effectively solves the challenge of managing and using skills-related data.\n**Parent Topic:** [HR Service Delivery release notes](..\/..\/release-notes\/hr-service-delivery\/hr-service-delivery-landing-rn.html \"The ServiceNow HR Service Delivery application improves the employee service experience by automating HR interactions and providing a single platform for all HR services. HR Service Delivery was enhanced and updated in the Vancouver release.\")\n\n\n\n## Penetration testing\n\n# Penetration testing {#ariaid-title1}\n\nPenetration testing in Application Vulnerability Response enables application owners to assess the security posture of their application. It is the manual testing of an application by the ethical hacking team.\n\n## Roles required {#pen_test_overview_avm__section_c25_d3r_qqb}\n\nPenetration testing requires the following roles:\n\nApp-Sec Manager: Contains security managers and application owners who manage the penetration testing assessment requests. It contains the following granular roles:\n\n* sn_vul.app_manage_pen_test_request\n* sn_vul.app_read_all\n* cmdb_read\n{#pen_test_overview_avm__ul_ypv_cbw_xqb}\n\nEthical Hacker: Contains members of the ethical hacking team who perform penetration testing of applications. It includes the following granular roles:\n\n* sn_vul.app_update_assignment_group\n* sn_vul.app_update_assigned_to\n* sn_vul.app_manage_manual_avits\n* sn_vul.app_manage_pen_test_request_config\n* itil\n* sn_vul.app_read_all\n* sn_vul.app_manage_pen_test_request\n* sn_vul.app_update_state\n{#pen_test_overview_avm__ul_elg_q1w_xqb}\n\nFor more information about these roles  see [Application Vulnerability Response user groups and roles](avm-manage-roles.html \"Before you can successfully remediate vulnerabilities with Application Vulnerability Response (AVR)  you must assign users to user groups.\").\n\nStarting with v19.0 of Vulnerability Response  if you are using the Veracode Vulnerability Integration  the penetration assessment tests in the Veracode Vulnerability Integration are manual findings from Veracode. They are not linked to any penetration test assessment requests you configure in Application Vulnerability Response. For more information about penetration test assessments from Veracode  see the [Veracode Vulnerability Integration](..\/..\/secops-integration-vr\/veracode\/concept\/veracode-vuln-integration.html \"The Vulnerability Response Integration with Veracode application uses data imported from the Veracode product to help you determine the impact and priority of flaws in your code.\").\n\n## Life cycle of penetration testing {#pen_test_overview_avm__section_xhp_hj3_qqb}\n\nAs an application owner  you can request the ethical hacking team for a penetration test assessment of your application. The ethical hacking team acts on this request and creates penetration test findings. These findings are manually-created Application Vulnerable Items (AVIs).\n\nThe penetration testing workflow covers the penetration testing life cycle from raising the testing request to resolving the findings of the ethical hacking team.\n\n## Requesting a penetration test assessment {#pen_test_overview_avm__section_ob1_xgp_qqb}\n\nStarting with v19.0  you can create new requests or copy existing requests at All \\> Penetration Test Assessment Requests \\> All.\n\nPrior to v19.0  as the application owner  you can request a penetration test assessment for your application using the ITSM service catalog.\n\n## Reviewing the penetration test assessment request {#pen_test_overview_avm__section_c4h_khp_qqb}\n\nThe ethical hacking team reviews and assesses the application and the scope of the penetration test assessment request  and adds it to the existing backlog.\n\n## Preparing an environment {#pen_test_overview_avm__section_mgr_rhp_qqb}\n\nThe ethical hacking team then sends a request to the application owner to provide an environment for them to start testing. Once the environment is ready  the application owner informs the ethical hacking team.\n\nFor more information about configuring test requests  see [Configure penetration testing](..\/task\/pen-test-config-v16-1.html \"You can configure the sprint duration and estimated effort for penetration testing assessment types. This provides the scheduling functionality for application owners  helping them determine a tentative time frame for their penetration test assessment requests.\").\n\n## Testing and reporting the penetration test findings {#pen_test_overview_avm__section_dgf_s3p_qqb}\n\nThe ethical hacking team tests the application and reports the findings to the application owner. The ethical hacking team also defines the Service Level Agreements (SLAs) for the penetration test findings using the remediation target date. These findings are the manually-created AVIs. The application owner in turn reviews the AVIs created by the ethical hacking team. They plan the fixes and assign them to the application team.\nNote: Remediation target rules do not apply to the penetration test findings.\n\nThe ethical hacking team can create a library of Application Vulnerability Entries (AVEs) and reuse them while reporting the AVIs. They can also track the status of the penetration test findings.\n\n## Fixing and validating the penetration test findings {#pen_test_overview_avm__section_vpq_lkp_qqb}\n\nAfter the penetration test findings are fixed and resolved by the application team  the fixes are validated manually and closed by the ethical hacking team.\n\n## Application Vulnerability Management reports {#pen_test_overview_avm__section_hjr_ylp_qqb}\n\nUse the reports available on the Application Vulnerability Management PA dashboard to track the penetration test findings.\nFigure 1. Penetration testing life cycle Penetration testing life cycle.\n\n\n\n# Penetration testing {#ariaid-title1}\n\nPenetration testing in Application Vulnerability Response enables application owners to assess the security posture of their application. It is the manual testing of an application by the ethical hacking team.\n\n## Roles required {#pen_test_overview_avm__section_c25_d3r_qqb}\n\nPenetration testing requires the following roles:\n\nApp-Sec Manager: Contains security managers and application owners who manage the penetration testing assessment requests. It contains the following granular roles:\n\n* sn_vul.app_manage_pen_test_request\n* sn_vul.app_read_all\n* cmdb_read\n{#pen_test_overview_avm__ul_ypv_cbw_xqb}\n\nEthical Hacker: Contains members of the ethical hacking team who perform penetration testing of applications. It includes the following granular roles:\n\n* sn_vul.app_update_assignment_group\n* sn_vul.app_update_assigned_to\n* sn_vul.app_manage_manual_avits\n* sn_vul.app_manage_pen_test_request_config\n* itil\n* sn_vul.app_read_all\n* sn_vul.app_manage_pen_test_request\n* sn_vul.app_update_state\n{#pen_test_overview_avm__ul_elg_q1w_xqb}\n\nFor more information about these roles  see [Manage Application Vulnerability Response user groups and roles](avm-manage-roles.html \"Before you can successfully remediate vulnerabilities with Application Vulnerability Response (AVR)  you must assign users to user groups.\").\n\n## Life cycle of penetration testing {#pen_test_overview_avm__section_xhp_hj3_qqb}\n\nAs an application owner  you can request the ethical hacking team for a penetration test assessment of your application. The ethical hacking team acts on this request and creates penetration test findings. These findings are manually-created Application Vulnerable Items (AVIs).\n\nThe penetration testing workflow covers the penetration testing life cycle from raising the testing request to resolving the findings of the ethical hacking team.\n\n## Requesting a penetration test assessment {#pen_test_overview_avm__section_ob1_xgp_qqb}\n\nAs the application owner  you can request a penetration test assessment for your application using the ITSM service catalog.\n\n## Reviewing the penetration test assessment request {#pen_test_overview_avm__section_c4h_khp_qqb}\n\nThe ethical hacking team reviews and assesses the application and the scope of the penetration test assessment request  and adds it to the existing backlog.\n\n## Preparing an environment {#pen_test_overview_avm__section_mgr_rhp_qqb}\n\nThe ethical hacking team then sends a request to the application owner to provide an environment for them to start testing. Once the environment is ready  the application owner informs the ethical hacking team.\n\n## Testing and reporting the penetration test findings {#pen_test_overview_avm__section_dgf_s3p_qqb}\n\nThe ethical hacking team tests the application and reports the findings to the application owner. The ethical hacking team also defines the Service Level Agreements (SLAs) for the penetration test findings using the remediation target date. These findings are the manually-created AVIs. The application owner in turn reviews the AVIs created by the ethical hacking team. They plan the fixes and assign them to the application team.\nNote: Remediation target rules do not apply to the penetration test findings.\n\nThe ethical hacking team can create a library of Application Vulnerability Entries (AVEs) and reuse them while reporting the AVIs. They can also track the status of the penetration test findings.\n\n## Fixing and validating the penetration test findings {#pen_test_overview_avm__section_vpq_lkp_qqb}\n\nAfter the penetration test findings are fixed and resolved by the application team  the fixes are validated manually and closed by the ethical hacking team.\n\n## Application Vulnerability Management reports {#pen_test_overview_avm__section_hjr_ylp_qqb}\n\nUse the reports available on the Application Vulnerability ManagementPA dashboard to track the penetration test findings.\nFigure 1. Penetration testing life cycle Penetration testing life cycle.\n\n\"\n\n",
        "QUERY": " 'What is ServiceNow's application pen test procedure?'",
        "GROUND_TRUTH": "ServiceNow's application penetration testing procedure involves several steps:\n\n1. Requesting a Penetration Test Assessment: As an application owner, you can request a penetration test assessment for your application. This request can be made using the ITSM service catalog.\n\n2. Reviewing the Penetration Test Assessment Request: The ethical hacking team reviews and assesses the application and the scope of the penetration test assessment request, and adds it to the existing backlog.\n\n3. Preparing an Environment: The ethical hacking team sends a request to the application owner to provide an environment for them to start testing. Once the environment is ready, the application owner informs the ethical hacking team.\n\n4. Testing and Reporting the Penetration Test Findings: The ethical hacking team tests the application and reports the findings to the application owner. These findings are manually-created Application Vulnerable Items (AVIs). The application owner reviews the AVIs created by the ethical hacking team and plans the fixes, assigning them to the application team.\n\n5. Fixing and Validating the Penetration Test Findings: After the penetration test findings are fixed and resolved by the application team, the fixes are validated manually and closed by the ethical hacking team.\n\n6. Application Vulnerability Management Reports: Reports available on the Application Vulnerability Management PA dashboard are used to track the penetration test findings.\n\nThe procedure requires two roles: the App-Sec Manager, who manages the penetration testing assessment requests, and the Ethical Hacker, who performs the penetration testing of applications."
    },
    {
        "id": 82,
        "CONTEXT": "\"## Introduction to the Sweagle CLI (command-line interface)\n\n|---------------------------------------------------------------|------------------------------------------------------------------------------|\n| ![Note icon](https:\/\/hi.service-now.com\/Note_25x.pngx \"Note\") | **IMPORTANT:** This functionality is \"General Available\" as of release 3.12. |\nThe Sweagle command-line interface (CLI) simplifies the REST API integration with Sweagle for the most common tasks. The CLI is a small  standalone package that encapsulates the most commonly used Sweagle REST API methods into an \"easier to use\" syntax directly from the command line.\n\nCompared to the REST API  the CLI offers capabilities that are a sequence of various API calls. As an example when using the uploadData option  the CLI will :\n\n1. open a data changeset\n2. upload the data from the provided source (in this case the CDI \"myCounter\" with value 5 in format type \"properties\") to a path in the data model (in this case baseData  settings)\n3. approve the data changeset and create a snapshot for each of the impacted CDS\n   .\/sweagle uploadData -as -n baseData uid -t props -d \"myCounter=5\"\n\nFor tech-savvy users  the Sweagle CLI can be a way to perform some of the user tasks directly from the terminal. But it can also be used in external scripts and process automation tools as a simplified syntax to automate tasks for uploading  validating  and consuming configuration data in the Sweagle platform.\n\n### Setup\n\nThe Sweagle CLI is a standalone package and there is no third-party installation required.\n\n#### 1. Download\n\nThe package is available for recent versions of MS Windows  \\*NIX  and macOS. Login to your Sweagle environment and download the package for your operating system from the help menu (top right corner).\n[![](sys_attachment.do?sys_id=db1098361bfa54d06531ea89bd4bcbde)](https:\/\/s3.amazonaws.com\/cdn.freshdesk.com\/data\/helpdesk\/attachments\/production\/43120676628\/original\/QPAsOgz0YEnHd9Yf2mcth2M3Pdecsdacvg.png?1585828408)\n\nAfter download  copy that file to the folder from where you want to run the Sweagle CLI. Also  for Linux and macOS  give the Sweagle file execution permissions (chmod +x sweagle)\n\n#### 2. Installation\n\n##### a. API token\n\nThe Sweagle CLI requires a valid API user to connect and allow (some of) the commands and actions on your data model. Copy the API token from one of the API users that are connected to your profile. In case you have no assigned API users  contact a user administrator who can set up an API user and assign it to your personal user details.\n\n##### b. Setup\n\nRun the one-time setup command (`.\/sweagle setup`) from the folder from where you want to run the Sweagle CLI. Follow the guided questions. Note that the GitHub integration is optional.\n\n**Tip**: Use the \"secure store of tokens\" option as it will encrypt your API token and other passwords required for CLI.\n\n##### c. Sanity Check\n\nYou can check that the Sweagle CLI is properly running and can reach your Sweagle environment by using the `.\/sweagle info` command\n\n##### d. Update Sweagle CLI\n\nThere is a built-in function for the Sweagle CLI to check for a new version of the CLI from the Sweagle environment  download it and replace the package with the new version.\n\n### How to use the CLI\n\nThere are 2 ways to use the Sweagle CLI\n\n* directly through the terminal and entering commands\n* starting in interactive mode\n\n#### 1 - Direct Through Terminal\n\nExecute commands directly from the command line. Note that the commands are case sensitive. There is a basic help function available for each of the commands at any level in the options using -h or --help.\n\nSweagle CLI command argument flags follow the git syntax. That means that multiple flags can be added in a short notation format. As an example  the uploadData command where you want to set the flags autoApprove=true and storeSnapshotResults=true can be done by adding \"-as\" to the terminal command.\n\n#### 2 - Interactive Mode\n\nStart the interactive mode through the command `.\/sweagle start`. This provides an interactive menu in which the user can navigate using the up and down arrow  select commands  enter filters  and use the results.\n\nWhen a command has been fully configured through the interactive menu and the result is on the terminal you can choose the option to \"show the CURL command\" or \"show the CLI command\" which you can copy-paste for your scripts and automation. In this way  the interactive mode is an easy way to compose your Sweagle CLI commands without having to read through the help information on how to use them.\n\n\n\n## GitHub integration with the Sweagle CLI\n\nThe Sweagle CLI offers a standard integration to recursively loop through a Github based repository and search for specific file extensions. Any file found is parsed and the configuration data is uploaded to Sweagle.\n\n### Setup\n\n#### Create Access Token In Github\n\nCreate a unique access token for your Github repository with access scope \"repo\". Note down the access token because Github will not show it again once you leave this page.\n\n[![](sys_attachment.do?sys_id=f69214be1b3e54d06531ea89bd4bcb1d)](https:\/\/s3.amazonaws.com\/cdn.freshdesk.com\/data\/helpdesk\/attachments\/production\/43126281921\/original\/wsY9catEGiWLQrUP-jflTxAZyQazaWeIbg.png?1588566454)\n\nNote that at the time of writing of this article  you have to select the checkbox at the scope level \"repo\". Only selecting the 5 list items without the top-level checkbox is not sufficient.\n\n#### Sweagle CLI Git Configuration\n\nThe GitHub integration is optional in the setup of the Sweagle CLI.\n\nIn case of a fully interactive installation of the Sweagle CLI  confirm that the GitHub integration should be set up and follow the instructions. Enter the Git access token.\n\nIf you have already a working Sweagle CLI  you can add the GitHub details by following the steps of the gitConfig option.\n\nIn case you only want to change the GitHub access token  you can update the token using the \"-t\" argument\n\n### Usage\n\n#### Interactive Mode\n\nThe CLI interactive mode will guide you through the steps in order to configure the recursive search throughout the repository and upload all found configuration data to Sweagle.\n\nNote that the remote repository should be an exact case sensitive name of the repository. It is currently not supported to limit the search to only a specific folder path in the repository (unlike with the terminal command mode where this is already implemented - see below).\n\n#### Terminal Command Mode\n\nThe CLI command mode offers various possibilities:\n\n1. provide a repository name and it will recursively scan through the content and filter all files with the filtered extensions as configured through the Sweagle gitConfig\n2. Provide a repository name and an optional folder name  and the Sweagle CLI will limit its search to only the provided folder path (use \/ for the path separation)\n3. provide a full path to a single file  and the Sweagle CLI will only upload that single file.\n\n**Example 1**: will parse through the content of the \"PRD\" folder in the repo named \"myApp\"  and upload any files found with one of the default extensions to Sweagle on the nodePath \"CRM environments\".\n\n**Example 2**: will upload the content of a single specific file in the repo to a specific path in Sweagle.\n\nIn case the recursive search through a repository (folder) should be limited to a specific set of file extensions  then first update the file extensions configuration to only the extensions required. After that  run the command as above.\n\n\n\n## Interactive Analysis\n\n# Interactive Analysis {#ariaid-title1}\n\nInteractive Analysis enables you to quickly explore data using visualizations.\n\nFrom any list of records  you can access an interactive set of reports on the list data. You can also manipulate the data by grouping  stacking  aggregating  and applying interactive filters. Click the visualization to drill down into the data. Click the information icon (Information icon.) to edit the source filter  view the list of applied filters  and copy the URL of the analysis.\nFigure 1. Interactive Analysis on Incident data Interactive analysis with information panel option.\n**Parent Topic:** [Performance Analytics fundamentals](use\/performance-analytics\/concept\/c_PAWidgetsAndDashboards.html \"Create and configure indicators and breakdowns. Collect data. Display calculated Performance Analytics scores.\")\n\n## Launch Interactive Analysis {#ariaid-title2}\n\nLaunch Interactive Analysis from any list.\nRole required: none.\n\nYou must have access to the list of records that you want to analyze.\n\n1. Navigate to any list.\n2. **Optional:** Configure the columns that are displayed on the list.  \n   The columns that appear on the list when you launch Interactive Analysis determine which fields are included in the analysis. The included fields determine which Group by and Stack by options are available  and which interactive filters appear by default.\n3. Right-click the column header for a reference  choice  date\/time  integer  or boolean field and select Launch Interactive Analysis.  \n   The column that you launch Interactive Analysis from is used as the default Group by value.\n4. **Optional:** Change how data is aggregated by selecting different values in the Group by and Stack by choice lists  or filter the data by applying one or more interactive filters.\n5. **Optional:** Drill down into a subset of the data by clicking a visualization  such as a bar in the bar chart or a cell in the heatmap.\n\n\"\n\n",
        "QUERY": " 'what is an interactive mode in Sweagle CLI?'",
        "GROUND_TRUTH": "The interactive mode in Sweagle CLI is a user-friendly feature that allows users to navigate through an interactive menu using the up and down arrows, select commands, enter filters, and use the results. It is initiated through the command `.\/sweagle start`. This mode is particularly useful for users who prefer a guided approach to using the CLI, as it provides an easy way to compose Sweagle CLI commands without having to read through the help information on how to use them. Once a command has been fully configured through the interactive menu and the result is on the terminal, users have the option to \"show the CURL command\" or \"show the CLI command\", which they can then copy and paste for their scripts and automation."
    },
    {
        "id": 83,
        "CONTEXT": "## Configuration file tracking\n\n# Configuration file tracking {#ariaid-title1}\n\nThe horizontal discovery process can find configuration files that belong to certain applications and add those configuration files to the CMDB. You can track the changes to these files by comparing them to previous versions.\nWarning: Configuration files contain sensitive system information. To prevent unauthorized access  ensure that access control lists (ACL) are placed on the Tracked Configuration file table \\[cmdb_ci_config_file_tracked\\]. Only allow authorized users to view this table or uncheck the Save Content setting.\n\n## Components for configuration file tracking {#tracked-config-files__section_otx_ygr_tz}\n\nCI type\n\n:\n\nPatterns\n\n:\n\nCMDB\n\n:\n\n## Dependency maps and application service maps {#tracked-config-files__section_hwl_zfg_5z}\n\nBoth dependency maps and application service maps display tracked configuration files. The relationship between a configuration file and its host is a contains relationship. The application contains the configuration file.\nFor example  this IIS web server contains three tracked configuration files:\nCI containing racked configuration files\nSometimes you organize CI types as a main CI type and its related CI types. On an application service map  Service Mapping shows changes to configuration files of related CIs for the main CIs in inclusions. In inclusions  the system treats applications hosted on a server as independent objects. For example  the Tomcat WAR CI appears separate from its host  the Tomcat CIs. In this case  Service Mapping shows changes to configuration files of Tomcat WAR when you select Tomcat. In addition  Service Mapping displays changes to configuration files of the hardware server hosting inclusions. In this example  it is a Linux server:\nFigure 1. Map showing an inclusion with a host\nMap showing an inclusion with a host\n\n## Deletion strategy {#tracked-config-files__section_deletion-strategy}\n\nYou can specify what you want to do with tracked configuration file CI records when discovery can no longer find them. You can keep the configuration file CI record  automatically delete it  delete only the CI relationships to it  or mark it absent.\n\n## Discovery patterns that support configuration file tracking by default {#tracked-config-files__patterns-for-file-tracking}\n\nThese patterns provide tracked file definitions by default:\n{#d67234e194}\n\n|                                                                                                                                                           Classifier                                                                                                                                                           |                     Pattern                      |                             CI Type                             |                                                                                  File path of tracked file                                                                                   |\n|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------|-----------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| [Apache Server](..\/reference\/r_DataCollDiscoApacheWebServers.html \"Discovery identifies and classifies information about Apache web servers on both Windows and Linux computers.\")                                                                                                                                             | Apache On Unix Pattern Apache On Windows Pattern | Apache Web Server \\[cmdb_ci_apache_web_server\\]                 | `$config_file`                                                                                                                                                                               |\n| [MySQL Server](c_MySQLDiscovery.html \"Discovery can identify an instance of MySQL that is running on UNIX or Windows operating systems.\")                                                                                                                                                                                      | MySQL server On Windows and Linux Pattern        | MySQL Instance \\[cmdb_ci_db_mysql_instance\\]                    | `$config_file`                                                                                                                                                                               |\n| [Microsoft IIS Server](..\/reference\/r_DataCollDiscoMicrosoftIISServers.html \"Discovery identifies and classifies information about Microsoft IIS servers.\")                                                                                                                                                                    | IIS                                              | Microsoft iis Web Server \\[cmdb_ci_microsoft_iis_web_server\\]   | `EVAL(javascript: var rtrn = '';var winDir = CTX.getCommandManager().shellCommand(\"echo %WinDir%\"  false  null  null  CTX);rtrn = winDir.trim() + '\\System32\\Inetsrv\\Config\\*.config';)` |\n| [Microsoft IIS Server](..\/reference\/r_DataCollDiscoMicrosoftIISServers.html \"Discovery identifies and classifies information about Microsoft IIS servers.\")                                                                                                                                                                    | IIS                                              | IIS Virtual Directory \\[cmdb_ci_iisdirectory\\]                  | `$install_directory + \"\\*.config\"`                                                                                                                                                           |\n| [TIBCO BusinessWorks and EMS](..\/..\/service-mapping\/concept\/mapping-services-tibco.html \"Discovery can find Tibco ActiveMatrix BusinessWorks and Enterprise Message Service (EMS). Service Mapping can discover application services containing Tibco BusinessWorks  Enterprise Message Service (EMS)  and their components.\") | ActiveMatrix BusinessWorks                       | ActiveMatrix Business Works \\[cmdb_ci_appl_tibco_matrix\\]       | `$config_file`                                                                                                                                                                               |\n| [TIBCO BusinessWorks and EMS](..\/..\/service-mapping\/concept\/mapping-services-tibco.html \"Discovery can find Tibco ActiveMatrix BusinessWorks and Enterprise Message Service (EMS). Service Mapping can discover application services containing Tibco BusinessWorks  Enterprise Message Service (EMS)  and their components.\") | Enterprise Message Service                       | Tibco Enterprise Message Service \\[cmdb_ci_appl_tibco_message\\] | `$config_file`                                                                                                                                                                               |\n| [Oracle](c_OracleDatabaseDiscovery.html \"Discovery can identify an Oracle database instance that is running on UNIX or Windows operating systems. It can also find Oracle clusterware  which runs Real Application Clusters (RAC).\")                                                                                           | Oracle DB on Windows Pattern                     | Oracle Instance \\[cmdb_ci_db_ora_instance\\]                     | `$install_directory + \"\networkAdmin\\*.ora\"`                                                                                                                                                |\n| [Oracle](c_OracleDatabaseDiscovery.html \"Discovery can identify an Oracle database instance that is running on UNIX or Windows operating systems. It can also find Oracle clusterware  which runs Real Application Clusters (RAC).\")                                                                                           | Oracle DB on Windows Pattern                     | Oracle Instance \\[cmdb_ci_db_ora_instance\\]                     | `$install_directory + \"\\dbs\\*.ora\"`                                                                                                                                                          |\n| [Oracle](c_OracleDatabaseDiscovery.html \"Discovery can identify an Oracle database instance that is running on UNIX or Windows operating systems. It can also find Oracle clusterware  which runs Real Application Clusters (RAC).\")                                                                                           | Oracle DB on Unix Pattern                        | Oracle Instance \\[cmdb_ci_db_ora_instance\\]                     | `$install_directory + \"\/dbs\/*.ora\"`                                                                                                                                                          |\n| [Oracle](c_OracleDatabaseDiscovery.html \"Discovery can identify an Oracle database instance that is running on UNIX or Windows operating systems. \n\n# Configuration file tracking {#ariaid-title1}\n\nThe horizontal discovery process can find configuration files that belong to certain applications and add those configuration files to the CMDB. You can track the changes to these files by comparing them to previous versions.\nWarning: Configuration files contain sensitive system information. To prevent unauthorized access  ensure that access control lists (ACL) are placed on the Tracked Configuration file table \\[cmdb_ci_config_file_tracked\\]. Only allow authorized users to view this table or uncheck the Save Content setting.\n\n## Components for configuration file tracking {#tracked-config-files__section_otx_ygr_tz}\n\nCI type\n\n:\n\nPatterns\n\n:\n\nCMDB\n\n:\n\n## Dependency maps and application service\n\nmaps {#tracked-config-files__section_hwl_zfg_5z}\n\nBoth dependency maps and application service maps display tracked configuration files. The relationship between a configuration file and its host is a contains relationship. The application contains the configuration file.\nFor example  this IIS web server contains three tracked configuration files:\nCI containing racked configuration files\nSometimes you organize CI types as a main CI type and its related CI types. On an application service map  Service Mapping shows changes to configuration files of related CIs for the main CIs in inclusions. In inclusions  the system treats applications hosted on a server as independent objects. For example  the Tomcat WAR CI appears separate from its host  the Tomcat CIs. In this case  Service Mapping shows changes to configuration files of Tomcat WAR when you select Tomcat. In addition  Service Mapping displays changes to configuration files of the hardware server hosting inclusions. In this example  it is a Linux server:\nFigure 1. Map showing an inclusion with a host\nMap showing an inclusion with a host\n\n## Deletion strategy {#tracked-config-files__section_deletion-strategy}\n\nYou can specify what you want to do with tracked configuration file CI records when discovery can no longer find them. You can keep the configuration file CI record  automatically delete it  delete only the CI relationships to it  or mark it absent.\n\n## Discovery patterns that support configuration file tracking by default {#tracked-config-files__patterns-for-file-tracking}\n\nThese patterns provide tracked file definitions by default:\n{#d65423e193}\n\n|                                                                                                                                                           Classifier                                                                                                                                                           |                     Pattern                      |                             CI Type                             |                                                                                  File path of tracked file                                                                                   |\n|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------|-----------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| [Apache Server](..\/reference\/r_DataCollDiscoApacheWebServers.html \"Discovery identifies and classifies information about Apache web servers on both Windows and Linux computers.\")                                                                                                                                             | Apache On Unix Pattern Apache On Windows Pattern | Apache Web Server \\[cmdb_ci_apache_web_server\\]                 | `$config_file`                                                                                                                                                                               |\n| [MySQL Server](c_MySQLDiscovery.html \"Discovery can identify an instance of MySQL that is running on UNIX or Windows operating systems.\")                                                                                                                                                                                      | MySQL server On Windows and Linux Pattern        | MySQL Instance \\[cmdb_ci_db_mysql_instance\\]                    | `$config_file`                                                                                                                                                                               |\n| [Microsoft IIS Server](..\/reference\/r_DataCollDiscoMicrosoftIISServers.html \"Discovery identifies and classifies information about Microsoft IIS servers.\")                                                                                                                                                                    | IIS                                              | Microsoft iis Web Server \\[cmdb_ci_microsoft_iis_web_server\\]   | `EVAL(javascript: var rtrn = '';var winDir = CTX.getCommandManager().shellCommand(\"echo %WinDir%\"  false  null  null  CTX);rtrn = winDir.trim() + '\\System32\\Inetsrv\\Config\\*.config';)` |\n| [Microsoft IIS Server](..\/reference\/r_DataCollDiscoMicrosoftIISServers.html \"Discovery identifies and classifies information about Microsoft IIS servers.\")                                                                                                                                                                    | IIS                                              | IIS Virtual Directory \\[cmdb_ci_iisdirectory\\]                  | `$install_directory + \"\\*.config\"`                                                                                                                                                           |\n| [TIBCO BusinessWorks and EMS](..\/..\/service-mapping\/concept\/mapping-services-tibco.html \"Discovery can find Tibco ActiveMatrix BusinessWorks and Enterprise Message Service (EMS). Service Mapping can discover application services containing Tibco BusinessWorks  Enterprise Message Service (EMS)  and their components.\") | ActiveMatrix BusinessWorks                       | ActiveMatrix Business Works \\[cmdb_ci_appl_tibco_matrix\\]       | `$config_file`                                                                                                                                                                               |\n| [TIBCO BusinessWorks and EMS](..\/..\/service-mapping\/concept\/mapping-services-tibco.html \"Discovery can find Tibco ActiveMatrix BusinessWorks and Enterprise Message Service (EMS). Service Mapping can discover application services containing Tibco BusinessWorks  Enterprise Message Service (EMS)  and their components.\") | Enterprise Message Service                       | Tibco Enterprise Message Service \\[cmdb_ci_appl_tibco_message\\] | `$config_file`                                                                                                                                                                               |\n| [Oracle](c_OracleDatabaseDiscovery.html \"Discovery can identify an Oracle database instance that is running on UNIX or Windows operating systems. It can also find Oracle clusterware  which runs Real Application Clusters (RAC).\")                                                                                           | Oracle DB on Windows Pattern                     | Oracle Instance \\[cmdb_ci_db_ora_instance\\]                     | `$install_directory + \"\networkAdmin\\*.ora\"`                                                                                                                                                |\n| [Oracle](c_OracleDatabaseDiscovery.html \"Discovery can identify an Oracle database instance that is running on UNIX or Windows operating systems. It can also find Oracle clusterware  which runs Real Application Clusters (RAC).\")                                                                                           | Oracle DB on Windows Pattern                     | Oracle Instance \\[cmdb_ci_db_ora_instance\\]                     | `$install_directory + \"\\dbs\\*.ora\"`                                                                                                                                                          |\n| [Oracle](c_OracleDatabaseDiscovery.html \"Discovery can identify an Oracle database instance that is running on UNIX or Windows operating systems. It can also find Oracle clusterware  which runs Real Application Clusters (RAC).\")                                                                                           | Oracle DB on Unix Pattern                        | Oracle Instance \\[cmdb_ci_db_ora_instance\\]                     | `$install_directory + \"\/dbs\/*.ora\"`                                                                                                                                                          |\n| [Oracle](c_OracleDatabaseDiscovery.html \"Discovery can identify an Oracle database instance that is running on UNIX or Windows operating systems. \n\n# Configuration file tracking {#ariaid-title1}\n\nThe horizontal discovery process can find configuration files that belong to certain applications and add those configuration files to the CMDB. You can track the changes to these files by comparing them to previous versions.\nWarning: Configuration files contain sensitive system information. To prevent unauthorized access  ensure that access control lists (ACL) are placed on the Tracked Configuration file table \\[cmdb_ci_config_file_tracked\\]. Only allow authorized users to view this table or uncheck the Save Content setting.\n\n## Components for configuration file tracking {#tracked-config-files__section_otx_ygr_tz}\n\nCI type\n\n:\n\nPatterns\n\n:\n\nCMDB\n\n:\n\n## Dependency maps and application service\n\nmaps {#tracked-config-files__section_hwl_zfg_5z}\n\nBoth dependency maps and application service maps display tracked configuration files. The relationship between a configuration file and its host is a contains relationship. The application contains the configuration file.\nFor example  this IIS web server contains three tracked configuration files:\nCI containing racked configuration files\nSometimes you organize CI types as a main CI type and its related CI types. On an application service map  Service Mapping shows changes to configuration files of related CIs for the main CIs in inclusions. In inclusions  the system treats applications hosted on a server as independent objects. For example  the Tomcat WAR CI appears separate from its host  the Tomcat CIs. In this case  Service Mapping shows changes to configuration files of Tomcat WAR when you select Tomcat. In addition  Service Mapping displays changes to configuration files of the hardware server hosting inclusions. In this example  it is a Linux server:\nFigure 1. Map showing an inclusion with a host\nMap showing an inclusion with a host\n\n## Deletion strategy {#tracked-config-files__section_deletion-strategy}\n\nYou can specify what you want to do with tracked configuration file CI records when discovery can no longer find them. You can keep the configuration file CI record  automatically delete it  delete only the CI relationships to it  or mark it absent.\n\n## Discovery patterns that support configuration file tracking by default {#tracked-config-files__patterns-for-file-tracking}\n\nThese patterns provide tracked file definitions by default:\n{#d91583e193}\n\n|                                                                                                                                                           Classifier                                                                                                                                                           |                     Pattern                      |                             CI Type                             |                                                                                  File path of tracked file                                                                                   |\n|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------|-----------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| [Apache Server](..\/reference\/r_DataCollDiscoApacheWebServers.html \"Discovery identifies and classifies information about Apache web servers on both Windows and Linux computers.\")                                                                                                                                             | Apache On Unix Pattern Apache On Windows Pattern | Apache Web Server \\[cmdb_ci_apache_web_server\\]                 | `$config_file`                                                                                                                                                                               |\n| [MySQL Server](c_MySQLDiscovery.html \"Discovery can identify an instance of MySQL that is running on UNIX or Windows operating systems.\")                                                                                                                                                                                      | MySQL server On Windows and Linux Pattern        | MySQL Instance \\[cmdb_ci_db_mysql_instance\\]                    | `$config_file`                                                                                                                                                                               |\n| [Microsoft IIS Server](..\/reference\/r_DataCollDiscoMicrosoftIISServers.html \"Discovery identifies and classifies information about Microsoft IIS servers.\")                                                                                                                                                                    | IIS                                              | Microsoft iis Web Server \\[cmdb_ci_microsoft_iis_web_server\\]   | `EVAL(javascript: var rtrn = '';var winDir = CTX.getCommandManager().shellCommand(\"echo %WinDir%\"  false  null  null  CTX);rtrn = winDir.trim() + '\\System32\\Inetsrv\\Config\\*.config';)` |\n| [Microsoft IIS Server](..\/reference\/r_DataCollDiscoMicrosoftIISServers.html \"Discovery identifies and classifies information about Microsoft IIS servers.\")                                                                                                                                                                    | IIS                                              | IIS Virtual Directory \\[cmdb_ci_iisdirectory\\]                  | `$install_directory + \"\\*.config\"`                                                                                                                                                           |\n| [TIBCO BusinessWorks and EMS](..\/..\/service-mapping\/concept\/mapping-services-tibco.html \"Discovery can find Tibco ActiveMatrix BusinessWorks and Enterprise Message Service (EMS). Service Mapping can discover application services containing Tibco BusinessWorks  Enterprise Message Service (EMS)  and their components.\") | ActiveMatrix BusinessWorks                       | ActiveMatrix Business Works \\[cmdb_ci_appl_tibco_matrix\\]       | `$config_file`                                                                                                                                                                               |\n| [TIBCO BusinessWorks and EMS](..\/..\/service-mapping\/concept\/mapping-services-tibco.html \"Discovery can find Tibco ActiveMatrix BusinessWorks and Enterprise Message Service (EMS). Service Mapping can discover application services containing Tibco BusinessWorks  Enterprise Message Service (EMS)  and their components.\") | Enterprise Message Service                       | Tibco Enterprise Message Service \\[cmdb_ci_appl_tibco_message\\] | `$config_file`                                                                                                                                                                               |\n| [Oracle](c_OracleDatabaseDiscovery.html \"Discovery can identify an Oracle database instance that is running on UNIX or Windows operating systems. It can also find Oracle clusterware  which runs Real Application Clusters (RAC).\")                                                                                           | Oracle DB on Windows Pattern                     | Oracle Instance \\[cmdb_ci_db_ora_instance\\]                     | `$install_directory + \"\networkAdmin\\*.ora\"`                                                                                                                                                |\n| [Oracle](c_OracleDatabaseDiscovery.html \"Discovery can identify an Oracle database instance that is running on UNIX or Windows operating systems. It can also find Oracle clusterware  which runs Real Application Clusters (RAC).\")                                                                                           | Oracle DB on Windows Pattern                     | Oracle Instance \\[cmdb_ci_db_ora_instance\\]                     | `$install_directory + \"\\dbs\\*.ora\"`                                                                                                                                                          |\n| [Oracle](c_OracleDatabaseDiscovery.html \"Discovery can identify an Oracle database instance that is running on UNIX or Windows operating systems. It can also find Oracle clusterware  which runs Real Application Clusters (RAC).\")                                                                                           | Oracle DB on Unix Pattern                        | Oracle Instance \\[cmdb_ci_db_ora_instance\\]                     | `$install_directory + \"\/dbs\/*.ora\"`                                                                                                                                                          |\n| [Oracle](c_OracleDatabaseDiscovery.html \"Discovery can identify an Oracle database instance that is running on UNIX or Windows operating systems.",
        "QUERY": " 'what is a tracked configuration file'",
        "GROUND_TRUTH": "A tracked configuration file, as described in KB12345, is a file that belongs to certain applications and is discovered through the horizontal discovery process. These files are added to the Configuration Management Database (CMDB). Changes to these files can be tracked by comparing them to their previous versions. It's important to note that these configuration files contain sensitive system information. To prevent unauthorized access, access control lists (ACL) should be placed on the Tracked Configuration file table [cmdb_ci_config_file_tracked]. Only authorized users should be allowed to view this table or the Save Content setting should be unchecked."
    },
    {
        "id": 84,
        "CONTEXT": "\"## Non-admin users can't update Date type fields (multiple records) via List v3\n\n## Description\n\nNon-admin users can't update date type fields (multiple records) via List v3.\n\n## Steps to Reproduce\n\n1. Enable List V3.  \n2. Go to 'kb_knowledge' table list and change List edit type to \"Save immediately (cell edit mode)\" in List Control.  \n3. Impersonate \"Bernard Laboy\" (a non-admin who has \"knowledge_manager\" role).  \n4. Go to 'kb_knowledge' table and add 'Valid To' field to the list view  if not present.  \n5. Select multiple records via 'Valid To' field column and double-click to update it.  \n6. You will get 500 status error in the console as well as the following server error:  \n...  \nSEVERE \\*\\*\\* ERROR \\*\\*\\* java.lang.NullPointerException  \njava.lang.RuntimeException: java.lang.NullPointerException  \nat com.glide.rest.handler.impl.ServiceHandlerImpl.handleInvocationTargetException(ServiceHandlerImpl.java:79)  \nat com.glide.rest.handler.impl.ServiceHandlerImpl.invokeService(ServiceHandlerImpl.java:50)  \nat com.glide.rest.processors.RESTAPIProcessor.process(RESTAPIProcessor.java:228)  \nat com.glide.processors.AProcessor.runProcessor(AProcessor.java:474)  \nat com.glide.processors.AProcessor.processTransaction(AProcessor.java:199)  \nat com.glide.processors.ProcessorRegistry.process0(ProcessorRegistry.java:178)  \nat com.glide.processors.ProcessorRegistry.process(ProcessorRegistry.java:167)  \n....  \n\nUser is able to update single record at a time in the list view though.\n\n## Workaround\n\nDisable List v3 overall or for specific table via List Control.\n\n**Related Problem: PRB1280817**\n\n\n\n## Vancouver security and notable fixes\n\nIf the node running the progress worker restarts (for example  as part of an upgrade or patch or crashes) it can cause the progress worker to show false information as 'Running'. The transactions get cancelled at the back end when the node gets restarted but the progress worker status keeps showing running\/in-progress.                                                                                                                                                                                                                 | Refer to the listed KB article for details.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n| Table Administration and Data Management PRB1391889                                                                              | The table_name field lists only the database views from the current scope                                                                                                                                       |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n| Table Administration and Data Management PRB1551027 [KB1002412](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB1002412) | Creation of a field fails when a field name is longer than 30 characters and the field name matches an existing field with some additional prefix text                                                          | When a field name is greater than 30 characters long  an alias for the field needs to be used due to limits on the maximum column name allowed in the backend database. During creation  an attempt is made internally to shorten the alias by removing any initial prefix text up to a first '_'.                                                                                                                                                                                                                                                                                                                                                                                                  | Refer to the listed KB article for details.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n| Table Administration and Data Management PRB1567442                                                                              | The user observes a 'You do not have access to the selected breakdown elements' warning message on Dashboards                                                                                                   | This in turn causes a 'Not a valid indicator  Breakdown  Element or aggregate combination' error on analytic hub.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n| Table Administration and Data Management PRB1631253 [KB1216793](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB1216793) | The activity formatter does not work as expected on the sc_task table after upgrading to Tokyo                                                                                                                  | In Tokyo releases  history is not properly built for the sc_task table on instances with a shard DB. This prevents comments  work notes  and field value updates from appearing in the Activity Formatter and record history. The updates are captured in sys_audit and sys_journal field  but not the sys_history_set.                                                                                                                                                                                                                                                                                                                                                                             | Refer to the listed KB article for details.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n| Table Database Views PRB1627014 [KB1263224](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB1263224)                     | Database view with prefix matching a child table produces inconsistent results                                                                                                                                  | When creating a Database View and using a prefix for a table which is matching the name of an existing table from its hierarchy can produce inconsistent results. For example  using 'task' as a prefix for the \\[sc_task\\] table is not advised because \\[sc_task\\] extends \\[task\\].                                                                                                                                                                                                                                                                                                                                                                                                              | Refer to the listed KB article for details.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n| Tags PRB1615831                                                                                                                  | Excluding conditions in reports or lists isn't working for tags when they're applied on a parent table                                                                                                          |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n| UI Actions PRB1581607 [KB1124166](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB1124166)                               | Platform's related list control UI action Omit new condition isn't working as expected                                                                                                                          | An error message displays.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | Refer to the listed KB article for details.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n| UI Actions PRB1626218 [KB1217683](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB1217683)                               | The Insert and Stay button in the alm_license table form context menu disappears after activating the 'Software Asset Management Foundation' plug-in                                                            |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | Refer to the listed KB article for details.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n| UI Actions PRB1641523 [KB1281885](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB1281885)                               | The Edit UI action in related lists stopped working for ITIL users when there's a 'create' ACL that requires another role on the referenced table                                                               | The system throws an error: 'Unable to execute UI Action because user does not have permission to create the current record'.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | Refer to the listed KB article for details.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n| UX Framework PRB1588976 [KB1207729](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB1207729)                             | A jelly-based form is loaded with a stale GlideRecord  causing outdated data in Next Experience UI form fields                                                                                                  | When using the Next Experience UI  opening a record in the 'Form' view can intermittently show outdated field values  which can include comments and work notes posted in the activity stream.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | Refer to the listed KB article for details.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n| UX Framework PRB1623599                                                                                                          | Upgrading an instance from Rome to Tokyo results in skipped errors for multiple sys_ux_lib_component  sys_ux_lib_component_attr  and sys_ux_macroponent records                                                 | Upgrading an instance from Rome to Tokyo results in skipped errors for multiple sys_ux_lib_component  sys_ux_lib_component_attr  and sys_ux_macroponent records  due to the files already existing in a different scope.                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | 1. Upgrade an instance from Rome to Tokyo. 2. Search sys_upgrade_history_log for the upgrade and 'Skipped error'. {#vancouver-security-notables__ol_gf4_zd1_2yb} Observe multiple sys_ux_lib_component  sys_ux_lib_component_attr  and sys_ux_macroponent records.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n| UX Framework PRB162682...",
        "QUERY": " 'User unable to update multiple records in list control'",
        "GROUND_TRUTH": "According to KB12345, non-admin users are unable to update multiple records of date type fields via List v3. This issue can be reproduced by enabling List V3, going to the 'kb_knowledge' table list, changing the List edit type to \"Save immediately (cell edit mode)\" in List Control, and impersonating a non-admin user with the \"knowledge_manager\" role. When you try to select multiple records via the 'Valid To' field column and double-click to update it, you will encounter a 500 status error in the console and a server error. However, the user is able to update a single record at a time in the list view. The workaround for this issue is to disable List v3 overall or for a specific table via List Control."
    },
    {
        "id": 85,
        "CONTEXT": "\"## Unable to install\/update an application customization on Production\n\n## Description\n\nUnable to install\/update an application customization on a production instance if there is a newer base version of the app that is not installed on any sub-prod instances.\n\n## Steps to Reproduce\n\n1. Install an older version of a store application that allows app customization on sub-prod and production instances.  \nEg: Workplace Reservation Management at version 1.8.5 (new version is 1.9.7)  \n\nOn Sub-Prod instance:\n2. Create an application customization for the app in Studio.\n3. Add some changes to the app (eg: table  fields  BR  etc).\n4. Publish the customized version of the app to application repository at version 1.0.0\n\nOn Production instance:\n5. Navigate to System Applications \\> All and search for the app (Workplace Reservation Management)\n6. Select the Customized Version as \"1.0.0 \\[Based on 1.8.5\\]\" and the Base Version as \"1.8.5\"\n7. Click \"Update\".\n\nExpected: The customized version of the app is installed on Production instance.\nActual: The install does not trigger as the \"Update\" button on the page doesn't work.\n\n## Workaround\n\n1. Upgrade the store application to the latest version (1.9.7) on a sub-prod instance.  \n2. Install application customization version (1.0.0) for the app on production.  \n\n**Related Problem: PRB1536634**\n\n\n\n## Upgrade Plan module  FAQ (Tokyo+)\n\n## The High-Level Data flow is as follows-\n\n1. Configure your DEV instance as a BUILDER instance.\n2. Upgrade your instance (T==\\> TPatch\\*\/U\/V)\n3. Review and [Resolve all your Skipped](https:\/\/docs.servicenow.com\/csh?topicname=uc-process-skipped-records.html&version=latest) records ( POST upgrade)\\*\\*\\* This is very important. If there are no action items for a skipped configuration and there is a new version coming in the upgrade  the file will be reverted to the base version ( this can be revisited later). But it is recommended to address all the skipped items.\n4. Install new Plugins\/applications ( Custom or Store) if any.\n5. Upgrade Centre=\\> Upgrade Plan\n6. Created upgrade plan when ready ( after making necessary changes) should be published which will create an app for your AppRepo. This will create a Global Scoped Application \"Upgrade Plan -- Target Version name\"\n7. Upgrade plan can be retrieved on any Consumer instance from \" My Company Applications\"\n8. Install the new version if you have made any changes to the upgrade plan. Any changes to the upgrade plan ( Refresh) will create a new version and publish Automatically. The name will be \"Upgrade Plan -- Version name\". \\*\\*\\* We recommend testing this on a SUB PROD consumer instance before installing this.\n9. Review the Upgrade plan and the Related list ( Individual versions for each application). This will contain installed apps as well as custom apps. If you do not want any of these apps to be excluded from the upgrade ( please mark them inactive).Instance Administrators can compare the versions and mark them as per the organisational requirement\/development plan\n10. Create an upgrade change for your CONSUMER instance.\n11. \"Upgrade Preview\" and this will Preview the Upgrade with Upgrade Plan. Predictions will give you the count. It says how many are Automatically resolved.\n12. During upgrade  the Upgrade engine will take care of App Updates\/Commit Customisation\/Resolve Skipped post upgrade files( as you did in the BUILDER instance)\n13. Upgrade Monitor will show you the relevant information.\n\n## Points to remember\n\n1. Upgrade Plan is created as a GLOBAL AppRepo Application with version incremented for each of Refresh( link)\n2. Upgrade plan is a GLOBAL application which hold config files which are in their own respective scope. GLOBAL application is to create\/host\/publish\/transfer\/install the config files in an easy way to consumer instances.\n3. Upgrade Plan should be installed on your CONSUMER instance before the upgrade.\n4. The upgrade plan will include all your Global customizations ( there will be a new app created for this as \"Global Customizations - Upgrade Plan\"). This app will remain on your app repository as an app and will be used for future upgrades to new versions. For every family releases the version number will be incremented.\n5. All Skipped records post-upgrade when addressed\/Resolved will be included in the sys_update_xml. These will be captured in the upgrade plan as customisations for respective Scope\/Package and will be included in Upgrade Plan\n6. This will also create applications with versions as installed on your BUILDER instance.\n7. This will create new versions for Store installed apps\/plugins if these are customised on that instance on their respective SCOPE. It will list the current version that is installed and the Customised version if available. Product doc: [Manage customizations to applications](https:\/\/docs.servicenow.com\/csh?topicname=manage-customizations-store-apps.html&version=latest)\n8. If you have multiple DEV instances publishing to AppRep and if the current BUILDER instance does not have up-to-date custom applications which were not installed ( but published) on App Repo from other DEV; this will not be included in the Upgrade Plan. It will give a warning saying there is a new version available in App Repo.\n9. The upgrade plan only contains the config. It does not contain any metadata. So if there is a Scoped app update coming in via Upgrade Plan; the metadata is going to be its customised package even though the Upgrade plan is installed as an App from AppRep.\n10. Every time you refresh the Upgrade Plan; it will automatically get published to AppRepo. We need to make sure the same version is installed on the CONSUMER instance.\n11. Soon after the Upgrade Plan is installed on the CONSUMER it will start downloading the individual components for the Upgrade Engine to Consume. These are stored in the Attachment table.\n12. During the Upgrade preview; It will list out the number of records that get auto-resolved and records that will not. You need to take action on the records which say to be reverted. \n\nThis happens if your Consumer instance has extra customisations which were not on your BUILDER instance. You can take note of these records and bring the customisations back to BUILDER and Refresh Upgrade Plan which will include these NEW customisations. Then this new version should be installed on the consumer instance before the upgrade.\n13. The upgrade plan will be consumed during the upgrade and will do the activities which are captured. These config files will be updated to their metadata on that respective SCOPE.\n14. Upgrade Plan will be considered for any Schema changes and there will be only one online alter if there is a schema change coming in via upgrade and another one on the Upgrade plan\n15. If there are any items in an Error state during Upgrade validation on the CONSUMER instance; the customer can remediate the issue and reprocess the Upgrade Plan to rectify this before the Version Upgrade. There will be a Status column that will specify the issue to remediate.\n\n## FAQ\n\n**1. Which instance should I configure as Builder?**\n\nYou should be making your DEV instance the builder instance. If you have multiple DEV instances; you should configure the MASTER\/MAIN DEV instance as your BUILDER instance\n\nBy Default all instances are marked as \"Consumer\" Driven by property glide.upgrade.plan.instance_type ( only available from T+)\n\nThese instances should stay as a Consumer instance\n\nTEST\n\nPROD.\n\nOnce the instance is configured as a BUILDER; you cannot change it to consumer and the clone preserves this setting.\n\n**2. Can I make my PROD a BUILDER instance?**\n\nYou should NOT make your PROD instance a Builder instance\n\nThe upgrade plan is linked to each upgrade. So once there is a plan created; your Global customisations will be linked to a \"Global Customisations - Upgrade Plan\" Application. You do not want that in PROD\n\n**3. What happens to the Store application installation on the upgrade plan for which we do not have entitlement on the Consumer ( PROD)**\n\nEntitlement issue\/Licensing issue ( State will be Error). These will not be auto-installed  and this should be addressed manually.\n\nThis will be the same for MAINT-only plugins. This will be changed to Error.\n\n**4. What happens if I do not want to install an application\/update which is already on a tested upgrade plan**\n\nWe have the option to make individual updates Inactive before your Actual Version upgrade on the target instance. So these will be skipped.\n\n**5. Can I disable the Installed upgrade plan in case of any last-minute change of plans?**\n\nWe can disable the upgrade plan together. Also  there is an option to create another upgrade(Refresh) plan ( new version) and install it as an Update on the consumer before the upgrade\n\n**6. When we install the customisation via the Upgrade plan; will it leave an entry on sys_update_version and sys_update_xml?**\n\nIf it is a new App installation without any customisation; it will not leave an entry on sys_update\\* tables. If it is a Store App\/Plugin and is customised; it will have those customisation records on sys_update\\* tables on the consumer instance\n\n**7. Can I install a deactivated component from the Upgrade plan at a later stage on the consumer?**\n\nThis is not possible at the moment and it will be there from U+. This will again dependent on if there is a new version in App Repo\/was it already upgraded on the instance from App Repo\/Store.\n\n**8. What will happen to an application which is already in a newer version installed than the version coming in Upgrade Plan?**\n\nIf the upgrade plan is having the same or an older version; it will not be consumed during the Upgrade.\n\n**9. How can I find out if the Upgrade plan was executed?**\n\nThis will be listed on the sys_upgrade_history_log table\" Auto resolved from\" column.\n\n**10. Is this available for Self Hosted customers?**\n\nIf the self-hosted customer has AppRepo connectivity they can utilise this feature.\n\n**11. What if there is a plugin\/app dependency on an app\/plugin on the upgrade plan?**\n\nThe upgrade plan will not look into the dependencies and auto-install them. The dependent plugins should be a part of the Upgrade Plan\n\n**12. Will this increase the time of the total upgrade?**\n\nThis will increase the Upgrade change duration and increase the time to upgrade  but post-upgrade activities are managed by the Upgrade engine automatically.\n\n**13. Will the Upgrade Plan capture the Update-sets?**\n\n\"\n\n",
        "QUERY": " 'Update not working if the newer base version of app not installed on sub-prod'",
        "GROUND_TRUTH": "According to KB12345, the issue arises when you try to install or update an application customization on a production instance, but there is a newer base version of the app that is not installed on any sub-prod instances. The steps to reproduce this issue are detailed in the article, and it is noted that the 'Update' button does not work as expected in this scenario. \n\nHowever, a workaround is provided: \n1. Upgrade the store application to the latest version (1.9.7) on a sub-prod instance.  \n2. Install application customization version (1.0.0) for the app on production.  \n\nThis issue is related to Problem PRB1536634."
    },
    {
        "id": 86,
        "CONTEXT": "\"## [Rome] Catalog UI Policies not working in Service Portal\n\n## Description\n\nCatalog UI Policies not working in Service Portal after Rome upgrade  when the variable internal name is not matching the one in the Catalog UI policy action.\n\n## Steps to Reproduce\n\n* Open any catalog item and create a variable with name \"u_test\"\n* Create a UI Policy and UI Policy Action for the above variable to hide it.\n* Export the UI Policy Action as XML and change the name (variable) to u_test_abc\n* Import back the UI Policy Action with changed name so that there is an UI Policy Action on a variable with name not matching with name of the variable\n* Test the catalog item in the native UI using try it button  you will see that the variable is hidden\n* Now open the catalog item in portal and see that the variable is not hidden\n\nThe reason this isn't working is  there is a mismatch of the variable internal name and the one defined in the catalog UI policy action.\nEven with this mismatch  this works in the previous release Quebec  and still works in DesktopUI.\n\n## Workaround\n\nThis issue can happen if\n\n1. There is a miss match between item_option_new (variable) record's name and \\`variable\\` field on catalog_ui_policy_action record.\n2. There is a miss match between item_option_new_set (variable set) record's \\`internal_name\\` and \\`variable\\` field on catalog_ui_policy_action record (In this case policy action is on entire variable set).\n\nThere are two possible workarounds for this issue\n\n1. Customer can (create and) set \\`glide.sc.ui_policy.use_cache\\` property value to \\`false\\`\n\n* This will take customer back to pre Rome behaviour where we do not use cache for fetching UI policies  so if there are many UI policies there might be performance some performance impact\n\n2. Customer can correct (sync) \\`variable\\` field on \\`catalog_ui_policy_action\\` record to match with \\`name\\`\/\\`internal_name\\` of corresponding Variable\/Variable Set\n\nTo sync  use below script. Replace \\`:itemId\\` with the sys_id of catalog item where UI policy issues are found\n\n\/\/ replace catalog item sys_id with :itemId\nvar catItemId = ':itemId';\nvar policyIds = \\[\\];\nvar itemPolicies = new GlideRecord('catalog_ui_policy');\nitemPolicies.addQuery('applies_to'  'item');\nitemPolicies.addQuery('catalog_item'  catItemId);\nitemPolicies.query();\nwhile(itemPolicies.next()) {\npolicyIds.push(itemPolicies.getUniqueValue());\n}\n\nvar catItem = new sn_sc.CatItem(catItemId);\nvar sets = catItem.getVariableSet();\n\nif(sets \\&\\& sets.length) {\nvar setPolicies = new GlideRecord('catalog_ui_policy');\nsetPolicies.addQuery('applies_to'  'set');\nsetPolicies.addQuery('variable_set'  'IN'  sets);\nsetPolicies.query();\n\nwhile(setPolicies.next()) {\npolicyIds.push(setPolicies.getUniqueValue());\n}\n}\ngs.info('policy to be synced = '+ policyIds)\nif (policyIds.length) {\nvar policyActions = new GlideRecord('catalog_ui_policy_action');\npolicyActions.addQuery('ui_policy'  'IN'  policyIds);\npolicyActions.query();\n\nwhile(policyActions.next()) {\nvar variable = policyActions.catalog_variable.getDisplayValue().split(\"IO:\")\\[1\\];\nif (!variable)\nvariable = policyActions.catalog_variable.split(\"IO:\")\\[1\\];\nvar option = new GlideRecord('item_option_new');\nif (option.get(variable)) {\npolicyActions.variable = option.name;\n} else {\noption = new GlideRecord(\"item_option_new_set\");\nif (option.get(variable))\npolicyActions.variable = option.internal_name;\n}\npolicyActions.update();\n}\n}\n\nNote: There is also a \\`catalog_variable\\` field on \\`catalog_ui_policy_action\\` which is in form of \\`IO:sys_id_of_variable\\`  No need to touch that.\n\nIf customer is choosing workaround 1  they should delete the property once they are in version where fix is available.\n\n**Related Problem: PRB1524152**\n\n\n\n## San Diego Patch 10\n\nEnsure the 'Display Data Table' checkbox is checked. 7. Create a new Service Portal page. 8. Drop the Report widget on the Portal page. 9. Configure the Report widget to use the 'Open Incidents by Assignment' report. 10. Repeat steps 8-9  placing the second widget instance right below the first widget. {#sandiego-patch-10__ol_ph3_4jz_fwb} Expected behavior: The table of the first widget isn't cut off. The table should be fully visible. Actual behavior: The table of the first widget is cut off.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n| Resource Management PRB1591007                                                                                         | 'Recalculate Resource Cost' doesn't update the planned cost                                                                                                                                                   | When rolling up  the decimal is ignored so it's not present on the requested_allocation record. The planned cost considers this decimal  which leads to a discrepancy.                                                                                                                                                                                                                                                                                                                                                                                                                          |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n| Scheduled Jobs PRB1594588                                                                                              | Transaction has no start date in SchedulerThread                                                                                                                                                              | Scheduler thread (glide.scheduler) is transactional in San Diego+  but the transaction does not have a start date attached. This can cause issues when node memory remediation starts  potentially killing the glide.scheduler thread every time it runs. There may be downstream effects to the glide.scheduler thread being killed by memory remediation in this way.                                                                                                                                                                                                                         |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n| Service Catalog PRB1538387 [KB1204327](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB1204327)                | Multi Row Variable Sets (MRVS) are overriding values when more than one set is on an order guide or catalog item                                                                                              |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | Refer to the listed KB article for details.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |\n| Service Catalog PRB1587013                                                                                             | The tab display name is missing when ordering from the Order Guide                                                                                                                                            | After upgrading to San Diego  the tab display name is missing when viewed in Firefox and Safari. This issue does not occur in MS Edge or Chrome.                                                                                                                                                                                                                                                                                                                                                                                                                                                | 1. Navigate to the Self Service Catalog. 2. Select New AD Client and adding Software in the Order AD Services category. 3. Fill in the Owner Variable. 4. File Select the use Case Variable. Create AD client AND add Software AND Standard client installation. 5. Click Choose Options {#sandiego-patch-10__ol_vh3_4jz_fwb} Notice that the 3rd tab title is blank.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |\n| Service Catalog PRB1611613                                                                                             | Service Catalog item fails in the San Diego version of the service portal                                                                                                                                     | The user cannot request catalog items in service portal.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | 1. Create a new sys_property - glide.sc.disable.question.cache and set the value to true. 2. Perform a cache flush. {#sandiego-patch-10__ol_gh3_4jz_fwb} Expected behavior: Item must be displayed without any error. Actual behavior: Cannot request catalog items in service portal.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |\n| Service Management Virtual Agent Topic B PRB1620647                                                                    | vaContext isn't accessible from topics in scoped apps                                                                                                                                                         | The base script including VATopicsHelper is showing that the error of 'vaContext' is not defined.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n| Service Mapping PRB1614707                                                                                             | When opening more than one tab of service mapping (from Service Dashboard)  the service name doesn't change                                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | 1. Create several operational services. 2. Open 'Service Operations Workspace' experience. 3. Navigate to the Service Dashboard. 4. Click a service and  on the opening dialog  click Service Map. 5. Click a different service and  on the opening dialog  click Service Map.. {#sandiego-patch-10__ol_phg_glz_fwb} Expected behavior: The corresponding service name is in the header of the opened Service Map tabs. Actual behavior: User gets the same service name of the first service map they opened regardless of the service that was picked.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |\n| Service Portal PRB1544103                                                                                              | An attachment in a catalog item is removed after clicking the Enter key in the other field in the portal's 'sc_cat_item' page                                                                                 |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n| Service Portal PRB1554172                                                                                              | User's comments can be duplicated multiple times for Service Portal                                                                                                                                           |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | 1. In a Chrome browser  simulate a slower network connection (about 28- kbits\/sec). 2. Open an existing from the ticket widget. 3. Open a case\/incident. 4. Post some long text from the portal side. 5. Before this text is posted  attach a file. The system sends multiple requests to\/for the same text multiple times. {#sandiego-patch-10__ol_a33_4jz_fwb} Expected behavior: Text and file are saved only once. Actual behavior: Text may be saved several times (minimum 2 times).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n| Service Portal PRB1579714                                                                                              | A variable with type Date\/Time displays placeholder text when in Read-Only                                                                                                                                    |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | 1. In a San Diego base instance  navigate to a catalog item that displays in the Service Catalog: Navigate to Service Catalog \\> Catalog Definitions \\> Maintain Items and open a catalog item record. 2. Scroll down to the Related Lists and select Variables. 3. Click New. You should be routed to a new page. 4. Fill out the form. \n\n## Vancouver security and notable fixes\n\n A password reset link redirects the user to the 'SSO login' page                                                                                                                                                |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | Refer to the listed KB article for details.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n| Service Portal PRB1460486     ...",
        "QUERY": " 'Unable to hide price in catalog request in Service Portal'",
        "GROUND_TRUTH": "no answer found"
    },
    {
        "id": 87,
        "CONTEXT": "\"## Unable to bulk re-archive restored records\n\n## Description\n\nIf you have performed a bulk restore of archived records (as described in [KB0680127](\/kb_view.do?sysparm_article=KB0680127 \"KB0680127\"))  you might expect that these restored records will be archived again when the Archive schedule job (Archiver) runs. However  Archiver by design  will not re-archive records that have been manually restored by a user. There might be various reasons that previously archived records are restored  so the Archiver cannot determine whether these restored records should be re-archived. You therefore need to take action to re-archive them  for example  clicking on the Restore record UI action on each record. Currently  there is no option to re-archive manually restored records in bulk.\n\n## Steps to Reproduce\n\nArchive a record using an active Archive rule.\n\nRestore that record using the Restore Record UI Action.\n\nNote that the regularly scheduled job - Archive runs and does not re-archive that record.\n\n## Workaround\n\n**Workaround (Pre Paris release):**\n\nTo re-archive restored records in bulk  consider adjusting the script provided below to clear entries from sys_archive_log ONLY for those restored records you want re-archived. You can put it in a scheduled script execution to make it run periodically to batch through a large recordset.\n\n**Notes**:\n\nThis script will NOT re-archive the records. It sets up the log so that on the next run of the Archive job  these records will get picked up and archived along with the other records that meet the criteria specified in the active Archive rules.\n\nBecause the sys_archive_log table could be large  you might want to consider options such as the following:\n\nBecause the OOB Archive job runs hourly  the script when executed will increase the number of records that fulfill the criteria specified in the active archive rule. This ight cause the Archive job to run past one hour if the default archive rule processing behavior or the number of active archive rules have been increased recently. This increases the likelihood of running into the known issue described in [KB0550967](\/kb_view.do?sysparm_article=KB0550967 \"KB0550967\") \"Archive threads can run in parallel on separate nodes for long running archive processes  degrading performance and adding noise errors to the log.\"\n\nTest in subproduction before implementing in Production.\n\n**Script**:\n\nvar gr = new GlideRecord('sys_archive_log');\ngr.addQuery('id'  recordToArchive);   \/\/ Use this to specify a record to re-archive\ngr.addNotNullQuery('restored');\u00a0 \u00a0 \u00a0  \/\/ Use this to limit to records which have been restored (i.e. records that are currently NOT archived)\ngr.addQuery('from_table'  tableName); \/\/ Use this to limit to records on the table you want to re-archive\ngr.query();\n\nwhile(gr.next()) {\n\u00a0 \u00a0 gs.print(\"Resetting archive log to re-archive record: \" + gr.id);\n\u00a0 \u00a0 if (gr.deleteRecord()) {\ngs.print(\"Log entry removed  record ready for re-archive\");\n} else {\ngs.print(\"Failed to remove entry from sys_archive_log\");\n}\n}\n**Paris Release and later:**\n\nIf the [Auto Rearchive checkbox](https:\/\/docs.servicenow.com\/csh?topicname=t_CreateAnArchiveRule.html&version=latest \"Auto Rearchive checkbox\") is checked on an archive rule  the restored record will be automatically rearchived after the specified auto rearchive duration (as part of the Archive job). Note that the Auto rearchive option will rearchive all restored records regardless of the condition in the rule.\n\nTo rearchive restored records conditionally  you can use the below script.\n\nvar gr = new GlideRecord('Table_To_Archive_From');\ngr.addQuery(); Write Query to fetch records you want to rearchive\ngr.query();\n\nwhile (gr.next()) {\nif (gr.get(gr.sys_id))\n{\nvar archiver = new GlideArchiveRecord();\narchiver.archive(gr);\n}\n}\n\n**Related Problem: PRB629054**\n\n\n\n## It is not possible to archive all cmdb_rel_ci record when CI records are Archived  leading to Compac\n\n## Description\n\nOn an instance that has the Data Archiving plugin installed and in use for the CMDB tables  the CMDB Identification and Reconcilliation engine can throws a huge number of errors like this in the syslog table:\nCompactRelation: failed to get details of CI bc4fe864135b9bc0574c75276144b09f: no thrown error \\| syslog \\| 000009c213579700f6a7f107d144b0b3 \\| system \\| 2018-07-15 21:36:31 \\| \\| 2 \\| com.glide.ui.ServletErrorListener \\| service_cache_mgr\nAnd the app node localhost log:\n2018-07-26 10:35:54 (605) Default-thread-3 AAD39C29136797C04DD2F027D144B0B4 #10927362 \/ngbsmprocessor.do Parameters -------------------------\nactionType=loadBasic\nmapScriptID=\nserviceMode=false\ncmd=get\nid=8f59d4dce19f71c424893534899bf84e\ncacheKill=1532626554567\n2018-07-26 10:35:55 (694) Default-thread-3 AAD39C29136797C04DD2F027D144B0B4 SEVERE \\*\\*\\* ERROR \\*\\*\\* service_cache_mgr : CompactRelation: failed to get details of CI 0008b8644fe49ec06433ab99f110c7cf\n2018-07-26 10:35:55 (696) Default-thread-3 AAD39C29136797C04DD2F027D144B0B4 SEVERE \\*\\*\\* ERROR \\*\\*\\* service_cache_mgr : CompactRelation: failed to get details of CI 001ae60913d2ba006203b2d96144b0ad\n2018-07-26 10:35:55 (698) Default-thread-3 AAD39C29136797C04DD2F027D144B0B4 SEVERE \\*\\*\\* ERROR \\*\\*\\* service_cache_mgr : CompactRelation: failed to get details of CI 009d6cd81337fac0b23ffea2e144b057\n2018-07-26 10:35:55 (700) Default-thread-3 AAD39C29136797C04DD2F027D144B0B4 SEVERE \\*\\*\\* ERROR \\*\\*\\* service_cache_mgr : CompactRelation: failed to get details of CI 00fc04514f126a806433ab99f110c70d\n.......\nThis code runs when the Dependency View ngbsmprocessor needs to collect details of all related CIs.\nThis causes 2 main problems:\n\n* ServiceCacheManager.getInboundRelations and .getOutboundRelations won't work properly  even though the data is there in the instance.\n* On an instance using the CMDB and discovery sources heavily  this can add such a large number of entries to syslog tables to cause disk space and performance issues.\n\nUnlike the CI Form  which realizes the CI is archived and redirects to the archived record  this code does not check to see if a CI record is archived and instead just throws an error.\nNeither the Archiving nor CMDB plugins have any feature for 'cascade archiving' related records such as cmdb_rel_ci when a CI is archived that would prevent a relationship record remaining after a CI is archived.\n\n## Steps to Reproduce\n\nOn a clean Kingston instance with demo data:\n1\/ Install 'Data Archiving' plugin\n2\/ For the purposes of this test  create an archive rule for the Linux Server table to archive any CIs with condition: name starts with lnux  which will cover 'lnux100' and 'lnux101' CIs\n3\/ Activae the rule  then manually run the 'Archive' scheduled job to avoid waiting\n4\/ Open the form for the \"Client Services\" business service CI. This has relationships with those server\n5\/ Open the Dependency view (BSM Map) from the button ion the CI relations section (which you'll notice doesn't show the archived Linux servers)\n6\/ Check the syslog entries  and you will see a pair of errors  one for each linux server\n2018-07-27 18:18:28 Error service_cache_mgr : CompactRelation: failed to get details of CI 53958ff0c0a801640171ec76aa0c8f86: no thrown error com.glide.ui.ServletErrorListener\n2018-07-27 18:18:28 Error service_cache_mgr : CompactRelation: failed to get details of CI 539747cac0a801640163e60735fbbf6e: no thrown error com.glide.ui.ServletErrorListener\n\n## Workaround\n\nFor Data Archiving there is an option to archive related records and whenever a rule is created for CMDB CI archival  the corresponding relations should also be archived.\n\nHowever  cmdb_rel_ci (CMDB relationship) table has two references to cmdb_ci table (parent and child). When creating a \"Archive Related Records\" rule  you can only select 1 of those fields  and therefore **this workaround is only half a workaround  but is better than nothing**.\n\n1. In your CMDB related archive rules  add a new Archive Related Records entry\n2. Select \"Relationships\" in drop down  and it will pre-select \"parent\" element and there is no way to add the \"child\" element as well.\n\n**Related Problem: PRB1296280**\n\n\n\n## San Diego Patch 10\n\nNavigate back to the studio window that was on hold at step 3  add the commit message and click Commit Files. 7. After the commit at step 6 completes  navigate to the studio window that was on hold at step 5  add the commit message  and then click Commit Files. 8. After the commit completes at step 8  check the sys_update_set list. {#sandiego-patch-10__ol_ah3_4jz_fwb} Expected behavior: Ups-2 and ups-3 are not deleted. Actual behavior: Ups-2 and ups-3 are deleted.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |\n| Survey Management PRB1564051                                                                                           | Incorrect survey responses are captured when the survey is taken from a non-desktop screen on Portal                                                                                                          | Incorrect survey responses are captured (metric value shows -1 in the asmt_metric_result record) when the survey is taken from a non-desktop screen on the portal.                                                                                                                                                                                                                                                                                                                                                                                                                              | 1. Log into any base instance. 2. Create a survey with images or open existing survey images. 3. Make sure that the survey has one string field for comments which hidden by default and should be shown as a dependent of one image. 4. Navigate to any survey in the portal. 5. Open inspector tool in Chrome. 6. Change the screen size  select the buttons in the image scale  and submit. 7. Check the asmt_metric_result record for the submitted survey. {#sandiego-patch-10__ol_zh3_4jz_fwb} Notice that the Comments value shows -1.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |\n| System Archiving PRB1614589                                                                                            | sys_attachment records should be deleted when their archived parent record is destroyed                                                                                                                       | sys_attachment records aren't deleted when a parent archived record is destroyed after upgrading to San Diego.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | 1. Pick a sys_email record. 2. Add a sys_attachment record to it. 3. Create or modify a sys_archive rule to archive the sys_email record. 4. Create a sys_archive_destroy rule to destroy the ar_sys_email. 5. Run it to destroy the ar_sys_email. {#sandiego-patch-10__ol_dh3_4jz_fwb} Expected behavior: The attached sys_attachment\/sys_attachment_doc records should be deleted. Actual behavior: The attached sys_attachment\/sys_attachment_doc records aren't deleted.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n| System Archiving PRB1616089                                                                                            | Re-archived records are destroyed according to their first archive date instead of their most recent                                                                                                          | If a record is archived  restored  then re-archived  a sys_archive_destroy rule will use the first archive date to determine when to delete the archived record  thereby prematurely deleting data that was not meant to be deleted until the second archive timestamp.                                                                                                                                                                                                                                                                                                                         | 1. Create (or re-use) a sys_archive_rule to target multiple sys_email records. 2. Manually run the rule to archive the records to ar_sys_email and sys_archive_log. 3. Wait five minutes. 4. Restore a single ar_sys_email record. 5. Run the same sys_archive rule again to re-archive the sys_email record you just restored. 6. Create a sys_archive_destroy rule to target ar_sys_email records older than 3 minutes and run it manually. {#sandiego-patch-10__ol_zdn_1lz_fwb} Expected behavior: The ar_sys_email record that was archived two should be left alone while all other records deleted Actual behavior: All ar_sys_email records are destroyed.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n| UX Framework PRB1592204                                                                                                | The 'Save as' list action doesn't work for most lists in Service Operation Workspace                                                                                                                          | Using the 'save as' feature doesn't work. It doesn't create a list under 'My Lists'.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | 1. Log in to a San Diego instance. 2. Install the sn_sow plugin. 3. Launch Service Operations Workspace. 4. Under Lists \\> Change \\> Open. 5. Click the 'gear' icon. 6. Click Save as. 7. Give it a title. 8. Click Save. 9. Open to 'My Lists'. {#sandiego-patch-10__ol_uh3_4jz_fwb} Actual behavior: There is no list created under 'My Lists'. Expected behavior: There is a list created under 'My Lists'.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n| Virtual Agent PRB1568236                                                                                               | When Dynamic Translation for Virtual Agent is enabled for a conversation  Dynamic Translation for Agent Chat (DTAC) does not work properly because queueEntryAccepted doesn't submit work to the hybrid queue | The notifyChatStarted method relies on the fact that it currently runs on the agent's thread  so it sets the live_agent_session language from the agent's glide session. The queueEntryAccepted method should retrieve the agent's session language and submit it along with the conversation ID via the hybrid queue in order to obtain a lock and avoid OptimisticLockExceptions.                                                                                                                                                                                                             | 1. Enable Dynamic Translation for Virtual Agent. 2. Enable DTAC. 3. Start a chat as a requester with a non-English language. 4. As an agent  accept the live agent request from the requester. {#sandiego-patch-10__ol_xh3_4jz_fwb} Expected behavior: DTAC should correctly set the liveagent_session_language and DTAC should work with the requester. Actual behavior: OptimisticLockException occurs when queueEntryAccepted and notifyChatSTarted tries to set the live_agent_session language and update the conversation context.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |\n| Workforce Optimization for IT Service Management PRB1625637                                                            |\"\n\n",
        "QUERY": " 'unable to archive records that were restored earlier'",
        "GROUND_TRUTH": "According to KB12345, the inability to re-archive restored records is by design. The Archiver will not re-archive records that have been manually restored by a user. This is because there might be various reasons that previously archived records are restored, so the Archiver cannot determine whether these restored records should be re-archived. \n\nTo re-archive restored records, you need to take action manually. For instance, you can click on the Restore record UI action on each record. However, there is currently no option to re-archive manually restored records in bulk.\n\nA workaround is provided in the article. For versions before the Paris release, you can adjust a script provided in the article to clear entries from sys_archive_log ONLY for those restored records you want re-archived. This script sets up the log so that on the next run of the Archive job, these records will get picked up and archived along with the other records that meet the criteria specified in the active Archive rules.\n\nFor the Paris release and later, if the Auto Rearchive checkbox is checked on an archive rule, the restored record will be automatically rearchived after the specified auto rearchive duration (as part of the Archive job). To rearchive restored records conditionally, you can use the script provided in the article."
    },
    {
        "id": 88,
        "CONTEXT": "\"## Public knowledge articles in edit mode reset the Attachment link to relative URL link\n\n## Description\n\nThe only way to link attachments to KB articles is via URL link in the format: **\/sys_attachment.do?sys_id=\\<sys_id\\>**\n\nThis makes end users not necessarily authenticated to an instance  but with access to public articles  unable to access the attachment link they receive from emails on public KB articles.\n\n## Steps to Reproduce\n\n- Login as KM administrator on any current instance.  \n- Create a KB article and upload a document\/picture as attachment.  \n- Mark some of the text and create a link to the attachment through the HTML editor.  \n- Choose the Attachment link  as opposed to URL.  \n- Select the document uploaded earlier.  \n- Save the KB article.  \n- Open the KB article in edit mode and select the link to edit it.  \nNotice the Attachment link has been reset to be an URL link. It is not possible to make it stay as an Attachment link  even deleting and recreating the attachment.  \nIf you send the KB article as email to users with no access to the instance  they cannot access the attachment.  \n\n## Workaround\n\nThis is expected behaviour and by design. In all current releases an attachment link to an article is always converted to a URL link before being stored into the database.\n\n**Related Problem: PRB686107**\n\n\n\n## Vancouver security and notable fixes\n\n[Vancouver release notes](..\/..\/release-notes\/family-release-notes.html \"The ServiceNow Vancouver release includes new products and applications  as well as additional features and fixes for existing products. Read the release notes to learn about the release  prepare for your upgrade  and upgrade your instance.\") \\> [Learn about the Vancouver release](..\/..\/release-notes\/concept\/rn-learn-landing-page.html \"The Vancouver release includes new features and improvements built on the Now Platform.\") \\>\n\n# Vancouver security and notable fixes {#ariaid-title1}\n\nThe Vancouver release contains important problem fixes.\n\nVancouver was released on August 3  2023.:\n\nImportant: For more information about how to upgrade an instance  see [ServiceNow upgrades](..\/upgrades\/reference\/upgrade.html \"The upgrade process moves your instance to a new ServiceNow release version. Understand the difference between upgrading and patching  release definitions  rollback and backup options  and how to test your non-production and production instance upgrades.\").\n\nFor more information about the release cycle  see the [ServiceNow Release Cycle](https:\/\/support.servicenow.com\/kb_view.do?sysparm_article=KB0547244).{#vancouver-security-notables__p_download-version}\nNote: This version is now available for use within all regulated market environments. For more information about services available in isolated environments  see [KB0743854](https:\/\/support.servicenow.com\/kb?id=kb_article_view&sysparm_article=KB0743854&_ga=2.238511747.200430442.1684856845-2052949275.1611611591).\nFor a downloadable  sortable version of the fixed problems in this release  click [here](https:\/\/downloads.docs.servicenow.com\/enus\/vancouver\/rn\/patches\/PRBs-V00.00.xlsx).\n\n## Security-related fixes {#vancouver-security-notables__section_qhl_2lv_xxb}\n\nVancouver includes fixes for security-related problems that affected certain ServiceNow\u00ae applications and the Now Platform\u00ae. We recommend that customers upgrade to this release for the most secure and up-to-date features. For more details on security problems fixed in Vancouver  refer to [KB1430552](https:\/\/support.servicenow.com\/kb?id=kb_article_view&sysparm_article=KB1430552).\n\n## Notable fixes {#vancouver-security-notables__section_shl_2lv_xxb}\n\n{#d129083e176}\n\n|                                                             Problem                                                              |                                                                                                Short description                                                                                                |                                                                                                                                                                                                                                                                                                                                             Description                                                                                                                                                                                                                                                                                                                                             |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Steps to reproduce                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |\n|----------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Activity Stream PRB1616546 [KB1213047](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB1213047)                          | Activity audit relation entries aren't captured when a record is deleted                                                                                                                                        | It generates an error: 'Activity.Rules \\*\\*\\* RULE FAILED with exception: table=sys_audit_relation \\*\\*\\* GlideRecord.setTableName - empty table name'.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | Refer to the listed KB article for details.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n| Activity Stream PRB1632844 [KB1219727](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB1219727)                          | Comments\/worknotes don't clear in the journal box after they're posted on Workspace                                                                                                                             | This issue is intermittent.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | 1. Log in to any Tokyo instance. 2. Open Agent Workspace. 3. Ensure that glide.ui.journal.use_html is set to true. 4. Open an existing record or create a record of incident\/case. 5. Change a field on the form. 6. Add a multiline text on the work notes. 7. Click Post. {#vancouver-security-notables__ol_rvl_zd1_2yb} Notice that comments\/worknotes don't clear in the journal box after they're posted on Workspace.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n| Activity Stream PRB1638410                                                                                                       | 'Need to pass' in isJournal prop to HTML compose fields prevent controls from uploading attachments                                                                                                             |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | 1. Ensure the glide.ui.journal.use_html system property is set to true. 2. Navigate to a record on the workspace. 3. Open the 'Network' tab in the developer console. 4. Copy and paste or drag an image into the rich text editor for work notes or comments. {#vancouver-security-notables__ol_d4r_sp2_gyb}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n| Activity Stream PRB1640708                                                                                                       | Activity Stream's 'Expand all' doesn't have a logical cap on the server-side  consuming resources                                                                                                               | The instance struggles to process and return a result.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n| Activity Stream PRB1648393 [KB1298601](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB1298601)                          | When the Activity Stream API is v1 and a 'sent' email with a recipient and an event creator resolve to the same username  the usernames fail to resolve                                                         | If a document record has an email and a journal  attachment  or field change event from the same user  all usernames fail to resolve in the document's activity stream. The name is either their user ID  their email  or possibly their sys_id.                                                                                                                                                                                                                                                                                                                                                                                                                                                    | Refer to the listed KB article for details.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n| Activity Stream PRB1650645                                                                                                       | A field change translation from English to Japanese is incorrect in Agent Workspace                                                                                                                             | The Japanese translation for field changes in 'Activities' isn't correct in Agent Workspace.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | 1. Enable the Japanese plugin. 2. Navigate to Agent Workspace. 3. Navigate to List \\> Incident \\> Open.. 4. Open any record. 5. Check the activity. 6. Observe that there's a field change in English. 7. Switch the language to Japanese. 8. Check the activity. 9. Observe that there's a field change in Japanese. {#vancouver-security-notables__ol_cyl_zd1_2yb} Observe that the Japanese translation for field changes in 'Activities' isn't correct.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n| Activity Stream PRB1656090                                                                                                       | Junk and undeliverable emails are shown in the activity stream in Agent Workspace                                                                                                                               | This issue is observed when email begins in 'Received' and takes 10+ seconds to be marked as 'Received-ignored'.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | 1. Create an email from an incident. 2. In a different tab  navigate to the sys_email table and change the email is step #1 type to 'Received'. \n\n A password reset link redirects the user to the 'SSO login' page                                                                                                                                                |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | Refer to the listed KB article for details.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n| Service Portal PRB1460486                                                                                                        | Load more results brings the duplicate records and loop through every time the user clicks 'Load more results'                                                                                                  | After setting pagination  the 'Load more results' link displays duplicate results.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | 1. Open any base instance. 2. Navigate to service portal. 3. Perform a search. 4. Once in the Faceted search widget  click 'Load more results'. {#vancouver-security-notables__ol_b5n_zd1_2yb} Notice that same list gets repeated.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |\n| Service Portal PRB1543669                                                                                                        | Knowledge articles' view count isn't displayed when above 999 views                                                                                                                                             | When using the Service Portal 'KB Category Page' widget  if a knowledge article has more than 999 views  the view count isn't displayed on the Service Portal 'Knowledge Categories' page.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | 1. In the latest instance  navigate to the '\/SP' page. 2. Select the 'Knowledge' link on the header. 3. Click each category. 4. Observe that the article previews' display an article view count. 5. Operating Systems \\> Windows \\> Windows Vista.. 6. Observe the default view count. 7. Export the XML file. 8. Manually update the sys_view_count to exceed 1000 views. 9. Upload the edited XML. 10. Repeat steps 1-...",
        "QUERY": " 'Unable to access the KB attachment if the KB article is sent as email to users'",
        "GROUND_TRUTH": "According to the KB12345 article, the issue where users are unable to access the KB attachment if the KB article is sent as an email is due to the design of the system. When a KB article is created and a document or picture is attached, the attachment link is converted to a URL link before being stored into the database. This URL link is in the format: \/sys_attachment.do?sys_id=<sys_id>. This means that end users who are not authenticated to an instance but have access to public articles are unable to access the attachment link they receive from emails on public KB articles. This is expected behavior and is by design in all current releases."
    },
    {
        "id": 89,
        "CONTEXT": "\"## [Rome] Catalog UI Policies not working in Service Portal\n\n## Description\n\nCatalog UI Policies not working in Service Portal after Rome upgrade  when the variable internal name is not matching the one in the Catalog UI policy action.\n\n## Steps to Reproduce\n\n* Open any catalog item and create a variable with name \"u_test\"\n* Create a UI Policy and UI Policy Action for the above variable to hide it.\n* Export the UI Policy Action as XML and change the name (variable) to u_test_abc\n* Import back the UI Policy Action with changed name so that there is an UI Policy Action on a variable with name not matching with name of the variable\n* Test the catalog item in the native UI using try it button  you will see that the variable is hidden\n* Now open the catalog item in portal and see that the variable is not hidden\n\nThe reason this isn't working is  there is a mismatch of the variable internal name and the one defined in the catalog UI policy action.\nEven with this mismatch  this works in the previous release Quebec  and still works in DesktopUI.\n\n## Workaround\n\nThis issue can happen if\n\n1. There is a miss match between item_option_new (variable) record's name and \\`variable\\` field on catalog_ui_policy_action record.\n2. There is a miss match between item_option_new_set (variable set) record's \\`internal_name\\` and \\`variable\\` field on catalog_ui_policy_action record (In this case policy action is on entire variable set).\n\nThere are two possible workarounds for this issue\n\n1. Customer can (create and) set \\`glide.sc.ui_policy.use_cache\\` property value to \\`false\\`\n\n* This will take customer back to pre Rome behaviour where we do not use cache for fetching UI policies  so if there are many UI policies there might be performance some performance impact\n\n2. Customer can correct (sync) \\`variable\\` field on \\`catalog_ui_policy_action\\` record to match with \\`name\\`\/\\`internal_name\\` of corresponding Variable\/Variable Set\n\nTo sync  use below script. Replace \\`:itemId\\` with the sys_id of catalog item where UI policy issues are found\n\n\/\/ replace catalog item sys_id with :itemId\nvar catItemId = ':itemId';\nvar policyIds = \\[\\];\nvar itemPolicies = new GlideRecord('catalog_ui_policy');\nitemPolicies.addQuery('applies_to'  'item');\nitemPolicies.addQuery('catalog_item'  catItemId);\nitemPolicies.query();\nwhile(itemPolicies.next()) {\npolicyIds.push(itemPolicies.getUniqueValue());\n}\n\nvar catItem = new sn_sc.CatItem(catItemId);\nvar sets = catItem.getVariableSet();\n\nif(sets \\&\\& sets.length) {\nvar setPolicies = new GlideRecord('catalog_ui_policy');\nsetPolicies.addQuery('applies_to'  'set');\nsetPolicies.addQuery('variable_set'  'IN'  sets);\nsetPolicies.query();\n\nwhile(setPolicies.next()) {\npolicyIds.push(setPolicies.getUniqueValue());\n}\n}\ngs.info('policy to be synced = '+ policyIds)\nif (policyIds.length) {\nvar policyActions = new GlideRecord('catalog_ui_policy_action');\npolicyActions.addQuery('ui_policy'  'IN'  policyIds);\npolicyActions.query();\n\nwhile(policyActions.next()) {\nvar variable = policyActions.catalog_variable.getDisplayValue().split(\"IO:\")\\[1\\];\nif (!variable)\nvariable = policyActions.catalog_variable.split(\"IO:\")\\[1\\];\nvar option = new GlideRecord('item_option_new');\nif (option.get(variable)) {\npolicyActions.variable = option.name;\n} else {\noption = new GlideRecord(\"item_option_new_set\");\nif (option.get(variable))\npolicyActions.variable = option.internal_name;\n}\npolicyActions.update();\n}\n}\n\nNote: There is also a \\`catalog_variable\\` field on \\`catalog_ui_policy_action\\` which is in form of \\`IO:sys_id_of_variable\\`  No need to touch that.\n\nIf customer is choosing workaround 1  they should delete the property once they are in version where fix is available.\n\n**Related Problem: PRB1524152**\n\n\n\n## Vancouver Patch 4 Hot Fix 1: Known Errors\n\n Software Asset Management Professional | \\[Accessibility\\] - License Usage - Focus is missing after clicking Run Reconciliation button from pop-up                                                                                                 |\n| [KB1274921](\/kb_view.do?sysparm_article=KB1274921) | PRB1504738 | Email                                  | Notification Backend - Duplicate push notification issue                                                                                                                                                  |\n| [KB0966640](\/kb_view.do?sysparm_article=KB0966640) | PRB1507712 | Agent Workspace                        | The calendar picker is picking invalid dates for today  yesterday  and all calendar left menu options.                                                                                                    |\n| [KB1005139](\/kb_view.do?sysparm_article=KB1005139) | PRB1539267 | PDF Generation                         | Korean letters are getting garbled when the PDF file is generated from a report                                                                                                                           |\n| [KB1005406](\/kb_view.do?sysparm_article=KB1005406) | PRB1541411 | Lists                                  | \\[Workspace WHC\\] Highlighted \/ selected \/ hover pages are indistinguishable in list pagination                                                                                                           |\n| [KB1166040](\/kb_view.do?sysparm_article=KB1166040) | PRB1572894 | Enterprise Portal                      | EVAM data source bypasses Query Business Rules on Project Workspace                                                                                                                                       |\n| [KB1123503](\/kb_view.do?sysparm_article=KB1123503) | PRB1573218 | Service Mapping                        | Inefficient GlideRecord query for 'sm_ci_field_data' from 'CiFieldDataDaoImpl' (function \"_size\") through 'CiChangeFlapperHandler' results in a warning as 'Table handling an extremely large result set' |\n| [KB1117843](\/kb_view.do?sysparm_article=KB1117843) | PRB1573869 | Software Asset Management Professional | Long reconciliation time due to SuiteEngine performance issues                                                                                                                                            |\n| [KB1124263](\/kb_view.do?sysparm_article=KB1124263) | PRB1589646 | UX Framework                           | Now Experience UI properties are not resolved for viewport sub-pages                                                                                                                                      |\n| [KB1198693](\/kb_view.do?sysparm_article=KB1198693) | PRB1601193 | Authentication                         | \\[Tokyo Partner Testing\\] Cannot register a new user account on Login page (Polaris view)                                                                                                                 |\n| [KB1501269](\/kb_view.do?sysparm_article=KB1501269) | PRB1609636 | UI Policy\/Client Script                | \\[UUU-Runtime-Performance\\]: \/api\/now\/ui\/polaris\/menu API call takes a long time to execute on first load if \"All\" menu is pinned and several application menus are expanded                              |\n| [KB1220333](\/kb_view.do?sysparm_article=KB1220333) | PRB1634193 |                                        | In Tokyo  after using 'ClearValue' in a catalog client script on Service Portal  the value of a checkbox a returns a blank string                                                                         |\n| [KB1273808](\/kb_view.do?sysparm_article=KB1273808) | PRB1641048 | Assessments                            | Field level ACL on core_company table blocks other apps from access                                                                                                                                       |\n| [KB1340471](\/kb_view.do?sysparm_article=KB1340471) | PRB1661584 |                                        | Non-admin Users are not triggering notifications for following records                                                                                                                                    |\n| [KB1448786](\/kb_view.do?sysparm_article=KB1448786) | PRB1679796 |                                        | 'Discard draft' button is not enabled for 'catalog_builder_editor' and 'catalog_admin' personas                                                                                                           |\n| [KB1514475](\/kb_view.do?sysparm_article=KB1514475) | PRB1685124 |                                        | Activity Stream Mention (@user) does not work in comments and work notes on Workspace                                                                                                                     |\n| [KB1517595](\/kb_view.do?sysparm_article=KB1517595) | PRB1694894 |                                        | Non-admin users are not able to add new notification devices                                                                                                                                              |\n| [KB1549188](\/kb_view.do?sysparm_article=KB1549188) | PRB1699994 |                                        | Playbook usage of DomainSupport.switchDomain doesn't use the closeable properly  which causes domain separation issues                                                                                    |\n| [KB1581512](\/kb_view.do?sysparm_article=KB1581512) | PRB1707643 |                                        | Getting error when attempting to activate a subflow inside the studio                                                                                                                                     |\n| [KB1556755](\/kb_view.do?sysparm_article=KB1556755) | PRB1708634 |                                        | MID Server Script Files get skipped in instance\/app upgrades  after Checksum values are automatically added                                                                                               |\n| [KB1584589](\/kb_view.do?sysparm_article=KB1584589) | PRB1724837 |                                        | NetApp Storage Cluster-Mode fails to bring Storage Volumes due to the following pattern step failing: 105 Match volume controllers for each node.                                                         |\n| [KB0598187](\/kb_view.do?sysparm_article=KB0598187) | PRB668806  | Project Portfolio Management           | Cannot list edit related list if the Related List Loading option 'After Form Loads' or 'On-demand' is selected                                                                                            |\n\n\n## Vancouver Patch 2 Hot Fix 1a: Known Errors\n\n Client Scripts do not save when using UIB in Firefox                                                                                                                                                      |\n| [KB0966596](\/kb_view.do?sysparm_article=KB0966596) | PRB1485591 | Lists                                  | Simple - List component is not available in the Dashboard Builder                                                                                                                                         |\n| [KB0966598](\/kb_view.do?sysparm_article=KB0966598) | PRB1492041 | Mobile Platform                        | Mobile Task button actions on WOT form page do not refresh the form once the Task has been accepted\/rejected                                                                                              |\n| [KB1123826](\/kb_view.do?sysparm_article=KB1123826) | PRB1501129 | Software Asset Management Professional | \\[Accessibility\\] - License Usage - Focus is missing after clicking Run Reconciliation button from pop-up                                                                                                 |\n| [KB1274921](\/kb_view.do?sysparm_article=KB1274921) | PRB1504738 | Email                                  | Notification Backend - Duplicate push notification issue                                                                                                                                                  |\n| [KB0966640](\/kb_view.do?sysparm_article=KB0966640) | PRB1507712 | Agent Workspace                        | The calendar picker is picking invalid dates for today  yesterday  and all calendar left menu options.                                                                                                    |\n| [KB1005139](\/kb_view.do?sysparm_article=KB1005139) | PRB1539267 | PDF Generation                         | Korean letters are getting garbled when the PDF file is generated from a report                                                                                                                           |\n| [KB1005406](\/kb_view.do?sysparm_article=KB1005406) | PRB1541411 | Lists                                  | \\[Workspace WHC\\] Highlighted \/ selected \/ hover pages are indistinguishable in list pagination                                                                                                           |\n| [KB1166040](\/kb_view.do?sysparm_article=KB1166040) | PRB1572894 | Enterprise Portal                      | EVAM data source bypasses Query Business Rules on Project Workspace                                                                                                                                       |\n| [KB1123503](\/kb_view.do?sysparm_article=KB1123503) | PRB1573218 | Service Mapping                        | Inefficient GlideRecord query for 'sm_ci_field_data' from 'CiFieldDataDaoImpl' (function \"_size\") through 'CiChangeFlapperHandler' results in a warning as 'Table handling an extremely large result set' |\n| [KB1117843](\/kb_view.do?sysparm_article=KB1117843) | PRB1573869 | Software Asset Management Professional | Long reconciliation time due to SuiteEngine performance issues                                                                                                                                            |\n| [KB1124263](\/kb_view.do?sysparm_article=KB1124263) | PRB1589646 | UX Framework                           | Now Experience UI properties are not resolved for viewport sub-pages                                                                                                                                      |\n| [KB1198693](\/kb_view.do?sysparm_article=KB1198693) | PRB1601193 | Authentication                         | \\[Tokyo Partner Testing\\] Cannot register a new user account on Login page (Polaris view)                                                                                                                 |\n| [KB1501269](\/kb_view.do?sysparm_article=KB1501269) | PRB1609636 | UI Policy\/Client Script                | \\[UUU-Runtime-Performance\\]: \/api\/now\/ui\/polaris\/menu API call takes a long time to execute on first load if \"All\" menu is pinned and several application menus are expanded                              |\n| [KB1220333](\/kb_view.do?sysparm_article=KB1220333) | PRB1634193 |                                        | In Tokyo  after using 'ClearValue' in a catalog client script on Service Portal  the value of a checkbox a returns a blank string                                                                         |\n| [KB1523063](\/kb_view.do?sysparm_article=KB1523063) | PRB1640897 |                                        | Translation prefixes are missing for Workspace menu                                                                                                                                                       |\n| [KB1273808](\/kb_view.do?sysparm_article=KB1273808) | PRB1641048 | Assessments                            | Field level ACL on core_company table blocks other apps from access                                                                                                                                       |\n| [KB1340471](\/kb_view.do?sysparm_article=KB1340471) | PRB1661584 |                                        | Non-admin Users are not triggering notifications for following records                                                                                                                                    |\n| [KB1362321](\/kb_view.do?sysparm_article=KB1362321) | PRB1671906 |                                        | CMDB Data Management Task required approval even when with policy having \"Needs review\" is false                                                                                                          |\n| [KB1448786](\/kb_view.do?sysparm_article=KB1448786) | PRB1679796 |                                        | 'Discard draft' button is not enabled for 'catalog_builder_editor' and 'catalog_admin' personas                                                                                                           |\n| [KB1514475](\/kb_view.do?sysparm_article=KB1514475) | PRB1685124 |                                        | Activity Stream Mention (@user) does not work in comments and work notes on Workspace                                                                                                                     |\n| [KB1564044](\/kb_view.do?sysparm_article=KB1564044) | PRB1689640 |                                        | HR Case \"Description\" \\[description\\] contains escape code for special characters                                                                                                                         |\n| [KB1516906](\/kb_view.do?sysparm_article=KB1516906) | PRB1693549 |                                        | CMDB Data Manager policy executions are stuck in Vancouver and 'cmdb_data_management_task' table has no 'has_manual_process' column                                                                       |\n| [KB1517595](\/kb_view.do?sysparm_article=KB1517595) | PRB1694894 |                                        | Non-admin users are not able to add new notification devices                                                                                                                                              |\n| [KB1533141](\/kb_view.do?sysparm_article=KB1533141) | PRB1696950 |                                        | Flow Designer Script Editor auto completion\/suggestions no longer working in flow design scripting Flow Designer Script Editor ( fd_data  Dot walking )                                                   |\n| [KB1531357](\/kb_view.do?sysparm_article=KB1531357) | PRB1698258 |                                        | Requests in Core UI with Polaris ON are resulting in pages hanging when urls are opened in a new tab                                                                                                      |\n| [KB1549188](\/kb_view.do?sysparm_article=KB1549188) | PRB1699994 |                                        | Playbook usage of DomainSupport.switchDomain doesn't use the closeable properly  which causes domain separation issues                                                                                    |\n| [KB1570742](\/kb_view.do?sysparm_article=KB1570742) | PRB1701019 |                                        | Location header redaction causing IH flows to fail                                                                                                                                                        |\n| [KB1532483](\/kb_view.do?sysparm_article=KB1532483) | PRB1701867 |                                        | Node log file browser transaction (ui_page_process.do) cannot terminate resulting in frequent garbage collection                                                                                          |\n| [KB1582083](\/kb_view.do?sysparm_article=KB1582083) | PRB1703666 |                                        | RLQuery join conditions being appended to last join can produce incorrect result set.                                                                                                                     |\n| [KB1556696](\/kb_view.do?sysparm_article=KB1556696) | PRB1705462 |                                        | After Vancouver upgrade  emails are being sent but remain in the Outbox                                                                                                                                   |\n| [KB1581512](\/kb_view.do?sysparm_article=KB1581512) | PRB1707643 |                                        | Getting error when attempting to activate a subflow inside the studio                                                                                                                                     |\n| [KB1556755](\/kb_view.do?sysparm_article=KB1556755) | PRB1708634 |                                        | MID Server Script Files get skipped in instance\/app upgrades  after Checksum values are automatically added                                                                                               |\n| [KB1570200](\/kb_view.do?sysparm_article=KB1570200) | PRB1712246 |                                        | Microsoft reconciliation Fails or Stalls Post-Upgrade to Vancouver                                                                                                                                        |\n\"\n\n",
        "QUERY": " 'ui policies are not working on the service portal'",
        "GROUND_TRUTH": "The issue of UI policies not working on the Service Portal, particularly after the Rome upgrade, occurs when the variable internal name does not match the one in the Catalog UI policy action. This mismatch can occur in two scenarios: a mismatch between the item_option_new (variable) record's name and the `variable` field on the catalog_ui_policy_action record, or a mismatch between the item_option_new_set (variable set) record's `internal_name` and `variable` field on the catalog_ui_policy_action record.\n\nThere are two possible workarounds for this issue:\n\n1. You can create and set the `glide.sc.ui_policy.use_cache` property value to `false`. This will revert to the pre-Rome behavior where the system does not use cache for fetching UI policies. However, if there are many UI policies, there might be some performance impact.\n\n2. You can correct (sync) the `variable` field on the `catalog_ui_policy_action` record to match with the `name`\/`internal_name` of the corresponding Variable\/Variable Set. The article provides a script to perform this synchronization. Replace `:itemId` with the sys_id of the catalog item where UI policy issues are found.\n\nIf you choose the first workaround, remember to delete the property once you are in a version where the fix is available."
    },
    {
        "id": 90,
        "CONTEXT": "\"## SAMP Reconciliation fails with error 'TypeError: Cannot read property \"install_condition\" from null'\n\n## Description\n\n* The SAMP reconciliation fails after 100% progress due to a race condition when the Import User Subscriptions job is also running at the same time.\n* The progress summary shows:\n\nTypeError: Cannot read property \"install_condition\" from null\n\nat sys_script_include.444d8294c32222006081face81d3aebf.script:514 (anonymous)\nat sys_script_include.444d8294c32222006081face81d3aebf.script:266 (anonymous)\nat sys_script_include.444d8294c32222006081face81d3aebf.script:247 (anonymous)\nat sys_script_include.30bbdf9587f52300923aa75fe5cb0b97.script:413 (anonymous)\nat sys_script_include.30bbdf9587f52300923aa75fe5cb0b97.script:400 (anonymous)\nat sys_script_include.6761b0dd0b1232001a17650d37673a77.script:868 (anonymous)\nat sys_script_include.6761b0dd0b1232001a17650d37673a77.script:824 (anonymous)\nat sys_script_include.6761b0dd0b1232001a17650d37673a77.script:54 (anonymous)\nat sys_trigger.f7006c6e471e59d0e1ce8a12736d4378:1\n\nPossibly related problems:\n[Reconciliation errors in the form 'Cannot read property \\<someSysId\\> from undefined' caused by SamNewPerCoreForMicrosoftLicenseCalculator loading all optimization records and Per Core VM allocations missing relationships](https:\/\/support.servicenow.com\/kb_view.do?sysparm_article=KB1227294 \"Reconciliation errors in the form 'Cannot read property <someSysId> from undefined' caused by SamNewPerCoreForMicrosoftLicenseCalculator loading all optimization records and Per Core VM allocations missing relationships\")\n[Workday reconciliation is failing if subscriptions are pulled from SSO or custom integration profile](https:\/\/support.servicenow.com\/kb_view.do?sysparm_article=KB1316426 \"Workday reconciliation is failing if subscriptions are pulled from SSO or custom integration profile\")\n\n## Steps to Reproduce\n\n1. Schedule the Reconciliation job to run at the same time as the SAM - Import User Subscriptions job.  \n   2. Check Recon Results for recon with status 'Failed' and Progress summary 'Reconciliation Complete'.  \n   3. Check the progress summary for the TypeError: cannot read property 'install_condition' from null.\n\n## Workaround\n\n- Use Case 1 -  \nMake sure the Import User Subscription and Software Reconciliation jobs are never scheduled to run at the same time.\n- Use Case 2 -\n\nIf you have verified both Import User Subscription and Software Reconciliation are running at different times  then consider changing the \"**SamAllocationSuiteEngine** \" Script Include ( \/sys_script_include.do?sys_id=444d8294c32222006081face81d3aebf ) to add the code below line at 435  so that the final code looks like this:\n\ngetSubscriptionSuitesOrComponents: function(publisherId  getSuites) {\n\/\/ First get all the licensable software models of this publisher\nvar subscriptionModels = \\[\\];\nvar subscriptionGa = new SampAggregate('samp_sw_subscription');\nsubscriptionGa.addQuery('software_model.manufacturer'  publisherId);\nsubscriptionGa.addNotNullQuery('user');\nsubscriptionGa.addNotNullQuery('licensable_software_model'); --- We are filtering record where licensable_software_model is empty.\nsubscriptionGa.groupBy('licensable_software_model');\nsubscriptionGa.query();\n\nNOTE: The reconciliation job processes the valid record where you already have a valid licensable_software_model value and show the data. This same job also populates the licensable_software_model value for the records where licensable_software_model is empty. Thus the records skipped in the current run will be automatically processed in the next run.\n\nif you see any record with licensable_software_model as empty and publisher as empty  then those records will not be processed. Those do not have any direct impact on the job execution. This needs to be investigated separately.\n\n**Related Problem: PRB1610760**\n\n\n\n## [Vancouver] Software Asset Management Professional (com.snc.samp) Microsoft Reconciliation Process F\n\n## Description\n\nPost-upgrade to the Vancouver release  some instances with the **Software Asset Management Professional** (com.snc.samp) plugin have experienced failures or stalling of the reconciliation process (recon stuck in progress). When this occurs  the following error messages may appear in the 'Reconciliation Progress Summary ? Reconciliation Progress Details' related list:\n\n1. TypeError: Cannot read property \"sys_id\" from undefined. Please review the stack trace error line  ensuring that the failure involves either the \"hostsToInstallTypes\" or \"vmsToInstallTypes\" object.  \n2. Logs reveal that the 'SAM - Software License Reconciliation Worker' is stuck. The stack trace suggests issues with the script include 'SamNewPerCoreWithCalForMicrosoftLicenseCalculator'.  \n3. JavaException: java.lang.StackOverflowError.  \n\nThis reconciliation failure is specifically linked to \"Microsoft Per-Core\" and \"Microsoft Per-Core (with CAL)\" license metrics.\n\n## Steps to Reproduce\n\nOn a Vancouver instance with \"Microsoft Per-Core\" and\/or \"Microsoft Per-Core (with CAL)\" entitlements  run reconciliation for Microsoft publisher and observe the symptom(s) described in the previous section.\n\n## Workaround\n\nThis problem has been fixed. If you are able to upgrade  review the \"Fixed In\" section to determine the latest version with a permanent fix that your instance can be upgraded to.\n\nIf you are encountering this issue on an affected version of Vancouver  but you are unable to upgrade to a fixed version at this time  kill the ongoing \"SAM - Software License Reconciliation Worker\"  then apply the update set attached to this Known Error article. Ensure that all related script includes are marked with \"replace_on_upgrade = true\" (in the sys_update_xml table) after committing the update set. This will prevent future conflicts with OOB script includes when upgrading to a fixed version.\n\n**FAQ**\n\n**Q:** How can I confirm if my instance is impacted by this issue?\n**A:** If you have the com.snc.samp plugin installed  and you have \"Microsoft per Core\" or \"Microsoft per Core (with CAL)\" entitlements  you can experience this issue when upgrading to an impacted version of Vancouver.\n\n**Q:** Will this affect non-prod instances  prod instances  or both?\n**A:** Both production and sub-production instances can be affected by this issue.\n\n**Q:** Can I implement the workaround myself?\n**A:** Yes. If you are on an impacted version of Vancouver and are experiencing this issue  please follow the steps detailed above and apply the update set attached to this article.\n\n**Q:** Once the workaround has been applied  do I need to do anything else?\n**A:** Once you've applied the update set  we recommend setting \"Replace on Upgrade\" to \"True\" for related script includes to prevent future conflicts when upgrading to a fixed version.\n\n**Q:** Will the solution cause any downtime?\n**A:** Implementing the workaround will not result in any downtime.\n\n**Q:** Which version(s) will contain the fix for this issue?\n**A:** This issue is fixed in Vancouver Patch 4 and above.\n\n**Related Problem: PRB1712246**\n\n\n\n## Reconciliation still fails for Microsoft publisher with the error ' TypeError: Cannot read property \n\n## Description\n\nAllocating VM that is missing a host relationship on a Microsoft Per Core license will cause reconciliation to fail with an error in the form \"Cannot read property \\<property name\\> from undefined\".\n\n## Steps to Reproduce\n\non a Tokyo Patch 7  or utah patch 1 create a vm without host and assign allocations to the its SQL Server license Per Core.\n\nrecon fails for SQL server product with stack trace:\n\nat sys_script_include.1e5ac217b10e0110fa9bf03fa4dd6856.script:1729 (anonymous)\nat sys_script_include.1e5ac217b10e0110fa9bf03fa4dd6856.script:1695 (anonymous)\nat sys_script_include.1e5ac217b10e0110fa9bf03fa4dd6856.script:2177 (anonymous)\nat sys_script_include.1e5ac217b10e0110fa9bf03fa4dd6856.script:2136 (anonymous)\nat sys_script_include.1e5ac217b10e0110fa9bf03fa4dd6856.script:189 (anonymous)\nat sys_script_include.602e129eb0276300fa9b028ca0d3b864.script:43 (anonymous)\nat sys_script_include.8a6dbe2887522300ede6f64936cb0b2c.script:113 (anonymous)\nat sys_script_include.8a6dbe2887522300ede6f64936cb0b2c.script:240 (anonymous)\nat sys_script_include.8a6dbe2887522300ede6f64936cb0b2c.script:114 (anonymous)\nat sys_script_include.8a6dbe2887522300ede6f64936cb0b2c.script:102 (anonymous)\nat sys_script_include.8a6dbe2887522300ede6f64936cb0b2c.script:94 (anonymous)\nat sys_script_include.74ed3a7d8dfed010fa9b4295b8773c71.script:25\nat sys_script_include.8a6dbe2887522300ede6f64936cb0b2c.script:36 (anonymous)\nat sys_script_include.30bbdf9587f52300923aa75fe5cb0b97.script:425 (anonymous)\nat sys_script_include.6761b0dd0b1232001a17650d37673a77.script:896 (anonymous)\nat sys_script_include.6761b0dd0b1232001a17650d37673a77.script:836 (anonymous)\nat sys_script_include.6761b0dd0b1232001a17650d37673a77.script:57 (anonymous)\nat sys_trigger.03ee762b975eedd0fbb979971153af9e:1\n\nit is a discovery issue since no host was stamped for this vm (type Virtualizes By missing in cmdb_rel_ci  where child = host  parent = vm)\n\nas a workaround  either customer needs to delete those allocations  or modify code in SamNewPerCoreForMicrosoftLicenseCalculator (sys_script_include_1e5ac217b10e0110fa9bf03fa4dd6856.xml) with\n\n_onlyUnique: function(value  index  self) {\n\nreturn self.indexOf(value) === index \\&\\& !gs.nil(value); \/\/ ADDED part from '\\&\\&'\n} \n\n## Workaround\n\nUpgrade to Utah Patch 5 or Tokyo Patch 10  backports were made as part of [BAK0071941](https:\/\/buildtools1.service-now.com\/x_snc_defect_backport.do?sys_id=00f3266d93eaa9d0b8d9b0627cba1015&sysparm_view=Scrum&sysparm_record_target=x_snc_defect_backport&sysparm_record_row=1&sysparm_record_rows=2&sysparm_record_list=parent_defect%3D5dde2900dbf8a114c7304b7c13961934%5EORDERBYDESCsys_updated_on) \/ [BAK0071942](https:\/\/buildtools1.service-now.com\/x_snc_defect_backport.do?sys_id=f4b4e2e1932ea9d0b8d9b0627cba10c7&sysparm_view=Scrum&sysparm_record_target=x_snc_defect_backport&sysparm_record_row=2&sysparm_record_rows=2&sysparm_record_list=parent_defect%3D5dde2900dbf8a114c7304b7c13961934%5EORDERBYDESCsys_updated_on).\n\n**Related Problem: PRB1665109**\n\n\"\n\n",
        "QUERY": " 'typeerror: cannot read property \"install_condition\" from null software reconciliation'",
        "GROUND_TRUTH": "The error 'TypeError: Cannot read property \"install_condition\" from null' occurs during the SAMP reconciliation process when the Import User Subscriptions job is running simultaneously. This is due to a race condition. To reproduce this error, you need to schedule the Reconciliation job to run at the same time as the SAM - Import User Subscriptions job. Then, check the Recon Results for recon with status 'Failed' and Progress summary 'Reconciliation Complete'. Finally, check the progress summary for the TypeError.\n\nThere are two workarounds suggested in the article. The first one is to ensure that the Import User Subscription and Software Reconciliation jobs are never scheduled to run at the same time. The second workaround involves modifying the \"SamAllocationSuiteEngine\" Script Include to add a specific code below line at 435. This code filters out records where the licensable_software_model is empty, allowing the reconciliation job to process valid records and populate the licensable_software_model value for the records where it is empty. These records will then be processed in the next run."
    },
    {
        "id": 91,
        "CONTEXT": "\"## Troubleshooting an unavailable instance or monitoring an outage\n\nTroubleshooting an unavailable instance or monitoring an outage\n\nSymptoms\n\n* Unable to connect to the instance.\n* Low or out of memory alerts display.\n* Instance is slow or sometimes unavailable.\n\nCause\n\n* Network issues exist between the instance and the user.\n* A significant change occurred.\n* Scripts created an inefficient query in the database or are running full table scans on large tables.\n* Nested queries exist in MySQL server. Many nested queries are capable of bringing the MySQL database to a halt under certain data conditions.\n* The Java virtual machine (JVM) memory utilization level is high or memory heap is not big enough to store all data.\n* A large number of calls are made to the database  which causes high disk input\/output (I\/O) on the database server.\n* The central processing unit (CPU) load on the server is too high.\n* There is high transaction concurrency.\n\nResolution\n\nThis step-by-step article describes how to diagnose and troubleshoot common performance issues related to the ServiceNow instance. Troubleshooting performance issues involve the use of a series of steps to isolate and determine the cause in order to take corrective actions as necessary. In a highly technological and innovative environment  it is not uncommon to experience occasional performance issues. Therefor  it is important to learn how to proactively prevent or minimize the occurrence of problems and  when they occur  diagnose the cause and take the appropriate corrective actions to quickly resolve the issue.\n\nOutages or downtime refers to the time span when the instance fails to provide its primary function. Eliminating outages poses a significant challenge for IT organizations that need to upgrade or migrate environments running ServiceNow. This is particularly true for applications that must provide continuous or near-continuous operations to users who increasingly expect uninterrupted availability of online services. Any outage of an application or website  even if that outage is scheduled or planned  has an impact on the revenue and reputation of our customers. It is imperative that Service Now proactively troubleshooting customer related outages to ensure minimal downtime and faster solutions.\n\nThe purpose of this article is to determine the source and resolve a performance issue and is not intended to diagnose performance issues related to all databases on a particular server. The steps are ordered in the most appropriate sequence to isolate the issue and identify the proper resolution. Please do not skip a step.\n\n|--------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| ![Prerequisite](\/sys_attachment.do?sys_id=92f66a611b200990b09633f2cd4bcba0 \"Prerequisite\") | **Prerequisite** : Prior to completing the following steps  collecting the necessary information needed to troubleshoot the performance issue is high recommended. For more information  see [Gathering node data via stats.do and threads.do](\/kb_view.do?sysparm_article=KB0517269 \"Gathering node data via stats.do and threads.do\"). |\n**To troubleshoot slow performance experienced by all users on a all application:**\n\n1. Test the network connectivity to the instance and verify there are no existing issues. For more information  see [Managing network connectivity](\/kb_view.do?sysparm_article=KB0517267 \"Managing network conectivity\").\n1. Verify if a node needs to be restarted. For more information  see [Identifying if a node needs to be restarted](\/kb_view.do?sysparm_article=KB0517327 \"Identifying if a node needs to be restarted\").\n1. Verify there are no inefficient queries running against the database. For more information  see [Identifying an tuning inefficient queries executing in the database (MySQL)](\/kb_view.do?sysparm_article=KB0517270 \"Identifying an tuning inefficient queries executing in the database (MySQL)\").\n1. Verify the instance is not running out of JVM memory. For more information  see [Identifying abnormal JVM memory utilization](\/kb_view.do?sysparm_article=KB0517274 \"Identifying abnormal JVM memory utilization\").\n1. Verify the disk I\/O does not indicate overuse or high levels of saturation. For more information  see [Identifying high disk I\/O on the database server](\/kb_view.do?sysparm_article=KB0517275 \"Identifying high disk I\/O on the database server\").\n1. Verify there are sufficient CPU resources to satisfy demand. For more information  see [Identifying high CPU utilization on the server](\/kb_view.do?sysparm_article=KB0517276 \"Identifying high CPU utilization on the server\").\n1. Verify the instance is not experiencing high concurrency. For more information  see [Identifying high transaction concurrency](\/kb_view.do?sysparm_article=KB0517277 \"Identifying high transaction concurrency\").   \n\nAnother Cause\n\n* Sometimes the sys_properties table contains the wrong name after clone or rename.\n\nResolution\n\nCheck if this is the case and update it accordingly.\n\nmysql\\> SELECT value FROM sys_properties WHERE name=\"instance_name\";\nmysql\\> SELECT value FROM sys_properties WHERE name=\"instance_id\";\n\nRun following commands to update 'instance_name' and 'instance_id' (You can get instance_id from datacenter)\n\nmysql\\> UPDATE sys_properties SET value='**\\<instance id\\>**' WHERE name=\"instance_id\";\nmysql\\> UPDATE sys_properties SET value='**\\<instance name\\>**' WHERE name='instance_name';\n\n\n\n## Agent Chat & Advanced Work Assignment Troubleshooting Guide\n\n\\[READ FIRST\\] For both Advanced Work Assignment and Agent Chat  potential issues are organized by scenario. Each scenario lists a series of steps you should follow and references KB articles that goes into more detail on the debugging step. If you have any feedback on improving this guide  message Meshach Adoe on Teams.\n\n## KEY ARTICLES\n\n*These documents are referenced throughout the article -- use this section to get easy access to key documents.*\n\n1. Guides to enable enhanced logging (see *FINAL STEPS* section for details)\n   1. ([DOCS](https:\/\/docs.servicenow.com\/bundle\/vancouver-servicenow-platform\/page\/administer\/advanced-work-assignment\/task\/awa-activate-logging.html)) Enable logging via `sys_logger_configuration`.\n   2. ([KB0786263](https:\/\/support.servicenow.com\/kb?id=kb_article_view&sysparm_article=KB0786263)) Enable AWA Event logging in `awa_service_channel`.\n   3. ([KB1209787](https:\/\/support.servicenow.com\/kb?id=kb_article_view&sysparm_article=KB1209787)) Enable client logging via `sys_properties` and debugging using TamperMonkey. If the instance is on or after TP10 or UP5  some client logging is already built in the instance and TamperMonkey might not be necessary.\n2. ([KB1220747](https:\/\/support.servicenow.com\/kb?id=kb_article_view&sysparm_article=KB1220747)) Splunk Queries for troubleshooting issues in Advanced Work Assignment and\/or Agent Chat.\n3. ([KB0655923](https:\/\/support.servicenow.com\/kb?id=kb_article_view&sysparm_article=KB0655923)\/[KB0994933](https:\/\/support.servicenow.com\/kb?id=kb_article_view&sysparm_article=KB0994933)) Capturing network traffic through a HAR file.\n4. ([KB1119870](https:\/\/support.servicenow.com\/kb?id=kb_article_view&sysparm_article=KB1119870)) Running the AWA health script.\n5. ([KB0998370](https:\/\/support.servicenow.com\/kb?id=kb_article_view&sysparm_article=KB0998370)) Diagnosing intermittent issues.\n\n## PREREQUISITES\n\n*If you have not worked with Agent Chat or AWA before  we recommend going through these guides to familiarize yourself with the terms and concepts referenced throughout this article.*\n\n1. [Basic Overview Training](https:\/\/trainingops.servicenow.com\/detail\/video\/6259348736001\/technical-support:-collaboration---advanced-work-assignments-basics?q=advanced%20work%20assignment) (40 mins)\n2. [(KB0956028) Understanding Advanced Work Assignment](https:\/\/support.servicenow.com\/kb?id=kb_article_view&sysparm_article=KB0956028)\n3. [(KB0955248) AWA Routing -- Order of Evaluation](https:\/\/support.servicenow.com\/kb?id=kb_article_view&sysparm_article=KB0955248)\n4. [Agent Chat \\& AWA Workshop Playlist](https:\/\/www.youtube.com\/playlist?list=PLkGSnjw5y2U6Xw39CzihKHQ_atSFOoAns)\n5. [Agent Chat FAQs](https:\/\/www.servicenow.com\/community\/agent-chat-routing-and-sidebar\/agent-chat-faq-s\/ta-p\/2306678)\n\n## PRELIMINARY CHECKS\n\n*Before troubleshooting for the specific issue  ensure to run these preliminary checks and act upon any recommendations.*\n\n1. Check instance vitals like CPU  Memory and Semaphores. If any issues are identified  consult the SWAT team under GCS Cloud Operations (led by Gurnish Anand).\n2. Run the AWA health script ([KB1119870](https:\/\/support.servicenow.com\/kb?id=kb_article_view&sysparm_article=KB1119870)). Act upon any warnings from the script and include the results when creating the case task.\n3. Run relevant Splunk dashboards and take note of any issues.\n   1. [Agent Chat AWA -- Health and Monitoring](https:\/\/splunk.servicenow.net\/en-US\/app\/search\/agent_chat_awa__health_and_monitoring)\n   2. [AWA -- Health of Core Platform Capabilities](https:\/\/splunk.servicenow.net\/en-US\/app\/search\/dashboard_awa): displays issues on core platform capabilities that AWA relies on.\n   3. [AgentChatAWA Interaction Lifecycle](https:\/\/splunk.servicenow.net\/en-US\/app\/search\/farza_test): run this dashboard if the issue occurs for a specific interaction.\n\n## AGENT CHAT\n\n### SCENARIO 1: Work items do not show up in the inbox OR Agent hears audio notification  but no work item appears OR Agent clicks on accept but sees no response OR Work item disappears before timer finishes.\n\n1. Check for any AWA failures under the \"Health - AWA Failures\" section while running the Splunk [Agent Chat AWA -- Health and Monitoring](https:\/\/servicenow-my.sharepoint.com\/personal\/meshach_adoe_servicenow_com1\/Documents\/164871) dashboard. If you see any  please create a case task for Dev-AgentChatAWA team.\n2. Check for any AMB or Record Watcher errors or performance issues.\n   1. Check if the core platform record watcher could be rejecting changes. Check the \"Health - Platform Record Watcher Rejections\" AND \"Monitoring - Top tables rejections\" sections while running the Splunk [Agent Chat AWA -- Health and Monitoring](https:\/\/servicenow-my.sharepoint.com\/personal\/meshach_adoe_servicenow_com1\/Documents\/164871) dashboard. If you see any errors show up  please create a case task for Dev-Platform AMB\\&RW and respective team who owns the tables in \"Monitoring - Top tables rejections\".\n   2. Check if AMB Performance and\/or instance performance may be degraded. Run the Splunk [Agent Chat AWA -- Health and Monitoring](https:\/\/splunk.servicenow.net\/en-US\/app\/search\/agent_chat_awa__health_and_monitoring)dashboard and check the \"Health - AMB Slowness for chats\" AND \"Monitoring - Instance Performance\" sections. If you notice delays more than 10 seconds on a consistent basis  please create a case task for CS-Performance team.\n   3. Check for any AMB errors or Record Watcher Rejections while running the Splunk [AWA -- Health of Core Platform Capabilities](https:\/\/splunk.servicenow.net\/en-US\/app\/search\/dashboard_awa) dashboard. If you see any errors show up  please create a case task for Dev-Platform AMB\\&RW.\n   4. Check the xmlstats page for statistics on AMB performance. `https:\/\/<instance_name>.service-now.com\/xmlstats.do?include=amb`\n      1. To help debug AMB errors  it might be useful to enable additional logging. This can be done by setting the sys_user_preference record `glide.amb.client.log.level=debug` (leaving user blank) and sys_properties `glide.amb.log.level=debug`.\n   5. Run more specific Splunk queries to trace down any patterns or sequence of events causing errors (see *[KB1220747](https:\/\/support.servicenow.com\/kb?id=kb_article_view&sysparm_article=KB1220747) - SECTION* *C* ).\n      1. Investigate whether any of the following issues might be occurring: check for threads marked as EXCESSIVE  check if AMB events got processed properly  check for any errors in WorkItem and Workload responders.\n3. ([KB1170427](https:\/\/support.servicenow.com\/kb?id=kb_article_view&sysparm_article=KB1170427)) Capture web session traffic and any network issues on the agent's client via a HAR file.\n   1. ([KB1156577](https:\/\/support.servicenow.com\/kb?id=kb_article_view&sysparm_article=KB1156577)) Check if the agent's client did receive the AMB message that carries the interaction offer information.\n   2. \n\nCheck the \"Health - Platform Record Watcher Rejections\" AND \"Monitoring - Top tables rejections\" sections while running the Splunk [Agent Chat AWA -- Health and Monitoring](https:\/\/servicenow-my.sharepoint.com\/personal\/meshach_adoe_servicenow_com1\/Documents\/164871) dashboard. If you see any errors show up  please create a case task for Dev-Platform AMB\\&RW and respective team who owns the tables in \"Monitoring - Top tables rejections\".\n   2. Check for any slowness in AMB performance. Run the Splunk [Agent Chat AWA -- Health and Monitoring](https:\/\/splunk.servicenow.net\/en-US\/app\/search\/agent_chat_awa__health_and_monitoring)dashboard and check the \"Health - AMB Slowness for chats\" AND \"Monitoring - Instance Performance\" sections. If you notice delays more than 10 seconds on a consistent basis  please create a case task for CS-Performance team.\n   3. Check for any AMB errors or Record Watcher Rejections while running the Splunk [AWA -- Health of Core Platform Capabilities](https:\/\/splunk.servicenow.net\/en-US\/app\/search\/dashboard_awa) dashboard. If you see any errors show up  please create a case task for Dev-Platform AMB\\&RW.\n   4. Check the xmlstats page for statistics on AMB performance: `https:\/\/<instance_name>.service-now.com\/xmlstats.do?include=amb`\n      1. To help debug AMB errors  it might be useful to enable additional logging. This can be done by setting the `sys_user_preference` record `glide.amb.client.log.level=debug` (leaving user blank) and `sys_properties` `glide.amb.log.level=debug`.\n   5. Run more specific Splunk queries to trace down any patterns or sequence of events causing errors (see *[KB1220747](https:\/\/support.servicenow.com\/kb?id=kb_article_view&sysparm_article=KB1220747) - SECTION* *C* ).\n      1. Investigate whether the following issues might be occurring: check for threads marked as EXCESSIVE  check if AMB events got processed properly  check for any errors in WorkItem and Workload responders.\n6. ([KB1170427](https:\/\/support.servicenow.com\/kb?id=kb_article_view&sysparm_article=KB1170427)) Capture web session traffic and any network issues on the agent's client via a HAR file.\n   1. ([KB1156577](https:\/\/support.servicenow.com\/kb?id=kb_article_view&sysparm_article=KB1156577)) Check if the agent's client did receive the AMB message that carries the interaction offer information.\n   2. If capturing via HAR file is not possible or if the customer prefers not to do so  an alternative way is in step 7.\n7. ([KB1209787](https:\/\/support.servicenow.com\/kb?id=kb_article_view&sysparm_article=KB1209787)) Enable client-side logging for Agent Chat by setting the system property `com.glide.awa.client_logging.enabled` as true.\n   1. If the instance is on or before TP10 or UP5  consider using TamperMonkey to post most logs to the server to determine whether the inbox cards were displayed\/removed  whether the inbox audio alert was played  and errors logged to the console. Ask customer to enable this for specific agents that are experiencing the problem.\n   2. If the agent's browser did receive the AMB message that triggers the incoming card notification  a log in the following format should show up: `2022-09-15 08:14:02 (062) Default-thread-56 C650FE004F06D510A671C43D94AD481A txid=52c5ba804f06 app=\"CI\" track=\"AWA\" interaction=\"48c532c48746d5107bae40c80cbb3580\" work_item=\"79c53e848746d5107bae40c80cbb3534\" User 46d44a23a9fe19810012d100cca80666 received an inbox update via AMB  with a time offset of 210 ms:`\n   3. One of the logs will also indicate the work item sys ID that was added to the specific agent's inbox: `Agent Inbox update: User '6816f79cc0a8016401c5a33be04be441' with current workitems (before) as [] and updated workitems (after) as [56bc063d43763110b1cee4c66bb8f2ff] at timestamp 'Wed Nov 29 2023 13:53:23 GMT-0800 (Pacific Standard Time)'.`\n8. ([KB1209787](https:\/\/support.servicenow.com\/kb?id=kb_article_view&sysparm_article=KB1209787)) Verify if agents were active on the workspace tab when the work item appears in the agent's inbox using TamperMonkey. Ask customer to enable this for specific agents that are experiencing the problem.\n   1. Identify the specific agent that was encountering the issue and the period at which it was occurring.\n   2. With TamperMonkey installed on the agent's browser  the following log will appear when an inbox card is displayed: `### INBOX LOGGER ### - IMS0000207 Inserted in DOM after 50 ms  at 2022-11-30T12:34:11.829Z with countdown 0:59 and visibilityState visible. Last activity was 3649 ms ago.` The log includes the visibility of the tab when the work item appears -- if the visibilityState is hidden  therefore the agent was not on the workspace tab at that point in time. It also includes the amount of time since the agent was last active on the tab.\n9. Consider the following potential root cases:\n   1. If you run into the \"Failed to accept work item\" log  one reason could be agent accepted the work item when the timer is about to expire.\n      1. By the time the agent accepts the inbox item  the TimeoutJob running in the background has already marked the work item as timeout. The duration of this timer is defined in the assignment rule.\n      2. Check if a security ACLs may be causing the log -- we use GlideRecordSecure to update the work item. Activate the glide.security.log.acl.warnings system property to see security ACL exceptions.\n10. Consider the following workarounds:\n    1. ([KB1382977](https:\/\/support.servicenow.com\/kb?id=kb_article_view&sysparm_article=KB1382977)) Recommend agents to enable desktop notifications for incoming chats while Dev-AgentChatAWA fixes the issue of audio notifications for inactive workspace tabs. \"\n\n",
        "QUERY": " 'troubleshoot performance issues in a instance'",
        "GROUND_TRUTH": "To troubleshoot performance issues in an instance, you can follow these steps:\n\n1. Test the network connectivity to the instance and verify there are no existing issues.\n2. Verify if a node needs to be restarted.\n3. Verify there are no inefficient queries running against the database.\n4. Verify the instance is not running out of JVM memory.\n5. Verify the disk I\/O does not indicate overuse or high levels of saturation.\n6. Verify there are sufficient CPU resources to satisfy demand.\n7. Verify the instance is not experiencing high concurrency.\n\nIn some cases, the sys_properties table might contain the wrong name after clone or rename. If this is the case, you should check and update it accordingly.\n\nFor each of these steps, the article provides links to more detailed guides on how to perform them. It's important to follow these steps in order and not to skip any of them to ensure a thorough troubleshooting process."
    },
    {
        "id": 92,
        "CONTEXT": "\"## The \"commit update set\" dialog never completes when the update set contains deletes for tables that \n\n## Description\n\nFor update sets that contain a delete for a table that does not exist  the update set will complete successfully but the progress worker that was doing the commit is never marked as complete and sits at 90-something percent complete. This is a non-fatal error.\n\n## Steps to Reproduce\n\nImport an update set that contains a table delete for a table not in the target system.\n\nCommit the update set.\n\nNote that the progress worker dialog never disappears.3.\n\nRefresh the form.\n\nNote that the status is complete.4.\n\nFind the root sys_execution_tracker record for this operation.\n\nNote that the tracker is still running and one of its children for \"dropping table\" is marked as failed.\n\n## Workaround\n\nClose (do **not** cancel) the dialog window and refresh the form to see the status marked as completed.\n\n**Related Problem: PRB714572**\n\n\n\n## Update set id 'global' is different than update set scope id\n\n# Solving errors on dashboards moved with update sets {#ariaid-title1}\n\nWhen you move a dashboard with an update set  if errors are shown on the Update Set Preview Problems tab of the Retrieved Update Set page  follow the instructions for each error to solve these problems.\n\n## Could not find a record in sys_grid_canvas for column canvas_page referenced in this\n\nupdate {#ariaid-title2}\n\nWhen you move a dashboard with an update set  the following error may occur: 'Could not find a record in sys_grid_canvas for column canvas_page referenced in this update'. To solve this error  move the canvas page from the source instance to the target instance.\nRole required: admin.\n\n1. In the Update Set Preview Problems related list  click the information icon (![Information icon](..\/..\/..\/common\/image\/Form_ReferenceLookupIcon.png)) next to the error.![Update Set Preview Problems tab with two errors.](..\/image\/update-set-preview-problems.png)\n2. In the pa_tabs record payload  copy the sys_id associated with the canvas_page field.  \n   ![Filter grid canvas on Sys ID](..\/image\/filter-on-sys-id.png)\n3. In the source instance  navigate to sys_grid_canvas.list.\n4. Filter the list on the sys_id copied in step 2.\n5. Right-click on the returned record and select Unload Canvas Page to add this record to the current update set.\n6. Transfer the update set to the target instance using standard update set functionality.  \n   For more information  see [Retrieve an update\n   set](..\/build\/system-update-sets\/task\/t_RetrieveAnUpdateSet.dita\/t_RetrieveAnUpdateSet.html).\n7. Repeat this task for all update set preview problems that have this error.\n\nThe missing dashboard tab content is moved to the target instance.\n\n## Update set id 'global' is different than update set scope id {#ariaid-title3}\n\nWhen you attempt to move a dashboard in a scoped app  the move fails when you try to retrieve the update set.\n\nDashboards comprise multiple tables. The Now Platform\u00ae does not track all of these tables. The untracked tables are always in the global scope. When you are moving a dashboard in the global scope  these tables do not present a problem. However  these tables block the move of dashboards in any other scope  because the scope of all components in an update set must be the same.\n\nFor a possible workaround  see [KB1555881](https:\/\/support.servicenow.com\/kb?id=kb_article_view&sysparm_article=KB1555881). You need access to Now Support.\n\n\n\n# Solving errors on dashboards moved with update sets {#ariaid-title1}\n\nWhen you move a dashboard with an update set  if errors are shown on the Update Set Preview Problems tab of the Retrieved Update Set page  follow the instructions for each error to solve these problems.\n\n## Could not find a record in sys_grid_canvas for column canvas_page referenced in this\n\nupdate {#ariaid-title2}\n\nWhen you move a dashboard with an update set  the following error may occur: 'Could not find a record in sys_grid_canvas for column canvas_page referenced in this update'. To solve this error  move the canvas page from the source instance to the target instance.\nRole required: admin.\n\n1. In the Update Set Preview Problems related list  click the information icon (![Information icon](..\/..\/..\/common\/image\/Form_ReferenceLookupIcon.png)) next to the error.![Update Set Preview Problems tab with two errors.](..\/image\/update-set-preview-problems.png)\n2. In the pa_tabs record payload  copy the sys_id associated with the canvas_page field.  \n   ![Filter grid canvas on Sys ID](..\/image\/filter-on-sys-id.png)\n3. In the source instance  navigate to sys_grid_canvas.list.\n4. Filter the list on the sys_id copied in step 2.\n5. Right-click on the returned record and select Unload Canvas Page to add this record to the current update set.\n6. Transfer the update set to the target instance using standard update set functionality.  \n   For more information  see [Retrieve an update set](..\/build\/system-update-sets\/task\/t_RetrieveAnUpdateSet.dita\/t_RetrieveAnUpdateSet.html).\n7. Repeat this task for all update set preview problems that have this error.\n\nThe missing dashboard tab content is moved to the target instance.\n\n## Update set id 'global' is different than update set scope id {#ariaid-title3}\n\nWhen you attempt to move a dashboard in a scoped app  the move fails when you try to retrieve the update set.\n\nDashboards comprise multiple tables. The Now Platform\u00ae does not track all of these tables. The untracked tables are always in the global scope. When you are moving a dashboard in the global scope  these tables do not present a problem. However  these tables block the move of dashboards in any other scope  because the scope of all components in an update set must be the same.\n\nFor a possible workaround  see [KB1555881](https:\/\/support.servicenow.com\/kb?id=kb_article_view&sysparm_article=KB1555881). You need access to Now Support.\n\n\"\n\n",
        "QUERY": " 'Tracker of update set with a table delete stuck at 90 percent though update set is success'",
        "GROUND_TRUTH": "The issue of the tracker of an update set with a table delete getting stuck at 90 percent, even though the update set is successful, occurs when the update set contains a delete for a table that does not exist. In this case, the progress worker that was doing the commit is never marked as complete and remains at 90-something percent complete. This is a non-fatal error. To resolve this issue, you should close (do not cancel) the dialog window and refresh the form. After refreshing, the status will be marked as completed. However, if you check the root sys_execution_tracker record for this operation, you will notice that the tracker is still running and one of its children for \"dropping table\" is marked as failed."
    },
    {
        "id": 93,
        "CONTEXT": "\"## MID Server TrustStore migration  to preserve cacerts when JRE is upgraded  fails when the same certi\n\n## Description\n\nIf the same ssl\/tls certificate has been added to cacerts more than once  then when a instance upgrade causes a MID Server Java upgrade  the custom additions to the cacerts files may be lost during the migration of the cacerts file.\n\nAn exception during this migration leaves the default cacerts file from Java  which only contains the common trusted root CA certificates  which means MID security policy certificate checks would only pass for connect endpoints using purchased certificates.\n\nOnce the MID Server is upgraded  this will cause an outage for any integrations where the endpoints use self-signed or internal CA generated certificates for the SSL\/TLS connection. This would include the instance connection if not using the normal hosted certificate  or is using a 'fake' interception\/inspection firewall certificate for the instance  that is also generated by an internal CA.\n\nThis is most likely to be seen for LDAP integrations  which usually require LDAPS  and often are implemented with an internal certificate.\n\n## Steps to Reproduce\n\n1. Using keytool  add the same internal CA certificate to agent\\hre\\lib\\securoty\\certs twice  using different aliases\n2. Upgrade the instance to a version that has a new JRE for the MID Server. Recent ones are Utah  Vancouver patch 4  Washington DC  so start on e.g. Vancouver Patch 3  then upgrade to patch 4.\n\n**Expected behaviour:**\nDuplicate certificates are copied as is. Or are merged. All other certificates would be preserved as there is nothing wrong with those.\n\n**Actual behaviour:**\nAll certifiacted added by the customer are lost. None are preserved.\nThe previous cacerts file is lost. It is not saved in a backup folder in agent\\work\\truststore_migration\n\nMID Server agent log will show:\n\n2023-11-09T04:35:14.807+0000 INFO (AutoUpgrade.3600.now) MIDTrustStoreMigrator:313 Migrating X.509 cert (alias:<alias name>  issuer: CN=rootCA DC=XXXXXX> DC=com  serial number: XXXXXX  version: 3  basic constraint: 2147483647)\n2023-11-09T04:35:14.822+0000 ERROR (AutoUpgrade.3600.now) MIDTrustStoreMigrator:210 Exception thrown during TrustStore migration.\njava.lang.IllegalStateException: Duplicate key snc-blackstoneca (attempted merging values [\n[\nVersion: V3\nSubject: .....\n\n])\nat java.base\/java.util.stream.Collectors.duplicateKeyException(Collectors.java:133)\nat java.base\/java.util.stream.Collectors.lambda$uniqKeysMapAccumulator$1(Collectors.java:180)\nat java.base\/java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169)\nat java.base\/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:177)\nat java.base\/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:177)\nat java.base\/java.util.HashMap$EntrySpliterator.forEachRemaining(HashMap.java:1764)\nat java.base\/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484)\nat java.base\/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474)\nat java.base\/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:913)\nat java.base\/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)\nat java.base\/java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:578)\nat com.service_now.mid.dist.upgrade.MIDTrustStoreMigrator.filterCerts(MIDTrustStoreMigrator.java:328)\nat com.service_now.mid.dist.upgrade.MIDTrustStoreMigrator.getCertsToMigrate(MIDTrustStoreMigrator.java:298)\nat com.service_now.mid.dist.upgrade.MIDTrustStoreMigrator.migrateCerts(MIDTrustStoreMigrator.java:182)\nat com.service_now.mid.dist.upgrade.MIDUpgrader.upgrade(MIDUpgrader.java:247)\nat com.service_now.monitor.AutoUpgrade.attemptUpgrade(AutoUpgrade.java:242)\nat com.service_now.monitor.AutoUpgrade.run(AutoUpgrade.java:125)\nat com.service_now.monitor.AMonitor.runit(AMonitor.java:154)\nat com.service_now.monitor.AMonitor.access$200(AMonitor.java:43)\nat com.service_now.monitor.AMonitor$MonitorTask.execute(AMonitor.java:147)\nat com.service_now.monitor.base.AMonitorTask.run(AMonitorTask.java:29)\nat java.base\/java.util.TimerThread.mainLoop(Timer.java:556)\nat java.base\/java.util.TimerThread.run(Timer.java:506)\n\n## Workaround\n\nThis problem is currently under review and targeted to be fixed in a future release. Subscribe to this Known Error article to receive notifications when more information will be available.\n\nThe workaround is to remove duplicates from cacerts before the upgrade.\n\n* Change directory to the agent\\jre\\lib\\security folder  then List what the cacerts file contains:  \n  C:\\ServiceNow_MID\\xxxx\\agent\\jre\\lib\\security\\>..\\..\\bin\\keytool -list -keystore cacerts -storepass changeit\n* This will list the alias and fingerprint of the certificate. If the fingerprint is the same as another  it's a duplicate. Then delete that specific certificate alias with:  \nC:\\ServiceNow_MID\\xxxx\\agent\\jre\\lib\\security\\>..\\..\\bin\\keytool -delete -keystore cacerts -storepass changeit -alias \\<alias\\>  \n\n**Related Problem: PRB1725231**\n\n\n\n## Troubleshooting Guide | SNAP-Snowsk8s healthcheck | SNCMON\n\n|---------------------------------------------------------------------------|----------------------------------------------------------------------------------------------|\n| ![Note](sys_attachment.do?sys_id=45349492db3d9f00fac7f4621f9619f1 \"Note\") | **Note**: The troubleshooting information below is intended for readers in the SysEng staff. |\n# Alert Details\n\n|---------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| **Alert Type Name:**      | SNAP-Snowsk8s healthcheck                                                                                                                                                                                                     |\n| **Signature:**            | \\<SEVERITY\\> Alert snap-snowsk8s-healthcheck on \\<SNAP_Snowsk8s_cluster\\>: \\<DATE\\> Content match failed                                                                                                                      |\n| **Source:**               | Xymon                                                                                                                                                                                                                         |\n| **Alert Description:**    | SNAP-Snowsk8s service is down                                                                                                                                                                                                 |\n| **Thresholds:**           | SNAP-Snowsk8s https healthcheck does not return the expected value                                                                                                                                                            |\n| **Clear Condition:**      | SNAP-Snowsk8s https helathcheck returns the expected value                                                                                                                                                                    |\n| **Causes:**               | The Tomcat server is not running  Clamd is not running  ClamAV virus definition is outdated.                                                                                                                                  |\n| **Impact:**               | Documents being uploaded\/downloaded to\/from instances are not being scanned                                                                                                                                                   |\n| **Priority:**             | P2 - High                                                                                                                                                                                                                     |\n| **Assignment Group:**     | This should be escalated directly to [Vladimir Nikolic](mailto:vladimir.nikolic@servicenow.com) and [Bhargavi Somasekar](mailto:bhargavi.somasekar@servicenow.com) (members of the Systems Engineering Core and India teams). |\n| **Customer Facing:**      | Yes                                                                                                                                                                                                                           |\n| **AutoINT:**              | No                                                                                                                                                                                                                            |\n| **Automation Available:** | No                                                                                                                                                                                                                            |\n# Escalation Contacts\n\n|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| ? ![Note](sys_attachment.do?sys_id=45349492db3d9f00fac7f4621f9619f1 \"Click to see the description\")Service owner (Click to see description) A Service Owner (i.e. product owner  OST requester team) is responsible for the overall performance and delivery of a specific service. They are accountable for: * ensuring that the service meets the needs of the business * managing the service throughout its lifecycle. * defining the service strategy  * ensuring that the service meets the required service level agreements (SLAs)  and managing the service improvement process and requests.                      | Escalate directly to [Vladimir Nikolic](mailto:vladimir.nikolic@servicenow.com) or [Bhargavi Somasekar](mailto:bhargavi.somasekar@servicenow.com) on Mattermost or MS Teams. |\n| ? ![Note](sys_attachment.do?sys_id=45349492db3d9f00fac7f4621f9619f1 \"Click to see the description\")Technical owner (Click to see description) A Technical Owner is a person or team responsible for the related technical infrastructure  hardware  software  or networking components around this alert and support the service to meet the stated service level agreement requirements.                                                                                                                                                                                                                                   | [Vladimir Nikolic](mailto:vladimir.nikolic@servicenow.com) from SysEng Core and [Bhargavi Somasekar](mailto:bhargavi.somasekar@servicenow.com) from SysEng India teams.      |\n| ? ![Note](sys_attachment.do?sys_id=45349492db3d9f00fac7f4621f9619f1 \"Click to see the description\")Level 1 Response Team (Click to see description) The Level 1 response team is responsible for: * responding to any alerts or notifications (Auto-INT) generated by monitoring systems. * triaging and investigating the alert * attempting to resolve it based on this TSG. * In many cases  this team would be Ops Site Reliability Engineering (SRE) as they are the first responder to several alerts  * it could be any team who initially receives the alert (i.e. SysEng  SWAT  CNS  etc.) especially if Auto-INT. | [Vladimir Nikolic](mailto:vladimir.nikolic@servicenow.com) from SysEng Core and [Bhargavi Somasekar](mailto:bhargavi.somasekar@servicenow.com) from SysEng India teams.      |\n| ? ![Note](sys_attachment.do?sys_id=45349492db3d9f00fac7f4621f9619f1 \"Click to see the description\")Level 2 Response Team (Click to see description) The Level 2 response team is responsible for: * escalation * investigating the root cause of any alerts * taking appropriate action to resolve any issues that are identified. * In many cases  this would also be the technical owner.                                                                                                                                                                                                                                 | N\/A                                                                                                                                                                          |\n| **Urgency:**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | **Escalate the alert within this window of time:** Immediate                                                                                                                 |\n# Triage Steps\n\nTry to identify the scope  time  and possible cause for the issue.\n\n* Triaging should take only a few minutes to execute \n* Results in the verification of an underlying incident  the incident type  impact  and urgency \n* Prepares the reader to perform investigation and resolution if necessary.\n\nFollow these steps:\n\n1. Locate the datacenter(s) affected (e.g. bwi\/phx).\n2. Locate Snowsk8s cluster(s) affected (e.g. c002.ifa).\n3. Confirm the exact time when the issue has started (based on monitoring output\/graphs).\n\n# Investigation Steps\n\nLocate the datacenter and Snowsk8s cluster with the affected workload.\n\n* Investigating should result in **finding the incident's cause**\n* **Prepares the reader**to resolve the incident\n\nFollow these steps:\n\n1. [Determine which cluster is running the affected snap-on-snowsk8s workload](https:\/\/datacenter.service-now.com\/nav_to.do?uri=%2Fx_snc_kcw_workload_instance_list.do%3Fsysparm_query%3Du_workload_deploymentLIKEsnap%26sysparm_first_row%3D1%26sysparm_view%3D).\n2. Log in to the cluster with affected snap-on-snowsk8s workload\n   * bssh sk8sops01.ifa400\n   * start_k8toolbox\n   * kubectl login \\<cluster\\> (e.g. c002.phx)\n3. Get the info about running pods (four per cluster)  containers (four per pod)  load balancer service  and logs from containers. If there's a container not ready  run 'kubectl describe \\>pod\\>' or try to log into the container's console.\n   * kubectl -n snap-on-snowsk8s get pods\n   * kubectl -n snap-on-snowsk8s logs \\<pod\\> -c tomcat (or nginx\/clamd\/freshclam)\n   * kubectl -n snap-on-snowsk8s describe pod \\<pod\\>\n   * kubectl -n snap-on-snowsk8s exec \\<pod\\> -c tomcat -- \/bin\/sh (container name can be as well nginx\/clamd\/freshclam)\n   * kubectl -n snap-on-snowsk8s get service snap-snowsk8s\n\n# Resolution Steps\n\n\n\n## Troubleshooting Guide | Alert connection on mms.smartmessagingsuite.com | Xymon\n\nRevision Log... **(Last updated 28-Jun-2022)**\n\n| Version |  Published  |                           Summary of Changes                            |\n|---------|-------------|-------------------------------------------------------------------------|\n| 1.0     | 3-Mar-2020  | Initial version                                                         |\n| 2.0     | 28-Jun-2022 | Updated the **Investigation Steps** section with additional validation. |\n# Alert Details\n\n|---------------------------|------------------------------------------------------------------------------|\n| **Name:**                 | Xymon:Alert connection on mms.smartmessagingsuite.com                        |\n| **Signature:**            | mms.smartmessagingsuite.com is not OK                                        |\n| **Source:**               | Xymon                                                                        |\n| **Description:**          | mms.smartmessagingsuite.com process is not running or port is not listening  |\n| **Thresholds:**           | N\/A                                                                          |\n| **Clear Condition:**      | Process is running \/ port is listening.                                      |\n| **Causes:**               | MMS Smartmessagingsuite service is stopped  unreachable  misconfigured  etc. |\n| **Impact:**               | AT\\&T customers will not receive SMS or MMS.                                 |\n| **Automation Available:** | N\/A                                                                          |\n**Additional Alert Details:**\n\n* Verify using the SRE Board [https:\/\/sre01.lhr1.service-now.com\/sre_board\/](https:\/\/sre01.lhr1.service-now.com\/sre_board\/) that there are no notes or changes ongoing.\n\n# Triage Steps\n\n1. Check the connection between relay servers to AT\\&T Smart Messaging Suite on Port 25. Xymon has the list of our [relay servers](https:\/\/monitor.service-now.com\/xymon\/mail-relay-servers \"relay servers\") to use for testing.\n2. Try to determine if it is networking-related from our end (connect from your local laptop to AT\\&T Smart Messaging Suite from COPR network).\n   **Note:** Mac users may need to install telnet first  as it is no longer standard in MacOS X (brew install telnet).\n\n*![](sys_attachment.do?sys_id=33603d7e1b044510c552c8031d4bcb67)*\n\n3. If you determine the cause is from the AT\\&T end  then you need to create a support ticket to AT\\&T EOD.\n\n# Investigation Steps\n\n1. Log into [AT\\&T Smart Messaging Suite](https:\/\/na1.smartmessagingsuite.com\/sei\/login.do?function=authenticate&customerId=90 \"AT&T Smart Messaging Suite\") and check the relay servers\/IPs listed in the queue  and verify for all IPs listed. (The user and password is listed in WPS under ATT MMS Gateway.) and also ensure **Authenticate by remote hosts** enabled.  \n   URL: [https:\/\/na1.smartmessagingsuite.com\/](https:\/\/na1.smartmessagingsuite.com\/)  \n   Click on **Company Setup** --\\>**Email API Preference** .\n\n2. Log in to the AT\\&T Smart Messaging Suite and check the messages queue.\n   **Note:** If the messages are not going through  then you might check networking on our side.\n3. Log in to the Smart Messaging Suite:  \n   1. Go to **Reports** \\> **Message Report** .  \n   2. In the column selection  deselect **Message** .  \n      **![](\/sys_attachment.do?sys_id=b3b2f9fa1b444510c552c8031d4bcb24)**\n   3. In the **Search Options**   select **timeframe** and select **User** \\> **All Users** .  \n   4. Click **Preview** .\n      **Note:** This is to verify whether or not traffic is working.\n\n# Resolution Steps\n\n* If it is a networking issue  work with the Network team to resolve it.\n* If the issue is on AT\\&T  then work with AT\\&T. Open a ticket with AT\\&T (the user and password will be in DCPS). See [KB0786303](\/kb_view.do?sysparm_article=KB0786303 \"KB0786303\") for further instructions.  \n  URL: [https:\/\/eod.wireless.att.com\/login.aspx](https:\/\/eod.wireless.att.com\/login.aspx)\n* Alternatively  a support ticket can be opened here:  \n  [https:\/\/help.sopranodesign.com\/hc\/en-us\/requests\/new](https:\/\/help.sopranodesign.com\/hc\/en-us\/requests\/new \"https:\/\/help.sopranodesign.com\/hc\/en-us\/requests\/new\")\n\n# Escalation\n\n|----------------------|---------------------------------------------------------------------------------------------------------|\n| **Alert Owner:**     | Monitoring_Dev                                                                                          |\n| **Technical Owner:** | [SRE](.\/kb_view.do?sysparm_article=KB0754260 \"SRE\") \/ [SA](.\/kb_view.do?sysparm_article=KB0754253 \"SA\") |\n| **Urgency:**         | Immediate                                                                                               |\n# Additional Resources\n\nClick the links below to read related articles and other resources.\n\n|                              Name                               |                             Description                             |\n|-----------------------------------------------------------------|---------------------------------------------------------------------|\n| [KB0753031](.\/kb_view.do?sysparm_article=KB0753031 \"KB0753031\") | Technical Service Overview \\| ATT Global SmartMessagingSuite (GSMS) |\n\"\n\n",
        "QUERY": " 'TLS handshake failure when connecting to an email server using a self-signed key'",
        "GROUND_TRUTH": "According to KB12345, if the same SSL\/TLS certificate has been added to cacerts more than once, an instance upgrade that causes a MID Server Java upgrade can result in the loss of custom additions to the cacerts files during the migration of the cacerts file. This can cause an outage for any integrations where the endpoints use self-signed or internal CA generated certificates for the SSL\/TLS connection. This could be the reason for the TLS handshake failure when connecting to an email server using a self-signed key. \n\nThe workaround suggested in the article is to remove duplicates from cacerts before the upgrade. This can be done by changing the directory to the agent\\jre\\lib\\security folder, listing what the cacerts file contains, and deleting the specific certificate alias if the fingerprint is the same as another, indicating it's a duplicate."
    },
    {
        "id": 94,
        "CONTEXT": "\"## Week Starts on date on the Time Card is not changing\n\nChanging the \"Week Starts On\" field on the Time Card does not save\n\n\n\nJakarta Patch 9c\n\n\n\nNo Time Sheet on the time card so \"Populate week starts on\" Business Rule was running.\n\n\"\n\n",
        "QUERY": " 'Time card with \"week starts on\" not working as expected'",
        "GROUND_TRUTH": "no answer found"
    },
    {
        "id": 95,
        "CONTEXT": "\"## Utah Patch 9 Hot Fix 1: Known Errors\n\n Simple - List component is not available in the Dashboard Builder                                                                                                                                                        |\n| [KB0966598](\/kb_view.do?sysparm_article=KB0966598) | PRB1492041 | Mobile Platform                        | Mobile Task button actions on WOT form page do not refresh the form once the Task has been accepted\/rejected                                                                                                             |\n| [KB0993213](\/kb_view.do?sysparm_article=KB0993213) | PRB1492353 | Platform Licensing                     | Inconsistencies with auto-calculation of per-user subscription allocation (System Entitlement)                                                                                                                           |\n| [KB1123826](\/kb_view.do?sysparm_article=KB1123826) | PRB1501129 | Software Asset Management Professional | \\[Accessibility\\] - License Usage - Focus is missing after clicking Run Reconciliation button from pop-up                                                                                                                |\n| [KB0966640](\/kb_view.do?sysparm_article=KB0966640) | PRB1507712 | Agent Workspace                        | The calendar picker is picking invalid dates for today  yesterday  and all calendar left menu options.                                                                                                                   |\n| [KB1000205](\/kb_view.do?sysparm_article=KB1000205) | PRB1513194 | Flow Designer                          | \\[Revisited\\] The output from the 'Get Catalog Variables for List Collector' type changed from string to GRProxy                                                                                                         |\n| [KB1080549](\/kb_view.do?sysparm_article=KB1080549) | PRB1539109 | Next Experience Unified Navigation     | POL_QE_RP1 - Next Experience does not allow user to login on Platform-only instance after installing 'Unified Navigation Admin Configuration' plugins and enabling Next Experience                                       |\n| [KB1005139](\/kb_view.do?sysparm_article=KB1005139) | PRB1539267 | PDF Generation                         | Korean letters are getting garbled when the PDF file is generated from a report                                                                                                                                          |\n| [KB1119866](\/kb_view.do?sysparm_article=KB1119866) | PRB1539950 | Core Platform                          | Activating the slushbucket 'Save' job results in the 'Created\/updated' field's entry reading as 'System'                                                                                                                 |\n| [KB1005406](\/kb_view.do?sysparm_article=KB1005406) | PRB1541411 | Lists                                  | \\[Workspace WHC\\] Highlighted \/ selected \/ hover pages are indistinguishable in list pagination                                                                                                                          |\n| [KB1120291](\/kb_view.do?sysparm_article=KB1120291) | PRB1557801 | UX Framework                           | The UXF does not trim strings for 'now-color' in UxFrameworkColorGenerator.java prior to parsing as an integer  which causes Next Experience pages to crash                                                              |\n| [KB1513421](\/kb_view.do?sysparm_article=KB1513421) | PRB1565976 | Key Management Framework               | Scheduled job fails daily because a Business Rule on sys_certificate deletes existing certificate attachments on update                                                                                                  |\n| [KB1117259](\/kb_view.do?sysparm_article=KB1117259) | PRB1569004 | IT Operations Management               | In Optimization and Health  old jobs are not being removed  if the plugin is installed after installation                                                                                                                |\n| [KB1224507](\/kb_view.do?sysparm_article=KB1224507) | PRB1569268 | Live Feed                              | Live feed notifications become active after upgrading to San Diego                                                                                                                                                       |\n| [KB1166040](\/kb_view.do?sysparm_article=KB1166040) | PRB1572894 | Enterprise Portal                      | EVAM data source bypasses Query Business Rules on Project Workspace                                                                                                                                                      |\n| [KB1157817](\/kb_view.do?sysparm_article=KB1157817) | PRB1597897 | Discovery                              | Azure tag changes still insert a new cmdb_key_value with the same key  rather than updating the existing cmdb_key_value                                                                                                  |\n| [KB1198693](\/kb_view.do?sysparm_article=KB1198693) | PRB1601193 | Authentication                         | \\[Tokyo Partner Testing\\] Cannot register a new user account on Login page (Polaris view)                                                                                                                                |\n| [KB1182326](\/kb_view.do?sysparm_article=KB1182326) | PRB1606990 | Service Portal                         | Fix self-closing tag issues for Service Portal widgets                                                                                                                                                                   |\n| [KB1221835](\/kb_view.do?sysparm_article=KB1221835) | PRB1607765 | Graph API                              | \"graphql_schema_admin\" role missing from \"write_role\" field on \"glide.graphql.introspection_enabled\" sys_properties record.                                                                                              |\n| [KB1573145](\/kb_view.do?sysparm_article=KB1573145) | PRB1628200 |                                        | Tokyo - child tables of cmdb extended tables do not correctly inherit the label of the parent column when it is created via update set. Only the original cmdb extended table column has the correct column label        |\n| [KB1443045](\/kb_view.do?sysparm_article=KB1443045) | PRB1631282 |                                        | The plugin com.snc.platform_document_management creates snc_internal role with a wrong sys_id                                                                                                                            |\n| [KB1504068](\/kb_view.do?sysparm_article=KB1504068) | PRB1642990 |                                        | Unable to create new case with French Language                                                                                                                                                                           |\n| [KB1292152](\/kb_view.do?sysparm_article=KB1292152) | PRB1648000 |                                        | \\[RPS\\] Anti virus scan prevents syncing of attachments using Remote Process Sync                                                                                                                                        |\n| [KB1309227](\/kb_view.do?sysparm_article=KB1309227) | PRB1658441 |                                        | System Notifications are not translatable with respective i18 language                                                                                                                                                   |\n| [KB1309212](\/kb_view.do?sysparm_article=KB1309212) | PRB1659016 |                                        | Variable summarizer in the standard ticket tab does not honour UI policies\/display submitted variable incorrectly                                                                                                        |\n| [KB1560292](\/kb_view.do?sysparm_article=KB1560292) | PRB1659842 |                                        | Unexpected behaviour for System Archive Related Records when the tables are in hierarchy of the table structure                                                                                                          |\n| [KB1518969](\/kb_view.do?sysparm_article=KB1518969) | PRB1662906 |                                        | Only 1 input field coming for JDBC step on clicking Test Data button                                                                                                                                                     |\n| [KB1446195](\/kb_view.do?sysparm_article=KB1446195) | PRB1670974 |                                        | Workspace error info message broken after Utah upgrade                                                                                                                                                                   |\n| [KB1362321](\/kb_view.do?sysparm_article=KB1362321) | PRB1671906 |                                        | CMDB Data Management Task required approval even when with policy having \"Needs review\" is false                                                                                                                         |\n| [KB1434114](\/kb_view.do?sysparm_article=KB1434114) | PRB1679148 |                                        | Thread got stuck in the JSQLParser call while trying to parse long query                                                                                                                                                 |\n| [KB1584714](\/kb_view.do?sysparm_article=KB1584714) | PRB1683928 |                                        | 'events process 0' job ran for days  and the stack kept showing 'system.upgraded' event stuck despite of upgrade was successfully completed.                                                                             |\n| [KB1514475](\/kb_view.do?sysparm_article=KB1514475) | PRB1685124 |                                        |\n\n## Tokyo Patch 10 Hot Fix 1a: Known Errors\n\n EVAM data source bypasses Query Business Rules on Project Workspace                                                                                                                                                              |\n| [KB1204179](\/kb_view.do?sysparm_article=KB1204179) | PRB1576905 | Platform Security                      | KMF signature records loaded for unchanged files during upgrade resulting in increased upgrade time                                                                                                                              |\n| [KB1274851](\/kb_view.do?sysparm_article=KB1274851) | PRB1581528 | Persistence                            | Queries still run on the database after the client has disconnected - \"Read timed out\" exception                                                                                                                                 |\n| [KB1124054](\/kb_view.do?sysparm_article=KB1124054) | PRB1589957 | NLU Workbench                          | \\[NLU-Procurement Service Management Model \\] Model training fails when a model has a reference to one static vocabulary source  such as purchasingTask                                                                          |\n| [KB1497593](\/kb_view.do?sysparm_article=KB1497593) | PRB1594523 | UX Framework                           | Blank page with console error for library-uxf after upgrading to Utah on Polaris (internal build)                                                                                                                                |\n| [KB1163104](\/kb_view.do?sysparm_article=KB1163104) | PRB1596329 | Flow Designer                          | Platform audits the deletion of 'sys_flow_context' and 'sys_flow_runtime_value' OOTB causing excessive 'sys_audit' record creations                                                                                              |\n| [KB1212248](\/kb_view.do?sysparm_article=KB1212248) | PRB1601946 | Software Asset Management Professional | License calculators for the Per device and Per User license metrics don't account for entitlements with unlimited rights                                                                                                         |\n| [KB1501269](\/kb_view.do?sysparm_article=KB1501269) | PRB1609636 | UI Policy\/Client Script                | \\[UUU-Runtime-Performance\\]: \/api\/now\/ui\/polaris\/menu API call takes a long time to execute on first load if \"All\" menu is pinned and several application menus are expanded                                                     |\n| [KB1220794](\/kb_view.do?sysparm_article=KB1220794) | PRB1614184 | Human Resources Service Management     | \\[AI Search\\] - HR case child table records are not being indexed.                                                                                                                                                               |\n| [KB1441913](\/kb_view.do?sysparm_article=KB1441913) | PRB1614636 | Instance Data Replication feature      | \\[regulated market\\] Creating a new Producer Replication Set with the Submit button causes an error                                                                                                                              |\n| [KB1577110](\/kb_view.do?sysparm_article=KB1577110) | PRB1629249 |                                        | \\[Polaris \/ Next Experience\\] User sees their Title in settings even when Read ACLs deny access                                                                                                                                  |\n| [KB1226146](\/kb_view.do?sysparm_article=KB1226146) | PRB1631245 |                                        | Columns are being re-glommed upon upgrade  causing excessive upgrade times                                                                                                                                                       |\n| [KB1443045](\/kb_view.do?sysparm_article=KB1443045) | PRB1631282 |                                        | The plugin com.snc.platform_document_management creates snc_internal role with a wrong sys_id                                                                                                                                    |\n| [KB1552329](\/kb_view.do?sysparm_article=KB1552329) | PRB1633332 |                                        | Similarity solutions with a Database View as input table are not updated and new records are not shown in prediction results until the solution is retrained                                                                     |\n| [KB1273281](\/kb_view.do?sysparm_article=KB1273281) | PRB1640075 |                                        | High heap memory consumption during PDF generations with Itext5                                                                                                                                                                  |\n| [KB1272717](\/kb_view.do?sysparm_article=KB1272717) | PRB1642966 |                                        | Incorrect Discovery classification due to SNMP OID \"1.3.6.1.4.1.9.1.2244\" Firewall device getting classified as Switch                                                                                                           |\n| [KB1290517](\/kb_view.do?sysparm_article=KB1290517) | PRB1645189 |                                        | Additional rows\/records are created in the sys_ui_list table when user views a list  can cause performance issue                                                                                                                 |\n| [KB1292152](\/kb_view.do?sysparm_article=KB1292152) | PRB1648000 |                                        | \\[RPS\\] Anti virus scan prevents syncing of attachments using Remote Process Sync                                                                                                                                                |\n| [KB1309227](\/kb_view.do?sysparm_article=KB1309227) | PRB1658441 |                                        | System Notifications are not translatable with respective i18 language                                                                                                                                                           |\n| [KB1307937](\/kb_view.do?sysparm_article=KB1307937) | PRB1659580 |                                        | Setting default branch causing source control to \"fetch\" before using the MID server - making it unable to commit to any repository                                                                                              |\n| [KB1560292](\/kb_view.do?sysparm_article=KB1560292) | PRB1659842 |                                        | Unexpected behaviour for System Archive Related Records when the tables are in hierarchy of the table structure                                                                                                                  |\n| [KB1343188](\/kb_view.do?sysparm_article=KB1343188) | PRB1664140 |                                        | service catalog topic blocks in virtual agent is truncating html fields                                                                                                                                                          |\n| [KB1336388](\/kb_view.do?sysparm_article=KB1336388) | PRB1666661 |                                        | Incorrect score displayed in Analytics Hub for a PA benchmark indicator                                                                                                                                                          |\n| [KB1335068](\/kb_view.do?sysparm_article=KB1335068) | PRB1668655 |                                        | Initialize function of client-callable Script Include is evaluated when the Script Include is shown on the Form                                                                                                                  |\n| [KB1433675](\/kb_view.do?sysparm_article=KB1433675) | PRB1669965 |                                        | HAM - Enabling CSDM Life Cycle plugin changed \"State\" value a number of records in the alm_hardware table to \"consumed\"                                                                                                          |\n| [KB1510175](\/kb_view.do?sysparm_article=KB1510175) | PRB1670529 |                                        | Parent-child table entries in replication sets break replication                                                                                                                                                                 |\n| [KB1362321](\/kb_view.do?sysparm_article=KB1362321) | PRB1671906 |                                        | CMDB Data Management Task required approval even when with policy having \"Needs review\" is false                                                                                                                                 |\n| [KB1430596](\/kb_view.do?sysparm_article=KB1430596) | PRB1673239 |                                        | Cursor flickering\/placement issues when typing in string fields (Description  work notes  etc.)                                                                                                                                  |\n| [KB1441282](\/kb_view.do?sysparm_article=KB1441282) | PRB1678154 |                                        | Table cleaner on internal system tables can have very severe impact                                                                                                                                                              |\n| [KB1434114](\/kb_view.do?sysparm_article=KB1434114) | PRB1679148 |                                        | Thread got stuck in the JSQLParser call while trying to parse long query                                                                                                                                                         |\n| [KB1514475](\/kb_view.do?sysparm_article=KB1514475) | PRB1685124 |                                        | Activity Stream Mention (@user) does not work in comments and work notes on Workspace                                                                                                                                            |\n| [KB1496723](\/kb_view.do?sysparm_article=KB1496723) | PRB1687769 |                                        | In Utah Patch 4 \/ Tokyo Patch 10  the table Cleaner is skipping many tables if they have queries greater than 30 sec. \n\n## Tokyo Patch 10 Hot Fix 1b: Known Errors\n\n HAM: Category Selector - List tables lack descriptive labeling                                                                                                                                                                   |\n| [KB1166040](\/kb_view.do?sysparm_article=KB1166040) | PRB1572894 | Enterprise Portal                      | EVAM data source bypasses Query Business Rules on Project Workspace                                                                                                                                                              |\n| [KB1204179](\/kb_view.do?sysparm_article=KB1204179) | PRB1576905 | Platform Security                      | KMF signature records loaded for unchanged files during upgrade resulting in increased upgrade time                                                                                                                              |\n| [KB1274851](\/kb_view.do?sysparm_article=KB1274851) | PRB1581528 | Persistence                            | Queries still run on the database after the client has disconnected - \"Read timed out\" exception                                                                                                                                 |\n| [KB1124054](\/kb_view.do?sysparm_article=KB1124054) | PRB1589957 | NLU Workbench                          | \\[NLU-Procurement Service Management Model \\] Model training fails when a model has a reference to one static vocabulary source  such as purchasingTask                                                                          |\n| [KB1497593](\/kb_view.do?sysparm_article=KB1497593) | PRB1594523 | UX Framework                           | Blank page with console error for library-uxf after upgrading to Utah on Polaris (internal build)                                                                                                                                |\n| [KB1163104](\/kb_view.do?sysparm_article=KB1163104) | PRB1596329 | Flow Designer                          | Platform audits the deletion of 'sys_flow_context' and 'sys_flow_runtime_value' OOTB causing excessive 'sys_audit' record creations                                                                                              |\n| [KB1212248](\/kb_view.do?sysparm_article=KB1212248) | PRB1601946 | Software Asset Management Professional | License calculators for the Per device and Per User license metrics don't account for entitlements with unlimited rights                                                                                                         |\n| [KB1501269](\/kb_view.do?sysparm_article=KB1501269) | PRB1609636 | UI Policy\/Client Script                | \\[UUU-Runtime-Performance\\]: \/api\/now\/ui\/polaris\/menu API call takes a long time to execute on first load if \"All\" menu is pinned and several application menus are expanded                                                     |\n| [KB1220794](\/kb_view.do?sysparm_article=KB1220794) | PRB1614184 | Human Resources Service Management     | \\[AI Search\\] - HR case child table records are not being indexed.                                                                                                                                                               |\n| [KB1441913](\/kb_view.do?sysparm_article=KB1441913) | PRB1614636 | Instance Data Replication feature      | \\[regulated market\\] Creating a new Producer Replication Set with the Submit button causes an error                                                                                                                              |\n| [KB1573145](\/kb_view.do?sysparm_article=KB1573145) | PRB1628200 |                                        | Tokyo - child tables of cmdb extended tables do not correctly inherit the label of the parent column when it is created via update set. Only the original cmdb extended table column has the correct column label                |\n| [KB1577110](\/kb_view.do?sysparm_article=KB1577110) | PRB1629249 |                                        | \\[Polaris \/ Next Experience\\] User sees their Title in settings even when Read ACLs deny access                                                                                                                                  |\n| [KB1226146](\/kb_view.do?sysparm_article=KB1226146) | PRB1631245 |                                        | Columns are being re-glommed upon upgrade  causing excessive upgrade times                                                                                                                                                       |\n| [KB1443045](\/kb_view.do?sysparm_article=KB1443045) | PRB1631282 |                                        | The plugin com.snc.platform_document_management creates snc_internal role with a wrong sys_id                                                                                                                                    |\n| [KB1273281](\/kb_view.do?sysparm_article=KB1273281) | PRB1640075 |                                        | High heap memory consumption during PDF generations with Itext5                                                                                                                                                                  |\n| [KB1272717](\/kb_view.do?sysparm_article=KB1272717) | PRB1642966 |                                        | Incorrect Discovery classification due to SNMP OID \"1.3.6.1.4.1.9.1.2244\" Firewall device getting classified as Switch                                                                                                           |\n| [KB1290517](\/kb_view.do?sysparm_article=KB1290517) | PRB1645189 |                                        | Additional rows\/records are created in the sys_ui_list table when user views a list  can cause performance issue                                                                                                                 |\n| [KB1292152](\/kb_view.do?sysparm_article=KB1292152) | PRB1648000 |                                        | \\[RPS\\] Anti virus scan prevents syncing of attachments using Remote Process Sync                                                                                                                                                |\n| [KB1309227](\/kb_view.do?sysparm_article=KB1309227) | PRB1658441 |                                        | System Notifications are not translatable with respective i18 language                                                                                                                                                           |\n| [KB1307937](\/kb_view.do?sysparm_article=KB1307937) | PRB1659580 |                                        | Setting default branch causing source control to \"fetch\" before using the MID server - making it unable to commit to any repository                                                                                              |\n| [KB1560292](\/kb_view.do?sysparm_article=KB1560292) | PRB1659842 |                                        | Unexpected behaviour for System Archive Related Records when the tables are in hierarchy of the table structure                                                                                                                  |\n| [KB1343188](\/kb_view.do?sysparm_article=KB1343188) | PRB1664140 |                                        | service catalog topic blocks in virtual agent is truncating html fields                                                                                                                                                          |\n| [KB1336388](\/kb_view.do?sysparm_article=KB1336388) | PRB1666661 |                                        | Incorrect score displayed in Analytics Hub for a PA benchmark indicator                                                                                                                                                          |\n| [KB1335068](\/kb_view.do?sysparm_article=KB1335068) | PRB1668655 |                                        | Initialize function of client-callable Script Include is evaluated when the Script Include is shown on the Form                                                                                                                  |\n| [KB1433675](\/kb_view.do?sysparm_article=KB1433675) | PRB1669965 |                                        | HAM - Enabling CSDM Life Cycle plugin changed \"State\" value a number of records in the alm_hardware table to \"consumed\"                                                                                                          |\n| [KB1510175](\/kb_view.do?sysparm_article=KB1510175) | PRB1670529 |                                        | Parent-child table entries in replication sets break replication                                                                                                                                                                 |\n| [KB1362321](\/kb_view.do?sysparm_article=KB1362321) | PRB1671906 |                                        | CMDB Data Management Task required approval even when with policy having \"Needs review\" is false                                                                                                                                 |\n| [KB1430596](\/kb_view.do?sysparm_article=KB1430596) | PRB1673239 |                                        | Cursor flickering\/placement issues when typing in string fields (Description  work notes  etc.)                                                                                                                                  |\n| [KB1441282](\/kb_view.do?sysparm_article=KB1441282) | PRB1678154 |                                        | Table cleaner on internal system tables can have very severe impact                                                                                                                                                              |\n| [KB1434114](\/kb_view.do?sysparm_article=KB1434114) | PRB1679148 |                                        | Thread got stuck in the JSQLParser call while trying to parse long query                 ...",
        "QUERY": " 'Syntax highlighting in scripts is not working after Utah upgrade'",
        "GROUND_TRUTH": "no answer found"
    },
    {
        "id": 96,
        "CONTEXT": "\"## Reference | Common Errors from \"Tembo Splunk Dashboard - Error View\"\n\n|---------------------------------------------------------------------------|----------------------------------------------------------------------------|\n| ![Note](sys_attachment.do?sys_id=45349492db3d9f00fac7f4621f9619f1 \"Note\") | **Note**: The information below is intended for readers in the SWAT staff. |\n## Table of Contents\n\n* [1. Overview](#OVERVIEW)\n* [2. Correlated errors](#CORRELATED_ERRORS)\n* [3. Common \"PostgresDB Errors\"](#POSTGRES_LOG)\n* [4. Common \"App Logs Errors\"](#APPNODE_LOG)\n* [5. Common \"SNOW API Errors\"](#SNOWAPI_LOG)\n* [6. Additional Resources](#ADDITIONAL_RESOURCES)\n\n1. Overview {#OVERVIEW}\n=======================\n\nThe purpose of this article is to provide information about the common errors found by the \"[Tembo Splunk Dashboard - Error View](.\/kb?id=kb_article_view&sysparm_article=KB1553837)\".\n\n2. Correlated errors {#CORRELATED_ERRORS}\n=========================================\n\nHere are a list of known errors that are correlated across the different logs:\n\n# |                            PostgresDB Errors                            | App Logs Errors |                                                                 SNOW API Errors                                                                 |                          Remarks                           |\n|-------------------------------------------------------------------------|-----------------|-------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------|\n| aborting backup due to backend exiting before pg_backup_stop was called |                 | \/glide\/snc-provision\/puppet\/modules\/backup\/lib\/backup\/backup-store.rb:\\<XXX\\>:in \\`\\<FUNCTION\\>': Backup not found: \\<FILENAME\\> (RuntimeError) | [PRB1709902](.\/problem.do?sysparm_query=number=PRB1709902) |\n|                                                                         |                 |                                                                                                                                                 |                                                            |\n3. Common \"PostgresDB Errors\" {#POSTGRES_LOG}\n\nHere are a list of common errors listed on the [\"PGS DB errors\" section on the Tembo Splunk Dashboard - Error View](.\/kb?id=kb_article_view&sysparm_article=KB1553837#POSTGRES_LOG).\n\nThe following sets of data  can also be retrieved by using the Splunk search via the [\"Research \\& Development\" section for \"PostgresDB Errors\"](.\/kb?id=kb_article_view&sysparm_article=KB1554168#POSTGRES_LOG)\n\n# |                                                          Error_message                                                           |      User      |                                                                                                                                                                 Remarks                                                                                                                                                                 |\n|----------------------------------------------------------------------------------------------------------------------------------|----------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| canceling statement due to lock timeout                                                                                          |                | Could be related to: [DEF0425255](https:\/\/buildtools1.service-now.com\/x_snc_defect_defect.do?sys_id=843a786293043910e3e7b0627cba1053) \/ [PRB1691462](.\/problem.do?sysparm_query=number=PRB1691462) If it happens after AHA Transfer and via \"autovacuum worker\"  it could be [PRB1713550](.\/problem.do?sysparm_query=number=PRB1713550) |\n| you don't own a lock of type ExclusiveLock                                                                                       |                | Likely related to: [DEF0425255](https:\/\/buildtools1.service-now.com\/x_snc_defect_defect.do?sys_id=843a786293043910e3e7b0627cba1053) \/ [PRB1691462](.\/problem.do?sysparm_query=number=PRB1691462) If it happens after AHA Transfer and via \"autovacuum worker\"  it could be [PRB1713550](.\/problem.do?sysparm_query=number=PRB1713550)   |\n| no pg_hba.conf entry for host \"\\[local\\]\"  user \"root\"  database \"root\"  no encryption                                           | root           |                                                                                                                                                                                                                                                                                                                                         |\n| no pg_hba.conf entry for host \"\\[local\\]\"  user \"dbi_backup\"  database \"postgres\"  no encryption                                 | dbi_backup     |                                                                                                                                                                                                                                                                                                                                         |\n| current transaction is aborted  commands ignored until end of transaction block                                                  |                |                                                                                                                                                                                                                                                                                                                                         |\n| duplicate key value violates unique con OR duplicate key value violates unique constraint \"XXX\"                                  |                |                                                                                                                                                                                                                                                                                                                                         |\n| canceling statement due to user request                                                                                          |                |                                                                                                                                                                                                                                                                                                                                         |\n| deadlock detected                                                                                                                |                |                                                                                                                                                                                                                                                                                                                                         |\n| column sys_user2.roles does not exist at character 8                                                                             |                | [PRB1711568](.\/problem.do?sysparm_query=number=PRB1711568)                                                                                                                                                                                                                                                                              |\n| aborting backup due to backend exiting before pg_backup_stop was called                                                          |                | [PRB1709902](.\/problem.do?sysparm_query=number=PRB1709902)                                                                                                                                                                                                                                                                              |\n| recovery is not in progress                                                                                                      | dbi_monitoring | [PRB1710675](.\/problem.do?sysparm_query=number=PRB1710675)                                                                                                                                                                                                                                                                              |\n| function pg_terminate_backend() does not exist at character 38                                                                   | dbi_monitoring | [PRB1711771](.\/problem.do?sysparm_query=number=PRB1711771)                                                                                                                                                                                                                                                                              |\n| Function swarm64da.to_decimal_mariadb() is deprecated. Remove this function call as pgNOW can perform the conversion implicitly. |                | [PRB1610441](.\/problem.do?sysparm_query=number=PRB1610441)                                                                                                                                                                                                                                                                              |\n| out of memory                                                                                                                    |                | **If** be=\"autovacuum worker\" then it is [PRB1711773](.\/problem.do?sysparm_query=number=PRB1711773)                                                                                                                                                                                                                                     |\n|                                                                                                                                  |                |                                                                                                                                                                                                                                                                                                                                         |\n4. Common \"App Logs Errors\" {#APPNODE_LOG}\n\nHere are a list of common errors listed on the [\"App Logs Errors\" section on the Tembo Splunk Dashboard - Error View](.\/kb?id=kb_article_view&sysparm_article=KB1553837#APPNODE_LOG).\n\nThe following sets of data  can also be retrieved by using the Splunk search via the [\"Research \\& Development\" section for \"App Logs Errors\"](.\/kb?id=kb_article_view&sysparm_article=KB1554168#APPNODE_LOG)\n\n# |                                            Error_message                                             |                                                                                                                                                                                    Remarks                                                                                                                                                                                     |\n|------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Cannot map object com.glide.rest.util.RESTRuntimeException                                           | **Note:** Only happens on buildtools1backend315 with tx_pattern_hash=-1243197801 via \/api\/x_snc_devops_auto\/github_trigger **ToDo** : Open incident\/case.                                                                                                                                                                                                                      |\n| Exception during batch statement commit to glide                                                     | ERROR: duplicate key value violates unique constraint \"\\<INDEX_NAME\\>\" **If** the SQL is \"INSERT INTO ire_mutex\"  then it is **Expected**.                                                                                                                                                                                                                                     |\n| The column index is out of range                                                                     |                                                                                                                                                                                                                                                                                                                                                                                |\n| Replica pool (postgresql_\\<HOST\\>_\\<PORT\\>) query execution failed org.postgresql.util.PSQLException | ERROR: canceling statement due to user request **To Check \\& Fix** : 1) May have some slow queries to review.                                                                                                                                                                                                                                                                  |\n| Replica pool (postgresql_\\<HOST\\>_\\<PORT\\>) query execution failed com.glide.db.GlideSQLException    | FAILED TRYING TO EXECUTE ON CONNECTION **To Check \\& Fix** : 1) \"Syntax Error or Access Rule Violation detected by database\" 2) \"General Data Exception detected by database\"                                                                                                                                                                                                  |\n| Unable to check autocommit state of the connection \\<NUMBER\\> org.postgresql.util.PSQLException      | This connection has been closed. **If** StackTrace goes through \"ConnectionWrapper.java:494\" ==\\> it is an **expected** fix for [DEF0280841](https:\/\/buildtools1.service-now.com\/x_snc_defect_defect.do?sys_id=d2657b541bca09907a36db56dc4bcb58) \/ [PRB1559725](.\/problem.do?sysparm_query=number=PRB1559725) \/ [KB1113084](.\/kb?id=kb_article_view&sysparm_article=KB1113084) |\n|                                                                                                      |                                                                                                                                                                                                                                                                                                                                                                                |\n5. Common \"SNOW API Errors\" {#SNOWAPI_LOG}\n\nHere are a list of common errors listed on the [\"SNOW API Errors\" section on the Tembo Splunk Dashboard - Error View](.\/kb?id=kb_article_view&sysparm_article=KB1553837#SNOWAPI_LOG).\n\nThe following sets of data  can also be retrieved by using the Splunk search via the [\"Research \\& Development\" section for \"SNOW API Errors\"](.\/kb?id=kb_article_view&sysparm_article=KB1554168#SNOWAPI_LOG)\n\n# |                                                                                     Error_message                                                                                      |    User    |                                                                                                                                                                                                                                                                                                                                                                                                                         Remarks                                                                                                                                                                                                                                                                                                                                                                                                                          |\n|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| ERROR: relation \"information_schema.\n\ninnodb_lock_waits\" does not exist                                                                                                                  | snccid     |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n| ERROR: syntax error at or near \".\"                                                                                                                                                     | snccid     | [PRB1710273](.\/problem.do?sysparm_query=number=PRB1710273)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n| ERROR: column \"table_rows\" does not exist                                                                                                                                              | snccid     | [PRB1710528](.\/problem.do?sysparm_query=number=PRB1710528)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n| relation \"sn_app_ism_m5r_check_result\" does not exist                                                                                                                                  | snccid     | The \"sn_app_ism_m5r_check_result\" is \"sn_app_ism_monitor_check_result\". It is part of \"ISM Monitoring\" (sn_app_ism_monitor) plugin. There is an ongoing program to rollout this plugin. This error only highlight that the specific instance  has yet to get the plugin. **Action** : just ignore. [PRB1620748](.\/problem.do?sysparm_query=number=PRB1620748) \/ [KB1226357](.\/kb?id=kb_article_view&sysparm_article=KB1226357) - ISM Rollout for Internal Instances. Rollouts are in phases: [CHG48880066](.\/change_request.do?sysparm_query=number=CHG48880066) \/ [CHG48518975](.\/change_request.do?sysparm_query=number=CHG48518975) \/ [CHG48691062](.\/change_request.do?sysparm_query=number=CHG48691062) \/ [CHG47023323](.\/change_request.do?sysparm_query=number=CHG47023323) \/ [CHG47023298](.\/change_request.do?sysparm_query=number=CHG47023298) |\n| \/glide\/snc-provision\/puppet\/modules\/backup\/lib\/backup\/backup-store.rb:227:in \\`close': Backup not found: \\<FILENAME\\> (RuntimeError)                                                   | dbi_backup | [PRB1709902](.\/problem.do?sysparm_query=number=PRB1709902)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n| \/glide\/snc-provision\/puppet\/modules\/backup\/lib\/backup\/backup-store.rb:\\<XXX\\>:in \\`validate': Backup not found: \\<BACKUP_FILE_NAME\\> (RuntimeError)                                    | dbi_backup | [PRB1711501](.\/problem.do?sysparm_query=number=PRB1711501) (duplicate of [PRB1709902](.\/problem.do?sysparm_query=number=PRB1709902))                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n| \/glide\/snc-provision\/puppet\/modules\/backup\/lib\/backup\/backup.rb:1130:in \\`close': Backup \\<BACKUP_FILE_NAME\\> truncated: reserved size (5G) reached (RuntimeError)                     | dbi_backup | [PRB1711502](.\/problem.do?sysparm_query=number=PRB1711502)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n| \/glide\/snc-provision\/.rbenv\/linux-2.6-libc-2.5-x86_64\/versions\/2.5.1\/lib\/ruby\/gems\/2.5.0\/gems\/system_sumo-1.0.12\/lib\/system_sumo.rb:401:in \\`join': execution expired (Timeout::Error) | dbi_backup | [PRB1711499](.\/problem.do?sysparm_query=number=PRB1711499)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n|                                                                                                                                                                                        |            |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n6. Additional Resources {#ADDITIONAL_RESOURCES}\n\n* [KB1553837](.\/kb?id=kb_article_view&sysparm_article=KB1553837) - Reference \\| Tembo Splunk Dashboard - Error View\n* [KB1554168](.\/kb?id=kb_article_view&sysparm_article=KB1554168) - Reference \\| Analysis with \"Tembo Splunk Dashboard - Error View\"\n* [KB1558863](.\/kb?id=kb_article_view&sysparm_article=KB1558863) - Reference \\| Common Errors from \"Tembo Splunk Dashboard - Error View\"  \n* [List of Problems](https:\/\/support.servicenow.com\/problem_list.do?sysparm_query=u_tags%3D%23Tembo_Splunk_Dashboard&sysparm_view=) created from this \"[Tembo Splunk Dashboard - Error View](.\/kb?id=kb_article_view&sysparm_article=KB1553837)\" project\n\nRevision Log... **(Last updated: 20-Dec-2023)**\n\n| Version |  Published  | Summary of Changes |\n|---------|-------------|--------------------|\n| 1.0     | 20-Dec-2023 | Initial version    |\n\n\n## NowTableService - model<Model: Decodable>(_ type: Model.Type  with sysId: SysID? = nil  from tableNa\n\n\nThe following code example shows how to call this method.\n\n## NowTableService - updateRecord(sysId: SysID  in tableName: String  withfields: \\[FieldName: FieldValue\\]  writeOptions: FieldWriteOptions? = nil  configuration: FieldReadConfiguration? = nil) async throws {#ariaid-title29}\n\nUpdates the specified record with the specified fields.\nYou can decode the data into a custom [Codable model](https:\/\/developer.apple.com\/documentation\/foundation\/archives_and_serialization\/encoding_and_decoding_custom_types)  or you can use the [NowTableService - paginator\\<Model: Decodable\\>(from tableName: String  path: String = Constants.resultPath  coder: Coder = .default  configuration: FetchConfiguration? = nil)](NowTableServiceiOSAPI.html#NTblServ-paginator_S_S_S_O \"Creates a paginator that enables the iteration of pages of decoded model(s) that handle nesting.\") function. Alternatively  you can use the convenience function convertToRecords() to transform data into a NowRecord object. The following shows how to convert a publisher to emit NowRecords:\n\nNote: All fields within a record may not be available for update. For example  fields that have a prefix of `sys_` are typically system parameters that are automatically generated and cannot be updated. Fields that are not specified and not auto-generated by the system are set to the associated data type's null value.\n{#d281684e9782}\n\n|     Name      |                                                                                                               Type                                                                                                               |                                                                                                                  Description                                                                                                                  |\n|---------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| sysId         | String                                                                                                                                                                                                                           | Sys_id of the record to return from the ServiceNow instance.                                                ...",
        "QUERY": " 'Syntax error or Access Rule violation detected by database'",
        "GROUND_TRUTH": "no answer found"
    },
    {
        "id": 97,
        "CONTEXT": "\"## Troubleshooting Guide | Alert connection on mms.smartmessagingsuite.com | Xymon\n\nRevision Log... **(Last updated 28-Jun-2022)**\n\n| Version |  Published  |                           Summary of Changes                            |\n|---------|-------------|-------------------------------------------------------------------------|\n| 1.0     | 3-Mar-2020  | Initial version                                                         |\n| 2.0     | 28-Jun-2022 | Updated the **Investigation Steps** section with additional validation. |\n# Alert Details\n\n|---------------------------|------------------------------------------------------------------------------|\n| **Name:**                 | Xymon:Alert connection on mms.smartmessagingsuite.com                        |\n| **Signature:**            | mms.smartmessagingsuite.com is not OK                                        |\n| **Source:**               | Xymon                                                                        |\n| **Description:**          | mms.smartmessagingsuite.com process is not running or port is not listening  |\n| **Thresholds:**           | N\/A                                                                          |\n| **Clear Condition:**      | Process is running \/ port is listening.                                      |\n| **Causes:**               | MMS Smartmessagingsuite service is stopped  unreachable  misconfigured  etc. |\n| **Impact:**               | AT\\&T customers will not receive SMS or MMS.                                 |\n| **Automation Available:** | N\/A                                                                          |\n**Additional Alert Details:**\n\n* Verify using the SRE Board [https:\/\/sre01.lhr1.service-now.com\/sre_board\/](https:\/\/sre01.lhr1.service-now.com\/sre_board\/) that there are no notes or changes ongoing.\n\n# Triage Steps\n\n1. Check the connection between relay servers to AT\\&T Smart Messaging Suite on Port 25. Xymon has the list of our [relay servers](https:\/\/monitor.service-now.com\/xymon\/mail-relay-servers \"relay servers\") to use for testing.\n2. Try to determine if it is networking-related from our end (connect from your local laptop to AT\\&T Smart Messaging Suite from COPR network).\n   **Note:** Mac users may need to install telnet first  as it is no longer standard in MacOS X (brew install telnet).\n\n*![](sys_attachment.do?sys_id=33603d7e1b044510c552c8031d4bcb67)*\n\n3. If you determine the cause is from the AT\\&T end  then you need to create a support ticket to AT\\&T EOD.\n\n# Investigation Steps\n\n1. Log into [AT\\&T Smart Messaging Suite](https:\/\/na1.smartmessagingsuite.com\/sei\/login.do?function=authenticate&customerId=90 \"AT&T Smart Messaging Suite\") and check the relay servers\/IPs listed in the queue  and verify for all IPs listed. (The user and password is listed in WPS under ATT MMS Gateway.) and also ensure **Authenticate by remote hosts** enabled.  \n   URL: [https:\/\/na1.smartmessagingsuite.com\/](https:\/\/na1.smartmessagingsuite.com\/)  \n   Click on **Company Setup** --\\>**Email API Preference** .\n\n2. Log in to the AT\\&T Smart Messaging Suite and check the messages queue.\n   **Note:** If the messages are not going through  then you might check networking on our side.\n3. Log in to the Smart Messaging Suite:  \n   1. Go to **Reports** \\> **Message Report** .  \n   2. In the column selection  deselect **Message** .  \n      **![](\/sys_attachment.do?sys_id=b3b2f9fa1b444510c552c8031d4bcb24)**\n   3. In the **Search Options**   select **timeframe** and select **User** \\> **All Users** .  \n   4. Click **Preview** .\n      **Note:** This is to verify whether or not traffic is working.\n\n# Resolution Steps\n\n* If it is a networking issue  work with the Network team to resolve it.\n* If the issue is on AT\\&T  then work with AT\\&T. Open a ticket with AT\\&T (the user and password will be in DCPS). See [KB0786303](\/kb_view.do?sysparm_article=KB0786303 \"KB0786303\") for further instructions.  \n  URL: [https:\/\/eod.wireless.att.com\/login.aspx](https:\/\/eod.wireless.att.com\/login.aspx)\n* Alternatively  a support ticket can be opened here:  \n  [https:\/\/help.sopranodesign.com\/hc\/en-us\/requests\/new](https:\/\/help.sopranodesign.com\/hc\/en-us\/requests\/new \"https:\/\/help.sopranodesign.com\/hc\/en-us\/requests\/new\")\n\n# Escalation\n\n|----------------------|---------------------------------------------------------------------------------------------------------|\n| **Alert Owner:**     | Monitoring_Dev                                                                                          |\n| **Technical Owner:** | [SRE](.\/kb_view.do?sysparm_article=KB0754260 \"SRE\") \/ [SA](.\/kb_view.do?sysparm_article=KB0754253 \"SA\") |\n| **Urgency:**         | Immediate                                                                                               |\n# Additional Resources\n\nClick the links below to read related articles and other resources.\n\n|                              Name                               |                             Description                             |\n|-----------------------------------------------------------------|---------------------------------------------------------------------|\n| [KB0753031](.\/kb_view.do?sysparm_article=KB0753031 \"KB0753031\") | Technical Service Overview \\| ATT Global SmartMessagingSuite (GSMS) |\n\n\n## Virtual Agent and actionable notification issues (ServiceNow for Microsoft 365 app)\n\nBelow given all issues are for BR based actionable notifications:\n\nIssue: Virtual Agent bot not responding or actionable notification not received to the user.\nIn most of cases bot is not configured properly or bot was not configured at all by customer. Please type 'Hi' in chat tab of \"ServiceNow for Teams app\" or \"ServiceNow for Microsoft 365\" if bot is not responding it means bot is not working then please follow below steps.\nPossible Resolution:\n\nMostly redoing the bot configuration and linking and unlinking the user will solve the issue.\nBelow is the links to configure Virtual Agent and Custom bot respectively customer can choose based on their requirement.\n[https:\/\/docs.servicenow.com\/csh?context=CSHelp:VirtualAgent-Messaging-Installation](https:\/\/docs.servicenow.com\/csh?context=CSHelp:VirtualAgent-Messaging-Installation)\n[https:\/\/docs.servicenow.com\/bundle\/tokyo-servicenow-platform\/page\/administer\/virtual-agent\/task\/teams-install-](https:\/\/docs.servicenow.com\/bundle\/tokyo-servicenow-platform\/page\/administer\/virtual-agent\/task\/teams-install-custom-app.html)[custom-app.html](https:\/\/docs.servicenow.com\/bundle\/tokyo-servicenow-platform\/page\/administer\/virtual-agent\/task\/teams-install-custom-app.html)\n[https:\/\/docs.servicenow.com\/bundle\/tokyo-servicenow-platform\/page\/administer\/virtual-agent\/concept\/va-integ-](https:\/\/docs.servicenow.com\/bundle\/tokyo-servicenow-platform\/page\/administer\/virtual-agent\/concept\/va-integ-single-teams.html)[single-teams.html](https:\/\/docs.servicenow.com\/bundle\/tokyo-servicenow-platform\/page\/administer\/virtual-agent\/concept\/va-integ-single-teams.html)\nPlease verify ServiceNow for Teams and Virtual Agent bot both are configured with same Microsoft tenant account.\n\nExample: CSTASK631634  CSTASK589726\nNote: If redoing bot configuration not solving problem please contact dev-ci-channels team as they are the owner of Virtual agent product.\n\n#### Issue: Duplicate actionable notification or notification is not received by User in Microsoft Teams\n\n* Please check if VA chat bot is working properly by typing 'Hi' in ServiceNow for Teams App chat tab if it's replying it means bot is working. Otherwise please configure VA chat bot by following ServiceNow document.\n* This kind of issue is mostly related to configuration issue. VA teams has provided KB article to do clean-up of setup. After clean up customer can redo configuration and regenerate manifest and upload it Microsoft Teams one more time. This should solve problem most of the times.\n* Unlink and link back user who is getting duplicate notifications.\n\n[https:\/\/buildtools1.service-now.com\/now\/nav\/ui\/classic\/params\/target\/kb_view.do%3Fsys_kb_id%3D189d67c11bf588d4d955639fbd4bcb9f](https:\/\/buildtools1.service-now.com\/now\/nav\/ui\/classic\/params\/target\/kb_view.do%3Fsys_kb_id%3D189d67c11bf588d4d955639fbd4bcb9f)\n[https:\/\/buildtools1.service-now.com\/now\/nav\/ui\/classic\/params\/target\/kb_view.do%3Fsys_kb_id%3Dd9f5b34fdbac941070b8c3d23996191d](https:\/\/buildtools1.service-now.com\/now\/nav\/ui\/classic\/params\/target\/kb_view.do%3Fsys_kb_id%3Dd9f5b34fdbac941070b8c3d23996191d)\n\nExample: CSTASK631634  CSTASK545294\n\n#### Issue: Actionable notifications is not being triggered\n\nIn most of cases Virtual Agent or custom bot is not configured or user is not linked properly.\nPlease check Business Rule 'MS teams: approval notification' is active.\nPlease check VA bot configuration.\nIf VA configuration is correct please link unlink user sometimes user linking is not correct.\n\nExample: [CSTASK589726](https:\/\/support.servicenow.com\/sn_customerservice_task.do?sys_id=95cef9a1479fe15011eaf24c736d4390&sysparm_record_list=assigned_to%3d3bf0083edbb620d03fa35583ca9619c5%5eORassigned_to%3dd0c48709db9620582e6a2183ca9619a2%5eORassigned_to%3d34cc3258dbdca344d3c92926ca961955%5eORassigned_to%3d5ce979e71b6cd95004cfc8031d4bcb69%5edescriptionDOES+NOT+CONTAINOutlook%5eORDERBYDESCsys_created_on&sysparm_record_row=54&sysparm_record_rows=363&sysparm_record_target=sn_customerservice_task&sysparm_view=case&sysparm_view_forced=true)  [CSTASK581200](https:\/\/support.servicenow.com\/sn_customerservice_task.do?sys_id=a5b0c1d9934fa150def533527cba10cf&sysparm_record_list=assigned_to%3d3bf0083edbb620d03fa35583ca9619c5%5eORassigned_to%3dd0c48709db9620582e6a2183ca9619a2%5eORassigned_to%3d34cc3258dbdca344d3c92926ca961955%5eORassigned_to%3d5ce979e71b6cd95004cfc8031d4bcb69%5edescriptionDOES+NOT+CONTAINOutlook%5eORDERBYDESCsys_created_on&sysparm_record_row=61&sysparm_record_rows=363&sysparm_record_target=sn_customerservice_task&sysparm_view=case&sysparm_view_forced=true)  [CSTASK589726](https:\/\/support.servicenow.com\/sn_customerservice_task.do?sys_id=95cef9a1479fe15011eaf24c736d4390&sysparm_record_list=assigned_to%3d3bf0083edbb620d03fa35583ca9619c5%5eORassigned_to%3dd0c48709db9620582e6a2183ca9619a2%5eORassigned_to%3d34cc3258dbdca344d3c92926ca961955%5eORassigned_to%3d5ce979e71b6cd95004cfc8031d4bcb69%5edescriptionDOES+NOT+CONTAINOutlook%5eORDERBYDESCsys_created_on&sysparm_record_row=54&sysparm_record_rows=363&sysparm_record_target=sn_customerservice_task&sysparm_view=case&sysparm_view_forced=true)  [CSTASK556578](https:\/\/support.servicenow.com\/sn_customerservice_task.do?sys_id=2a946f21c3d6e99084d3f41d050131e7&sysparm_record_list=assigned_to%3d3bf0083edbb620d03fa35583ca9619c5%5eORassigned_to%3dd0c48709db9620582e6a2183ca9619a2%5eORassigned_to%3d34cc3258dbdca344d3c92926ca961955%5eORassigned_to%3d5ce979e71b6cd95004cfc8031d4bcb69%5edescriptionDOES+NOT+CONTAINOutlook%5eORDERBYDESCsys_created_on&sysparm_record_row=78&sysparm_record_rows=363&sysparm_record_target=sn_customerservice_task&sysparm_view=case&sysparm_view_forced=true)  [CSTASK548999](https:\/\/support.servicenow.com\/sn_customerservice_task.do?sys_id=9373e07ddb0ae554e3a1c4be13961905&sysparm_record_list=assigned_to%3d3bf0083edbb620d03fa35583ca9619c5%5eORassigned_to%3dd0c48709db9620582e6a2183ca9619a2%5eORassigned_to%3d34cc3258dbdca344d3c92926ca961955%5eORassigned_to%3d5ce979e71b6cd95004cfc8031d4bcb69%5edescriptionDOES+NOT+CONTAINOutlook%5eORDERBYDESCsys_created_on&sysparm_record_row=83&sysparm_record_rows=363&sysparm_record_target=sn_customerservice_task&sysparm_view=case&sysparm_view_forced=true)  [CSTASK548913](https:\/\/support.servicenow.com\/sn_customerservice_task.do?sys_id=deaf6fa11b0ea5d018e935df034bcbea&sysparm_record_list=assigned_to%3d3bf0083edbb620d03fa35583ca9619c5%5eORassigned_to%3dd0c48709db9620582e6a2183ca9619a2%5eORassigned_to%3d34cc3258dbdca344d3c92926ca961955%5eORassigned_to%3d5ce979e71b6cd95004cfc8031d4bcb69%5edescriptionDOES+NOT+CONTAINOutlook%5eORDERBYDESCsys_created_on&sysparm_record_row=84&sysparm_record_rows=363&sysparm_record_target=sn_customerservice_task&sysparm_view=case&sysparm_view_forced=true)\n\n#### Issue: Is customisation possible in BR based VA bot Actionable notifications\n\n* Customisation is not possible of BR based Actionable Notifications. All the script includes are read only.\n* Please suggest customer to use VA based Actionable notifications if they want customisation.\n\n#### Issue: VA bot Actionable notifications stuck at send reply\n\nIn most of cases send reply Business Rule is disabled or customer has some customisation on ServiceNow side in actionable script include files or Business Rules.\n\n* Please verify if OOTB 'MS teams: approval notification update' Business Rule enabled.\n* Please verify customer customisation on ServiceNow side it could be BR or Adaptive Card related script include e.g.  \n  MSTeamsNotificationBuilderUtil.\n* Please check system logs for any error and debug on that line.\n\nExample: CSTASK603115\n\n#### Issue: VA bot Actionable notifications customer wants to navigate to different portal link from \"View Details\"\n\nPlease ask customer to change their default portal. BR based adaptive card links in actionable notifications built using default portal. So whichever portal will be default portal it will redirect to that portal.\nExample: CSTASK530776\n\nIssue: VA bot actionable notifications Approval  Reject buttons are not visible\n\nPlease check if customer has Glide Virtual Agent plugin installed. Actionable notifications needs licensing for Glide Virtual Agent.\n\nPlease check for any customisation done by customer on OOTB ServiceNow script include related to actionable notification. If yes please ask to revert to OOTB then test. If it works fine with OOTB files then please ask to check their customisation.\n\n\n\n## SMS Sent using Twilio doesn't get delivered and\/or message status stays undelivered | Notify Plugin\n\nSome customers may see their messages are no longer getting delivered without any changes made to the Notify\/Twilio module on their ServiceNow instances.\n\nThis could be happening because of the recent mandates for A2P 10DLC registration for sending SMSs to the US.\n\nFor more details on this mandate refer - [https:\/\/www.twilio.com\/blog\/a2p-10dlc-registration](https:\/\/www.twilio.com\/blog\/a2p-10dlc-registration)\n\nIf you want to determine if your messages are being blocked for this very reason follow below steps\n\n1. Login to Twilio Console ([https:\/\/www.twilio.com\/console](https:\/\/www.twilio.com\/console)).\n2. Ensure you choose the Sub-account connected to the instance where you are seeing SMSs being undelivered.\n3. Navigate to Monitor -\\> Logs -\\> Messaging.\n4. Try to locate the SMS record corresponding to ones sent on the instance.\n5. Verify the status to determine the reason for failure.\n\nIf the status reflects the missing A2P 10 DLC registration error  you need to follow the steps listed in above document from Twilio to get yourself compliant.\n\nIf the message shows as delivered on the Twilio console  it could mean that ServiceNow instance might not be able to receive callback from Twilio but the message was indeed delivered to the recipient. In order to verify this do the following.\n\n1. On the ServiceNow Instance navigate to Notify \\> Twilio Direct Configuration\n2. Ensure you are on the \"Twilio Direct Driver\" scope\n3. Click on Test Callbacks UI Action\n4. Use a known Number to send out an SMS\/Call to verify if the instance is able to receive callbacks.\n\nIf the instance is not receiving callbacks  use the below steps to determine if you have an IP restriction in place\n\n1. On the ServiceNow Instance navigate to System Security \\> IP Address Access Controls.\n2. Filter with type as Deny.\n3. Check if there is a Deny entry present with IP range\n\nIf there is an Deny entry it means your instance is not allowed to be accessed via Public Internet. This is the reason your message statuses are not getting updated on ServiceNow as Twilio updates are not making it to ServiceNow.\n\n\"\n\n",
        "QUERY": " 'SMS notifications not getting to ATT users'",
        "GROUND_TRUTH": "If AT&T users are not receiving SMS notifications, it could be due to issues with the mms.smartmessagingsuite.com process not running or the port not listening. The first step in troubleshooting this issue is to check the connection between relay servers to AT&T Smart Messaging Suite on Port 25. If it's determined that the issue is networking-related from our end, you should try to connect from your local laptop to AT&T Smart Messaging Suite from the COPR network. \n\nIf the cause is from the AT&T end, you need to create a support ticket to AT&T EOD. You can also log into AT&T Smart Messaging Suite and check the relay servers\/IPs listed in the queue and verify for all IPs listed. \n\nIf it's a networking issue, work with the Network team to resolve it. If the issue is on AT&T's end, then work with AT&T. Open a ticket with AT&T (the user and password will be in DCPS)."
    },
    {
        "id": 98,
        "CONTEXT": "\"## Scheduled job \"SAM - Subscription Maintenance\" can't run properly in SAM Foundation because SAMPRese\n\n## Description\n\nScheduled job \"SAM - Subscription Maintenance\" can't run properly in SAM Foundation so that it fails to update entitlement status\n\n## Steps to Reproduce\n\nGo to the scheduled job \"SAM - Subscription Maintenance\". Click \"Execute Now\".\n\n## Go to System logs and observe that there is warning as below:\n\norg.mozilla.javascript.EcmaError: \"SAMPReserveEntitlementUtil\" is not defined.\nCaused by error in \\<refname\\> at line 10\n\n## 7: }\n\n8:\n9: function sampUpdateStateStatus() {\n==\\> 10: SAMPReserveEntitlementUtil.processExpiredReserveEntitlements();\n11:\n12: var today = gs.daysAgo(0);\n13: var grEnt = new GlideRecord('alm_license');\n\n## Workaround\n\nComment out the line below under the function sampUpdateStateStatus() of the \"SAM - Subscription Maintenance\" Scheduled Script Execution:\n\nSAMPReserveEntitlementUtil.processExpiredReserveEntitlements();\n\n**Related Problem: PRB1475316**\n\n\n\n## Changes with Software Asset Management Foundations plugin activation\n\nOnce you activate the plugin Software Asset Management Foundations  you may see below visible\/flow changes.\n\n**Plugin** : Software Asset Management Foundation (**ID**: com.snc.sams)\n\n**Dependent Plugins**:\n\n* Software Asset Management Core (com.snc.sam.core)\n* Normalization Data Services Client (com.glide.data_services_canonicalization.client)\n* GlideQuery (com.sn_glidequery)\n\n**Other Plugins Activated\/Updated**:\n\n* Asset Management (com.snc.asset_management)\n* Asset Management Workspace (com.sn_itam_workspace)\n* CMDB Page Templates (sn_cmdb_pg_templts)\n* Configuration Management (CMDB) (com.snc.cmdb)\n* Model Management (com.snc.model)\n* Natural Language Query (com.snc.nlq)\n\n**Summary of changes**:\n\n**Company Records** : The core_company table will be have normalized company records added and its related normalized mappings from ServiceNow content. Once you complete the guided set-up you will be able to see only the unique companies (with normalized=TRUE) and its alias names as Normalized mappings. Please find more details of functionality as [Normalization Data Services](https:\/\/docs.servicenow.com\/bundle\/vancouver-platform-administration\/page\/administer\/normalization\/concept\/c_NormalizationOverview.html \"Normalization Data Services\").\n\n**Application\/Module(s)**: As part of the above plugin you see new applications called Software Asset  Normalization Data Services applications and its related modules are added\/updated. (These labels might have updated if you are already using Legacy SAM application)\n\n**Tables**: You will see tables like Software Entitlement (alm_license)  Device Allocations (alm_entitlement_asset)   User Allocations (alm_entitlement_user)  Software Installations (cmdb_sam_sw_install) are updated. These tables are labeled differently in Legacy SAM plugin. Once SAMF\/P is added these are renamed and the functionality also changed. If you have the legacy-SAM active already and SAMF is activated you will see the changes as below. Else these are all new additions.{#c_SAMMigrationSAMF__table_kn3_h25_lz__entry__3}\n\n|       **Table**       |  **Change**  | **Previous Label** |     **New Label**      |\n|-----------------------|--------------|--------------------|------------------------|\n| alm_license           | Label Update | Software License   | Software Entitlement   |\n| alm_entitlement_user  | Label Update | User Entitlement   | User Allocations       |\n| alm_entitlement_asset | Label Update | Device Entitlement | Device Allocations     |\n| cmdb_sam_sw_install   | New Table    | --                 | Software Installations |\n{#c_SAMMigrationSAMF__table_kn3_h25_lz}\n\n**Dictionary\/Column changes**:\n\n* Inference mandatory field - For software models that have suite components (to bundle software models)  the Inference mandatory field value in the Software Suite \\[cmdb_m2m_suite_model\\] table is transferred to a new Mandatory field\n* Rights field - The Software Entitlements (formerly Software Licenses) Rights field value in the License Entitlements \\[alm_entitlement\\] table is transferred to a new Purchased rights field  and name changed from Rights to Active rights\n* The Software model field for a software entitlement allocation (Software Entitlement \\[alm_license\\] table) is automatically set to the software model on the entitlement (License Entitlements \\[alm_entitlement\\] table)\n* The quantity for a software entitlement allocation (License Entitlements \\[alm_entitlement\\] table) is set to 1 unless there are multiple allocations.\n* If there are multiple software entitlement allocations for the same user or device  the allocations are aggregated into one record  the quantity is set to the count of aggregated records  and duplicate allocations are not allowed.\n\n**Forms\/Views**:\n\n* Software Models  Entitlements (formerly Software License)  Discovery Models  and Software Installations form and list layouts are modified.\n* The field manufacturer on Software model is labeled as Publisher.\n* Some of the fields are marked as mandatory and some UI policies  client scripts added on software model or software entitlement to address certain use cases like license metric  metric type  quantity etc.\n\n**Functionality**:\n\n* If you do not have any SAM related plugins earlier  then you might encounter the changes related to softwares as below:\n  1. The software used to updated to cmdb_software_instance table. So after this plugin you may see some data missing from cmdb_software_instance OR the latest data not being updated to cmdb_software_instance. Reason for this is with any SAM related plugin the target table for softwares is changed from cmdb_software_instance to targeted to cmdb_sam_sw_install. This applies to all OOB applications like discovery  SCCM  other SG integrations.\n  2. After the SAM is installed and if you would like to have all the softwares from older table (cmdb_software_instance - Software Instance) to new table (cmdb_sam_sw_install - Software Installations) then you can use the OOB [Migrate software installations process](https:\/\/docs.servicenow.com\/bundle\/vancouver-it-asset-management\/page\/product\/software-asset-management2\/task\/t_MigrateSWInstalls.html \"Migrate software installations process\").\n* The license counters from legacy-SAM are disabled as we have new functionality called reconciliation introduced from SAM-F\/SAMP.\n* Legacy-SAM used to match the discovery models with related software models and populate the same. This is now deactivated.\n\n**Customizations**:\n\n* Any customizations to SAMF or its dependent plugins are skipped during the SAMF\/SAMP activation. These customizations are supposed to be reviewed and strongly recommended to revert to OOB (***if you wish to use SAM functionality*** ). Please [refer](https:\/\/support.servicenow.com\/kb?id=kb_article_view&sysparm_article=KB0726511)\n\n**List of file additions\/updates**:\n\n\n\n## [Vancouver] Software Asset Management Professional (com.snc.samp) Microsoft Reconciliation Process F\n\n## Description\n\nPost-upgrade to the Vancouver release  some instances with the **Software Asset Management Professional** (com.snc.samp) plugin have experienced failures or stalling of the reconciliation process (recon stuck in progress). When this occurs  the following error messages may appear in the 'Reconciliation Progress Summary ? Reconciliation Progress Details' related list:\n\n1. TypeError: Cannot read property \"sys_id\" from undefined. Please review the stack trace error line  ensuring that the failure involves either the \"hostsToInstallTypes\" or \"vmsToInstallTypes\" object.  \n2. Logs reveal that the 'SAM - Software License Reconciliation Worker' is stuck. The stack trace suggests issues with the script include 'SamNewPerCoreWithCalForMicrosoftLicenseCalculator'.  \n3. JavaException: java.lang.StackOverflowError.  \n\nThis reconciliation failure is specifically linked to \"Microsoft Per-Core\" and \"Microsoft Per-Core (with CAL)\" license metrics.\n\n## Steps to Reproduce\n\nOn a Vancouver instance with \"Microsoft Per-Core\" and\/or \"Microsoft Per-Core (with CAL)\" entitlements  run reconciliation for Microsoft publisher and observe the symptom(s) described in the previous section.\n\n## Workaround\n\nThis problem has been fixed. If you are able to upgrade  review the \"Fixed In\" section to determine the latest version with a permanent fix that your instance can be upgraded to.\n\nIf you are encountering this issue on an affected version of Vancouver  but you are unable to upgrade to a fixed version at this time  kill the ongoing \"SAM - Software License Reconciliation Worker\"  then apply the update set attached to this Known Error article. Ensure that all related script includes are marked with \"replace_on_upgrade = true\" (in the sys_update_xml table) after committing the update set. This will prevent future conflicts with OOB script includes when upgrading to a fixed version.\n\n**FAQ**\n\n**Q:** How can I confirm if my instance is impacted by this issue?\n**A:** If you have the com.snc.samp plugin installed  and you have \"Microsoft per Core\" or \"Microsoft per Core (with CAL)\" entitlements  you can experience this issue when upgrading to an impacted version of Vancouver.\n\n**Q:** Will this affect non-prod instances  prod instances  or both?\n**A:** Both production and sub-production instances can be affected by this issue.\n\n**Q:** Can I implement the workaround myself?\n**A:** Yes. If you are on an impacted version of Vancouver and are experiencing this issue  please follow the steps detailed above and apply the update set attached to this article.\n\n**Q:** Once the workaround has been applied  do I need to do anything else?\n**A:** Once you've applied the update set  we recommend setting \"Replace on Upgrade\" to \"True\" for related script includes to prevent future conflicts when upgrading to a fixed version.\n\n**Q:** Will the solution cause any downtime?\n**A:** Implementing the workaround will not result in any downtime.\n\n**Q:** Which version(s) will contain the fix for this issue?\n**A:** This issue is fixed in Vancouver Patch 4 and above.\n\n**Related Problem: PRB1712246**\n\n\"\n\n",
        "QUERY": " 'SAM subscription maintenance failing to update entitlement status'",
        "GROUND_TRUTH": "The \"SAM - Subscription Maintenance\" scheduled job is failing to update the entitlement status in SAM Foundation due to an error in the script. The error message in the system logs indicates that \"SAMPReserveEntitlementUtil\" is not defined. This error is caused by a line in the function sampUpdateStateStatus() of the \"SAM - Subscription Maintenance\" Scheduled Script Execution. The workaround suggested in the article is to comment out the line \"SAMPReserveEntitlementUtil.processExpiredReserveEntitlements();\" under the function sampUpdateStateStatus(). This issue is related to Problem PRB1475316."
    },
    {
        "id": 99,
        "CONTEXT": "\"## Rubrik discovery - WindowsCluster CIs are populated into table Windows Server\n\n## Description\n\nDue to limitation of the Rubrik REST API call for \"Rubrik Hosts\" returning insufficient data for Windows Cluster CIs  these CIs were populated into Windows Server table (cmdb_ci_win_server) causing duplicate records  which already exists under cmdb_ci_cluster. The limitation is affecting also MSSQL Instances and MSSQL databases  associated with the Windows Cluster CIs in the Rubrik context.\n\nAs part of previous implementation  the Rubrik REST API for \"MSSQL Instances\" and \"MSSQL DBs\" is returning Instances and associated Data bases related to Windows Clusters. From other side the Rubrik REST API for \"WIndows Clusters\" is returning insufficient data for those CIs  which cannot be uniquely identified. The data for Windows Clusters is populated into the CMDB table for Windows Servers  which is leading to duplicate CIs in different CMDB tables (cmdb_ci_cluster and cmdb_ci_win_server).\n\n## Steps to Reproduce\n\n1. Configure Rubrik discovery schedule of type serverless. Make sure credentials of type \"Basic Auth\" are configured associated to the Rubrik cluster.  \n2. Execute discovery  make sure pattern \"Rubrik Windows Host (LP)\" is triggered and populating data into table \"cmdb_ci_win_server\".  \n3. Observe that in table \"cmdb_ci_win_server\" CIs with Operating system type \"WindowsCluster\" are populated.\n\n## Workaround\n\nDue to the issue described above  information for Windows Clusters is not retrieved and respectively they are not populated as CIs into the CMDB. Also MSSQL Instances and MSSQL Data bases  associated with Windows Clusters are filtered out from the discovery into the Rubrik context.\n\nThis issue is fixed as part of PRB1503346 \/\/ DEF0217774 related to plugin 'Discovery and Service Mapping Patterns' version 1.0.81\n\n**Related Problem: PRB1503346**\n\n\n\n## Manage Unclassed Hardware CIs\n\n* Is there an established relationship between the VR team and the CMDB team?\n\n{#_Toc43211776}**Remediation Plays**\n\n{#_Toc43211777}**Summary** The table below lists and summarizes each of the remediation plays in the playbook. Details are included later.\n\n|-------------------------------------|-------------------------|-------------------------------------------------------------------------------|\n| **Play Name**                       |                         |                                                                               |\n| Review your data                    | What this play is about | Review the data present in unclassed CI                                       |\n| Review your data                    | Required tasks          | List the CIs in Unclassed Hardware class                                      |\n| Fix Play: Update discovered items   | What this play is about | This play helps you analyze the discovered items and assign the correct class |\n| Fix Play: Update discovered items   | Required tasks          | List the discovered items and manually select the appropriate class           |\n| Fix Play -- Reclassify CIs manually | What this play is about | List the CIs which needs to be reclassified                                   |\n| Fix Play -- Reclassify CIs manually | Required tasks          | Reclassify identified CIs                                                     |\n| Fix Play -- Retire unused CIs       | What this play is about | Retire the unused CIs in Unclassed hardware items class                       |\n| Fix Play -- Retire unused CIs       | Required tasks          | Create and run the schedule job                                               |\n| Data Governance                     | What this play is about | Monitor and maintain CIs in Unclassed hardware items class                    |\n| Data Governance                     | Required tasks          | Review the data and fix appropriately                                         |\n{#_Toc43211778}**Play 1 - Review your data**\n\n**What this Play is about**\n\nThis play helps in listing all the CIs in Unclassed Hardware\n\n**Required tasks**\n\n1. On the Navigator type cmdb_ci_unclassed_hardware.list. This will list the unclassified hardware CIs\n2. Group by Discovery Source to help identify from where these CIs originated\n3. Work iteratively with CMDB team to ensure that CMDB has the CIs for which VIs are getting discovered. Fix discovery issues like \n   1. Discovery credential issues\n   2. Issues related to discovery IP ranges\n   3. Availability of MID Servers for all the target servers\n\n{#_Toc43211779}**Play 2 -- Update discovered items to the correct class**\n\n**What this Play is about**\n\nThis play helps you analyze the discovered items and assign the correct class\n\nRepeat this process until all you have left is items that are in the \"it will never match\" category.\n\n**Required tasks**\n\n1. Adjust the columns on the Discovered Items module to also include: Host tag  NetBIOS  Operating system  DNS Name  Fully Qualified domain name\n2. Now  group by Operating system. Investigate this list by looking at that OSs with the largest count down to the smallest.\n3. Look to solve the biggest patterns first  to navigation search box  type cmdb_ci_unclassed_hardware.list. to display the CIs\n4. Ensure that the Class field is displayed in the list.\n5. If you do not see this attribute  personalize the list to add the Class field.\n6. Double-click the Class value for the CI  and select a new class.\n7. Click the green check box to confirm your selection.\n\n{#_Toc43211780}**Play 3 - Fix Play Reclassify CIs manually**\n\n**What this Play is about**\n\nReclassify CIs manually\n\n**Required tasks**\n\n1. Locate the CI that you want to reclassify and display it in a list view.\n2. You can use the application navigator. Or for example  if the CI is a server  then in the navigation search box  type cmdb_ci_unclassed_hardware.list. to display the CIs\n3. Ensure that the Class field is displayed in the list.\n4. If you do not see this attribute  personalize the list to add the Class field.\n5. Double-click the Class value for the CI  and select a new class.\n6. Click the green check box to confirm your selection.\n\n**Play 4 - Fix Play : Fix Play : Retire Unmatched CIs**\n\n**What this Play is about**\n\nCreate a scheduled job that looks at all non \"retired\" Unclassed Hardware items to see if they are still in use. What I mean is  I would see if there is a reference to the Unclassed Hardware item in the Discovered Items module. If there is not a reference in the Discovered Items module to that CI  then set that CI Status to \"Retired\".\n\n**Required tasks**\n\n1. Download \\& upload the [xml file](sys_attachment.do?sys_id=607b9fa01bf39190acdc54e56b4bcb7c)\n2. Go to scheduled job and review the job \"Retire UnMatched CIs\"\n3. Make the job active\n\n{#_Toc43211781}**Data Governance**\n\n**What this Play is about**\n\nMonitor and maintain data in Unclassed hardware\n\n**Required tasks**\n\n1. Regularly execute play 1 to review CIs in Unclassed Hardware\n2. \n\n## PRB with: Rubrik Node (LP) pattern  step: 22. Populate cmdb_ci_rubrik_node value for \"source_native_\n\n## Description\n\nRubrik Node (LP) pattern  step: 22. Populate cmdb_ci_rubrik_node\nvalue for \"source_native_key\"\ncurrently it's: \"rubrik_node_\" + $cluster_nodes\\[\\].node_id + \"_\" + \"cmdb_ci_rubrik_cluster\\[1\\].cluster_id\"  \nthis should be rectified as  \n\"rubrik_node_\" + $cluster_nodes\\[\\].node_id + \"_\" + $cmdb_ci_rubrik_cluster\\[1\\].cluster_id\n\n## Steps to Reproduce\n\nCheck: Rubrik Node (LP) pattern  step: 22. Populate cmdb_ci_rubrik_node\nvalue for \"source_native_key\"\n\n## Workaround\n\nOpen pattern: Rubrik Node (LP)\n\nIn step: 22. Populate cmdb_ci_rubrik_node  locate value for \"source_native_key\"\n\nchange:\n\"rubrik_node_\" + $cluster_nodes\\[\\].node_id + \"_\" + \"cmdb_ci_rubrik_cluster\\[1\\].cluster_id\"\n\nto:\n\"rubrik_node_\" + $cluster_nodes\\[\\].node_id + \"_\" + $cmdb_ci_rubrik_cluster\\[1\\].cluster_id\n\n**Related Problem: PRB1632205**\n\n\"\n\n",
        "QUERY": " 'Rubrik LP returns inadequate data for the Server CIs'",
        "GROUND_TRUTH": "The Rubrik REST API call for \"Rubrik Hosts\" has a limitation where it returns insufficient data for Windows Cluster CIs. This issue results in these CIs being populated into the Windows Server table (cmdb_ci_win_server), causing duplicate records that already exist under cmdb_ci_cluster. This limitation also affects MSSQL Instances and MSSQL databases associated with the Windows Cluster CIs in the Rubrik context. \n\nTo reproduce this issue, you need to configure the Rubrik discovery schedule of type serverless with credentials of type \"Basic Auth\" associated with the Rubrik cluster. Then, execute discovery and ensure that the pattern \"Rubrik Windows Host (LP)\" is triggered and populating data into the table \"cmdb_ci_win_server\". You will observe that in the table \"cmdb_ci_win_server\", CIs with Operating system type \"WindowsCluster\" are populated.\n\nThe workaround for this issue is that the information for Windows Clusters is not retrieved and they are not populated as CIs into the CMDB. Also, MSSQL Instances and MSSQL Databases associated with Windows Clusters are filtered out from the discovery into the Rubrik context. This issue has been fixed as part of PRB1503346 \/\/ DEF0217774 related to the plugin 'Discovery and Service Mapping Patterns' version 1.0.81."
    },
    {
        "id": 100,
        "CONTEXT": "\"## Work Instruction | How to extend an instance\n\nRevision Log... **(Last updated: 21-Nov-2022)**\n\n| Version |  Published  | Summary of Changes |\n|---------|-------------|--------------------|\n| 1.0     | 21-Nov-2022 | Initial version    |\n|---------------------------------------------------------------------------------|\n--------------------------------------------------------------------------------|\n| ![Warning](sys_attachment.do?sys_id=ce445c92db3d9f00fac7f4621f96191c \"Warning\") | **Warning**: The work instructions below are for use by SRE-DevOps staff only. |\n# Overview\n\nThis article details the steps required to perform an **extend** (the life span) of an instance that is about to be retired. The extension defers the date that the instance will be retired further into the future.\n\n# Risks \\& Cautions\n\n* Continually extending instance retirements holds onto **storage** that might be needed by other customers and instances.\n* The number of extensions that a customer can request depends on the instance type  instance purpose  user category  and user sub-category and is controlled by the [Instance Expiration Defaults](https:\/\/datacenter.service-now.com\/nav_to.do?uri=%2Fu_expiration_defaults_list.do) table. For example  the owner of a Marketing Dedicated Sub-production instance can request up to 1 000 extensions.\n* If a customer reaches the limit of extensions  they'll see a message like this:  \n  SRE DevOps however can still extend the life span of that instance using the Instructions below; this is when you would use the **Override Extension Rules** checkbox.\n\n\n# Instructions\n\nFollow these steps:\n\n1. Log in to [datacenter](https:\/\/datacenter.service-now.com\/).\n2. Go to the [Service Catalog](https:\/\/datacenter.service-now.com\/nav_to.do?uri=%2Fcatalog_home.do%3Fsysparm_view%3Dcatalog_default).\n3. Go to the **Instances** section and click on the **View all items** link.\n4. Scroll down to **Request Extension of Instance Expiration Date** and click on it.\n5. The fields in the following form appear:\n   1. Fill in the **Instance** name.\n   2. A **Change Request** is not required.\n   3. **More information** should be added to this step about when and under what circumstances you would need to check the **Override Expiration Rules** checkbox.\n\n# Verification Steps\n\nN\/A\n\n# Rollback\n\nN\/A\n\n# Escalation {#ESCALATION}\n\nN\/A\n\n# Additional Resources\n\nN\/A\n\n\n\n## Reactivate suspended temp instance\n\nIf we use the catalog item Request Extension of Instance Expiration Date for a recently suspended temp or employee instance  the instance will be removed from the suspended state and back to normal.\n\nLink to the catalog item:\n\n[https:\/\/support.servicenow.com\/com.glideapp.servicecatalog_cat_item_view.do?sysparm_id=569f65f36f9a5100160ef92aea3ee492](https:\/\/support.servicenow.com\/com.glideapp.servicecatalog_cat_item_view.do?sysparm_id=569f65f36f9a5100160ef92aea3ee492)\n\n\n\n## Request an extension for a deferred remediation task in Application Vulnerability Response  \n\n# Request an extension for a deferred remediation task in Application Vulnerability Response {#ariaid-title1}\n\nRequest an extension for a deferred remediation task (VUL) before it reaches its deferred until due date. As a remediation owner  you're no longer required to wait until the deferred due date to make this request.\nRole required: sn_vul.vulnerability_admin or sn_vul.admin (deprecated)\nYou can also request an extension from the Vulnerability Response Workspaces. For more information  see .\n\n1. Navigate to All \\> Application Vulnerability Response \\> Remediation Tasks.\n2. Open a deferred remediation task.\n3. Select Request Extension.\n4. Fill in the fields on the form  as appropriate.  \n   {#d262774e121}\n   |         Field          |                                                                                                   Description                                                                                                   |\n   |------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n   | Extend Until           | Select the date when the Defer state expires and the remediation task is reactivated.                                                                                                                           |\n   | Reason                 | Enter the reason for deferring the issue. Choices include: * Awaiting maintenance window * Fix unavailable * Risk accepted * Mitigating control in place * Other {#extend-defer-app-vuln-group__ul_r55_vb1_xdb} |\n   | Additional information | Enter any other relevant information.                                                                                                                                                                           |\n   {#extend-defer-app-vuln-group__table_s52_yvn_r6}\n5. Select Request Approval.  \n   On approval of the request  the changes on the remediation task are rolled down to the application vulnerable items as well  which are part of the remediation task. However  the application vulnerable items should be in the deferred state and their current until data precedes the updated until date. An email is also triggered on submission of the request and subsequent action by the approver.  \n   Note:\n   * The request goes through two levels of approval.\n   * The Request Extension button is unavailable for remediation tasks that are created as part of an exception rule.\n   * If a deferred remediation task is extended again  the extend deferral count increases in the backend.\n   {#extend-defer-app-vuln-group__ul_dk2_tn1_1yb}\n\n\"\n\n",
        "QUERY": " 'request extension of instance expiration date'",
        "GROUND_TRUTH": "To request an extension of an instance expiration date, follow these steps:\n\n1. Log in to the datacenter at https:\/\/datacenter.service-now.com\/.\n2. Navigate to the Service Catalog at https:\/\/datacenter.service-now.com\/nav_to.do?uri=%2Fcatalog_home.do%3Fsysparm_view%3Dcatalog_default.\n3. Go to the Instances section and click on the View all items link.\n4. Scroll down to Request Extension of Instance Expiration Date and click on it.\n5. Fill in the form that appears:\n   - Fill in the Instance name.\n   - A Change Request is not required.\n   - More information should be added to this step about when and under what circumstances you would need to check the Override Expiration Rules checkbox.\n\nPlease note that continually extending instance retirements holds onto storage that might be needed by other customers and instances. The number of extensions that a customer can request depends on the instance type, instance purpose, user category, and user sub-category and is controlled by the Instance Expiration Defaults table."
    },
    {
        "id": 101,
        "CONTEXT": "\"## Metadata Consumer job might not receive new metadata messages when initializing a new consumer set.\n\n## Description\n\nThis issues can occurs when a consumer instance create a new consumer set. Producer instance approve the consumer subscription. However  the consumer instance never gets the approval from the producer instance.\n\nChecking idr_replication_log on consumer  IDRMetadataConsumer job is not receiving any metadata records.\ni.e.\n\nsource = IDRMetadataConsumerJob - system\nmessage = finished: IDRMetadataConsumerJob  IDRConsumerJob#metadata: 0 \n\nThe reason for metadata consumer job not receiving any metadata is due to the fact that we have 7 days retention policy for metadata topics. If for 7 days  an instance hasn't received any metadata message  kafka would lose their offset for their metadata topic. Now if after 7 days  they try to do a poll on kafka  since their metadata offset is null  poll won't return any messages.\n\n## Steps to Reproduce\n\n1) have two instances that have active sets but haven't create\/updates their sets for 7 days.  \n2) Create a new producer replication set on one instance  \n3) create the corresponding consumer set   \n4) When producer sends approval  consumer will not receive it\n\n## Workaround\n\nThe following scripts will work with instances on Orlando or later.\n\nNavigate to the idr_replication_log table and add \"Created\" to list view.\n\nUse the filter shown below to get the logs for metadata:\n*Source* is \"IDRMetadataConsumerJob\"\n*Message* does not contain \"IDRConsumerJob#metadata: 0\"\nThen order by Created time (z-a)\n\nIf 0 metadata value and 'created' less than 4 days old; no need to resolve\nif non-zero metadata and 'created' less than 4 days old; no need to resolve\n\nif non-zero metadata and 'Created' more than 4 days old; run scripts below review the 'offset' and update accordingly.\n\n#### Check the current offset:\n\nvar target = Packages.com.glide.idr.cluster.ClusterInfoProvider.getInstanceId();\nvar topicAddress = Packages.com.glide.idr.commons.Constants.getReadTopicAddress(target);\nvar topicClass = new Packages.com.glide.mq.clients.api.Topic(topicAddress);\ntry {\nvar provider = function () { return new Packages.com.glide.idr.protocol.deserialization.IDRV0Deserializer()}; \u00a0 \u00a0\nvar job = \"IDRMetadataConsumerJob\";\n\n\/* Note: 'getConsumer' function does take a different number of parameters based on the release version.\nPlease confirm the instance release version and change the code to use the correct format of the 'getConsumer' function call.*\/\n\n\/\/This is for pre-Rome\nvar consumer = new Packages.com.glide.mq.clients.api.MQClientManager.getConsumer(job  provider);\n\n\/\/This is for Rome\n\/\/var consumer = new Packages.com.glide.mq.clients.api.MQClientManager.getConsumer(job  provider  \"\"  \"\");\n\n\/\/This is for San Diego\n\/\/var consumer = new Packages.com.glide.mq.clients.api.MQClientManager.getConsumer(job  null  provider  \"\"  \"\");\n\n} finally {\nconsumer.close();\n}\n**If \"*Current Offset*\" from the above script returns as NULL:**\n\nDisable the IDRMetadataConsumer job\n\n#### **Update offset**\n\n\/\/NOTE: Do not alter the script as it does alter the offset value and can cause further issues if the incorrect value is set.\n\nvar target = Packages.com.glide.idr.cluster.ClusterInfoProvider.getInstanceId();\nvar topicAddress = Packages.com.glide.idr.commons.Constants.getReadTopicAddress(target);\nvar topicClass = new Packages.com.glide.mq.clients.api.Topic(topicAddress);\ntry {\nvar provider = function () { return new Packages.com.glide.idr.protocol.deserialization.IDRV0Deserializer()};\nvar job = \"IDRMetadataConsumerJob\";\n\n\/* Note: 'getConsumer' function does take a different number of parameters based on the release version.\nPlease confirm the instance release version and change the code to use the correct format of the 'getConsumer' function call.*\/\n\n\/\/This is for pre-Rome\nvar consumer = new Packages.com.glide.mq.clients.api.MQClientManager.getConsumer(job  provider);\n\n\/\/This is for Rome\n\/\/var consumer = new Packages.com.glide.mq.clients.api.MQClientManager.getConsumer(job  provider  \"\"  \"\");\n\n\/\/This is for San Diego\n\/\/var consumer = new Packages.com.glide.mq.clients.api.MQClientManager.getConsumer(job  null  provider  \"\"  \"\");\n\nconsumer.subscribe(\ntopicClass\n);\nvar endOffset = consumer.endOffset(topicClass); \/\/leaving the print statement for debugging\ngs.print(\"End Offset: \" + endOffset); consumer.subscribe(\ntopicClass\n);\nconsumer.poll();\nif (null === endOffset || endOffset.getRawOffset() == 0) {\nvar offset = new Packages.com.glide.mq.clients.api.Offset(topicClass  0);\nconsumer.seek(topicClass  offset);\n} else {\nvar newOffset = new Packages.com.glide.mq.clients.api.Offset(topicClass  endOffset.getRawOffset() - 1);\nconsumer.seek(topicClass  newOffset);\n}\nconsumer.poll();\nconsumer.commit();\n} finally {\nconsumer.close();\n}\nThen Re-run the 1st script to validate the offset\n\nIf the \"Current Offset\" is no longer NULL re-activate the IDRMetadataConsumer job.\n\n**Related Problem: PRB1433180**\n\n\n\n## Service Bridge Onboarding Troubleshooting\n\n## Table of Contents\n\n* [Service Bridge Onboarding Overview](#Onboarding_Overview)\n* [Service Bridge Registration Error](#Contact_Provider_Error)\n* [Service Bridge Key Exchange Request Failed](#Failed_Exchange)\n* [Service Bridge \"Connect to Provider\" not responding](#Connect_to_Provider)\n* [Service Bridge Mismatch Version Support](#Mismatch_Version)\n\n# Service Bridge Onboarding Overview {#Onboarding_Overview}\n\n# Service Bridge Registration Error {#Contact_Provider_Error}\n\nWhen you are onboarding a new Service Bridge Consumer and you get an error message of \"Please contact provider for support\" on the Providers Service Bridge Registration page after clicking the \"Connect to Provider\" on the consumer verify the follow possible causes for a possible solution.\n\n**Versions**\n\nService Bridge Version 1.0.4 or greater\n\n**Provider Instance**\n\n1. Ensure that you are logged into the provider instance as the Contact user on the Registration task.\n2. Verify the hidden inputs on the Registration page has all the values seen below populated and that those values are valid and correct.\n\n# Service Bridge Key Exchange Request Failed {#Failed_Exchange}\n\nWhen you are onboarding a new Service Bridge Provider or Consumer and you get an error message of \"Key Exchange Request failed sending to source instance for approval\" try and follow the below steps until you are able to successfully complete the Key exchange.\n\n**Versions**\n\nService Bridge Version 1.0.4 or greater\n\n**Provider Instance**\n\nThere is a \"Retry Key Exchange Request\" UI Action on the Registration task. If that continues to fail you can follow the below steps to complete the Resource Exchange Request manually.\n\n1. Navigate to All \\> Key Management \\> Resource Exchange Requests \\> New\n2. On the form  fill in the fields\n   * Exchange Frequency: Adhoc\n   * Source Instance Sys Id (found on consumer's instance stats.do)\n   * Source Instance Host\n     * Example:testnode1788-node1.service-now.com\n   * Crypto Specifications: com_snc_platform_security_oauth_glideencrypter\n3. Click Submit\n\n**Consumer Instance**\n\nThere is a \"Retry Key Exchange Request\" UI Action on the Provider connection record. If that continues to fail you can follow the below steps to complete the Resource Exchange Request manually.\n\n1. Navigate to All \\> Key Management \\> Resource Exchange Requests \\> New\n2. On the form  fill in the fields\n   * Exchange Frequency: Adhoc\n   * Source Instance Sys Id (found on provider's instance stats.do)\n   * Source Instance Host\n     * Example:testnode1788-node1.service-now.com\n   * Crypto Specifications: com_snc_platform_security_oauth_glideencrypter\n3. Click Submit\n\n# Service Bridge \"Connect to Provider\" not responding {#Connect_to_Provider}\n\nWhen you are onboarding a new Service Bridge Consumer and the \"Connect to Provider\" UI Action on the Consumer's Provider connection record not responding  verify the OAuth Client record was created on submit of the Provider connection record with the following values:\n\n* Table: Application Registries (oauth_entity)\n* Name: \\<Provider connection record number\\>\n* Redirect URL: \\<Provider URL\\>\/oauth_redirect.do\n\nIf the record does not exist run the following script in Background scripts as Admin user and replace \\<Provider connection number\\> with the Number and \\<Provider URL\\> with the Providers URL found on the Provider connection record on the consumer:\n\n# Service Bridge Mismatch Version Support {#Mismatch_Version}\n\nWhen you are onboarding a new Service Bridge Provider or Consumer that has Version 1.0.4 or 1.0.5 installed to an Instance that is running Service Bridge Version 1.0.2 you now need to ensure KMF Key Exchange is done on Version 1.0.2 or Onboarding cannot exchange the encrypted resources required to complete the onboarding process.\n\n**Versions**\n\nService Bridge Provider Version 1.0.2\n\nService Bridge Consumer Version 1.0.2\n\n**Consumer Running Service Bridge Version 1.0.2**\n\n* After you create a new Provider Connection record STOP and follow the below steps before clicking \"Connect to Provider\" UI Action.\n  1. Navigate to All \\> Key Management \\> Resource Exchange Requests \\> New\n  2. On the form  fill in the fields\n     * Exchange Frequency: Adhoc\n     * Source Instance Sys Id (found on provider's instance stats.do)\n     * Source Instance Host\n       * Example:testnode1788-node1.service-now.com\n     * Crypto Specifications: com_snc_platform_security_oauth_glideencrypter\n  3. Click Submit\n  4. From the list search for the Resource Exchange Request record with values:\n     * (**If there is more than one record you only need to Approve\/Update 1 Request)**\n     * Status: Pending Approval\n     * Crypto Module: com_snc_platform_security_oauth_glideencrypter\n     * Target Instance Host: URl for your Provider Instance\n  5. Set the Status from Pending Approval to Request Approved\n  6. Click the **Update Request** button  **Do not click the \"Update\" button**\n* Install the attached Update Set [KB1584604 Workaround for Consumer.xml](sys_attachment.do?sys_id=b6d8fa7097577550d4743dae2153af19)\n  1. Navigate to All \\> System Update Sets \\> Retrieved Update Sets \\> Import XML\n  2. Preview the Update Set and Commit\n* Install the attached Update Set [KB1584604 Cleanup for Consumer.xml](sys_attachment.do?sys_id=72d8fa7097577550d4743dae2153af16)\n  1. Navigate to All \\> System Update Sets \\> Retrieved Update Sets \\> Import XML\n  2. \n\nPreview the Update Set and Commit\n* Return back to the Provider Connection Record and click the \"Connect to Provider\"\n\n**Provider** **Running Service Bridge Version 1.0.2**\n\n* After you create a new Registration task record for new Consumer **STOP** and follow the below steps before clicking the connection URL link or providing it to your consumer.\n  1. Navigate to All \\> Key Management \\> Resource Exchange Requests \\> New\n  2. On the form  fill in the fields\n     * Exchange Frequency: Adhoc\n     * Source Instance Sys Id (found on consumer's instance stats.do)\n     * Source Instance Host\n       * Example:testnode1788-node1.service-now.com\n     * Crypto Specifications: com_snc_platform_security_oauth_glideencrypter\n  3. Click Submit\n* Install the attached Update Set\n  1. Navigate to All \\> System Update Sets \\> Retrieved Update Sets \\> Import XML\n  2. Preview the Update Set and Commit\n* Return to the Registration task you need to copy the connection URL link and add the following to the end of the URL:\n  * %26instance_id%3D\\<Your Instance Id found on your stats.do page\\>\n  * Ex: [http:\/\/localhost:8081\/nav_to.do?uri=sn_sb_con_provider_connection.do?sys_id%3D-1%26sysparm_query%3Durl%3Dhttps%3A%2F%2Fempressprovider.service-now.com%2F%26instance_id%3D2fd978d5db23c5d8b847ac44d49619ac](http:\/\/localhost:8081\/nav_to.do?uri=sn_sb_con_provider_connection.do?sys_id%3D-1%26sysparm_query%3Durl%3Dhttps%3A%2F%2Fempressprovider.service-now.com%2F%26instance_id%3D2fd978d5db23c5d8b847ac44d49619ac)\n* Once the Consumer creates a new Provider Connection record in their instance with connection URL you provided a Resource Key Exchange Request will be set to you.\n* **Before** the Consumer clicks on the \"Connect to Provider\" you need to Approve their KMF Resource Key Exchange request they sent to you.\n  1. Navigate to All \\> Key Management \\> Resource Exchange Requests\n  2. From the list search for the Resource Exchange Request record with values:\n     * (**If there is more than one record you only need to Approve\/Update 1 Request)**\n     * Status: Pending Approval\n     * Crypto Module: com_snc_platform_security_oauth_glideencrypter\n     * Target Instance Host: URl for your Provider Instance\n  3. Set the Status from Pending Approval to Request Approved\n  4. Click the **Update Request** button  **Do not click the \"Update\" button**\n* Now the Consumer can \"Connect to Provider\" from their instance\n\n\"\n\n",
        "QUERY": " 'producer instance not approving a new consumer instance'",
        "GROUND_TRUTH": "The issue of a producer instance not approving a new consumer instance occurs when a consumer instance creates a new consumer set. The producer instance approves the consumer subscription, but the consumer instance never receives the approval from the producer instance. This issue can be reproduced by having two instances that have active sets but haven't created or updated their sets for 7 days. Then, create a new producer replication set on one instance and the corresponding consumer set on the other. When the producer sends approval, the consumer will not receive it.\n\nThe reason for this issue is that there is a 7-day retention policy for metadata topics. If an instance hasn't received any metadata message for 7 days, Kafka would lose their offset for their metadata topic. Now, if after 7 days they try to do a poll on Kafka, since their metadata offset is null, the poll won't return any messages.\n\nTo resolve this issue, navigate to the idr_replication_log table and add \"Created\" to list view. Use the filter to get the logs for metadata. If the 'created' is less than 4 days old and the metadata value is 0 or non-zero, there is no need to resolve. If the 'created' is more than 4 days old and the metadata value is non-zero, run the provided scripts to review the 'offset' and update accordingly. If the \"Current Offset\" from the script returns as NULL, disable the IDRMetadataConsumer job, update the offset using the provided script, and then re-activate the IDRMetadataConsumer job."
    },
    {
        "id": 102,
        "CONTEXT": "\"## can't save \"variable types\" in Questions of Catalog Builder template when system language is Japanes\n\n## Description\n\nCan't save \"variable types\" in Questions of Catalog Builder template when system language is Japanese  this work in English.\n\n## Steps to Reproduce\n\n1. install Japanese language plugin if not   \n2. open any existing template in catalog bulider  \n3. open any exising template from catalog bulider  \n4. click Questions  \n5. click the wheel icon in the middle of screen  \n6. move any variable type to left side collection then save( button in the right bottom)  \n7. reopen it found it's not saved\n\n## Workaround\n\nthe workaround is to change the setup of the catalog builder template in the English system language until this is fixed.\n\n**Related Problem: PRB1592304**\n\n\n\n## Insert and stay function ignore OOB business rule: Check for same name variables\n\n## Description\n\nInsert and stay function ignore OOB business rule: Check for same name variables\n\n## Steps to Reproduce\n\nThe issue is reproducible on Newer release Utah and Vancouver.\n\nHop any OOB instance on Utah or Vancouver --\\> sample empmisrafil1\nOpen any catalog item.\n\\>\\> add new variable and save\n\\>\\> add another variable with the same name like above step and save.\nyou notice you see an error: The catalog item xxxx has 1 other variable(s) with the same name associated with it. Keep the names of variables associated with a catalog item unique\nThis is due the Br: Check for same name variables\n\n\\>\\> Open recently new created variable  change only the question  make sure name is the same and use insert or insert and stay\nYou notice you are able to create a new variable with the same name.\n\nInvestigation Summary:\n\n- The issue was not observed in the Tokyo version.  \n- This behaviour is observed on Utah and Vancouver\n\n## Workaround\n\nWorkaround for the issue can be found in the attached update set (Workaround update set for PRB1708103.xml). Please commit the update set to fix the issue.\n\n**Related Problem: PRB1708103**\n\n\n\n## Vancouver Patch 2 Hot Fix 1: Known Errors\n\n Mobile Task button actions on WOT form page do not refresh the form once the Task has been accepted\/rejected                                                                 |\n| [KB1123826](\/kb_view.do?sysparm_article=KB1123826) | PRB1501129 | Software Asset Management Professional | \\[Accessibility\\] - License Usage - Focus is missing after clicking Run Reconciliation button from pop-up                                                                    |\n| [KB1274921](\/kb_view.do?sysparm_article=KB1274921) | PRB1504738 | Email                                  | Notification Backend - Duplicate push notification issue                                                                                                                     |\n| [KB0966640](\/kb_view.do?sysparm_article=KB0966640) | PRB1507712 | Agent Workspace                        | The calendar picker is picking invalid dates for today  yesterday  and all calendar left menu options.                                                                       |\n| [KB1005139](\/kb_view.do?sysparm_article=KB1005139) | PRB1539267 | PDF Generation                         | Korean letters are getting garbled when the PDF file is generated from a report                                                                                              |\n| [KB1005406](\/kb_view.do?sysparm_article=KB1005406) | PRB1541411 | Lists                                  | \\[Workspace WHC\\] Highlighted \/ selected \/ hover pages are indistinguishable in list pagination                                                                              |\n| [KB1166040](\/kb_view.do?sysparm_article=KB1166040) | PRB1572894 | Enterprise Portal                      | EVAM data source bypasses Query Business Rules on Project Workspace                                                                                                          |\n| [KB1198693](\/kb_view.do?sysparm_article=KB1198693) | PRB1601193 | Authentication                         | \\[Tokyo Partner Testing\\] Cannot register a new user account on Login page (Polaris view)                                                                                    |\n| [KB1501269](\/kb_view.do?sysparm_article=KB1501269) | PRB1609636 | UI Policy\/Client Script                | \\[UUU-Runtime-Performance\\]: \/api\/now\/ui\/polaris\/menu API call takes a long time to execute on first load if \"All\" menu is pinned and several application menus are expanded |\n| [KB1194889](\/kb_view.do?sysparm_article=KB1194889) | PRB1614390 | Knowledge Management                   | \\[Cherry Picked Utah R2\\] SFSTRY0050850: The CSS in a KB's article body isn't working                                                                                        |\n| [KB1220333](\/kb_view.do?sysparm_article=KB1220333) | PRB1634193 |                                        | In Tokyo  after using 'ClearValue' in a catalog client script on Service Portal  the value of a checkbox a returns a blank string                                            |\n| [KB1523063](\/kb_view.do?sysparm_article=KB1523063) | PRB1640897 |                                        | Translation prefixes are missing for Workspace menu                                                                                                                          |\n| [KB1273808](\/kb_view.do?sysparm_article=KB1273808) | PRB1641048 | Assessments                            | Field level ACL on core_company table blocks other apps from access                                                                                                          |\n| [KB1362321](\/kb_view.do?sysparm_article=KB1362321) | PRB1671906 |                                        | CMDB Data Management Task required approval even when with policy having \"Needs review\" is false                                                                             |\n| [KB1448786](\/kb_view.do?sysparm_article=KB1448786) | PRB1679796 |                                        | 'Discard draft' button is not enabled for 'catalog_builder_editor' and 'catalog_admin' personas                                                                              |\n| [KB1514475](\/kb_view.do?sysparm_article=KB1514475) | PRB1685124 |                                        | Activity Stream Mention (@user) does not work in comments and work notes on Workspace                                                                                        |\n| [KB1564044](\/kb_view.do?sysparm_article=KB1564044) | PRB1689640 |                                        | HR Case \"Description\" \\[description\\] contains escape code for special characters                                                                                            |\n| [KB1533141](\/kb_view.do?sysparm_article=KB1533141) | PRB1696950 |                                        | Flow Designer Script Editor auto completion\/suggestions no longer working in flow design scripting Flow Designer Script Editor ( fd_data  Dot walking )                      |\n| [KB1549188](\/kb_view.do?sysparm_article=KB1549188) | PRB1699994 |                                        | Playbook usage of DomainSupport.switchDomain doesn't use the closeable properly  which causes domain separation issues                                                       |\n| [KB1556696](\/kb_view.do?sysparm_article=KB1556696) | PRB1705462 |                                        | After Vancouver upgrade  emails are being sent but remain in the Outbox                                                                                                      |\n| [KB1556755](\/kb_view.do?sysparm_article=KB1556755) | PRB1708634 |                                        | MID Server Script Files get skipped in instance\/app upgrades  after Checksum values are automatically added                                                                  |\n| [KB0598187](\/kb_view.do?sysparm_article=KB0598187) | PRB668806  | Project Portfolio Management           | Cannot list edit related list if the Related List Loading option 'After Form Loads' or 'On-demand' is selected                                                               |\n\"\n\n",
        "QUERY": " 'Problem saving variable types in Catalog Builder Questions in Japanese'",
        "GROUND_TRUTH": "According to the KB12345 article, there is a known issue where \"variable types\" cannot be saved in Questions of Catalog Builder template when the system language is set to Japanese. This issue can be reproduced by following these steps:\n\n1. Install the Japanese language plugin if not already installed.\n2. Open any existing template in the catalog builder.\n3. Click on Questions.\n4. Click the wheel icon in the middle of the screen.\n5. Move any variable type to the left side collection then save (button in the right bottom).\n6. Upon reopening, you will find that it's not saved.\n\nThe workaround for this issue, until it is fixed, is to change the setup of the catalog builder template in the English system language."
    },
    {
        "id": 103,
        "CONTEXT": "\"## AIS system property may trigger full cache flush results in sever performance issues\n\n## Description\n\nThis is for the sys_properties \"glide.ais.ha.enable_auto_failover\" and \"glide.ais.event_queue_writable\" in relation to AI Search (AIS) node provisioning which can result in performance impact if certain conditions are met. This is due to a full cache flush being triggered when the AIS node is reprovisioned.\n\nFor context  with the use of AI Search (AIS)  one AIS node is provided per customer which is shared across all instances (irrespective if it is a subproduction or production instance).\n\nOn the AIS node  it contains dedicated AIS partitions for each instance. When a clone is initiated  when the capacity on the AIS node is not enough to provision a new AIS partition  it triggers a process called \"Drain and Upsize AIS node\" after the clone is complete. This occurs when the source instance is a larger size than the target instance on a clone.\n\nWhen \"Drain and Upsize AIS node\" occurs  we update the \"glide.ais.ha.enable_auto_failover\" and \"glide.ais.event_queue_writable\" sys_properties for all instances of a customer which has AIS partitions. This can even happen if a customer has a new instance and requests to provision AI search for a newly created instance.\n\nThe update in sys_properties after the \"Drain and Upsize AIS node\" process as part of the AIS provisioning initiates a full cache flush upon update on all instances. This full cache flush can impact performance on both the source  target  and other instances owned by the same Company.\n\nThe timing of the \"Drain and Upsize AIS node\" is important to note since it occurs after the clone. The reason is because of the size of the clone  as some clones can take more than two days. For example  if a clone is initiated over the weekend  the process to reprovision the AIS node can occur during core business hours if a clone takes the entirety of a weekend.\n\nIf a clone does take the entire weekend  the \"Drain and Upsize AIS node\" can occur during regular business hours where a full cache flush can occur when the sys_properties are updated on all instances which results in impact on a production instance in their core hours.\n\n## Steps to Reproduce\n\nIssue cannot be reproduced on-demand.\n\n## Workaround\n\nServiceNow will update the sys_properties ignore_cache= true for both \"glide.ais.ha.enable_auto_failover\" and \"glide.ais.event_queue_writable\" as a maintenance for identified instances that have AIS enabled.\n\n### FAQs\n\nQ: Can I reschedule or opt-out of this maintenance?\n\nA: Modifying this property is important to prevent future potential performance impact. We do not allow a reschedule  but if an opt-out exception is requested please let us know in the communication record opened. With the exception request  we require that the customer administrator run the script attached to this KB (PRB1578851 write audit script.txt) as soon as possible in scripts background under global scope on the instance mentioned in the communication record.\n\nQ: Will this maintenance have any service impact?\n\nA: Modifying the system properties will not have any service impact. This is a simple change to update both system properties. Node restart is NOT required during the maintenance.\n\nQ: What testing has been done before performing this maintenance on my instance?\n\nA: Testing has been done on internal instances for this maintenance.\n\nQ: After the clone  which instances will potentially be impacted?\n\nA: Any\/all of your instances can be affected by this issue  however production instances typically contain more live data where performance impact can be a bigger concern.\n\nQ: How are these two properties used?\n\nA:\n\n* \"glide.ais.ha.enable_auto_failover\" : This will control automatic failover to backup AIS service in case Primary AIS service goes down.\n* \"glide.ais.event_queue_writable\" : This is to control AIS events in the sysevents table.\n\nQ: How can I tell if the \"Drain and Upsize AIS node\" will initiate?\n\nA: This occurs on clones from instances with AIS partitions  or provisioning AI search (doesn't mattern if it's on a source or target instance for a clone).\n\n**Related Problem: PRB1578851**\n\n\n\n## An overview of Java Flight Recorder (JFR) events\n\nThe above output is sorted on 'Duration' so we immediately know that during the period the JVM was profiled the longest running business rule was:\n\n* Named 'Run SLAs' which triggers 'after' an operation (i.e. insert \/ update \/ delete) and executed for 1.644s\n* Was executed as part of a \/x_sapda_prod_secin_product_security_incidents.do transaction on thread Default-thread-13 with transaction number 733156\n* Allocated 103Mb of data in memory whilst it executed\n\nIn the above example as all 3 business rules shown had execution of \\>= 100ms they would have been considered 'slow' and  therefore  would have been logged as a 'Slow business rule' in localhost logs. Note  however  that the event contains additional information about the business rule which is not normally logged.\n\nIn addition to this the JFR records details of \\*all\\* business rules executed regardless of their duration -- this means we can see similar data for business rules which executed in microseconds:\n\nAgain we could see some of this data by enabling session debug within the instance however data in the JFR is arguably easier to read and can be collected even in situations where we have no access to the instance and \/ or we are unable to reproduce a performance issue for some reason.\n\nAnother significant benefit of these events is that they also allow the stack which triggered the event to be displayed. If we go back to 'Run SLAs' business rule and click on it at the bottom of JDK Mission Control we see output similar to the following:\n\nThis immediately tells us that:\n\n* The user clicked a UI action (sys_ui_action record with sys_id 42da42d00a0a0b340066377beb6dd099)\n* This triggered a GlideRecord() insert in to a table which then caused 'after' business rules against that table to trigger\n\nHistorically this information would only be available if you were to modify the business rule to throw a stack to the localhost log when it was executed however JFR events make this available with no need to make any changes to the underlying instance.\n\n**Filtering events**\n\nOne final feature which can be particularly powerful is the ability to filter events in various ways. Throughout this document we've been referring to a particular \/x_sapda_prod_secin_product_security_incidents.do transaction and know that this executed the 'Run SLAs' business rule but what if we wanted to see all of the business rules executed just by this transaction?\n\nBy default the event browser will show you all instances of a given type of event regardless of parent thread \/ transaction and so on. It might be possible to manually look through this list and concentrate on only the items you are interested in however when this involves many thousands of events this approach simply isn't practical. Instead we can filter such that the only rows displayed are those we are actually interested in.\n\nIn JDK Mission Control this is achieved by right clicking on an event and selecting 'Show filter'  i.e.:\n\nThis loads a new section above the list of events which displays 'Filter the table'. By right clicking in this section a custom filter can be created  i.e.\n\nThis populates the filter with the selected attribute:\n\nThis can then be modified as required (i.e. the '==' can be changed to a different operator and the value being searched for can be typed to the right).\n\nAs an example if we wanted to only display business rules where the name contains the string 'and' the filter could be constructed as follows which only displays the information we are interested in:\n\n**An example of using events to troubleshoot a performance issue**\n\nLets say that a performance issue is able to be reproduced and  using application node logs  we can see details of the corresponding transaction as follows:\n\n2023-11-16 05:31:24 (514) Default-thread-13 DB5B11611BAAFD102DEA628F7B4BCB17 txid=fb1d9da130af tx_pattern_hash=-743341645 EXCESSIVE \\*\\*\\* End #733156 \/x_sapda_prod_secin_product_security_incidents.do  user: I533959  impersonated by: james.ford@snc  total time: 0:00:10.177  processing time: 0:00:10.177  CPU time: 0:00:08.387  SQL time: 0:00:01.483 (count: 2 620)  business rule: 0:00:01.810 (count: 92)  ACL time: 0:00:02.613  UI Action time: 0:00:00.001  Script time: 0:00:02.989 (count: 365)  source: 148.139.0.228  type: form  origin scope: x_sapda_prod_secin  app scope: x_sapda_prod_secin\n\nAt this point we might want to investigate why the transaction spends \\~2.6s evaluating ACLs. Note  however  that:\n\n* The above output does not tell us how many ACLs were evaluated\n* By default the JVM will only log ACL evaluations which take \\>= 5ms\n\n\n\n## Troubleshooting an unavailable instance or monitoring an outage\n\nTroubleshooting an unavailable instance or monitoring an outage\n\nSymptoms\n\n* Unable to connect to the instance.\n* Low or out of memory alerts display.\n* Instance is slow or sometimes unavailable.\n\nCause\n\n* Network issues exist between the instance and the user.\n* A significant change occurred.\n* Scripts created an inefficient query in the database or are running full table scans on large tables.\n* Nested queries exist in MySQL server. Many nested queries are capable of bringing the MySQL database to a halt under certain data conditions.\n* The Java virtual machine (JVM) memory utilization level is high or memory heap is not big enough to store all data.\n* A large number of calls are made to the database  which causes high disk input\/output (I\/O) on the database server.\n* The central processing unit (CPU) load on the server is too high.\n* There is high transaction concurrency.\n\nResolution\n\nThis step-by-step article describes how to diagnose and troubleshoot common performance issues related to the ServiceNow instance. Troubleshooting performance issues involve the use of a series of steps to isolate and determine the cause in order to take corrective actions as necessary. In a highly technological and innovative environment  it is not uncommon to experience occasional performance issues. Therefor  it is important to learn how to proactively prevent or minimize the occurrence of problems and  when they occur  diagnose the cause and take the appropriate corrective actions to quickly resolve the issue.\n\nOutages or downtime refers to the time span when the instance fails to provide its primary function. Eliminating outages poses a significant challenge for IT organizations that need to upgrade or migrate environments running ServiceNow. This is particularly true for applications that must provide continuous or near-continuous operations to users who increasingly expect uninterrupted availability of online services. Any outage of an application or website  even if that outage is scheduled or planned  has an impact on the revenue and reputation of our customers. It is imperative that Service Now proactively troubleshooting customer related outages to ensure minimal downtime and faster solutions.\n\nThe purpose of this article is to determine the source and resolve a performance issue and is not intended to diagnose performance issues related to all databases on a particular server. The steps are ordered in the most appropriate sequence to isolate the issue and identify the proper resolution. Please do not skip a step.\n\n|--------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| ![Prerequisite](\/sys_attachment.do?sys_id=92f66a611b200990b09633f2cd4bcba0 \"Prerequisite\") | **Prerequisite** : Prior to completing the following steps  collecting the necessary information needed to troubleshoot the performance issue is high recommended. For more information  see [Gathering node data via stats.do and threads.do](\/kb_view.do?sysparm_article=KB0517269 \"Gathering node data via stats.do and threads.do\"). |\n**To troubleshoot slow performance experienced by all users on a all application:**\n\n1. Test the network connectivity to the instance and verify there are no existing issues. For more information  see [Managing network connectivity](\/kb_view.do?sysparm_article=KB0517267 \"Managing network conectivity\").\n1. Verify if a node needs to be restarted. For more information  see [Identifying if a node needs to be restarted](\/kb_view.do?sysparm_article=KB0517327 \"Identifying if a node needs to be restarted\").\n1. Verify there are no inefficient queries running against the database. For more information  see [Identifying an tuning inefficient queries executing in the database (MySQL)](\/kb_view.do?sysparm_article=KB0517270 \"Identifying an tuning inefficient queries executing in the database (MySQL)\").\n1. Verify the instance is not running out of JVM memory. For more information  see [Identifying abnormal JVM memory utilization](\/kb_view.do?sysparm_article=KB0517274 \"Identifying abnormal JVM memory utilization\").\n1. Verify the disk I\/O does not indicate overuse or high levels of saturation. For more information  see [Identifying high disk I\/O on the database server](\/kb_view.do?sysparm_article=KB0517275 \"Identifying high disk I\/O on the database server\").\n1. Verify there are sufficient CPU resources to satisfy demand. For more information  see [Identifying high CPU utilization on the server](\/kb_view.do?sysparm_article=KB0517276 \"Identifying high CPU utilization on the server\").\n1. Verify the instance is not experiencing high concurrency. For more information  see [Identifying high transaction concurrency](\/kb_view.do?sysparm_article=KB0517277 \"Identifying high transaction concurrency\").   \n\nAnother Cause\n\n* Sometimes the sys_properties table contains the wrong name after clone or rename.\n\nResolution\n\nCheck if this is the case and update it accordingly.\n\nmysql\\> SELECT value FROM sys_properties WHERE name=\"instance_name\";\nmysql\\> SELECT value FROM sys_properties WHERE name=\"instance_id\";\n\nRun following commands to update 'instance_name' and 'instance_id' (You can get instance_id from datacenter)\n\nmysql\\> UPDATE sys_properties SET value='**\\<instance id\\>**' WHERE name=\"instance_id\";\nmysql\\> UPDATE sys_properties SET value='**\\<instance name\\>**' WHERE name='instance_name';\n\n\"\n\n",
        "QUERY": " 'performance issue on production'",
        "GROUND_TRUTH": "The performance issue on production could be related to the AI Search (AIS) system properties \"glide.ais.ha.enable_auto_failover\" and \"glide.ais.event_queue_writable\". When a clone is initiated and the capacity on the AIS node is not enough to provision a new AIS partition, it triggers a process called \"Drain and Upsize AIS node\". This process updates the mentioned system properties for all instances of a customer which has AIS partitions. This update initiates a full cache flush on all instances, which can impact performance on both the source, target, and other instances owned by the same company. This performance impact can be particularly noticeable during core business hours if the \"Drain and Upsize AIS node\" process occurs after a long clone operation. \n\nAs a workaround, ServiceNow will update the sys_properties ignore_cache= true for both \"glide.ais.ha.enable_auto_failover\" and \"glide.ais.event_queue_writable\" as a maintenance for identified instances that have AIS enabled. This change will not have any service impact and does not require a node restart during the maintenance."
    },
    {
        "id": 104,
        "CONTEXT": "\"## ACC agent installer logs passwords in plain text\n\n## Description\n\nUpon installing the agent client collector agent on Windows platform  the MSI installer logs the password used for the service account in plain text on the installer log.\n\nThis is detected as a security issue by some security scanners and while the impact is limited  it does expose a vulnerability on the target machine.\n\n## Steps to Reproduce\n\n1. Install ACC agent on a test Windows machine.\n2. Read the Agent_Installation.log to find the plain text password for the local user account\n\n## Workaround\n\nThis problem has been fixed. If you are able to upgrade  review the Fixed In section to determine the latest version with a permanent fix your instance can be upgraded to.\n\n**Related Problem: PRB1633872**\n\n\n\n## How to reduce the logging for Agent Client Collector in the System Log\n\n## sn_agent - Agent Client Collector framework (ACC-F)\n\nHere are the system properties to control the level of logging from ACC-F\/-V\n\nThe November 2023 release  ACC-F v3.3.1  added this instance system property as the first part of a logging improvement project:\nsn_agent.logging.verbosity\nhttps:\/\/\\<instance name\\>.service-now.com\/sys_properties.do?sys_id=1c2ecc15435031108b19b1b58ab8f2f1\n\nThat has been added to the docs:\nhttps:\/\/docs.servicenow.com\/bundle\/vancouver-it-operations-management\/page\/product\/agent-client-collector\/reference\/acc-framework-configuration-properties.html\n\"The minimum level of messages that are logged for Agent Client Collector Framework. Messages with the indicated status and anything more severe are logged\nOptions:\nerror\nwarn\ninfo\ndebug\nDefault: warn\"\n\nThis change also decreased the data collection logging a lot  by defaulting to Warning (and Errors) only. Info messages will no longer be written to syslog by default.\n\nYou can change it to error to not even get the warnings. Unless you are actively debugging ACC issues  you can leave this at the minimum 'error' level  and temporarily increase the log level when actually debugging.\n\nFYI: The second half of this project is to add a feature to allow turning on the debug for an Agent install  from the instance  instead of having to manually edit the acc.yml file. Keep an eye on the ACC-F release notes in early 2024 to see when that appears.\n\n## sn_acc_visibility - Agent Client Collector for Visibility\n\nACC-V is planning to do something similar to ACC-F  around mid-2024. Until then  this workaround can be used  which is based on this KB article:\n[KB0714743 - Application Logging and the logging.destination and logging.verbosity System Properties](\/kb?id=kb_article_view&sysparm_article=KB0714743)\n\nCreate these System Properties in the sys_properties table:\n\n* sn_acc_visibility.logging.destination = db   \n  Specifying db in the Value field for this property indicates that the log messages for the scoped application should be written to both the localhost logs and the syslog table.\n* sn_acc_visibility.logging.verbosity = error   \n  to only log errors  not info and warning  so a a lot less syslog records as a result.\n\n## Other ACC app scopes\n\nThe same idea as KB0714743 can be used for other scopes too. Any scripts running in the scope  and using gs.debug()  gs.info()  gs.warn() or gs.error() will get filtered using the scope specific property  at a lower platform level.\n\n* \\<scope name\\>.logging.destination=db\n* \\<scope name\\>.logging.verbosity=error\n\nThere may also be app-specific properties that can be set  so please check the properties documentation for the app first.\n\n\n\n## Agent Chat & Advanced Work Assignment Troubleshooting Guide\n\nCheck the \"Health - Platform Record Watcher Rejections\" AND \"Monitoring - Top tables rejections\" sections while running the Splunk [Agent Chat AWA -- Health and Monitoring](https:\/\/servicenow-my.sharepoint.com\/personal\/meshach_adoe_servicenow_com1\/Documents\/164871) dashboard. If you see any errors show up  please create a case task for Dev-Platform AMB\\&RW and respective team who owns the tables in \"Monitoring - Top tables rejections\".\n   2. Check for any slowness in AMB performance. Run the Splunk [Agent Chat AWA -- Health and Monitoring](https:\/\/splunk.servicenow.net\/en-US\/app\/search\/agent_chat_awa__health_and_monitoring)dashboard and check the \"Health - AMB Slowness for chats\" AND \"Monitoring - Instance Performance\" sections. If you notice delays more than 10 seconds on a consistent basis  please create a case task for CS-Performance team.\n   3. Check for any AMB errors or Record Watcher Rejections while running the Splunk [AWA -- Health of Core Platform Capabilities](https:\/\/splunk.servicenow.net\/en-US\/app\/search\/dashboard_awa) dashboard. If you see any errors show up  please create a case task for Dev-Platform AMB\\&RW.\n   4. Check the xmlstats page for statistics on AMB performance: `https:\/\/<instance_name>.service-now.com\/xmlstats.do?include=amb`\n      1. To help debug AMB errors  it might be useful to enable additional logging. This can be done by setting the `sys_user_preference` record `glide.amb.client.log.level=debug` (leaving user blank) and `sys_properties` `glide.amb.log.level=debug`.\n   5. Run more specific Splunk queries to trace down any patterns or sequence of events causing errors (see *[KB1220747](https:\/\/support.servicenow.com\/kb?id=kb_article_view&sysparm_article=KB1220747) - SECTION* *C* ).\n      1. Investigate whether the following issues might be occurring: check for threads marked as EXCESSIVE  check if AMB events got processed properly  check for any errors in WorkItem and Workload responders.\n6. ([KB1170427](https:\/\/support.servicenow.com\/kb?id=kb_article_view&sysparm_article=KB1170427)) Capture web session traffic and any network issues on the agent's client via a HAR file.\n   1. ([KB1156577](https:\/\/support.servicenow.com\/kb?id=kb_article_view&sysparm_article=KB1156577)) Check if the agent's client did receive the AMB message that carries the interaction offer information.\n   2. If capturing via HAR file is not possible or if the customer prefers not to do so  an alternative way is in step 7.\n7. ([KB1209787](https:\/\/support.servicenow.com\/kb?id=kb_article_view&sysparm_article=KB1209787)) Enable client-side logging for Agent Chat by setting the system property `com.glide.awa.client_logging.enabled` as true.\n   1. If the instance is on or before TP10 or UP5  consider using TamperMonkey to post most logs to the server to determine whether the inbox cards were displayed\/removed  whether the inbox audio alert was played  and errors logged to the console. Ask customer to enable this for specific agents that are experiencing the problem.\n   2. If the agent's browser did receive the AMB message that triggers the incoming card notification  a log in the following format should show up: `2022-09-15 08:14:02 (062) Default-thread-56 C650FE004F06D510A671C43D94AD481A txid=52c5ba804f06 app=\"CI\" track=\"AWA\" interaction=\"48c532c48746d5107bae40c80cbb3580\" work_item=\"79c53e848746d5107bae40c80cbb3534\" User 46d44a23a9fe19810012d100cca80666 received an inbox update via AMB  with a time offset of 210 ms:`\n   3. One of the logs will also indicate the work item sys ID that was added to the specific agent's inbox: `Agent Inbox update: User '6816f79cc0a8016401c5a33be04be441' with current workitems (before) as [] and updated workitems (after) as [56bc063d43763110b1cee4c66bb8f2ff] at timestamp 'Wed Nov 29 2023 13:53:23 GMT-0800 (Pacific Standard Time)'.`\n8. ([KB1209787](https:\/\/support.servicenow.com\/kb?id=kb_article_view&sysparm_article=KB1209787)) Verify if agents were active on the workspace tab when the work item appears in the agent's inbox using TamperMonkey. Ask customer to enable this for specific agents that are experiencing the problem.\n   1. Identify the specific agent that was encountering the issue and the period at which it was occurring.\n   2. With TamperMonkey installed on the agent's browser  the following log will appear when an inbox card is displayed: `### INBOX LOGGER ### - IMS0000207 Inserted in DOM after 50 ms  at 2022-11-30T12:34:11.829Z with countdown 0:59 and visibilityState visible. Last activity was 3649 ms ago.` The log includes the visibility of the tab when the work item appears -- if the visibilityState is hidden  therefore the agent was not on the workspace tab at that point in time. It also includes the amount of time since the agent was last active on the tab.\n9. Consider the following potential root cases:\n   1. If you run into the \"Failed to accept work item\" log  one reason could be agent accepted the work item when the timer is about to expire.\n      1. By the time the agent accepts the inbox item  the TimeoutJob running in the background has already marked the work item as timeout. The duration of this timer is defined in the assignment rule.\n      2. Check if a security ACLs may be causing the log -- we use GlideRecordSecure to update the work item. Activate the glide.security.log.acl.warnings system property to see security ACL exceptions.\n10. Consider the following workarounds:\n    1. ([KB1382977](https:\/\/support.servicenow.com\/kb?id=kb_article_view&sysparm_article=KB1382977)) Recommend agents to enable desktop notifications for incoming chats while Dev-AgentChatAWA fixes the issue of audio notifications for inactive workspace tabs. \"\n\n",
        "QUERY": " 'Password exposed in the Agent installation logs'",
        "GROUND_TRUTH": "The issue of passwords being exposed in the Agent installation logs occurs when installing the agent client collector agent on a Windows platform. The MSI installer logs the password used for the service account in plain text on the installer log. This is detected as a security issue by some security scanners and while the impact is limited, it does expose a vulnerability on the target machine. To reproduce this issue, you can install the ACC agent on a test Windows machine and read the Agent_Installation.log to find the plain text password for the local user account. However, this problem has been fixed. If you are able to upgrade, review the Fixed In section to determine the latest version with a permanent fix your instance can be upgraded to. The related problem is PRB1633872."
    },
    {
        "id": 105,
        "CONTEXT": "\"## MID Server upgrade fails leaving MID Server Down  due to Cisco AMP antivirus preventing the upgrade \n\n## Description\n\n**MID Server upgrade fails** leaving MID Server Down  due to **Cisco Advanced Malware Protection (AMP)** for Endpoints preventing MID Server upgrade process replacing the Wrapper executable file.\n\nThe Upgrade starts  the MID Server launches the dist upgrade process and shuts itself down. The Dist-upgrade process failed with FileNotFoundException and stops due to the wrapper-windows-x86-64.exe file being locked\/blocked by Cisco AMP.\n\n**NOTE:** This PRB is specific to MID Server outages caused by Cisco AMP during upgrades  but this symptom is not always caused by Cisco AMP. The same symptom has also been reported on server not running Cisco AMP.\n\nBefore applying the work-around we need to make sure that the root cause is Cisco AMP. To do this we need to verify the wrapper.log and also make sure that Cisco AMP is running.\n\n**Verifying the error message in wrapper.log**\n\nThe MID Server wrapper.log will show this at the end (assuming no manual attempt was made to start it since):\n\nMay 12  2020 2:57:33 PM com.snc.dist.mid_upgrade.UpgradeMain run\nSEVERE: com.snc.dist.mid_upgrade.UpgradeException: java.io.FileNotFoundException: C:\\ServiceNowAgent\bin\\wrapper-windows-x86-64.exe (Access is denied)\ncom.snc.dist.mid_upgrade.UpgradeException: java.io.FileNotFoundException: C:\\ServiceNowAgent\bin\\wrapper-windows-x86-64.exe (Access is denied)\nat com.snc.dist.mid_upgrade.UpgradeMain.migrateToTarget(UpgradeMain.java:840)\nat com.snc.dist.mid_upgrade.UpgradeMain.run(UpgradeMain.java:313)\nat java.lang.Thread.run(Thread.java:748)\nCaused by: java.io.FileNotFoundException: C:\\ServiceNowAgent\bin\\wrapper-windows-x86-64.exe (Access is denied)\nat java.io.FileOutputStream.open0(Native Method)\nat java.io.FileOutputStream.open(FileOutputStream.java:270)\nat java.io.FileOutputStream.(FileOutputStream.java:213)\nat java.io.FileOutputStream.(FileOutputStream.java:162)\nat org.apache.commons.io.FileUtils.doCopyFile(FileUtils.java:1142)\nat org.apache.commons.io.FileUtils.doCopyDirectory(FileUtils.java:1446)\nat org.apache.commons.io.FileUtils.doCopyDirectory(FileUtils.java:1444)\nat org.apache.commons.io.FileUtils.copyDirectory(FileUtils.java:1388)\nat org.apache.commons.io.FileUtils.copyDirectory(FileUtils.java:1317)\nat com.snc.dist.mid_upgrade.UpgradeMain.migrateToTarget(UpgradeMain.java:837)\n... 2 more\n\nMay 12  2020 2:57:33 PM com.snc.dist.mid_upgrade.UpgradeMain appendMidLogs\nINFO: Flushing logs\n<< UPGRADE LOG END >>\n\nVerifying Cisco AMP is running\n\nOpen \"Task Manager\" and make sure that CiscoAMP is running\n## Steps to Reproduce\n\n1. Install a MID Server on a Windows host running Cisco Advanced Malware Protection (AMP)\n2. Cause the MID Server to upgrade\n3. Some upgrades will fail to upgrade at the point that the old agent\\bin\\wrapper-windows-x86-64.exe is deleted\n\n## Workaround\n\nTo resolve the issue  you need to add an exclusion set including the following exclusions to the policy applied for the MID Server host machine on Cisco AMP Console:\n\n* File Scan for wrapper-windows-x86-64.exe under agent\\bin folder (with Apply to child processes marked)\n* File Scan for java.exe under agent\\jre\\bin folder (with Apply to child processes marked)\n* Wildcard for the MID Server folder\n\nTo add the exclusions\n\n1. Make sure you have proper privilege to add an exclusion to Cisco AMP\n2. On the Cisco AMP console  \n   1. Create a new Exclusion set\n   2. Add a new \"File Scan\" exclusion to the exclusion set to exclude the wrapper-windows-x86-64.exe existing under agent\\bin folder. The details of how you can add this exclusion is described in [KB0867749](\/kb_view.do?sysparm_article=KB0867749 \"KB0867749\") When you define this exclusion make sure  \n      * You use the complete path for wrapper-windows-x86-64.exe under agent folder. For example for a MID Server with the agent folder path C:\\Midservers\\mid1\\agent you need to exclude \"C:\\Midservers\\mid1\\agent\\bin\\wrapper-windows-x86-64.exe\"\n      * You mark **\"Apply to child processes\"**when you define the exclusion\n   3. Add a new \"File Scan\" exclusion to the exclusion set for java.exe. This File Scan exclusion is defined to exclude the java.exe existing under agent\\jre\\bin folder. The details of how you can add this exclusion is described in [KB0867749](\/kb_view.do?sysparm_article=KB0867749 \"KB0867749\") When you define this exclusion make sure   \n      * You use the complete path for java.exe under agent folder. For example for a MID Server with the agent folder path C:\\Midservers\\mid1\\agent you need to exclude \"C:\\Midservers\\mid1\\agent\\jre\\bin\\java.exe\"\n      * You mark **\"Apply to child processes\"** when you define the exclusion  \n   4. Add a wildcard exclusion for the MID Server folder. For example for the agent folder \"C:\\Midservers\\mid1\\agent\" you can add \"C\\Midservers\\\\*\". The details are available in [KB0867749](\/kb_view.do?sysparm_article=KB0867749 \"KB0867749\").  \n   5. Add the new exclusion set to the policy applied for the MID Server host machine.\n3. Make sure the policy is synchronized on the MID Server host machine before starting MID upgrade. If you are not familiar with this process refer to [KB0867749](\/kb_view.do?sysparm_article=KB0867749 \"KB0867749\").\n4. Cause the MID Server to upgrade.\n\n**Related Problem: PRB1408516**\n\n\n\n## If the MID Server gives up waiting for the Upgrade process to start  but it does eventually start  f\n\n## Description\n\nSince a check was added in the Tokyo version (PRB1538125)  a MID Server upgrading itself will not shut itself down until it has confirmation that the Upgrade process that takes over has started properly.\nIt waits for a hard-coded 10 minutes  before giving up. The MID Server then stays up and deletes the temporary upgrade files.\n\nHowever  the upgrade Process may eventually start and carry on  killing the main MID Server service processes  and then deleting the files from the agent folder  that it had intended to replace. But the files it was going to replace them with  extracted from the ZIP file  are no longer in the temp folder.\n\nThis leaves the MID Server down  and with files missing that prevent it being started again.\n\n## Steps to Reproduce\n\nThis is difficult to reproduce on demand without finding a way to slow down the bootstrapping of the upgrade process.\n\nThe normal log line for the starting of the upgrade process is show:\n2023-10-22T01:16:20.029-0400 INFO (AutoUpgrade.3600) \\[MIDUpgrader:501\\] Stopping MID server. Bootstrapping upgrade.\n2023-10-22T01:16:20.029-0400 INFO (AutoUpgrade.3600) \\[MIDUpgrader:502\\] MIDUpgrader.startUpgradeRunner()  OperationalState=UPGRADING\n\nSome time later  the MID Server starts to log that it is still waiting for the upgrade process to start:\n2023-10-22T01:35:05.525-0400 INFO (AutoUpgrade.3600) \\[MIDUpgrader:375\\] DistUpgrade file is not present.\n2023-10-22T01:35:05.525-0400 INFO (AutoUpgrade.3600) \\[MIDUpgrader:323\\] Waiting for upgrade process started signal...\n\nThe agent log will show this warning 10 minute later:\n2023-10-22T01:45:06.952-0400 WARN (AutoUpgrade.3600) \\[MIDIssueLogger:83\\] Creating unique MID issue with message \"Unable to receive upgrade process status\" for source \"Couldn't Launch Upgrade Process\"\n\nThe MID Server now assumes it can carry on as normal after logging the failed upgrade  and will clean up the temp files it extracted from the downloaded upgrade ZIP files.\n\nHowever  wrapper.log shows the upgrade process did eventually start after all  and signal to the MID Server service using the DistUpgrade file  but it is too late:\nINFO \\| jvm 1 \\| 2023\/10\/22 01:55:19.964 \\| Oct 22  2023 1:55:19 AM com.snc.dist.mid_upgrade.DistUpgradeUtil writeStartedStatusToFile\nINFO \\| jvm 1 \\| 2023\/10\/22 01:55:19.964 \\| INFO: File path: C:\\ServiceNow Mid Servers\\....\\agent\\conf\\DistUpgrade\n\nIt then goes on to kill the mid server service\nINFO \\| jvm 1 \\| 2023\/10\/22 02:06:15.453 \\| INFO: Killing process (PID = 7064).\nINFO \\| jvm 1 \\| 2023\/10\/22 02:06:29.104 \\| INFO: Killing process (PID = 8016).\nand delete files:\nINFO \\| jvm 1 \\| 2023\/10\/22 02:09:33.368 \\| Oct 22  2023 2:09:33 AM com.snc.dist.mid_upgrade.UpgradeMain wipeDirs\nand fail to replace them:\nINFO \\| jvm 1 \\| 2023\/10\/22 02:09:33.369 \\| INFO: Copying files to MID server installation path.\nINFO \\| jvm 1 \\| 2023\/10\/22 02:09:33.369 \\| Oct 22  2023 2:09:33 AM com.snc.dist.mid_upgrade.UpgradeMain run\nINFO \\| jvm 1 \\| 2023\/10\/22 02:09:33.369 \\| SEVERE: com.snc.dist.mid_upgrade.UpgradeException: java.io.FileNotFoundException: Source 'C:\\ServiceNow Mid Servers\\....\\agent\\work\\upgrade_temp\\6339952203872892431\\agent' does not exist\n\nWith missing binaries  the MID Server service can now not be manually started\n2023\/10\/23 07:27:40 \\| Starting the ServiceNow MID Server_... service...\n2023\/10\/23 07:27:41 \\| Launching a JVM...\n2023\/10\/23 07:27:42 \\| Error: Could not find or load main class org.tanukisoftware.wrapper.WrapperStartStopApp\n2023\/10\/23 07:27:42 \\| Caused by: java.lang.ClassNotFoundException: org.tanukisoftware.wrapper.WrapperStartStopApp\n\n## Workaround\n\nThis problem is currently under review and targeted to be fixed in a future release. Subscribe to this Known Error article to receive notifications when more information will be available.\n\nThe installation will now have missing files that will need to be replaced. This process can be used to repair it:\n[KB0713557 How to manually restore or upgrade a MID Server after a failed auto-upgrade](https:\/\/support.servicenow.com\/kb?id=kb_article_view&sysparm_article=KB0713557)\n\nIf you are attempting to prevent this issue from occurring in a future upgrade (rather than repairing a past upgrade)  the following steps are another workaround:\n\n(1) From the navigator  go to MID Server \\> Properties (or open the ecc_agent_property table).\n(2) Click \"New\" to create a new property\n(3) Fill in the form with the following values:\nName: mid.skip.dist.signal.for.shutdown\nValue: true\nMID Server: (leave blank)\n(4) Restart all the MID servers.\n\nUltimately  the performance of the MID Server host should be investigated  because there is no valid reason for a simple java program to take more than 10 minutes to start up.\n\nThe host should have a minimum of4 CPU cores  per MID Server install on that host. This is a [documented minimum requirement](https:\/\/docs.servicenow.com\/en-US\/bundle\/vancouver-servicenow-platform\/page\/product\/mid-server\/reference\/r_MIDServerSystemRequirements.html) for a MID Server install to be supported.\n\n\n\n## Configure Custom MID server Upgrade URL\n\n### Description:\n\nWhen the MID Server is being upgraded  upgrade packages are downloaded from **https:\/\/install.service-now.com** . This is a publicly accessible server and some customers' access to **https:\/\/install.service-now.com** may be blocked  which causes the MID Server upgrade to fail.\n\nTo avoid this issue  you can customize the **https:\/\/install.service-now.com** URL to your own web server to download the upgrade packages. The OOB system property **mid.install_server.base_uri** is set to **https:\/\/install.service-now.com**by default and can be modified by a maint user to set a custom web server URL. Then the MID Server upgrade will download packages from the custom web server URL instead.\n\n**There are two steps to configure the MID Server URL. These steps must be repeated for every family release.**\n\n* Download the MID Server packages and upload them your own web server  which can be reached by your MID Servers.\n* Modify the **mid.install_server.base_uri** property to include your web server base URL.\n\n**Download the MID upgrade packages and upload to the your own Web Server which can be reached by MID servers:**\n\n1) Run the following script in background script to get the upgrade packages URLs required for MID upgrade.\n\n**Note:** If the MID Server's OS is Linux  replace Windows with Linux in the following script.\n\n`\/***********************\/`\n\n`var midBuildStamp = gs.getProperty('mid.buildstamp');`\n\n`var os = 'windows'; \/\/ for linux OS replace windows with linux`\n\n`var result = {`\n\n` 'result': {`\n\n` midPackage: []`\n\n` }`\n\n`};`\n\n`var packages = [`\n\n` new MIDPackage('mid-core'  midBuildStamp  'universal'  'universal') `\n\n` new MIDPackage('mid-upgrade'  midBuildStamp  'universal'  'universal') `\n\n` new MIDPackage('mid-jre'  midBuildStamp  os  'x86-64')`\n\n`];`\n\n`for (var i = 0; i < packages.length; ++i) {`\n\n` var pack = packages[i];`\n\n` var uriFactory = new MIDPackageUriFactory(pack  'auto_upgrade');`\n\n` uriFactory.setMidSysId(\"\");`\n\n` result.result.midPackage.push({`\n\n` 'filename': pack.getLegacyFilename() `\n\n` 'url': uriFactory.getUris()`\n\n` });`\n\n`}`\n\n`gs.info(result.result.midPackage[0].url.toString());`\n\n`gs.info(result.result.midPackage[1].url.toString());`\n\n`gs.info(result.result.midPackage[2].url.toString());`\n\n`gs.info(result.result.midPackage[1].url.toString().replace('universal.universal'  'preUpgradeCheck'));`\n\n`\/***********************\/`\n\n**Result:**The output of the script will result in 4 URLs.\n\n2) Copy paste each URL in a browser and download these 4 files from **install-service-now.com**.\n3) Place these files in your web server where your MID Server can access them.\n\n**Example:** `https:\/\/install.service-now.com\/glide\/distribution\/builds\/package\/mid-core\/2023\/10\/08\/mid-core-10-09-2020_10-08-2023_2043.universal.universal.zip file`\n\nshould be placed in:\n\n`https:\/\/{yourserverIPorDomain}\/glide\/distribution\/builds\/package\/mid-core\/2023\/10\/08\/mid-core-10-09-2020_10-08-2023_2043.universal.universal.zip `\n\n**Note:**The web server where the upgrade files are placed should be reachable to MID Server. Make sure the upgrade packages are accessible without any credentials.\n\n**Modify the mid.install_server.base_uri system property to include the Web Server Base URL:**\n\n1) Default is **https:\/\/install.service-now.com**.\n2) User must have the **maint** role to modify this property and set it to a custom Web Server URL  for example: https:\/\/{yourserverIPorDomain}\n\n**Note:** This could be one time change and no need to modify this property for every release  unless Web Server URL changes.\n\nOnce these steps are completed  you can trigger the MID Server upgrade to download from the new URL.\n\n\"\n\n",
        "QUERY": " 'Mid server upgrade failure with Cisco AMP endpoints'",
        "GROUND_TRUTH": "The MID Server upgrade failure with Cisco AMP endpoints occurs when the Cisco Advanced Malware Protection (AMP) prevents the MID Server upgrade process from replacing the Wrapper executable file. This results in the MID Server shutting down. \n\nTo verify if Cisco AMP is the root cause, check the MID Server wrapper.log for a FileNotFoundException related to the wrapper-windows-x86-64.exe file. Also, ensure that Cisco AMP is running on your system.\n\nTo resolve this issue, you need to add an exclusion set in the policy applied for the MID Server host machine on the Cisco AMP Console. The exclusion set should include:\n\n1. File Scan for wrapper-windows-x86-64.exe under the agent\\bin folder (with \"Apply to child processes\" marked)\n2. File Scan for java.exe under the agent\\jre\\bin folder (with \"Apply to child processes\" marked)\n3. A wildcard for the MID Server folder\n\nAfter adding the exclusions, ensure the policy is synchronized on the MID Server host machine before starting the MID upgrade."
    },
    {
        "id": 106,
        "CONTEXT": "\"## Oracle LMS : Options data table is not updated and cleaned up\n\n## Description\n\nThe Oracle options LMS scan data (cmdb_ci_orcl_options) is getting duplicated values OR showing the latest and oldest data. Where we expect to have latest values only.\n\n## Steps to Reproduce\n\n* Disable few oracle options\n* Run discovery for oracle db on unix pattern\n* Verify if the options data is updated and if the records is cleaned up.\n\n## Workaround\n\nChange the LMS Oracle Options identifier entrees as below:\n\n**Before the Identifier Entries** :\nCorrelation ID  DB_NAME  MACHINE_ID  Option name\n\n**New Identifier Entries** :\nCorrelation ID\n\n**Related Problem: PRB1475943**\n\n\n\n## Vancouver Patch 3 Hotfix 1\n\n[Vancouver release notes](..\/..\/release-notes\/family-release-notes.html \"The ServiceNow Vancouver release includes new products and applications  as well as additional features and fixes for existing products. Read the release notes to learn about the release  prepare for your upgrade  and upgrade your instance.\") \\> [Learn about the Vancouver release](..\/..\/release-notes\/concept\/rn-learn-landing-page.html \"The Vancouver release includes new features and improvements built on the Now Platform.\") \\>\n\n# Vancouver Patch 3 Hotfix 1 {#ariaid-title1}\n\nThe Vancouver Patch 3 Hotfix 1 release contains fixes to these problems.\n\nBuild information::   Build date: 10-27-2023_1806: Build tag: glide-vancouver-07-06-2023__patch3-hotfix1-10-26-2023\nImportant: For more information about how to upgrade an instance  see [ServiceNow upgrades](..\/upgrades\/reference\/upgrade.html \"The upgrade process moves your instance to a new ServiceNow release version. Understand the difference between upgrading and patching  release definitions  rollback and backup options  and how to test your non-production and production instance upgrades.\").\n\nFor more information about the release cycle  see the [ServiceNow Release Cycle](https:\/\/support.servicenow.com\/kb_view.do?sysparm_article=KB0547244).{#vancouver-patch-3-hf-1-PO__p_download-version}\nNote: This version is now available for use within all regulated market environments. For more information about services available in isolated environments  see [KB0743854](https:\/\/support.servicenow.com\/kb?id=kb_article_view&sysparm_article=KB0743854&_ga=2.238511747.200430442.1684856845-2052949275.1611611591).\n\n## Fixed problem {#vancouver-patch-3-hf-1-PO__section_igd_zgg_gzb}\n\n{#d285406e136}\n\n|             Problem             |                                             Short description                                              | Description |                                                                                                                                                                   Steps to reproduce                                                                                                                                                                    |\n|---------------------------------|------------------------------------------------------------------------------------------------------------|-------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Activity Stream PRB1704891      | Activity stream flickers when typing on the Compose section in the configurable workspace                  |             | 1. Open a CSM\/FSM workspace. 2. From list of changes  open any record. 3. Change the mode to show only the activity stream. 4. Typing anything in the Compose section  with spaces and new lines. 5. Post  then type more in the Compose section. Notice that the activity stream flickers intermittently. {#vancouver-patch-3-hf-1-PO__ol_bh1_c3g_gzb} |\n| Database Persistence PRB1703666 | RLQuery join conditions are appended to the last join and can produce an incorrect results set             |             |                                                                                                                                                                                                                                                                                                                                                         |\n| List Administration PRB1697784  | Opening cases and case tasks from the dashboard loads records with the Task table in the URL               |             | 1. Open the CSM workspace. 2. Click on any case or case task. {#vancouver-patch-3-hf-1-PO__ol_ixq_kjg_gzb} Actual behavior: Case and case task records open with the Task table in the URL. Expected behavior: Case and case task records open with the sn_customerservice_case and sn_customerservice_task tables in the URL.                          |\n| MID Server PRB1708403           | MID server processes ECC messages multiple times  resulting in duplicate input records for a single output |             | 1. Set up a discovery schedule for SD Lab discovery with IP Network ranges 10.11.128.0\/22 and 10.11.144.0\/23. 2. Allow it to run for 24 hours. {#vancouver-patch-3-hf-1-PO__ol_xpd_yjg_gzb} Notice that there are ECC queue messages marked as errors due to the duplicate inputs during the 24 hour range of run time.                                 |\n{#vancouver-patch-3-hf-1-PO__all-other-fixes}\n\n## Fixes included {#vancouver-patch-3-hf-1-PO__section_jgd_zgg_gzb}\n\nUnless any exceptions are noted  you can safely upgrade to this release version from any of the versions listed below. These prior versions contain PRB fixes that are also included with this release. Be sure to upgrade to the latest listed patch that includes all of the PRB fixes you are interested in.\n\n* [Vancouver Patch 2](vancouver-patch-2.html \"The Vancouver Patch 2 release contains important problem fixes.\")\n* [Vancouver Patch 1](vancouver-patch-1.html \"The Vancouver Patch 1 release contains important problem fixes.\")\n* [Vancouver security and notable fixes](vancouver-security-notables.html \"The Vancouver release contains important problem fixes.\")\n* [All other Vancouver fixes](vancouver-all-other-fixes.html \"The Vancouver release contains important problem fixes.\")\n{#vancouver-patch-3-hf-1-PO__ul_nbj_sz4_5yb}  \n**Parent Topic:** [Available patches and hotfixes](..\/..\/release-notes\/available-versions.html \"The Vancouver family includes patches and hotfixes.\")\n\n\n\n## Creation of duplicate Scanned application when no CI look up rules are matched\n\n## Description\n\nStarting with version 19.0.4 of Vulnerability Response that was released in August and November 2023  a duplicate discovered application record for a scanned application is created if lookup rules do not find a match to existing records. The duplicate record is created and referenced as a discovered application. As a result  this duplicate discovered application record also changes whenever the scanned application record is updated  which increases the table size and thus might impact workflow performance downstream. Here\n\n## Steps to Reproduce\n\n* On running the AVR integration  if there is no CI lookup rule that matches to a CI.\n* Once the integration is run  check the scanned application table for duplicate records for the same scanned application.\n\n## Workaround\n\n**Solution:**\n\n* If you are using a version of Vulnerability Response that was released prior to August 2023  you must upgrade to the Vulnerability Response application released in February 2024 to fix the issue.\n* If you have concerns about upgrading  alternatively  you can update your version. Updates have been provided for the versions of Vulnerability Response that were released in August and November of 2023.\n* If you are using the version of Vulnerability Response that was released in August 2023  use the August update set ([august_update_set](sys_attachment.do?sys_id=717e78e2478ff150f93138ce536d43ac)) to fix the issue.\n* If you are using the version of Vulnerability Response that was released in November 2023  use the November update set ([nov_update_set](sys_attachment.do?sys_id=f17ef8e2478ff150f93138ce536d4336)) to fix the issue.\n* Attached is a [on-demand scheduled job](sys_attachment.do?sys_id=b17ef8e2478ff150f93138ce536d43de) that you can use to remove the duplicate scanned applications records if they are already created.\n\n**For the update:**\n\nWhen updating to a new version of Vulnerability Response  you must first revert back to the Store release version of the script include and then perform the update to avoid any conflicts. This helps you avoid potential conflicts  as the fix for this issue starts with the February 2024 version of Vulnerability Response.\n\n**Related Problem: PRB1717292**\n\n\"\n\n",
        "QUERY": " 'LMS scan data showing redundant or duplicate values'",
        "GROUND_TRUTH": "The issue of Oracle LMS scan data showing redundant or duplicate values is due to the Oracle options LMS scan data (cmdb_ci_orcl_options) getting duplicated values or showing both the latest and oldest data, instead of only the latest values. This can be reproduced by disabling a few Oracle options, running discovery for Oracle DB on Unix pattern, and then verifying if the options data is updated and if the records are cleaned up. \n\nThe workaround provided in the article suggests changing the LMS Oracle Options identifier entries. The original identifier entries are Correlation ID, DB_NAME, MACHINE_ID, and Option name. The new identifier entries should only include the Correlation ID. This change should help in preventing the creation of duplicate values in the Oracle LMS scan data. \n\nThis issue is related to problem PRB1475943."
    },
    {
        "id": 107,
        "CONTEXT": "\"## Upgrade Rollback\n\nThey roll back to Helsinki Patch 4. They decide later to upgrade to Helsinki Patch 6 but cannot do so because the HI upgrade dashboard believes they are on Helsinki Patch 7. The resolution is to submit a TASK to CS - Admin to update the Assigned WAR to Helsinki Patch 4.\n\n**Why did the upgrade not record for rollback?**\n\n* Upgrade recording level property (**glide.rollback.upgrade_recording_level** ):  \n  * family: \\[DEFAULT\\] Only record in-family upgrades\n  * all: Record every upgrade no matter what\n  * none: Do not record any upgrade\n* In-family upgrade compares glide.war to glide.war.assigned at the start of the upgrade\n* Check the log file at the start of upgrade_complete for details about **\"in-family\"** decision:  \n  * Upgrading within the family (glide)  recording changes for rollback\n  * Unable to parse one or both file names  not recording upgrade for rollback\n  * NOT upgrading within the family ('%s' vs. '%s')  not recording changes for rollback\n  * Will be followed by from=\\[\\<glide.war\\>\\]  to=\\[\\<glide.war.assigned\\>\\]\n\n**Why is my recorded rollback incomplete?**\n\n* Check for exceptions during rollback\n* Property **glide.rollback.recording.on_exception** determines how the system responds:  \n  * DEFAULT: continue_without_rollback -- Rollback will stop recording  mark rollback invalid  but continue the upgrade\n  * abort: Upgrade is aborted - this setting is really only used in testing\/validating patch builds as a way to fail fast\n  * continue: Rollback continues  basically ignoring the error  and the upgrade continues\n\n**Why wasn't \\<change\\> recorded?**\n\n* Check the table exclusion list in RollbackRecorderUtils.java\n* Check localhost logs around the time of change\n\n**Is the upgrade rollback stuck?**\n\n* If the upgrade appears to be stuck UI facing  it's probably on a large table (Ex: If a CMDB table is 82m records it can take an hour to process) - give it time. You can follow the localhost logs otherwise for live updates. It will look something like this below:\n\n|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 2018-11-07 08:50:50 (516) worker.1 worker.1 txid=43570c831365 WARNING \\*\\*\\* WARNING \\*\\*\\* Lazy Writer Queue is long  current size: 9100  peak size: 10002 2018-11-07 08:50:50 (516) worker.1 worker.1 txid=43570c831365 WARNING \\*\\*\\* WARNING \\*\\*\\* Current lazy writer delay: 0:00:01.855 2018-11-07 08:50:50 (526) worker.1 worker.1 txid=43570c831365 WARNING \\*\\*\\* WARNING \\*\\*\\* Lazy Writer Queue is long  current size: 9200  peak size: 10002 2018-11-07 08:50:50 (526) worker.1 worker.1 txid=43570c831365 WARNING \\*\\*\\* WARNING \\*\\*\\* Current lazy writer delay: 0:00:01.865 2018-11-07 08:50:50 (534) worker.1 worker.1 txid=43570c831365 WARNING \\*\\*\\* WARNING \\*\\*\\* Lazy Writer Queue is long  current size: 9300  peak size: 10002 2018-11-07 08:50:50 (534) worker.1 worker.1 txid=43570c831365 WARNING \\*\\*\\* WARNING \\*\\*\\* Current lazy writer delay: 0:00:01.873 2018-11-07 08:50:50 (542) worker.1 worker.1 txid=43570c831365 WARNING \\*\\*\\* WARNING \\*\\*\\* Lazy Writer Queue is long  current size: 9400  peak size: 10002 2018-11-07 08:50:50 (542) worker.1 worker.1 txid=43570c831365 WARNING \\*\\*\\* WARNING \\*\\*\\* Current lazy writer delay: 0:00:01.881 2018-11-07 08:50:50 (552) worker.1 worker.1 txid=43570c831365 WARNING \\*\\*\\* WARNING \\*\\*\\* Lazy Writer Queue is long  current size: 9500  peak size: 10002 2018-11-07 08:50:50 (552) worker.1 worker.1 txid=43570c831365 WARNING \\*\\*\\* WARNING \\*\\*\\* Current lazy writer delay: 0:00:01.891 2018-11-07 08:50:50 (560) worker.1 worker.1 txid=43570c831365 WARNING \\*\\*\\* WARNING \\*\\*\\* Lazy Writer Queue is long  current size: 9600  peak size: 10002 2018-11-07 08:50:50 (560) worker.1 worker.1 txid=43570c831365 WARNING \\*\\*\\* WARNING \\*\\*\\* Current lazy writer delay: 0:00:01.899 2018-11-07 08:50:50 (568) worker.1 worker.1 txid=43570c831365 WARNING \\*\\*\\* WARNING \\*\\*\\* Lazy Writer Queue is long  current size: 9700  peak size: 10002 2018-11-07 08:50:50 (568) worker.1 worker.1 txid=43570c831365 WARNING \\*\\*\\* WARNING \\*\\*\\* Current lazy writer delay: 0:00:01.907 2018-11-07 08:50:50 (641) worker.1 worker.1 txid=43570c831365 WARNING \\*\\*\\* WARNING \\*\\*\\* Lazy Writer Queue is long  current size: 9800  peak size: 10002 2018-11-07 08:50:50 (641) worker.1 worker.1 txid=43570c831365 WARNING \\*\\*\\* WARNING \\*\\*\\* Current lazy writer delay: 0:00:01.980 2018-11-07 08:50:50 (649) worker.1 worker.1 txid=43570c831365 WARNING \\*\\*\\* WARNING \\*\\*\\* Lazy Writer Queue is long  current size: 9900  peak size: 10002 2018-11-07 08:50:50 (649) worker.1 worker.1 txid=43570c831365 WARNING \\*\\*\\* WARNING \\*\\*\\* Current lazy writer delay: 0:00:01.988 |\n**Which development team needs to be contacted in case of further inquiries\/concerns?**\n\n* Make a task for Dev-Persistence\n\n### Rollback Progress UI\n\n## Rollback in progress\n\n## Rollback almost complete\n\n## Rollback complete\n\n### Upgrade Log \/ Upgrade History\n\nThe following example shows how the upgrade log looks during a downgrade. The lines at the bottom are the completion of the recent upgrade. The next lines above are the rollback process.\n\nThe upgrade history will show empty cells upon rollback. For comparison  the bottom record shows an upgrade from Helsinki P4 to Helsinki P5. The rollback is on top\\& from Helsinki P5 to Helsinki P4.\n\n### Post Rollback Validation\n\n\n\n## Troubleshooting Lazy Writer Issues\n\n# Description\n\nLazy Writer is a mechanism (background thread) that we use in the platform to defer updates to the database so that user sessions can go on about their business (not unlike an ASYNC business rule).\nEach application server node will have a glide.lazy.writer thread that handles these deferred updates.\n\nLazy writer handles asynchronous writes to sysevent  audit  sys_user_presence. When the instance is processing a large number of user presences updates and sysevent updates  lazy writer can cause row lock contention as all the rows in a large batch are locked until the transaction is committed. User presence updates have direct impact on UI transactions  and end users will start to experience slowness.\n\nWhen the Lazy Writer queue is full then its writes become Sync writes and that causes the lock contention\n\n# Procedure\n\n1. Check threads.do and localhost logs  \n   Check the \/threads.do output and see if multiple threads are in the following stack trace:\nat com.glide.db.DBCompositeAction.executeLazy(DBCompositeAction.java:291)\n   Check the localhost_log.$(date +\"%Y-%m-%d\").txt and look for threads with the following error:   FAILED TRYING TO EXECUTE ON CONNECTION 15: UPDATE sys_user_presence\n\n   2018-04-03 07:36:28 (640) Presence-thread-3 58A5E0FCDB9517C8DAF72FEB0B961945 SEVERE \\*\\*\\* ERROR \\*\\*\\* FAILED TRYING TO EXECUTE ON CONNECTION 15: UPDATE sys_user_presence SET \\`ua_time\\` = '2018-04-03T14:35:36.978Z'  \\`path\\` = '\/hrportal'  \\`sys_updated_by\\` = '__USERID__'  \\`sys_mod_count\\` = 29230  \\`sys_updated_on\\` = '2018-04-03 14:35:37'  \\`user_agent\\` = 'Mozilla\/5.0 (Windows NT 10.0; WOW64; Trident\/7.0; rv:11.0) like Gecko'  \\`status\\`= NULL WHERE sys_user_presence.\\`sys_id\\` = '2327c028db4cba00b9b178f9bf9619d5' \/\\* dell079  gs:58A5E0FCDB9517C8DAF72FEB0B961945  tx:35e34dfcdbd957c8daf72feb0b9619e2 \\*\/   **Lock wait timeout exceeded; try restarting transaction**   **java.sql.SQLException: Lock wait timeout exceeded; try restarting transaction**   at org.mariadb.jdbc.internal.SQLExceptionMapper.get(SQLExceptionMapper.java:149)   at org.mariadb.jdbc.internal.SQLExceptionMapper.throwException(SQLExceptionMapper.java:106)   at org.mariadb.jdbc.MySQLStatement.executeQueryEpilog(MySQLStatement.java:268)   at org.mariadb.jdbc.MySQLStatement.execute(MySQLStatement.java:296)   at org.mariadb.jdbc.MySQLStatement.execute(MySQLStatement.java:387)   at sun.reflect.GeneratedMethodAccessor22.invoke(Unknown Source)   at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)   at java.lang.reflect.Method.invoke(Method.java:498)   at com.glide.db.StatementWrapper.invoke(StatementWrapper.java:40)   at com.sun.proxy.$Proxy7.execute(Unknown Source)   at com.glide.db.DBI.executeStatement0(DBI.java:921)   at com.glide.db.DBI.executeStatement(DBI.java:877)   at com.glide.db.DBI.executeStatement(DBI.java:850)   at com.glide.db.DBAction.executeAsResultSet(DBAction.java:283)   at com.glide.db.DBCompositeAction.executeAsResultSet(DBCompositeAction.java:139)   at com.glide.db.DBCompositeAction.executeAsResultSet0(DBCompositeAction.java:92)   at com.glide.db.DBAction.executeAndReturnTable(DBAction.java:247)   at com.glide.db.DBAction.executeNormal(DBAction.java:236)   at com.glide.db.DBAction.executeAndReturnException(DBAction.java:197)   at com.glide.db.DBAction.execute(DBAction.java:136)   **at com.glide.db.DBCompositeAction.executeLazy(DBCompositeAction.java:291)**   **at com.glide.db.DBCompositeAction.executeLazy(DBCompositeAction.java:275)**   **at com.glide.db.DBCompositeAction.executeLazy(DBCompositeAction.java:271)**   at com.glide.ui.ng.NGPresenceService.update(NGPresenceService.java:132)   at com.glide.ui.ng.NGPresenceService.updatePresence(NGPresenceService.java:92)   at sun.reflect.GeneratedMethodAccessor470.invoke(Unknown Source)   at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)   at java.lang.reflect.Method.invoke(Method.java:498)   at com.glide.rest.handler.impl.ServiceHandlerImpl.invokeService(ServiceHandlerImpl.java:43)   at com.glide.rest.processors.RESTAPIProcessor.process(RESTAPIProcessor.java:228)   at com.glide.processors.AProcessor.runProcessor(AProcessor.java:415)   at com.glide.processors.AProcessor.processTransaction(AProcessor.java:186)   at com.glide.processors.ProcessorRegistry.process0(ProcessorRegistry.java:178)   at com.glide.processors.ProcessorRegistry.process(ProcessorRegistry.java:167)   at com.glide.ui.GlideServletTransaction.process(GlideServletTransaction.java:49)   at com.glide.sys.Transaction.run(Transaction.java:1977)   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)   at java.lang.Thread.run(Thread.java:748)\n\n   1.1 Check the issue is not due to excessive sys_user_presence updates  ie: via the localhost on the application nodes:\n\n   grep \"UPDATE sys_user_presence\" localhost_log.$(date +\"%Y-%m-%d\").txt \\|awk '{ print $4 }' \\|sort \\|uniq -c\n\n   sample output:   \\[app128153.sjc111:\/glide\/nodes\\]$ grep \"UPDATE sys_user_presence\" localhost_log.$(date +\"%Y-%m-%d\").txt \\|awk '{ print $4 }' \\|sort \\|uniq -c   23936 glide.lazy.writer\\[glide\\]   12299 Presence-thread-1   12291 Presence-thread-2   12289 Presence-thread-3   12290 Presence-thread-4\n\n   We would expect to only see glide.lazy.writer updates.   In the above example  this shows excessive updates by Presence Threads\n\n2. Check INNOTOP \/ Process List and also observe the INSERT\/UPDATE\/DELETE lock contention  \n3. Check the application node logs  and observe errors similar to;  \n   2018-04-03 07:58:18 (254) Presence-thread-3 CDE47834DB9113C84C931AAC0B961959 **WARNING \\*\\*\\* WARNING \\*\\*\\* Current lazy writer delay: 0:01:18.809**\n2018-04-03 07:58:19 (087) **glide.lazy.writer\\[glide\\] SYSTEM SEVERE \\*\\*\\* ERROR \\*\\*\\* Exception during batch statement commit to** glide:dell_1:emc:jdbc:mysql:\/\/db160020.iad106.service-now.com:3402\/  **falling back to single commits: Deadlock found when trying to get lock; try restarting transaction**\n2018-04-03 07:58:19 (092) glide.lazy.writer\\[glide\\] SYSTEM \\[0:00:01.394\\] Statement batcher: 1000\n2018-04-03 07:58:19 (389) glide.lazy.writer\\[glide\\] SYSTEM Time: 0:00:00.223 id: dell_1\\[glide.17\\] for: UPDATE sys_user_presence SET \\`ua_time\\` = '2018-04-03T14:56:27.335Z'  \\`path\\` = '\/hrportal'  \\`sys_updated_by\\` = ...\n   4.1 Check SPLUNK (Visualization tab)   sourcetype=appnode_localhost_log instance=__INSTANCE__ \"lock_wait\" \\| timechart count span=1m\n\n\n4.2 Check Big Data\n\n4. Once confirmed  review binlogs and application node logs to determine if single (small group) of sessions are the cause. Use logs to identify the table(s) being updated too frequently. See [KB0657202](https:\/\/support.servicenow.com\/kb_view.do?sysparm_article=KB0657202 \"KB0657202\")  \n5. Work backwards to establish the root cause of the update flood - initiating transactions from that sessionID in the application node logs. Possible causes could be widgets on \/sp Service Portal pages generating excessive 'sys_user_presence' updates.  \n   Comparitive numbers:\nGE during peak times sees around 100 per second (no issue)\nDell during outage event P1s saw 500 updates per second to sys_user_presence which was the cause of slow performance\n   6. Upgrade to a fixed release.  \n   PRB1267824 optimizes the LazyWriter to avoid using db transactions unnecessarily so that it does not hold row locks for an unnecessarily long period of time.  \n\n# Applicable Versions\n\nFixed in:\n\nJakarta Patch 9\nKingston Patch 5\n\n# Additional Information\n\nRelated properties:\nglide.db.lazy.writer.wait_threads\nglide.db.lazy.writer.debug\nglide.db.lazy.writer.use_transaction\nglide.db.allow.lazy.payload\n\n[KB0657202](https:\/\/support.servicenow.com\/kb_view.do?sysparm_article=KB0657202 \"KB0657202\") TSE Binlog Query Examples\n\n\n\n## Reference | Datacenter Instance Health\n\n|---------------------------------------------------------------------------|--------------------------------------------------------------------------------|\n| ![Note](sys_attachment.do?sys_id=45349492db3d9f00fac7f4621f9619f1 \"Note\") | **Note**: The information below is intended for readers in the SRE-Devops Team |\n1. Overview {#OVERVIEW}\n=======================\n\nThe purpose of this article is to provide information on how to determine Datacenter Instance's health.\n\n2. BigData Dashboard {#HEADING_2}\n=================================\n\nAccess the BigData dashboard [here](https:\/\/statsnow.servicenow.net\/dashboard\/datacenter-health?timestamp=6h%2C0&instance=datacenter). It will show the data from the past 6 hours and you can change that from the dropdown.\n\nScreenshot:\n\nBelow are the sections on this dashboard:\n\n**1. Datacenter Transactions Count**\n\nThis section represents transactions count during the selected time frame along with historical data.\n\nThe below line represents data for a selected time  and the orange line represents historical data.\n\nThis can range anywhere from 200 to 10 000 depending on peak\/off-peak usage.\n\nIf you see a sudden drop that does not match the historic data:\n\na. Check the 'App Node JVM Memory' and 'XML Stats Response Code' graphs\n\n1. if there is a sudden drop  check all app node's health if there were any restarts\/crashes.\n2. Check if currently  all app nodes have status=operational \\[ruckus -b datacenter n\\]\n\nb. Check if there is any active alert \\[ruckus -b datacenter a\\]. If yes  work with SRE further on these alerts.\n\nc. Check Servlet stats of all nodes \\[ruckus -b datacenter v\\]. Investigate if anything is in RED.\n\nd. If all looks good  wait for 30 mins and check the transactions count again.\n\n**2. Datacenter User Transactions Average Response Time**\n\nThe graph is in milliseconds  Blue lines represent data for a selected time and the orange line represents historic data.\n\nIf you see response time going beyond 60K milliseconds (60 seconds):\n\na. Check if there is any active alert \\[ruckus -b datacenter a\\]. If yes  see if any of the active alerts can cause a performance issue. Work with SRE.\n\nb. Check the 'App Node JVM Memory'  'Database Avg Response Time for Application Servers'  'DB Memory Consumption'  'CPU IO Wait' Graphs and investigate further to locate the bottleneck.\n\nc. Check Servlet stats of all nodes \\[ruckus -b datacenter v\\]. Investigate if anything is in RED.\n\nd. Leverage [DC Status](https:\/\/search.servicenow.net\/en-US\/app\/search\/dc_status?form.otherInstance=datacenter&form.pickedTime.earliest=-12h%40h&form.pickedTime.latest=now&form.binCount=200&form.by_node_checkbox=true&form.instPick=datacenter) Splunk dashboard to analyze and find the bottleneck. \\[summarised below\\]\n\ne. See what user transactions are taking more than 1 minute\n\n1. Log in to the datacenter  get firefighter access\n2. Go to \"Active Transactions\" and capture a few of the long-running transaction\n3. Search for this long-running transaction in Splunk and gather all transaction logs to understand the transaction that is delayed\n4. Evaluate if you can replicate the delay\n\nf. If all looks good  engage network engineering to ensure there is no network-related issue.\n\n**3. Datacenter Unprocessed Events Count**\n\nThis graph shows the number of unprocessed events for the selected time  blue lines are for selected time and orange lines are for historic data.\n\nIf you see this number above 1000 for longer than 10 minutes:\n\na. Log in to the datacenter and navigate to System Diagnostics \\> Diagnostics Page.\n\nb. In the system overview section  check the number of Events pending.\n\nc. This number should be a minimum or less than 1000. If this number is more than 1000 (considered as the threshold)  a large number of events are stuck in the instance.\n\nd. Follow [KB0547784](.\/kb?id=kb_article_view&sysparm_article=KB0547784)on Troubleshooting the stuck event process. \\[don't kill any transaction without proper due diligence\\]\n\n**4. Datacenter Scheduler Queue Length**\n\nSchedulers are the workers that onboard transactions for background processing.\n\nIf the Scheduler queue is larger than 1000:\n\na. List worker threads \\[ruckus -b datacenter wW\\] and look for the ones marked in Red  they are the long-running jobs holding worker threads\n\nb. Check if anything is running for hours  and debug this long-running job \\[do not cancel anything without proper review and approval\\].\n\nc. If there are multiple long-running jobs holding worker threads  this can cause a queue of waiting jobs\n\nd. Check JVM threads and if you see anything in red \\[i.e. if cpu is red  work with SRE  if age is red investigate\\]\n\ne. Check if all nodes are properly configured and have their worker threads onboarded \\[ruckus -b datacenter w\\]\n\n1. Execute \\[ruckus -b datacenter w\\]\n2. Ensure column \\[workers\\] is not empty for any of the nodes. If empty  healthcheck the node it is empty for and add the node back to the cluster to remediate.\n\n**5. Datacenter API_INT Max Queue Depth**\n\nThis Graph represents the depth requests count on API_INT \\[Integration Semaphore\\]\n\n\"\n\n",
        "QUERY": " 'Lazy Writer Queue is long'",
        "GROUND_TRUTH": "The 'Lazy Writer Queue is long' warning is related to the Lazy Writer mechanism in the system. This mechanism is a background thread used in the platform to defer updates to the database, allowing user sessions to continue their operations. Each application server node has a glide.lazy.writer thread that handles these deferred updates. \n\nThe Lazy Writer handles asynchronous writes to sysevent, audit, sys_user_presence. When the instance is processing a large number of user presences updates and sysevent updates, the Lazy Writer can cause row lock contention as all the rows in a large batch are locked until the transaction is committed. This can impact UI transactions and cause slowness for end users.\n\nWhen the Lazy Writer queue is full, its writes become synchronous, leading to lock contention. If you see warnings about the Lazy Writer Queue being long, it means that the queue is full and the system is experiencing delays in processing updates.\n\nTo troubleshoot this issue, you can check threads.do and localhost logs, check INNOTOP \/ Process List for lock contention, review binlogs and application node logs to determine if a single session or small group of sessions are causing the issue, and upgrade to a fixed release if necessary."
    },
    {
        "id": 108,
        "CONTEXT": "\"## Automated Test Framework - Rescheduled ATF test suites and parameterized tests consume too much memo\n\n## Description\n\nWhen an ATF test suite that contains many child suites and\/or parameterized tests is rescheduled due to a mutually exclusive test running  the suite takes a long time to be rescheduled and will use a lot of the instance memory. In extreme cases  the instance may run out of memory and terminate the ATF test execution  leaving the test suite run incomplete.\n\nThis issue becomes worse with bigger test suite hierarchies and\/or mixing child test suites with parameterized tests; for example  a test suite containing many child suites  each of which has child suites of their own with multiple parameterized tests in them will use a significant amount of memory when it is rescheduled.\n\n## Steps to Reproduce\n\n1) Create a large ATF test suite containing multiple child suites (which may have child suites of their own) and parameterized tests. This can be done in the \"Automated Test Framework (ATF) \\> Tests\" and \"Automated Test Framework (ATF) \\> Suites\" modules  \n\nFor context  this issue was observed with the following setup:\nOne root suite containing 10 child suites  each of which has a single child suite of their own\nEvery suite mentioned above (including the root suite) contained 10 parameterized tests  each of which had 3 parameter sets\nThe grand total number of tests is 21 suites \\* 10 tests per suite = 210 tests\n\n2) Run the test suite  \n3) At some point during the test suite run  force it to reschedule  \n\nNOTE: One way to do this is to create a long running test (e.g. a test with a \"Run Server Side Script\" step that uses gs.sleep() to sleep for 10 minutes)  then navigate to the Mutually Exclusive Tests related list on the test form  click \"Add Mutual Exclusion\" and choose one of the tests in the suite. Then run the test and the suite in parallel; the mutual exclusion rule on the test will make the suite reschedule when it tries to run the mutually exclusive test (at that point the long-running test can be cancelled to allow the suite to run)\n\n4) Wait \\~10 seconds for the test suite to resume execution (at this point there's no need for it to reschedule again  so the test started in Step 3 can be completed)  \n\nExpected: The suite run should resume normal execution after being rescheduled\nObserved: The suite run takes a very long time (several minutes) to resume execution. While waiting for the test to resume execution  more and more memory is consumed; in extreme cases  the instance may run out of memory and kill the test run\n\n## Workaround\n\nAs mentioned in this description  this issue occurs when a test suite execution is rescheduled  and gets worse with large test suite hierarchies. This means there are two potential workarounds for this issue:\n\n1. Update the tests in the suite so they are less likely to be rescheduled. In most cases this means updating the tests so that they don't rely on updating pre-existing record data. Check the \"Mutually Exclusive Tests\" related list on the tests in the suite to see which tests have record conflicts with other tests. See below for some documentation on Parallel Testing and Mutually Exclusive Tests\n2. Break down large test suites hierarchies; in particular  removing child test suites from their parents (\"flattening\" the test suite) and running them separately will significantly reduce the impact of this issue\n\nParallel testing documentation:\n\n[https:\/\/docs.servicenow.com\/bundle\/rome-application-development\/page\/administer\/auto-test-framework\/concept\/parallel-testing.html](https:\/\/docs.servicenow.com\/bundle\/rome-application-development\/page\/administer\/auto-test-framework\/concept\/parallel-testing.html)\n\n[https:\/\/docs.servicenow.com\/bundle\/rome-application-development\/page\/administer\/auto-test-framework\/concept\/mutual-exclusion-rule.html](https:\/\/docs.servicenow.com\/bundle\/rome-application-development\/page\/administer\/auto-test-framework\/concept\/mutual-exclusion-rule.html)\n\n**Related Problem: PRB1539992**\n\n\n\n## San Diego Patch 10\n\nIf the 'content_active' field for a lifecycle is updated  the corresponding 'Active' field should also be updated in the Software Product Lifecycles \\[sam_sw_product_lifecycle\\] table.                                                                                                                                                                                                                                                                                                                                               | Refer to the listed KB article for details. |\n| UI Policies PRB1562733                                                                                            | Issue with reference qualifier script implementation in the new seismic modal form                                                                                | When a user updates the assignment group and clicks in the assigned-to reference field  the 'getEncodedRecord' function in the field.js should receive the correct value for 'isNewRecord' as specified in the configuration of the form. Instead  the value of 'isNewRecord' is undefined.                                                                                                                                                                                                                                                                                                                                                     |                                             |\n{#sandiego-patch-10__notable-fixes}\n\n## All other fixes {#sandiego-patch-10__section_d3c_hbd_fwb}\n\n{#d120127e431}\n\n|                                                        Problem                                                         |                                                                                               Short description                                                                                               |                                                                                                                                                                                                                                                                                           Description                                                                                                                                                                                                                                                                                           |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Steps to reproduce                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |\n|------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Access Control PRB1581854                                                                                              | Archive table ACLs can slow down other form loads when there's a significant number of archived tables in the instance                                                                                        | The performance problem takes place when the property glide.security.enable_archive_table_acls is set to true.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | 1. Ensure glide.security.enable_archive_table_acls is set to true. 2. Open an instance with many archived tables. 3. Notice the form shows slowness on load. {#sandiego-patch-10__ol_x2r_n4s_gwb}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n| Agent Chat PRB1625505                                                                                                  | The Agent Chat window is empty due to a failure in processing a picker control that doesn't have a matched value                                                                                              | Messages should be shown in the chat window but it appears blank.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | 1. Have a picker control in a topic. 2. Have an agent available in one browser. 3. Run the topic from the web client and force the value of one of the options to be something different in Chrome Inspect. 4. Select that option. 5. Notice that the topic fails and requester is redirected to Live agent. 6. Accept the work item from the live agent. Notice that the chat window is blank. {#sandiego-patch-10__ol_yg3_4jz_fwb}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |\n| Agent Workspace PRB1604316                                                                                             | A 'Choice' field value clears on Agent Workspace when using a client script  but works fine on platform                                                                                                       | The value for 'Tax Years' clears on clicking anywhere in the form.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n| AI Search Glide PRB1553655                                                                                             | When a KB is updated with a new version  search suggestions results in \/sp continue to point to the old version                                                                                               |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n| API Access Policies PRB1614023                                                                                         | OAuth token expiration period and PA\\&Reporting performance issues                                                                                                                                            | When a user schedules multiple exports close in time  all exports have the same token that has an expiration time. Not all the requests from the same user have the same timeout  as the expiration time is fixed when the token is created. User incidents have some export requests not succeed because the token expired while waiting in the queue to export.                                                                                                                                                                                                                               |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n| Authentication PRB1595695                                                                                              | KMFInstanceTypeDetector improperly detects non-prod DC instances                                                                                                                                              | Sometimes  datacenter instances do not have the glide.installation.production property set to true  which causes KMFInstanceTypeDetector to improperly classify them as unknown.                                                                                                                                                                                                                                                                                                                                                                                                                | 1. Provision a DC instance. 2. Set this property to false: glide.installation.production. 3. Generate a key pair using ensureAll. {#sandiego-patch-10__ol_oh3_4jz_fwb} Expected behavior: The Key Pair uses an external generator (for example  Vault). Actual behavior: The Key Pair is generated internally by KMF. Certificates don't contain 'vault' in their issuer.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n| Automated Test Framework (ATF) PRB1539992 [KB1001156](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB1001156) | Rescheduled ATF test suites and parameterized tests consume too much memory                                                                                                                                   | The suite run takes a very long time (several minutes) to resume execution. While waiting for the test to resume execution  more and more memory is consumed. In extreme cases  the instance may run out of memory and kill the test run.                                                                                                                                                                                                                                                                                                                                                       | Refer to the listed KB article for details.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |\n| Change Management PRB1567689                                                                                           | Typo in the JS output from the STTRMTransition where 'conditon' should be 'condition'                                                                                                                         | In the evaluateConditions method when it converts the condition to JS  the key used is 'conditon' instead of 'condition'.                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n| Change Management PRB1613166                                                                                           | Change of behavior required in the 'Change POST' API  in which no arguments should create a default change request in the same UI form                                                                        | It shouldn't give an error requiring the user to specify either a model or a type.                                                                                                                                                                                                                  ...",
        "QUERY": " 'Large ATF test suite hierarchies takes a lot of time when rescheduled'",
        "GROUND_TRUTH": "According to KB12345, when an Automated Test Framework (ATF) test suite that contains many child suites and\/or parameterized tests is rescheduled, it takes a long time to be rescheduled and uses a lot of the instance memory. This issue becomes worse with larger test suite hierarchies and\/or mixing child test suites with parameterized tests. In extreme cases, the instance may run out of memory and terminate the ATF test execution, leaving the test suite run incomplete.\n\nThe problem can be reproduced by creating a large ATF test suite containing multiple child suites and parameterized tests, running the test suite, and forcing it to reschedule at some point during the test suite run. The suite run takes a very long time to resume execution after being rescheduled, and more and more memory is consumed during this time.\n\nThere are two potential workarounds for this issue:\n1. Update the tests in the suite so they are less likely to be rescheduled. This usually means updating the tests so that they don't rely on updating pre-existing record data.\n2. Break down large test suite hierarchies; in particular, removing child test suites from their parents (\"flattening\" the test suite) and running them separately will significantly reduce the impact of this issue."
    },
    {
        "id": 109,
        "CONTEXT": "\"## Bad identification rule of kuberneters service pods and node\n\n## Description\n\nWe observed only kubernetes Services records in different clusters with same name and namespace are getting reconciled to currently discovered cluster.\n\nDue to following CI identification rules become Independent.\n\n* cmdb_ci_kubernetes_service\n* cmdb_ci_kubernetes_node\n* cmdb_ci_kubernetes_pod\n\n## Steps to Reproduce\n\nCreate a service with the same name and namespace in two different clusters.\nRun discovery on those clusters.\nOnly one service CI will be created.\n\n## Workaround\n\nImport [kubernetes_services.xml](https:\/\/buildtools1.service-now.com\/sys_attachment.do?sys_id=6ffb4a73db8d59d0875c7907689619b7&sysparm_this_url=kb_knowledge.do%3Fsys_id%3De990da6bdbc555d0875c790768961987%26sysparm_view%3D%26sysparm_domain%3Dnull%26sysparm_domain_scope%3Dnull) update set only Kubernetes service CI is impacted.\n\nImport the [pods_services_nodes.xml](https:\/\/buildtools1.service-now.com\/sys_attachment.do?sys_id=2bfb4a73db8d59d0875c7907689619b3&sysparm_this_url=kb_knowledge.do%3Fsys_id%3De990da6bdbc555d0875c790768961987%26sysparm_view%3D%26sysparm_domain%3Dnull%26sysparm_domain_scope%3Dnull) update set if all the above mentioned CI are impacted.\n\nOR\n\nManually change the following CI identification rule to dependent.\n\n* cmdb_ci_kubernetes_service\n* cmdb_ci_kubernetes_node\n* cmdb_ci_kubernetes_pod\n\nAfter this change please run Kubernetes discovery and discovery should not fail with payload error or duplicate record error.\n\nImpacted CI should realign to their cluster records.\n\n**Related Problem: PRB1595478**\n\n\n\n## Discovery configured with a Load Balanced MID cluster is failing due to new business rule 'Restrict \n\n## Description\n\nIn Vancouver release  Discovery using a schedule configured with a Load Balanced MID cluster results in certain patterns failing to launch with an error message - **Error while execution pattern \"\\<Pattern Name\\>**\". This is due to below business rule preventing MID user A from being able to create output records for MID user B when logged in as different user.\n\n**Restrict record creation for other mids -**https:\/\/\\<instance_name\\>.service-now.com\/sys_script.do?sys_id=23176a40777261103406f24a8c5a996d\n\n## Steps to Reproduce\n\n1. Create a Load Balanced MID server Cluster\n2. Create a Discovery Schedule configured to execute using the MID Server Cluster\n3. Observe that sometimes certain patterns will fail to execute with generic error 'Error while execution pattern \"\\<Pattern Name\\>\"  \n   Note: This issue can also occur during auto-select MID server.\nThis will lead to the scheduled discovery to stop from execution and will also see an error message - \"Access Denied. MID user:\"\n\n## Workaround\n\nThe issue is fixed in **Vancouver Patch 5** and is available for upgrade.\n\nIf you are unable to upgrade anytime soon  please follow the steps to apply the workaround that will update the Business Rule \"**Restrict record creation for other mids** \" and will provide the relief. Use either of the steps below -\n\n1. Modify the condition of the business rule -\n   * On the navigation filter type - sys_script.LIST\n   * Search with sys_id contains \"23176a40777261103406f24a8c5a996d\"\n   * Set the condition to: current.state == \"ready\" \\&\\& current.topic == 'Command'\n2. Users should have both \"admin \" role to perform below action.\n\n- Import the update set \"[sys_remote_update_set_dd8259e20cb6b550f8778b6c4da37a20.xml](sys_attachment.do?sys_id=388edf6e97cf39500af678ce2153af8d)\" file\n- Commit the update set.  \nThis file will update the Business Rule \"Restrict record creation for other mids\". Make sure the Business Rule is updated and ensure the issue is resolved.  \n\n**Related Problem: PRB1711976**\n\n\n\n## Vancouver Patch 3 Hotfix 1\n\n[Vancouver release notes](..\/..\/release-notes\/family-release-notes.html \"The ServiceNow Vancouver release includes new products and applications  as well as additional features and fixes for existing products. Read the release notes to learn about the release  prepare for your upgrade  and upgrade your instance.\") \\> [Learn about the Vancouver release](..\/..\/release-notes\/concept\/rn-learn-landing-page.html \"The Vancouver release includes new features and improvements built on the Now Platform.\") \\>\n\n# Vancouver Patch 3 Hotfix 1 {#ariaid-title1}\n\nThe Vancouver Patch 3 Hotfix 1 release contains fixes to these problems.\n\nBuild information::   Build date: 10-27-2023_1806: Build tag: glide-vancouver-07-06-2023__patch3-hotfix1-10-26-2023\nImportant: For more information about how to upgrade an instance  see [ServiceNow upgrades](..\/upgrades\/reference\/upgrade.html \"The upgrade process moves your instance to a new ServiceNow release version. Understand the difference between upgrading and patching  release definitions  rollback and backup options  and how to test your non-production and production instance upgrades.\").\n\nFor more information about the release cycle  see the [ServiceNow Release Cycle](https:\/\/support.servicenow.com\/kb_view.do?sysparm_article=KB0547244).{#vancouver-patch-3-hf-1-PO__p_download-version}\nNote: This version is now available for use within all regulated market environments. For more information about services available in isolated environments  see [KB0743854](https:\/\/support.servicenow.com\/kb?id=kb_article_view&sysparm_article=KB0743854&_ga=2.238511747.200430442.1684856845-2052949275.1611611591).\n\n## Fixed problem {#vancouver-patch-3-hf-1-PO__section_igd_zgg_gzb}\n\n{#d285406e136}\n\n|             Problem             |                                             Short description                                              | Description |                                                                                                                                                                   Steps to reproduce                                                                                                                                                                    |\n|---------------------------------|------------------------------------------------------------------------------------------------------------|-------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Activity Stream PRB1704891      | Activity stream flickers when typing on the Compose section in the configurable workspace                  |             | 1. Open a CSM\/FSM workspace. 2. From list of changes  open any record. 3. Change the mode to show only the activity stream. 4. Typing anything in the Compose section  with spaces and new lines. 5. Post  then type more in the Compose section. Notice that the activity stream flickers intermittently. {#vancouver-patch-3-hf-1-PO__ol_bh1_c3g_gzb} |\n| Database Persistence PRB1703666 | RLQuery join conditions are appended to the last join and can produce an incorrect results set             |             |                                                                                                                                                                                                                                                                                                                                                         |\n| List Administration PRB1697784  | Opening cases and case tasks from the dashboard loads records with the Task table in the URL               |             | 1. Open the CSM workspace. 2. Click on any case or case task. {#vancouver-patch-3-hf-1-PO__ol_ixq_kjg_gzb} Actual behavior: Case and case task records open with the Task table in the URL. Expected behavior: Case and case task records open with the sn_customerservice_case and sn_customerservice_task tables in the URL.                          |\n| MID Server PRB1708403           | MID server processes ECC messages multiple times  resulting in duplicate input records for a single output |             | 1. Set up a discovery schedule for SD Lab discovery with IP Network ranges 10.11.128.0\/22 and 10.11.144.0\/23. 2. Allow it to run for 24 hours. {#vancouver-patch-3-hf-1-PO__ol_xpd_yjg_gzb} Notice that there are ECC queue messages marked as errors due to the duplicate inputs during the 24 hour range of run time.                                 |\n{#vancouver-patch-3-hf-1-PO__all-other-fixes}\n\n## Fixes included {#vancouver-patch-3-hf-1-PO__section_jgd_zgg_gzb}\n\nUnless any exceptions are noted  you can safely upgrade to this release version from any of the versions listed below. These prior versions contain PRB fixes that are also included with this release. Be sure to upgrade to the latest listed patch that includes all of the PRB fixes you are interested in.\n\n* [Vancouver Patch 2](vancouver-patch-2.html \"The Vancouver Patch 2 release contains important problem fixes.\")\n* [Vancouver Patch 1](vancouver-patch-1.html \"The Vancouver Patch 1 release contains important problem fixes.\")\n* [Vancouver security and notable fixes](vancouver-security-notables.html \"The Vancouver release contains important problem fixes.\")\n* [All other Vancouver fixes](vancouver-all-other-fixes.html \"The Vancouver release contains important problem fixes.\")\n{#vancouver-patch-3-hf-1-PO__ul_nbj_sz4_5yb}  \n**Parent Topic:** [Available patches and hotfixes](..\/..\/release-notes\/available-versions.html \"The Vancouver family includes patches and hotfixes.\")\n\n\"\n\n",
        "QUERY": " 'Kubernetes discovery failing with duplicate record error'",
        "GROUND_TRUTH": "The issue of Kubernetes discovery failing with a duplicate record error is due to the identification rules of certain Configuration Items (CI) becoming independent. These CIs include cmdb_ci_kubernetes_service, cmdb_ci_kubernetes_node, and cmdb_ci_kubernetes_pod. The problem arises when a service with the same name and namespace is created in two different clusters and discovery is run on those clusters, resulting in the creation of only one service CI.\n\nTo resolve this issue, you can import the provided update sets if the impacted CI is only the Kubernetes service or all the mentioned CIs. Alternatively, you can manually change the CI identification rule to dependent for the mentioned CIs. After making these changes, you should run Kubernetes discovery again. The discovery should not fail with a payload error or duplicate record error, and the impacted CI should realign to their cluster records. This issue is related to problem PRB1595478."
    },
    {
        "id": 110,
        "CONTEXT": "\"## Key Management Framework - On-Premise to Cloud Migration Guidelines\n\n## Introduction\n\nCurrently (2023 and onward)  customers who are migrating from on-premise to cloud do not have the ability to automatically migrate their KMF encrypted data. This is because on-premise instances use customer-generated keystores which ServiceNow does not have access to. This document details guidance needed for on-premise to cloud migrations. Customer will need to be informed that they may need to manually re-enter Password2 data into their respective tables.\n\nNOTE: This document applies to Password2 data ONLY -- if your customer also supports Column Level Encryption  please reach out to the Column Level Encryption Team for guidance.\n\nStep 1: Analyze the KMF Password2 Splash Zone\n\n1. Log into your on-premise instance with your admin account\n2. Via the navigator  go to System Definition \\> Script - Background Script\n3. Execute the provided script in global scope to find all KMF Password2 encrypted data -- see Appendix A\n4. Examine the output of the script (check the end of summary)\n   * If there are only a handful of records  it will be more efficient to manually re-input the Password2 data on the cloud instance -- continue to Step 2\n   * If there is a large number for records  please create a case  and in turn ask the support engineer to create a case task to the KMF Team to discuss further options\n\nStep 2: Password2 Data Re-Entry\n\n1. Move forward with your on-premise to cloud migration\n2. Once the migration is complete  on the cloud instance  verify that the instance is KMF healthy and operational -- if it is not  open a case with support with the recommendation that a case task for the KMF Team is opened\n   * To check KMF Health  via the navigator  go to Key Management \\> Health (Diagnostics)\n   * Note  you need to have the appropriate KMF role (KMF Admin or KMF Cryptographic Manager) to view the KMF Health page -- see [Key Management Framework roles](https:\/\/docs.servicenow.com\/csh?topicname=kmf-roles.html&version=latest) documentation for further details\n3. On the cloud instance  manually re-input the Password2 data -- the instance will encrypt the data with cloud-based cryptographic keys\n   * Use the output in Step 1 to locate the appropriate records for re-entry\n   * If there are issues recalling what the Password2 data (plaintext value) was  you should be able to decrypt the KMF Password2 encrypted data on your **on-premise** instance -- see Appendix B\n   * Note  if you see an access denied error message  likely you'll need to adjust the module access policy that is blocking your decryption attempt -- see [Module access policy overview](https:\/\/docs.servicenow.com\/csh?topicname=module_access_policy_overview.html&version=latest) and [Create a module access policy](https:\/\/docs.servicenow.com\/bundle\/vancouver-platform-security\/page\/administer\/key-management-framework\/task\/create-module-access-policy.html) documentation for more details\n   * Note  you can delete Module Access Policies as needed once you're done with the decryption process\n   * Note  if you run into problems decrypting  please open a case\n   * Note  there could be cases where the record in question is no longer applicable for the cloud instance -- in such cases  you can leave the record alone or delete the record (customer discretion)\n4. Once complete  we recommend that you test the changes that you made  depending on your use case\n\n## Appendix A\n\n## Appendix B\n\n\n\n## Enabling KMF for On-Premise Instances\n\n# Overview\n\nThis KB details the steps to enable KMF for on-premise instances.\n\n# Background\n\nKMF is a ServiceNow application that relies on a key hierarchy to envelope encrypt customer data encryption keys.? This key hierarchy is backed by a Hardware Security Module (HSM) that's reachable to datacenter instances.? However  KMF can operate on-premise (with certain limitations) with a local keystore in the place of the datacenter HSM.\n\n* KMF has been out-of-box since Quebec\n* KMF is used by other applications as a dependent feature  for its cryptography and key management needs\n* Re the limitations:\n  * The general limitation is tied to a KMF sub-feature called Key Exchange\n  * In the cloud environment  Key Exchange relies on an internal PKI system  and set of key pairs (Instance Asymmetric Encryption Key  Instance Signature Key)  associated to the PKI system\n  * Key Exchange specifically uses these key pairs for authentication of the exchange  and cryptographic protection of the transferred keys\n  * For on-premise  there is no internal PKI system available for the instance to leverage - thus Key Exchange is not supported\n  * There exists a workaround to get Key Exchange working for on-premise instances  but this would require consultation (on what's the best path forward)\n  * If there are questions this workaround  please contact the Key Management Framework Team or Secrets Management Team\n\n# Steps\n\n1. Generate the keystore and an Instance Root Key (IRK  to be stored in the keystore). The recommended local keystore is BCFKS. The IRK is a 256-bit key that KMF uses for envelope encryption.  \n   Note: when generating the IRK  when prompted for a password  elect to rely on the keystore password.  \n   Note: the keystore must be accessible to all nodes - in other words  all nodes must be sourcing the same Instance Root Key (e.g. generate the keystore once and copy it to all nodes or use a common share with one keystore).  \n   To generate the keystore from scratch (along with the IRK)  just execute the following command:\n   # keytool -genseckey -alias 256bitkey -keyalg aes -keysize 256 -providername BCFIPS -providerclass org.bouncycastle.jcajce.provider.BouncyCastleFipsProvider -providerpath \\<PATH_OF_bc-fips_USUALLY_UNDER \/glide\/nodes\/\\<node\\>\/lib\/jsw\/bc-fips-1.0.2.jar\\> -keystore \/glide\/nodes\/\\<NODE\\>\/conf\/overrides.d\/\\<NAME_OF_KEYSTORE\\>.bcfks -storetype bcfks -storepass \\<PASSWORD\\>\n\n   An example of the above command is provided below:\n\n   # keytool -genseckey -alias 256bitkey -keyalg aes -keysize 256 -providername BCFIPS -providerclass org.bouncycastle.jcajce.provider.BouncyCastleFipsProvider -providerpath ..\/..\/lib\/jsw\/bc-fips-1.0.2.jar -keystore **keystorekmf.bcfks** -storetype bcfks -storepass **changeit**\n\n2. Verify KMF plugin on the instance.  \n   After creating the keystore and the IRK  install the KMF Plugin properties  if not already installed. This does not apply to Quebec instances as KMF is OOB starting in Quebec.  \n   * Search KMF Plugins procedure: Logon to your instance \\> Click to 'All' \\> In the Search bar  write \"v_plugin.list\" \\> Once page opened \\> Search 'System Plugins' change filter to 'ID' and put 'com.glide.kmf.global'  press 'Enter'.\n     For example:\n\n* Checking all KMF plugins are installed.  \n     Logon to your instance \\> Click to 'All' \\> In the Search bar  write \"v_plugin.list\" \\> Once page opened \\> Search 'System Plugins' change filter to \"Name\" and put 'Key Management' \\> press 'Enter'. This will list out all the KMF Plugins.   \n     Please note that the above steps require MAINT access. If you do not have this access  you can go to https:\/\/\\<instance FQDN\\>\/xmlstats.do and search for the following plugins:\n* com.glide.kmf.global\n* com.glide.kmf.codesigning\n* com.glide.kmf\n* (note: depending on other plugins you have enabled  there might be more containing \"kmf\")\n\nRepair KMF plugins if needed (need Maint access)\n\n1. Checking the status of the KMF keystore properties. None of the keystore properties configured under the KMF should have a result come out 'Null'.  \n   Logon to your instance \\> Click to 'All' \\> In the Search bar \\> Under System Definition \\> Scripts -Background \\> put the below mentioned KMF properties to extract information \\> Run Script 'Global'.  \n   You need to put the below entries in 'Script-Background' to verify it:\n   gs.info(\"kmf.file.keystore.enabled: \" + gs.getProperty('kmf.file.keystore.enabled'));   gs.info(\"kmf.file.keystore.path: \" + gs.getProperty('kmf.file.keystore.path'));   gs.info(\"kmf.file.keystore.name: \" + gs.getProperty('kmf.file.keystore.name'));   gs.info(\"kmf.file.keystore.type: \" + gs.getProperty('kmf.file.keystore.type'));   gs.info(\"kmf.file.keystore.imk.alias: \" + gs.getProperty('kmf.file.keystore.imk.alias'));\n\n   You would expect the below output come after execution  which is expected if KMF is not configured:\n\n2. For configuring the KMF properties  we need to create the file properties configuration file.  \n   kmf.file.keystore.name would be set to \"keystorekmf.bcfks\" (Example Covert PKCS12 to BCFKS) and kmf.file.keystore.imk.alias would be set to \"256bitkey\" (Example Create Instance Root Key).\n   **IMPORTANT: if the kmf.file.keystore.path is not specified  it defaults to \"conf\" -- the path resolves then to ${glide.home.dist}\/conf. ${glide.home.dist} is the root folder of the glide instance.**\n\n   **IMPORTANT: for the kmf.file.keystore.path do not set an absolute path to the keystore location  it should be relative to ${glide.home.dist}.**\n\n   # vi \/glide\/nodes\/\\<Node_name\\>\/conf\/overrides.d\/glide.kmf.keystore.properties and change the file owner\/group to servicenow.\n\n   * kmf.file.keystore.enabled = true\n   * kmf.file.keystore.path = conf\/overrides.d\n   * kmf.file.keystore.name = \\<Keystore_name\\>.bcfks\n   * kmf.file.keystore.type = BCFKS\n   * kmf.file.keystore.password = \\<Password_youput_while_Creating_Root_Instance_Key\\>\n   * kmf.file.keystore.imk.alias = 256bitkey\n   For example:\n3. Reload the properties.  \n   Note: if KMF is already installed and the instance running  a re-load of system properties in memory is needed after populating the system properties in-file.\n   Logon to your instance \\> Click to 'All' \\> In the Search bar \\> Under System Definition \\> Scripts -Background \\> put the below mentioned KMF properties to reload properties \\> Run Script 'Global' \\> hit Enter\n\n   * GlideProperties.reload();  \n4. Re-checking the status of KMF.  \n   Logon to your instance \\> Click to 'All' \\> In the Search bar \\> Under System Definition \\> Scripts -Background \\> put the below mentioned KMF properties to reload properties \\> Run Script 'Global'.\nYou need to put the below entries in 'Script-Background' to verify it.   gs.info(\"kmf.file.keystore.enabled: \" + gs.getProperty('kmf.file.keystore.enabled'));\ngs.info(\"kmf.file.keystore.path: \" + gs.getProperty('kmf.file.keystore.path'));\ngs.info(\"kmf.file.keystore.name: \" + gs.getProperty('kmf.file.keystore.name'));\ngs.info(\"kmf.file.keystore.type: \" + gs.getProperty('kmf.file.keystore.type'));\ngs.info(\"kmf.file.keystore.imk.alias: \" + gs.getProperty('kmf.file.keystore.imk.alias'));\n   Now you would see details about the properties configured as per above. For example:\n\n\n# Validation and Test On-Premise KMF\n\nThis section describes a few ways you can verify\/test KMF on the on-premises instance.\n\n**1. KMF Health Page**\nReleased in San Diego  the KMF Health Page provides current state information. In order to access this page  you need to have the role assigned (can be seen by maint).\n\nLogon to your instance \\> Click to 'All' \\> In the Search bar \\> Key Management \\> Diagnostics\n\nFor on-premises instances  the KMF Health Page should indicate the following:\n\n* Key Secure \\> Disabled\n* File Key Store \\> Operational\n* Glide Encrypter \\> Operational\n* Instance Key Encryption \\> Operational\n* Instance HMAC Key \\> Operational\n* Vault PKI \\> Disabled (Disabled for all sub-items)\n* EJBCA PKI \\> Disabled (Disabled for all sub-items)\n* Instance PKI \\> Malfunction (Malfunction for all sub-items)  \n    Snapshot example follows:\n\n**2. KMF CryptoOperation API - Wrap \/ Unwrap Test**\n\nLogon to your instance \\> Click to 'All' \\> In the Search bar \\> Under System Definition \\> Scripts -Background \\> put below data \\> Run Script 'Global'\n\n\/\/ Symmetric key wrapping\nvar op1 = new sn_kmf_ns.KMFCryptoOperation(\"instance_level_glide_encrypter\" \"SYMMETRIC_WRAPPING\").withAlgorithm(\"AES\").withInputFormat(\"KMFNone\");\nvar wrappedKey = op1.doOperation(\"myplainkey\");\n\n\/\/ Symmetric key unwrapping\nvar op2 = new sn_kmf_ns.KMFCryptoOperation(\"instance_level_glide_encrypter\" \"SYMMETRIC_UNWRAPPING\").withInputFormat(\"FORMATTED\").withAlgorithm(\"AES\");\nvar unwrappedKey = GlideStringUtil.base64Decode(op2.doOperation(wrappedKey));\ngs.info(\"Unwrapped: \" + unwrappedKey);\n\nYou would expect to see below output (example):\n\nIf you repeat above step  you would below output which is expected.\n\n**![](sys_attachment.do?sys_id=ee4d571747e7a1903b05ff48436d43b8)**\n\n# Misc. Troubleshooting\n\n1. First and foremost  per the KMF Health Page check  if the page reports the presence of demo data  execute the following command in background scripts in global scope. For clearing demo data  you need to run the below script.   \n\nLogon to your instance \\> Click to 'All' \\> In the Search bar \\> Under System Definition \\> Scripts -Background \\> put below data \\> Run Script 'Global'\n\n* new SNC.KMFAutomationInterface().cleanUpKMFDemoData();\n\n2. Per the KMF Health Page indicates missing Instance Key Encryption Key and\/or Instance HMAC Key and missing components in the Glide Encrypter  execute the following command in background scripts in global scope:\n\nLogon to your instance \\> Click to 'All' \\> In the Search bar \\> Under System Definition \\> Scripts -Background \\> put below data \\> Run Script 'Global'\n\n* (new SNC.KMFGlideEncrypterAPI()).generateGlideEncrypterModule();\n\n3. Reload the properties in the background.\n\nLogon to your instance \\> Click to 'All' \\> In the Search bar \\> Under System Definition \\> Scripts -Background \\> put the below mentioned KMF properties to reload properties \\> Run Script 'Global' \\> hit Enter\n\n* GlideProperties.reload();\n\n4. In most cases  it's more straight forward to generate the keystore from scratch as described above. However  alternative to creating the keystore from scratch is to convert an existing JKS\/PKCS12 keystore to a BCFKS keystore. Below would be the notional command  with an example.\n\n* Re keystore  you may try to convert keystore JKS  PKCS12 to BCFKS. Below is the notional command.  \n  # keytool -importkeystore -v -srckeystore \\<PATH_OF_KEYSTORE\\>\/\\<KEYSTORE\\> -srcstoretype \\<CURRENT_FORMAT_TYPE_OF_KEYSTORE\\> -providername BCFIPS -providerclass org.bouncycastle.jcajce.provider.BouncyCastleFipsProvider -providerpath \\<PATH_OF_bc-fips_USUALLY_UNDER \/glide\/nodes\/\\<node\\>\/lib\/jsw\/bc-fips-1.0.2.jar\\> -destkeystore \/glide\/nodes\/\\<NODE\\>\/conf\/overrides.d\/\\<NAME_OF_KEYSTORE\\>.bcfks -deststoretype BCFKS\n\n  An example of the above command is provided below - it assumes you are at location: \/glide\/nodes\/\\<NODE\\>\/conf\/overrides.d\/:\n\n  # keytool -importkeystore -v -srckeystore \/glide\/java\/jre\/lib\/security\/cacerts -srcstoretype JKS -providername BCFIPS -providerclass org.bouncycastle.jcajce.provider.BouncyCastleFipsProvider -providerpath .\"\n\n",
        "QUERY": " 'kmf on-prem to cloud migration'",
        "GROUND_TRUTH": "To migrate KMF from on-premise to cloud, follow these steps:\n\n1. Analyze the KMF Password2 Splash Zone: Log into your on-premise instance with your admin account. Navigate to System Definition > Script - Background Script. Execute the provided script in global scope to find all KMF Password2 encrypted data. Examine the output of the script. If there are only a few records, it will be more efficient to manually re-input the Password2 data on the cloud instance. If there is a large number of records, create a case and ask the support engineer to create a case task to the KMF Team to discuss further options.\n\n2. Password2 Data Re-Entry: Proceed with your on-premise to cloud migration. Once the migration is complete, on the cloud instance, verify that the instance is KMF healthy and operational. If it is not, open a case with support with the recommendation that a case task for the KMF Team is opened. On the cloud instance, manually re-input the Password2 data. The instance will encrypt the data with cloud-based cryptographic keys. Use the output in Step 1 to locate the appropriate records for re-entry. Once complete, test the changes that you made, depending on your use case.\n\nRemember, this document applies to Password2 data ONLY. If your customer also supports Column Level Encryption, please reach out to the Column Level Encryption Team for guidance."
    },
    {
        "id": 111,
        "CONTEXT": "\"## Business Rules FAQ\n\n## Table of Contents\n\n* [Why are variables on sc_task not returning expected results in business rule?](#mcetoc_1eivfhctq2i)\n* [Why am I seeing incident states and journal entries multiple times on record?](#mcetoc_1eivfhctq2j)\n* [Why is my business rule not triggering?](#mcetoc_1eivfhctq2k)\n* [Why is my inactive business rule still running?](#mcetoc_1eivfhctq2k)\n* [Why is my business rule not entering the if\/for\/while loop?](#mcetoc_1eivfhctq2k)\n* [Why is my business rule not working for a specific record?](#mcetoc_1eivfhctq2k)\n\n# **Frequently Asked Questions** {#mcetoc_1eivfhctq2g}\n\nThe goal of this article is to answer generic frequent requests\/questions ServiceNow Technical Support receives in relation to clones. I if you have follow-up questions  please contact Technical Support.\n\nThis KB will also be helpful: [Business Rule Best Practices](\/kb?id=kb_article_view&sysparm_article=KB0722983 \"Business Rule Best Practices\")\n\n### Why are variables on sc_task not returning expected results in business rule? {#mcetoc_1eivfhctq2i}\n\nThis can happen due to the scope of the variables. If the variable has global selected  the variable is available for all catalog tasks within service catalog workflows or execution plans by default. If deselected  the variable must be associated with individual catalog tasks.\n\nHere is some documents to help:\n\n[https:\/\/docs.servicenow.com\/bundle\/quebec-application-development\/page\/script\/business-rules\/concept\/c_UsingPredefinedGlobalVariables.html](https:\/\/docs.servicenow.com\/bundle\/quebec-application-development\/page\/script\/business-rules\/concept\/c_UsingPredefinedGlobalVariables.html)\n\n[https:\/\/docs.servicenow.com\/bundle\/quebec-servicenow-platform\/page\/product\/service-catalog-management\/task\/t_CreateAVariableForACatalogItem.html](https:\/\/docs.servicenow.com\/bundle\/quebec-servicenow-platform\/page\/product\/service-catalog-management\/task\/t_CreateAVariableForACatalogItem.html)\n\nAlso to note making the variable global is only specific to that catalog item  it should not affect any other variable with the same name on another catalog item.\n\n### Why am I seeing incident states and journal entries multiple times on record? {#mcetoc_1eivfhctq2j}\n\nNormally this issue is occurring due to some looping in your logic. If you are able to reproduce the issue I would always advise turning on Business rule debugging and see if any business rule on the incident is running twice. In most cases a business rule should only run once on a record  so seeing a business rule twice on a record is a clear sign that duplicate information can be found on a record.\n\nI would check that business rule and also any other business rules if they have any current.update() in the code. If so this is your biggest culprit. This normally happens as a business rule runs do to a change on a record maybe an update  you then run actions because of that business rule to change that record  then run current.update() which would run another update on that record which then would trigger the Business rule to run again which causes duplicates.\n\nIf you are looking to update a record in the business rule that triggered from that same record  use a before update BR. This will allow you to catch the update and then adjust the update statement before it's pushed to the database. This would mean no need for a second update that will cause the duplication.\n\n### Why is my business rule not triggering? {#mcetoc_1eivfhctq2k}\n\nThe first thing to check if a business rule is not triggering is to turn on debug business rules and reproduce the issue. Look to see if the business rule is actually being found and check if potentially it's not meeting the set condition. If you see the business rule but failing the set condition  you will need to walk through the condition and try and find which part is not correct. Some conditions can have multiple things to check  so if you can manually validate all of them to see an issue first  then if no luck you can try and remove all the conditions and add them back one at a time to try and narrow down which condition is the issue.\n\nAnother method is to remove the checks from the condition and add them into the script in the business rules and use gs.info statements to display the values so you can see live what data is found by this business rule  sometimes you may find the data you are getting is not what you thought it was. This will then help you adjust and fix up the conditions.\n\nIf by chance you don't see your business rule triggering at all  this could be a few reasons but the most common is a setWorkflow(false) in your code which will stop business logic from running which is business rules. This is used in code to stop certain things from triggering in the system when doing specific work  but it can also have the after effect of business rules not triggering. If you have this then would look for any business rules that have setWorkflow(false) and see if they may be triggering before this Business rule and causing this issue. \n\n## Creation of duplicate Scanned application when no CI look up rules are matched\n\n## Description\n\nStarting with version 19.0.4 of Vulnerability Response that was released in August and November 2023  a duplicate discovered application record for a scanned application is created if lookup rules do not find a match to existing records. The duplicate record is created and referenced as a discovered application. As a result  this duplicate discovered application record also changes whenever the scanned application record is updated  which increases the table size and thus might impact workflow performance downstream. Here\n\n## Steps to Reproduce\n\n* On running the AVR integration  if there is no CI lookup rule that matches to a CI.\n* Once the integration is run  check the scanned application table for duplicate records for the same scanned application.\n\n## Workaround\n\n**Solution:**\n\n* If you are using a version of Vulnerability Response that was released prior to August 2023  you must upgrade to the Vulnerability Response application released in February 2024 to fix the issue.\n* If you have concerns about upgrading  alternatively  you can update your version. Updates have been provided for the versions of Vulnerability Response that were released in August and November of 2023.\n* If you are using the version of Vulnerability Response that was released in August 2023  use the August update set ([august_update_set](sys_attachment.do?sys_id=717e78e2478ff150f93138ce536d43ac)) to fix the issue.\n* If you are using the version of Vulnerability Response that was released in November 2023  use the November update set ([nov_update_set](sys_attachment.do?sys_id=f17ef8e2478ff150f93138ce536d4336)) to fix the issue.\n* Attached is a [on-demand scheduled job](sys_attachment.do?sys_id=b17ef8e2478ff150f93138ce536d43de) that you can use to remove the duplicate scanned applications records if they are already created.\n\n**For the update:**\n\nWhen updating to a new version of Vulnerability Response  you must first revert back to the Store release version of the script include and then perform the update to avoid any conflicts. This helps you avoid potential conflicts  as the fix for this issue starts with the February 2024 version of Vulnerability Response.\n\n**Related Problem: PRB1717292**\n\n\n\n## Duplicate alerts and alert CI binding fails when domain separation is activated with Alert Playbook \n\n## Description\n\nOn domain separated instances with event management and Alert Playbook we might see:\n\n1. \"Duplicate alerts\" (some alerts have identical message_key values as existing alerts)\n2. Problems with CI binding to alerts  where CIs exist and should bind  but they don't\n3. Intermittently Event Rules are not found for events in the non-global domain  \n   4. Intermittently event rule in the wrong Domain is applied\n\nThese problems occur with the installation or existence of \"Service Operations Workspace Alert Mngmt\" store application  on versions that don't include the workaround (DEF0457725).\n\nThe problem is that PD Engine doesn't switch domains properly so when the Alert Playbook flows of SOW Alert Mngmt run there are domain problems where alerts and CIs are looked for in the previous processed domain instead of the current domain. The snapshot of the Alert Playbook flows were created before Utah which causes them to trigger the PD Engine flows.\n\nThe root cause lays in Playbook Experience code which mis-uses domain separation function call and fails to revert (close) domain changes.\nThe root cause of these problems is a shared problem to two of them and it is supposed to be handled by DEF0444365 ([PRB1699994](https:\/\/support.servicenow.com\/problem.do?sys_id=3db3f4eb939d35d0101833527cba1076){#x_snc_defect_defect.problem_link_link}).\n\nA permanent workaround is supposed to be delivered with DEF0457725 (P[RB1710690](https:\/\/support.servicenow.com\/problem.do?sys_id=8a1d80f3c34a71149c9971dc7a0131bf){#x_snc_defect_defect.problem_link_link}). For an immediate workaround  scroll down to the **Workaround** section.\n\n## Steps to Reproduce\n\n**The following reproduction steps will only reproduce the CI binding problem  which is a problem that is shared with the duplicate alerts problem  so it's enough.**\n\n**Prerequisites:**\n\n1. Order an instance Utah Patch 5\n2. Install Domain Separation plugin\n3. Install Event Management plugin\n4. Install Playbook Experience application\n5. Install Service Operations Workspace Alert Mngmt (app-sow-hlt-em) store application  version 21.2.4\n\nNote: Issue's 3 4 mentioned in the Description are seen in the Vancouver release as well\n\n**Reproduction:**\n\n1. Run the script below\n2. Navigate to em_event\n3. Choose the two records and change their State column to Ready at the same time (bulk\/multiple update)\n\n**EXPECTED:**\n\\* Both alerts are bound to their CIs\n\n**ACTUAL:**\n\\* The event that was processed first will have its alert bound to a CI\n\\* The other alert won't be bound\n\n**SCRIPT:**\n\n\/\/ ========== START SCRIPT ==========\nclear();\ncreate();\n\nfunction create() {\ngs.info('CSTASK655438 - Creating events')\n\nvar CI_NAME_1 = 'lnux1';\nvar CI_NAME_2 = 'lnux2';\nvar COMPANY_NAME_1 = 'CompanyDomain1';\nvar COMPANY_NAME_2 = 'CompanyDomain2';\nvar TOP_DOMAIN_ID = '774190f01f1310005a3637b8ec8b70ef';\nvar domTop1Id = getDomainId('Domain1'); \/\/ domain without CI\nvar domTop2Id = getDomainId('Domain2'); \/\/ domain with CI\ngetSetCiInDomain(CI_NAME_1  domTop1Id  COMPANY_NAME_1);\ngetSetCiInDomain(CI_NAME_2  domTop2Id  COMPANY_NAME_2);\n\nvar msgKey1 = 'MN_' + new GlideDateTime().getValue() + \"_\" + Math.random();\ncreateEvent(domTop1Id  msgKey1  CI_NAME_1);\n\ngs.sleep(5000);\n\nvar msgKey2 = 'MN_' + new GlideDateTime().getValue() + \"_\" + Math.random();\ncreateEvent(domTop2Id  msgKey2  CI_NAME_2);\n\ngs.sleep(5000);\n\n\/\/ updateEvents(\\[domTop1Id  domTop2Id\\]  \\[msgKey1  msgKey2\\]);\n\nfunction getSetCiInDomain(ciName  domainId  companyName) {\nvar gr = new GlideRecord('core_company');\ngr.addQuery('name'  companyName);\ngr.addQuery('sys_domain'  domainId);\ngr.query();\nif (!gr.next()) {\ngr.initialize();\ngr.setValue('name'  companyName);\ngr.setValue('sys_domain'  domainId);\ngr.insert();\n}\nvar companyId = gr.getUniqueValue();\n\nvar gr = new GlideRecord('cmdb_ci_linux_server');\ngr.addQuery('name'  ciName);\ngr.addQuery('sys_domain'  domainId);\ngr.addQuery('company'  companyId);\ngr.query();\nif (gr.next())\nreturn;\n\ngr.initialize();\ngr.setValue('sys_domain'  domainId);\ngr.setValue('name'  ciName);\ngr.setValue('company'  companyId);\ngr.insert();\n}\n\nfunction getDomainId(name) {\nvar gr = new GlideRecord('domain');\ngr.get('name'  name);\nif (gr.isValidRecord())\nreturn gr.getUniqueValue();\n\ngr.initialize();\ngr.setValue('parent'  TOP_DOMAIN_ID);\ngr.setValue('name'  name);\ngr.setValue('type'  'Customer');\nreturn gr.insert();\n}\n\nfunction createEvent(domainId  msgKey  ciName) {\nvar gr = new GlideRecord('em_event');\ngr.initialize();\ngr.description = 'Test event domain: ' + domainId;\ngr.metric_name = 'eve11';\ngr.source = 'MN';\ngr.severity = (1 + Math.floor(Math.random() \\* 4)).toString();\ngr.node = ciName;\ngr.sys_domain = domainId;\ngr.state = 'Ignored';\ngr.additional_info = JSON.stringify({\n\n});\ngr.message_key = msgKey;\ngr.insert();\n}\n\nfunction updateEvents(domainIds  msgKeys) {\nvar gr = new GlideRecord('em_event');\ngr.addQuery('sys_domain'  'IN'  domainIds.join(' '));\ngr.addQuery('message_key'  'IN'  msgKeys.join(' '));\ngr.orderBy('sys_created_on');\ngr.query();\nwhile (gr.next()) {\ngr.setValue('state'  'Ready');\ngr.update();\ngs.sleep(2000);\n}\n}\n\ngs.info('CSTASK655438 - Finished creating events')\n}\nfunction clear() {\nvar agr = new GlideRecord('em_alert');\nagr.addQuery('state'  'IN'  'Open  Reopen Flapping');\nagr.query();\nagr.setValue('state'  'Closed');\nagr.updateMultiple();\n\nvar tables = \\[\n'sa_agg_group_alert_staging' \n'sa_agg_pattern_alert' \n'sa_analytics_alert' \n'sa_rca_group' \n'sa_rca_group_alert' \n'sa_performance_statistics' \n'sys_step_pattern' \n'em_agg_group' \n'em_agg_group_alert' \n'em_agg_group_cmdb' \n'em_agg_group_prediction' \n'em_alert' \n'em_event' \n'em_alert_anomaly' \n'em_alert_history' \n'em_impact_graph' \n'em_alert_extra_data'\n\\];\n\nfor (var ti = 0; ti \\< tables.length; ti++) {\nvar table = tables\\[ti\\];\nvar gr = new GlideRecord(table);\ngr.query();\ngr.deleteMultiple();\n}\n}\n\n\/\/ ========== END SCRIPT ==========\n\n## Workaround\n\nBy reactivating the Alert Playbook flows we can create new snapshots that will make those flows run on the new Flow Engine rather than PD Engine  so the domain switch problem will stop happening:\n\n1. Navigate to \"Process Automation Designer\" from the navigation menu  change scope to \"Service Operations Workspace Alert Mngmt\"\n2. Open the \"Alert Playbook\" flow\n3. \"\n\n",
        "QUERY": " 'Journal entries duplication issue on the record'",
        "GROUND_TRUTH": "The duplication of journal entries on a record is usually due to some looping in your logic. If you can reproduce the issue, it is advised to turn on Business rule debugging and see if any business rule on the incident is running twice. A business rule should only run once on a record, so seeing a business rule twice on a record is a clear sign that duplicate information can be found on a record. \n\nYou should check that business rule and also any other business rules if they have any current.update() in the code. This usually happens as a business rule runs due to a change on a record maybe an update, you then run actions because of that business rule to change that record, then run current.update() which would run another update on that record which then would trigger the Business rule to run again which causes duplicates.\n\nIf you are looking to update a record in the business rule that triggered from that same record, use a before update BR. This will allow you to catch the update and then adjust the update statement before it's pushed to the database. This would mean no need for a second update that will cause the duplication."
    },
    {
        "id": 112,
        "CONTEXT": "\"## Duplicate Discovery of Switch in One Discovery Schedule\n\n* **Probe-based Discovery for Switch**\n\nDuring Probe-based discovery of a switch with multiple IP addresses in the same range  discovery picks one IP address to discover and ignores the other IP addresses. The IP addresses that were skipped show up in the discovery log as **Identified  Ignored extra IP** . In the example shown below  discovery updated only 1 CI (with IP 3.134.56.5) and ignored the other 10 IP addresses. This behavior is expected and documented in KB0677974 [One configuration item identified for multiple IP addresses](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB0677974 \"One configuration item identified for multiple IP addresses\").\n\n* **Pattern-based Discovery for Switch**\n\nDiscovering the same switch using Pattern-based discovery (Network Switch Pattern) causes multiple discoveries to occur. Instead of discovering 1 IP address and ignoring other IPs  the pattern re-discovers the switch multiple times. In the example below  when running one discovery schedule  the switch is re-discovered 8 times. Only 3 of the IP addresses were ignored.\n\n* **The duplicate discoveries cause multiple issues** :  \n  * The discovery schedule takes longer to complete.\n  * The Discovery Log and ECC Queue fills up with additional messages from the duplicate discoveries. In the Discovery Log  the duplicate discoveries start within seconds of each other (Created time stamp).\n  * Additional bandwidth is being used across the network.\n  * The CI Audit History fills up with additional IP Address changes for each duplicate discovery. The ip_address field on the switch keeps flip-flopping back and forth. In the example  the IP address flip-flops 61 times during 1 discovery schedule run.  \n* **Probable Cause**\n\nIn Probe-based discovery  there is enough information in the Identification phase to continue discovering 1 IP address and ignore the others. In Pattern-based discovery  after the Classify phase there isn't enough information to determine which IP addresses to ignore. The switch pattern (with combined Identify and Exploration phases) continues discovery on the IP address without knowledge of the other IPs. The result is duplicate discoveries on the same switch.\n\n* **Knowledge Articles**\n\nThere are 2 similar KB articles that identify the issue in detail  but the 2 properties (glide.discovery.ip_based.active = false and glide.discovery.device.duplicate.ip.optimization = true) in the KB does **NOT** resolve the duplicate discoveries. The KB articles are as follows:\n\nKB0781931 [Horizontal Discovery does not detect Extra IP address in the same discovery schedules](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB0781931 \"Horizontal Discovery does not detect Extra IP address in the same discovery schedules\")\n\nKB0759072 [Pattern based discovery does not detect extra IP Addresses leading to concurrent discoveries and higher DB load](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB0759072 \"Pattern based discovery does not detect extra IP Addresses leading to concurrent discoveries and higher DB load\")\n\n* **Work around**\n\nA work around for the duplicate discoveries is to exclude all but one IP address for each switch in the discovery schedule. The process for excluding the IP addresses is outlined in community article [Excluding an IP from discovery](https:\/\/community.servicenow.com\/community?id=community_question&sys_id=1ecb1123dbe2c8945ed4a851ca961912&view_source=searchResult \"Excluding an IP from discovery\").\n\n\n\n## Validate discovery results\n\n# Schedule a horizontal Discovery {#ariaid-title1}\n\nA Discovery schedule determines what horizontal Discovery searches for  when it runs  and which MID Servers are used. Create a Discovery schedule for your local environment or a schedule for discovering the resources in your cloud service account.\nEnsure that your Discovery schedule conforms to security best practices  such as limiting the range of discovery targets and using the most secure credentials.\n\nMake sure to [test your\ncredentials](..\/product\/credentials\/task\/t_CreateCredential.dita\/t_CreateCredential.html) before you run a schedule. Bad credentials are a leading cause of failed discoveries.\n\nRoles required: admin  discovery_admin\nYou can use a Discovery schedule to launch horizontal Discovery  which uses probes  sensors  and pattern operations to scan your network for CIs. Use this procedure to create a schedule manually from the Discovery schedule form.\n\nService Mapping also provides a Discovery schedule for top-down Discovery. See [Schedule a top-down discovery by Service Mapping](..\/..\/service-mapping\/task\/t_CreateDiscoSchedForCITypes.html \"After Service Mapping discovers configuration items (CIs) belonging to your application service for the first time  it then rediscovers CIs to find changes and updates. Create or modify discovery schedules to control how often Service Mapping rediscovers services or CIs. For example  you may create custom discovery schedules to avoid redundant stress on the infrastructure.\") for more information.\nUse the Discovery schedule module in the Discovery application to:\n\n* Configure a schedule to discover resources in your cloud service account.\n* Configure a schedule to discover certificates from URL scans.\n* Configure device identification by IP address or other identifiers.\n* Determine if credentials are used in device probes.\n* Name the MID Server to use for a particular type of Discovery.\n* Create or disable a schedule that controls when the Discovery runs in your network.\n* Configure the use of multiple Shazzam probes for load balancing.\n* Configure the use of multiple MID Servers for load balancing.\n* Run a Discovery schedule manually.\n* Run Discovery on a single IP address.\n{#t_CreateADiscoverySchedule__ul_ytx_x4b_5p}  \nNote: To view the run-results of your schedules for both IP-based and Cloud Discovery  use the summaries on the [Discovery Home page](..\/concept\/discovery-home-page.html \"The Discovery Home page provides a summary of discoveries that were triggered by cloud and non-cloud schedules. You can view any errors that occurred during a Discovery and find remediation suggestions. Use the Home page view to examine the cloud resources discovered for the service accounts you selected in a cloud schedule.\"). The Home page publishes the details of any errors that might have occurred and displays possible actions to take to remediate problems.\n\n1. Navigate to All \\> Discovery \\> Discovery schedules to create a new record.\n2. Select the type of schedule to open:  \n   * New: Creates a new horizontal schedule for discovering components in your network.\n   * Quick Discovery: Runs an horizontal Discovery on a single IP address without requiring a schedule.\n   * Create a Cloud Discovery schedule: Creates a schedule  using the Discovery Manager wizard  for discovering resources in a cloud service account.\n   {#t_CreateADiscoverySchedule__ul_j3h_tq1_zhb}\n3. Complete the Discovery schedule form  using the fields in the table. {#t_CreateADiscoverySchedule__ph_Step3Content}\n4. Right-click in the header of the record and select Save from the context menu. {#t_CreateADiscoverySchedule__cmd_Step4}\n5. To create a range of IP addresses to discover  click Quick Ranges under Related Links.  \n   Note: To improve security  limit the range of discovery targets to exclude unnecessary networks and devices.  \n   Figure 1. Discovery schedule ![Discovery schedule](..\/image\/DiscoverySchedule.png)  \n   {#d280650e329}{#t_CreateADiscoverySchedule__row_name578654}{#t_CreateADiscoverySchedule__row_Active739273}{#t_CreateADiscoverySchedule__row_MaxRunTime787656}{#t_CreateADiscoverySchedule__row_Run893873}\n   |            Field            |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n   |-----------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n   | Name                        | Enter a unique  descriptive name for your schedule.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n   | Discover                    | Select one of the following scan types: * Configuration items: Uses Discovery identifiers to match devices with CIs in the CMDB and update the CMDB appropriately. Perform a simple discovery by selecting a specific MID Server to scan for all protocols (SSH  WMI  and SNMP). Or  perform advanced discoveries with discovery behaviors. When you select a behavior  the MID Server field is not available. * IP addresses: Scans devices without the use of credentials. These scans discover all the active IP addresses in the specified range and create device history records  but do not update the CMDB. IP address scans also show multiple IP addresses that are running on a single device. Identify devices by class and by type  such as Windows computers and Cisco network gear. The Max range size Shazzam probe property determines the maximum number of IP addresses Shazzam scans. \n\n## Schedule a horizontal Discovery\n\n# Schedule a horizontal Discovery {#ariaid-title1}\n\nA schedule determines what horizontal searches for  when it runs  and which are used. Create a Discovery schedule for your local environment or a schedule for discovering the resources in your cloud service account.\nEnsure that your Discovery schedule conforms to security best practices  such as limiting the range of discovery targets and using the most secure credentials.\n\nMake sure to [test your credentials](..\/..\/credentials\/task\/t_CreateCredential.html \"Create and test the credentials that Discovery  Service Mapping  Cloud Management  and Orchestration require to access hardware and software in your network.\") before you run a schedule. Bad credentials are a leading cause of failed discoveries.\n\nRoles required: admin  discovery_admin\nYou can use a Discovery schedule to launch horizontal Discovery  which uses probes  sensors  and pattern operations to scan your network for CIs. Use this procedure to create a schedule manually from the Discovery schedule form.\n\nService Mapping also provides a Discovery schedule for top-down Discovery. See [Schedule a top-down discovery by Service Mapping](..\/..\/service-mapping\/task\/t_CreateDiscoSchedForCITypes.html \"After discovers configuration items (CIs) belonging to your application service for the first time  it then rediscovers CIs to find changes and updates. Create or modify discovery schedules to control how often rediscovers services or CIs. For example  you may create custom discovery schedules to avoid redundant stress on the infrastructure.\") for more information.\nUse the Discovery schedule module in the Discovery application to:\n\n* Configure a schedule to discover resources in your cloud service account.\n* Configure a schedule to discover certificates from URL scans.\n* Configure device identification by IP address or other identifiers.\n* Determine if credentials are used in device probes.\n* Name the to use for a particular type of .\n* Create or disable a schedule that controls when the runs in your network.\n* Configure the use of multiple Shazzam probes for load balancing.\n* Configure the use of multiple for load balancing.\n* Run a Discovery schedule manually.\n* Run on a single IP address.\n{#t_CreateADiscoverySchedule__ul_ytx_x4b_5p}  \nNote: To view the run-results of your schedules for both IP-based and Cloud Discovery  use the summaries on the [Discovery Home page](..\/concept\/discovery-home-page.html#discovery-home-page \"The Discovery Home page provides a summary of discoveries that were triggered by cloud and non-cloud schedules. You can view any errors that occurred during a Discovery and find remediation suggestions. Use the Home page view to examine the cloud resources discovered for the service accounts you selected in a cloud schedule.\"). The Home page publishes the details of any errors that might have occurred and displays possible actions to take to remediate problems.\n\n1. Navigate to All \\> Discovery \\> Discovery schedules to create a new record.\n2. Select the type of schedule to open:  \n   * New: Creates a new horizontal schedule for discovering components in your network.\n   * Quick Discovery: Runs an horizontal on a single IP address without requiring a schedule.\n   * Create a Cloud Discovery schedule: Creates a schedule  using the Discovery Manager wizard  for discovering resources in a cloud service account.\n   {#t_CreateADiscoverySchedule__ul_j3h_tq1_zhb}\n3. Complete the Discovery schedule form  using the fields in the table. {#t_CreateADiscoverySchedule__ph_Step3Content}\n4. Right-click in the header of the record and select Save from the context menu. {#t_CreateADiscoverySchedule__cmd_Step4}\n5. To create a range of IP addresses to discover  click Quick Ranges under Related Links.  \n   Note: To improve security  limit the range of discovery targets to exclude unnecessary networks and devices.  \n   Figure 1. Discovery schedule ![Discovery schedule](..\/image\/DiscoverySchedule.png)  \n   {#d2445748e319}{#t_CreateADiscoverySchedule__row_name578654}{#t_CreateADiscoverySchedule__row_Active739273}{#t_CreateADiscoverySchedule__row_MaxRunTime787656}{#t_CreateADiscoverySchedule__row_Run893873}\n   |            Field            |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |\n   |-----------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------...",
        "QUERY": " 'Issue in detecting additional IP address during horizontal discovery'",
        "GROUND_TRUTH": "The issue in detecting additional IP addresses during horizontal discovery is due to the difference in the way Probe-based and Pattern-based discoveries work. In Probe-based discovery, the system picks one IP address to discover and ignores the other IP addresses in the same range. This is expected behavior and is documented in KB0677974. However, in Pattern-based discovery, the system re-discovers the switch multiple times instead of ignoring the extra IP addresses. This leads to multiple issues such as longer discovery schedules, additional messages in the Discovery Log and ECC Queue, increased network bandwidth usage, and frequent changes in the IP address field on the switch. The cause of this issue is that after the Classify phase in Pattern-based discovery, there isn't enough information to determine which IP addresses to ignore. Therefore, the switch pattern continues discovery on the IP address without knowledge of the other IPs, resulting in duplicate discoveries. A workaround for this issue is to exclude all but one IP address for each switch in the discovery schedule. The process for excluding the IP addresses is outlined in the community article \"Excluding an IP from discovery\"."
    },
    {
        "id": 113,
        "CONTEXT": "\"## Why you should not increase the max length of the User ID field [sys_user.user_name] which is limite\n\nThe User ID is a **unique identifier** for the user's ServiceNow login user name. The maximum length of the User ID field in the User \\[sys_user\\] table is 40  which means the User ID in the User table can only have 40 or fewer characters:\n\nSo if you enter\/update a User ID manually  the platform will not allow you to enter more than 40 characters. Also  if you create\/update a user in other ways than manually (via an integration  XML upload  etc.) and provide a User ID that has more than 40 characters  only the first 40 characters will be kept and saved.\n\nYou may have a requirement to have User IDs with more than 40 characters and the platform would allow you to increase the Max Length of the \\[sys_user.user_name\\] field. However  **we advised not to increase the max length of the User ID field**because it might create issues.\n\nWhile some of the issues caused by changing this value have been addressed over the years  the fact is that the platform has not been designed to handle User IDs with more than 40 characters.\n\n**What else is dependent on the User ID field?**\n\nThe User ID is used to populate the **Created by** and **Updated by** fields. These 2 read-only fields are automatically added to every table that is being created (except if the table extends another one as the 2 fields from the parent will be used). The max length of these 2 fields is 40.\n\n**What are the issues which can arise by increasing the max length of the user ID field?**\n\n1. The User IDs will be truncated to 40 characters when they are stored in the **Created by** and **Updated by** fields. This can lead to seeing values that look the same if the first 40 characters of different User IDs happen to be the same. **That can raise various issues such as:** \\* Confusion among users  \n   \\* Reports and filters returning incorrect results  \n   \\* Failing ACLs  \n   \\* Flow Designer flow getting stuck when the **Run As** field is set to 'User who initiates session'  \n   \\* etc.  \n2. Users with a User ID longer than 40 characters will also not be able to attach files to a record.\n\n\n\n## NowAnalyticsSDK - setUserId(userId: String)\n\n# NowAnalyticsSDK interface - Android {#ariaid-title1}\n\nThe NowAnalyticsSDK interface provides functions that enable you to configure analytics properties  user settings  and events for managing a collection of user analytics data.\n{#d306977e68}\n\n|     Name      |                                                                                                                   Type                                                                                                                   |                 Description                  |\n|---------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------|\n| configuration | [NowServiceConfiguration](..\/..\/NowServiceConfiguration\/concept\/NowServiceConfigurationAndroidAPI.html \"The NowServiceConfiguration class enables you to configure the ServiceNow instance URL and package name for a feature service.\") | Configuration to associate with the service. |\nTable 1. Properties{#NowAnalyticsAndroidInterface__table_vx2_klw_5pb}\n**Parent Topic:** [Mobile SDK API reference - Android](..\/..\/..\/..\/..\/..\/app-store\/dev_portal\/API_reference\/MobileSDKAndroid\/concept\/MobileSDKAndroidAPI.html \"The Mobile SDK for Android provides the classes necessary to interface Android devices with the ServiceNow platform.\")\n\n## NowAnalyticsSDK - addEvent(eventName: String) {#ariaid-title2}\n\nAdds an application event  such as a user reaching a specific level or screen. These events appear on the dashboard in the order that they occurred.\n{#d306977e230}\n\n|      Name       |  Type  |        Description        |\n|-----------------|--------|---------------------------|\n| named eventName | String | Name of the event to add. |\nTable 2. Parameters{#NAnaly-addEvent_S__table_ikl_dhm_tpb}\n{#d306977e270}\n\n| Type | Description |\n|------|-------------|\n| None |             |\nTable 3. Returns{#NAnaly-addEvent_S__table_jkl_dhm_tpb}\nThe following code example shows how to call this function to add an application event.\n\n## NowAnalyticsSDK - addEvent(eventName: String  props: MutableMap\\<String  Any\\>) {#ariaid-title3}\n\nAdds an application event  such as a user reaching a specific level or screen  and enables the setting of custom properties on the event. These events appear on the dashboard in the order that they occurred.\n{#d306977e356}\n\n|   Name    |                                            Type                                             |                                                                                                                                                                                                                  Description                                                                                                                                                                                                                  |\n|-----------|---------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| eventName | String                                                                                      | Name of the event to add.                                                                                                                                                                                                                                                                                                                                                                                                                     |\n| props     | [MutableMap](https:\/\/kotlinlang.org\/api\/latest\/jvm\/stdlib\/kotlin.collections\/-mutable-map\/) | Custom property key-value pairs for the event. The total size of the event name  property key and value cannot exceed 300 bytes per property. Properties that exceed this limit are ignored. Property keys may not contain dot ('.') or dollar ('$') signs  if they do  they are trimmed. Strings are UTF-8 encoded. Supported value types: * Integer * Double * Float * String * Url * Boolean * Date {#NAnaly-addEvent_S_S__ul_g41_kxy_spb} |\nTable 4. Parameters{#NAnaly-addEvent_S_S__table_ps4_j3m_tpb}\n{#d306977e438}\n\n| Type | Description |\n|------|-------------|\n| None |             |\nTable 5. Returns{#NAnaly-addEvent_S_S__table_qs4_j3m_tpb}\nThe following code example shows how to call this function to add an application event that includes custom properties for the event.\n\n## NowAnalyticsSDK - addScreenAction(actionName: String) {#ariaid-title4}\n\nAdds a custom action. These actions appear in the user dashboard as part of the session data and describes a screen change in an application.\n{#d306977e525}\n\n|    Name    |  Type  |                           Description                           |\n|------------|--------|-----------------------------------------------------------------|\n| actionName | String | Name of the action to add to the screen  such as MyButtonClick. |\nTable 6. Parameters{#NAnaly-addScreenAction_S__table_mcv_4km_tpb}\n{#d306977e568}\n\n| Type | Description |\n|------|-------------|\n| None |             |\nTable 7. Returns{#NAnaly-addScreenAction_S__table_ncv_4km_tpb}\nThe following code example shows how to call this function to add a custom action to the current screen.\n\n## NowAnalyticsSDK - appendToUserProperty(propertyName: String  item: String) {#ariaid-title5}\n\nAppends the specified item to the specified user property list.\n{#d306977e654}\n\n|     Name     |  Type  |                      Description                      |\n|--------------|--------|-------------------------------------------------------|\n| propertyName | String | Name of the user property to append the list item to. |\n| item         | String | List item to append to the property.                  |\nTable 8. Parameters{#NAnaly-appendToUserProperty_S_S__table_hnp_wgj_5pb}\n{#d306977e706}\n\n| Type | Description |\n|------|-------------|\n| None |             |\nTable 9. Returns{#NAnaly-appendToUserProperty_S_S__table_inp_wgj_5pb}\nThe following code example shows how to call this function to append a case to the specified list.\n\n## NowAnalyticsSDK - configure(instanceURL: URL  configureCallbacks: ConfigureCallbacks?) {#ariaid-title6}\n\nConfigures the specified ServiceNow instance URL and callbacks to NowAnalytics.\nOnce this method is called  the actual configuration is fetched from the associated ServiceNow instance and synced automatically.\n{#d306977e802}\n\n|        Name        |        Type        |                                                                                                Description                                                                                                 |\n|--------------------|--------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| instanceURL        | URL                | URL of the ServiceNow instance to associate with the NowAnalyticfs service.                                                                                                                                |\n| configureCallbacks | ConfigureCallbacks | Optional. Callback to execute whenever there is an error communicating to the ServiceNow instance. This callback can be called at any point in time and can be called multiple times for multiple reasons. |\nTable 10. Parameters{#NAnaly-configure_S_O__table_zhj_yhj_5pb}\n{#d306977e860}\n\n| Type | Description |\n|------|-------------|\n| None |             |\nTable 11. Returns{#NAnaly-configure_S_O__table_a3j_yhj_5pb}\nThe following code example shows how to call this function to configure the specified instance.\n\n## NowAnalyticsSDK - deleteCurrentUserData() {#ariaid-title7}\n\nDeletes all local and remote data for the current user.\nThis method also unsets the current active user and opts this device out of future analytics tracking.\nNote: This method performs a synchronous call to the SNAnalytics servers. \n\n## NowAnalyticsSDK - deleteCurrentUserData()\n\nIt is therefore recommended that you perform this on a side thread.\n{#d306977e951}\n\n| Name | Type | Description |\n|------|------|-------------|\n| None |      |             |\nTable 12. Parameters{#NAnaly-deleteCurrentUserData__table_tgk_jjj_5pb}\n{#d306977e989}\n\n|  Type   |                                                                                                                                Description                                                                                                                                |\n|---------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Boolean | Flag that indicates whether the deletion was successful. Possible values: * true: Deletion was successful. * false: Deletion failed. Failure may occur if the SNAnalytics servers cannot be reached  such as when there is no connectivity. The method should be retried. |\nTable 13. Returns{#NAnaly-deleteCurrentUserData__table_ugk_jjj_5pb}\nThe following code example shows how to call this function to delete the current user's data.\n\n## NowAnalyticsSDK - getTrackingConsent() {#ariaid-title8}\n\nReturns the current user's consent response to analytics tracking.\n{#d306977e1087}\n\n| Name | Type | Description |\n|------|------|-------------|\n| None |      |             |\nTable 14. Parameters{#NAnaly-getTrackingConsent__table_xlm_cmj_5pb}\n{#d306977e1125}\n\n|  Type   |                                                                                                     Description                                                                                                     |\n|---------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Boolean | Flag that indicates whether the current user consented to having their data collected. Possible values: * true: User agreed to having their data collected. * false: User opted out of having their data collected. |\nTable 15. Returns{#NAnaly-getTrackingConsent__table_ylm_cmj_5pb}\nThe following code example shows how to call this function to request the current user's consent response to analytics tracking.\n\n## NowAnalyticsSDK - incUserProperty(propertyName: String  value: Double) {#ariaid-title9}\n\nIncrements or decrements the value of the specified numeric property by the specified value.\n{#d306977e1222}\n\n|     Name     |  Type  |                                    Description                                     |\n|--------------|--------|------------------------------------------------------------------------------------|\n| propertyName | String | Name of the user property to increment.                                            |\n| value        | Double | Value to increment the property by. Enter a negative value to decrement the value. |\nTable 16. Parameters{#NAnaly-incUserProperty_S_D__table_nmn_zmj_5pb}\n{#d306977e1274}\n\n| Type | Description |\n|------|-------------|\n| None |             |\nTable 17. Returns{#NAnaly-incUserProperty_S_D__table_omn_zmj_5pb}\nThe following code example shows how to call this function.\n\n## NowAnalyticsSDK - installJavascriptInterface(webView: WebView) {#ariaid-title10}\n\nEnables the calling of javaScript.SNMobileAnalytics methods from within a {@link android.webkit.WebView}  using JavaScript.\n{#d306977e1361}\n\n|  Name   |   Type    |                   Description                    |\n|---------|-----------|--------------------------------------------------|\n| webView | WKWebView | The {@link android.webkit.WebView} to attach to. |\nTable 18. Parameters{#NAnaly-installJSInterface_O__table_cwq_14j_5pb}\n{#d306977e1401}\n\n| Type | Description |\n|------|-------------|\n| None |             |\nTable 19. Returns{#NAnaly-installJSInterface_O__table_dwq_14j_5pb}\nThe following code example shows how to call this function to enable the calling of javaScript.SNMobileAnalytics methods.\n\n## NowAnalyticsSDK - removeUserProperty(propertyName: String) {#ariaid-title11}\n\nDeletes the specified property for the current user.\n{#d306977e1487}\n\n|     Name     |  Type  |             Description              |\n|--------------|--------|--------------------------------------|\n| propertyName | String | Name of the user property to delete. |\nTable 20. Parameters{#NAnaly-removeUserProp_S__table_ovp_jrj_5pb}\n{#d306977e1527}\n\n| Type | Description |\n|------|-------------|\n| None |             |\nTable 21. Returns{#NAnaly-removeUserProp_S__table_pvp_jrj_5pb}\nThe following code example shows how to call this function to remove the \"Temp Cases\" property.\n\n## NowAnalyticsSDK - setListener(nowAnalyticsListener: NowAnalyticsListener?) {#ariaid-title12}\n\nSets a listener on the NowAnalytics' events such as session changes and automatic screen detections.\nNote: Every time you call this method  the listener is overridden.\n{#d306977e1618}\n\n|         Name          |         Type          |                                                                                                                                                                                                                                                                          Description                                                                                                                                                                                                                                                                          |\n|-----------------------|-----------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| nowAnalytics?Listener | NowAnalytics?Listener | The interface that NowAnalytics notifies for session events. * onNowAnalyticsScreenDetected: Called when NowAnalytics automatically detects a screen. * onNowAnalyticsSessionEnded: Called after a session ends. * onNowAnalyticsSessionEnding: Called just before a session is about to finish  allowing you to prevent the session from ending. * onNowAnalyticsSessionStarted: Called when a session has started. * onNowAnalyticsSessionStarting: Called just before a session is about to start  allowing you to prevent the session from being started. |\nTable 22. Parameters{#NAnaly-setListener_O__table_zsr_s3p_5qb}\n{#d306977e1675}\n\n| Type | Description |\n|------|-------------|\n| None |             |\nTable 23. Returns{#NAnaly-setListener_O__table_atr_s3p_5qb}\nThe following code example shows how to call this function to set the NowAnalyticsListener().\n\n## NowAnalyticsSDK - setTrackingConsent(consentGiven: Boolean) {#ariaid-title13}\n\nSets the current user's consent response to analytics tracking.\n{#d306977e1764}\n\n|     Name     |  Type   |                                                                                           Description                                                                                           |\n|--------------|---------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| consentGiven | Boolean | Flag that indicates the current user's consent for analytics tracking. Valid values: * true: User consented to having their data tracked. * false: User opted out of having their data tracked. |\nTable 24. Parameters{#NAnaly-setTrackingConsent_B__table_tq5_2sj_5pb}\n{#d306977e1814}\n\n| Type | Description |\n|------|-------------|\n| None |             |\nTable 25. Returns{#NAnaly-setTrackingConsent_B__table_uq5_2sj_5pb}\nThe following code example shows how to call this function to set the current user's consent response to analytics tracking.\n\n## NowAnalyticsSDK - setUserId(userId: String) {#ariaid-title14}\n\nSets the application-specific user identifier. Pass nil to log the current user out.\n{#d306977e1901}\n\n|  Name  |  Type  |                                                        Description                                                        |\n|--------|--------|---------------------------------------------------------------------------------------------------------------------------|\n| userId | String | Unique application-specific user identifier. This identifier is used to aggregate reports for that user in the dashboard. |\nTable 26. Parameters{#NAnaly-setUserId_S__table_j3y_ctj_5pb}\n{#d306977e1941}\n\n| Type | Description |\n|------|-------------|\n| None |             |\nTable 27. Returns{#NAnaly-setUserId_S__table_k3y_ctj_5pb}\"\n\n",
        "QUERY": " 'increase user id character limit'",
        "GROUND_TRUTH": "The User ID in the ServiceNow platform is a unique identifier for the user's login username. The maximum length of the User ID field is 40 characters. If you attempt to enter or update a User ID with more than 40 characters, the platform will only keep and save the first 40 characters. \n\nWhile the platform allows you to increase the max length of the User ID field, it is strongly advised not to do so. This is because the platform has not been designed to handle User IDs with more than 40 characters. \n\nIncreasing the max length of the User ID field can lead to several issues. For instance, User IDs will be truncated to 40 characters when they are stored in the 'Created by' and 'Updated by' fields. This can cause confusion among users, incorrect results in reports and filters, failing ACLs, and issues with the Flow Designer flow. Additionally, users with a User ID longer than 40 characters will not be able to attach files to a record."
    },
    {
        "id": 114,
        "CONTEXT": "\"## Signing Powershell scripts\n\n**Note: The following steps to sign Powershell script is not a supported feature. These are the recommended steps to be followed by the customers. We don't have the ability to support customers apart from the following steps.**\n\n**Issue**\n\n**1. Customer is NOT experiencing a failure but they want PowerShell scripts signed.**\n\n* Some customers like Defense Information Security Agency (DISA) Security Technical Implementation Guidelines (STIG) have policies that have this as a requirement\n* This customer wants IntegrationHub action's script signed  specifically Active Directory Spoke. They want us to **force and require signing** of the scripts.\n\n**2. Customer experiences a failure because their Windows host PowerShell ExecutionPolicy is AllSigned or RemoteSigned  and ServiceNow PowerShell scripts are not signed**\n\n* This customer had errors when testing Orch activities and credentials. When the customer changed EP to \"Unrestricted\" it worked.\n* Usually  scripts execute regardless of ExecutionPolicy on local MID but may fail per customer configuration.\n\n**Signing Powershell Scripts**\n\nIf a customer must sign their PowerShell scripts  use the following guidance\n\n1. Powershell scripts can be found in table ecc_agent_script_file (*query: Parent = Powershell*)\n2. First  sign **PSScript.ps1**. Both IntegrationHub and Orchestration run scripts on the MID using this as a wrapper script\n3. We recommend signing other infrastructure Powershell scripts (*query:* *Parent = Powershell and Directory = false*)\n4. If remote targeting is used then you will need to sign **ExecuteRemote.ps1** as well. This script handles executing script on the target machine\n5. Now go ahead and sign all the other Powershell scripts  \n   * For example - if you want to sign AD spoke Powershell scripts  you can find these scripts with query ***Parent = AdSpoke*** on table ecc_agent_script_file\n\n**Note: As of now  inline scripts cannot be signed. During runtime  we store the inline script in the temp file on the MID server and delete it after the execution of the Powershell step. If the customer needs to sign these  then they must be converted to script files.**\n\n**Saving the signed scripts**\n\n1. Save the signed scripts as an attachment.   \n   * Select Use Attachment field and attach the script file to the ecc_agent_script_file record\n   * Note: The attachment name should be the same as the ecc_agent_script_file name field\n2. We are saving the script as an attachment to avoid OOB formatting when the script is saved inline\n3. A disadvantage of this approach is that we lose versioning of the ecc_agent_script_file records\n\n**Verify scripts are valid**\n\n1. Wait for the scripts to get downloaded to MID. It takes around 10 seconds\n2. Verify the content of the signed scripts on MID. The scripts can be found in ..***mid_folder_path\/agent\/scripts\/Powershell*** or look for log messages similar to  \n   *02\/09\/21 07:16:02 (822) FileSync:ecc_agent_script_file Already synchronized C:\\\\<mid_folder_path\\>\\agent\\scripts\\PowerShell\\PSScript.ps1*\n3. To validate the scripts on PowerShell ISE using the command: ***Get-AuthenticodeSignature \"\\<file path\\>\"***\n4. Run Powershell test to confirm if the signature works\n\n**Debugging issues with signed scripts**\n\n1. Don't update the Powershell script directly in Windows. We regularly sync the ecc_agent_script_file scripts with MID. This sync job will override the local scripts with the instance version\n2. If the scripts fail to execute due to auth failures verify the Powershell script still as signature and verify the signature is valid by using the command:***Get-AuthenticodeSignature \"\\<file path\\>\"***\n3. If Powershell script contains the signature and fails to validate using the above command you will have to resign the script\n\n\n\n## San Diego Patch 10\n\n|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n| Configuration Management Database (CMDB) PRB1595340                                                                    | Highly connected CIs cause significant performance drop for CIUtils when collecting related CIs                                                                                                               | In cases where a CI has a high number of relationships in the cmdb_rel_ci table  the CIUtils.servicesAffectedByCI method can run very long  on the order of an hour or more. The script include calls to the CMDB java layer  which is where performance improvements are needed.                                                                                                                                                                                                                                                                                                               |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n| Configuration Management Database (CMDB) PRB1606520                                                                    | A reconciliation rule doesn't work with a filter condition                                                                                                                                                    |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | 1. Create a reconciliation rule with the priority 100  the attributes life_cycle_stage and life_cycle_stage_status and the filter condition life_cycle_stage is not 'End of Life' or life_cycle_stage_status is not 'Retired'.Using class manager will add the 'End of life' and retired as sysIds so change them to strings from the table (cmdb_reconciliation_definition) view of the record. 2. Create a reconciliation rule with priority 200  the attributes life_cycle_stage and life_cycle_stage_status. 3. Run the below payload to insert a record: {#sandiego-patch-10__ol_mh3_4jz_fwb}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |\n| Configuration Management Database (CMDB) PRB1608834                                                                    | Payload Merger throws null point error (NPE) when there is a marker from the merged partial payload                                                                                                           |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | 1. Create a Discovery source. 2. Modify the lookup rule on hardware for the network adapter from name  mac_address to ip_address  and mac_address. 3. Import the attached partial payloads and partial payloads indices. 4. Modify the linuxServer.json in app-cmdb-enhanced-ire-tests to the input payload text. 5. Comment the payload1 in the verifyUniqueCompletePayloadMerge test in PayloadMergerIT. 6. Run the test case verifyUniqueCompletePayloadMerge. {#sandiego-patch-10__ol_jh3_4jz_fwb} Observe that there is a NPE.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |\n| Content Experiences (Backports Only) PRB1619001                                                                        | iframe elements are removed when saving  preventing embedded videos from being used                                                                                                                           | The EmpExHTMLSanitizerConfig is using the HTMLSanitizerConfig policy that does not include iframes.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | 1. Navigate to Content Publishing \\> Content Library. 2. Click New \\> Rich Content. 3. Add a Video block and select it on the canvas. 4. Under Properties  change the Provider to YouTube or Vimeo. 5. Save the content and target the content to the ESC portal. {#sandiego-patch-10__ol_ch3_4jz_fwb} Expected behavior: The created content once rendered in the Portal should include the embedded video. Actual behavior: The embedded video is missing.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n| Contextual Search PRB1594709                                                                                           | The 'Search as' feature of the contextual search resets the session domain to a user's default domain                                                                                                         |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | 1. In an instance with Domain Separation  find the Table Configuration for Incident (Platform) and enable 'Search as' based on the Caller field. 2. Create an incident with ACME Employee as caller. 3. Go to the 'TOP' domain and access the incident. 4. Open the contextual search and access the Search as tab. 5. Refresh the page. 6. Check the domain picker. {#sandiego-patch-10__ol_rh3_4jz_fwb} Actual Behavior: The domain picker is reset to 'global'. Expected Behavior: The domain picker shows 'TOP'.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |\n| Core Platform PRB1608252                                                                                               | Missing important information in the scheduled job 'Completed' log message                                                                                                                                    |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | 1. Look at the scheduled jobs starting and completion log message in the localhost logs. 2. Verify that it has the expected run time (next action)  actual run time  and created time attributes. {#sandiego-patch-10__ol_kh3_4jz_fwb} Expected behavior: Important attributes are present in the log message. Actual behavior: These important attributes are missing in the log message.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n| Core Platform PRB1620355                                                                                               | Calling JSUtil.nil or JSUtil.notNil on an array allocates 6 GB of unnecessary garbage memory                                                                                                                  |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n| Core Platform PRB1635585 [KB1221999](https:\/\/support.servicenow.com\/kb?id=kb_article_view&sysparm_article=KB1221999)   | StaticContentServer caches the existence and nonexistence of snc_node_disable.html                                                                                                                            | After instance migration  some instances do not come online when nodes are re-pointed to a new DB.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | Refer to the listed KB article for details.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |\n| Database Persistence PRB1625868                                                                                        | Secondary database should log 'registrar is full' only when debug mode is enabled                                                                                                                             |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n| Discovery PRB1603003                                                                                                   | Inefficient check for duplicate IP addresses causes high DB CPU usage and adds latency to the ASYNC: Discovery - Sensors jobs                                                                                 | The code that checks for duplicate IP addresses (CIs that have network adapters with IP addresses that are the same) isn't scaleable. This causes high DB CPU and latency for Discovery.                                                                                                                                                                                                                                                                                                                                                                                                        | Run Discovery with duplicate IP addresses.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n| Discovery PRB1608469 [KB1182402](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB1182402)                      | The 'Windows - ADM' probe doesn't launch                                                                                                                                                                      | On San Diego  if no Powershell or admin share access is detected  Windows Discovery doesn't launch Application Dependency Mapping (ADM). Thus  no application patterns launch for running processes.                                                                                                                                                                                                                                                                                                                                                                                            | Refer to the li...",
        "QUERY": " 'How to verify whether powershell scripts are valid?'",
        "GROUND_TRUTH": "To verify whether PowerShell scripts are valid, follow these steps:\n\n1. Wait for the scripts to get downloaded to MID. It usually takes around 10 seconds.\n2. Verify the content of the signed scripts on MID. The scripts can be found in the directory: mid_folder_path\/agent\/scripts\/Powershell. Alternatively, you can look for log messages similar to \"02\/09\/21 07:16:02 (822) FileSync:ecc_agent_script_file Already synchronized C:\\\\<mid_folder_path>\\agent\\scripts\\PowerShell\\PSScript.ps1\".\n3. To validate the scripts on PowerShell ISE, use the command: Get-AuthenticodeSignature \"\\<file path>\".\n4. Run a PowerShell test to confirm if the signature works.\n\nIf you encounter issues with signed scripts, do not update the PowerShell script directly in Windows. The ecc_agent_script_file scripts are regularly synced with MID and this sync job will override the local scripts with the instance version. If the scripts fail to execute due to authentication failures, verify the PowerShell script still has a signature and verify the signature is valid by using the command: Get-AuthenticodeSignature \"\\<file path>\". If the PowerShell script contains the signature and fails to validate using the above command, you will have to resign the script."
    },
    {
        "id": 115,
        "CONTEXT": "\"## Troubleshooting excessive Service Portal widget reloads\n\n**Background:**\n\nWhen working with ServiceNow instances it is common to see transactions executing where the transaction URL starts with \/API\/now\/sp\/rectangle. For example:\n\n2021-06-23 05:25:23 (739) Default-thread-191 FABB35231B387C90A8ADA9F36B4BCB3F txid=9e5df9e71b74 \\*\\*\\* Start #33598 \/api\/now\/sp\/rectangle\/37cf5aeadbf96450c100e16e13961972  user: admin\n2021-06-23 05:25:23 (766) Default-thread-191 FABB35231B387C90A8ADA9F36B4BCB3F txid=9e5df9e71b74 \\*\\*\\* End #33598 \/api\/now\/sp\/rectangle\/37cf5aeadbf96450c100e16e13961972  user: admin  total time: 0:00:00.037  processing time: 0:00:00.037  SQL time: 0:00:00.007 (count: 14)  business rule: 0:00:00.007 (count: 3)  ACL time: 0:00:00.001  source: 199.91.137.61   type:rest  method:POST  api_name:now\/sp  resource:now\/sp\/rectangle\/{rectangle}  version:Default  user_id:6816f79cc0a8016401c5a33be04be441  response_status:201\n\nThese transactions indicate that a particular Service Portal widget is reloading \/ refreshing. Note that:\n\n* The sys_id in the transaction URL indicates the widget instance which is reloading  i.e.:\n\n\\[db178151.ytz3:\/home\/users\/james.ford\\]$ pbrun snow query empjfordparis \"select sp_column  sp_widget from sp_instance where sys_id = '37cf5aeadbf96450c100e16e13961972'\"\n+------+----------------------------------+----------------------------------+\n\\| Port \\| sp_column \\| sp_widget \\|\n+------+----------------------------------+----------------------------------+\n\\| 3520 \\| 2f2f92eadbf96450c100e16e1396191f \\| 887f56eadbf96450c100e16e139619f1 \\|\n+------+----------------------------------+----------------------------------+\n\n* The sys_id is contained in sp_instance.sp_widget field indicates the underlying Service Portal widget being used  i.e.:\n\n\\[db178151.ytz3:\/home\/users\/james.ford\\]$ pbrun snow query empjfordparis \"select name from sp_widget where sys_id = '887f56eadbf96450c100e16e139619f1'\"\n+------+---------------------+\n\\| Port \\| name \\|\n+------+---------------------+\n\\| 3520 \\| Copy of My Requests \\|\n+------+---------------------+\n\nThe sp_widget table contains further definitions around how the widget operates  i.e.:\n\n* css: Cascading Style Sheet (CSS) data defining aspects of how the widget should be displayed in the end users browser\n* template: An HTML template for the widget defining how end users can interact with the widget\n* script: Server-side JavaScript which generally describes how data to be displayed in the widget is built\n* client_script: Client-side JavaScript which dictates how the widget operates within the end users browser\n\nIt is entirely normal for certain Service Portal widgets to periodically reload however if this happens too frequently it can cause various issues  i.e.:\n\n* Contention on default threads\/semaphore pool on application nodes could cause an impact on multiple end users\n* Multiple \/API\/now\/sp\/rectangle transactions being created for a users session in quick succession - as transactions for a single session are (by default) executed serially these transactions can stack up behind one another and be impacted by session wait - this could cause significant degradation to end user experience\n\nFor example  consider the following scenario - a user initiates a large number of reloads (5) of a given widget instance in a short period of time (136ms):\n\n2021-06-22 08:10:34 (555) http-35 New transaction 8F99C91F1BFC3850078D6397624BCB17 #1405044 \/api\/now\/sp\/rectangle\/dfa819256f8a0380903ed3b0be3ee400\n2021-06-22 08:10:34 (567) http-39 New transaction 8F99C91F1BFC3850078D6397624BCB17 #1405045 \/api\/now\/sp\/rectangle\/dfa819256f8a0380903ed3b0be3ee400\n2021-06-22 08:10:34 (597) http-16 New transaction 8F99C91F1BFC3850078D6397624BCB17 #1405046 \/api\/now\/sp\/rectangle\/dfa819256f8a0380903ed3b0be3ee400\n2021-06-22 08:10:34 (646) http-40 New transaction 8F99C91F1BFC3850078D6397624BCB17 #1405047 \/api\/now\/sp\/rectangle\/dfa819256f8a0380903ed3b0be3ee400\n2021-06-22 08:10:34 (691) http-44 New transaction 8F99C91F1BFC3850078D6397624BCB17 #1405048 \/api\/now\/sp\/rectangle\/dfa819256f8a0380903ed3b0be3ee400\n\nCorresponding transactions are then executed serially - each reloads of the widget takes approximately 1 - 2 seconds so by the time the final transaction completes its performance is significantly degraded due to session wait (8.17s):\n\n2021-06-22 08:10:36 (672) Default-thread-11 8F99C91F1BFC3850078D6397624BCB17 txid=4099919b1b78 \\*\\*\\* Start #1405044 \/api\/now\/sp\/rectangle\/dfa819256f8a0380903ed3b0be3ee400  user: xxxxxx\n...\n2021-06-22 08:10:38 (287) Default-thread-1 8F99C91F1BFC3850078D6397624BCB17 txid=c899119f1b38 \\*\\*\\* Start #1405045 \/api\/now\/sp\/rectangle\/dfa819256f8a0380903ed3b0be3ee400  user: xxxxxx\n...\n2021-06-22 08:10:39 (837) Default-thread-8 8F99C91F1BFC3850078D6397624BCB17 txid=0499919b1b78 \\*\\*\\* Start #1405046 \/api\/now\/sp\/rectangle\/dfa819256f8a0380903ed3b0be3ee400  user: xxxxxx\n...\n2021-06-22 08:10:41 (371) Default-thread-7 8F99C91F1BFC3850078D6397624BCB17 txid=c099dd9b1b78 \\*\\*\\* Start #1405047 \/api\/now\/sp\/rectangle\/dfa819256f8a0380903ed3b0be3ee400  user: xxxxxx\n...\n2021-06-22 08:10:42 (867) Default-thread-1 8F99C91F1BFC3850078D6397624BCB17 txid=409911131b78 \\*\\*\\* Start #1405048 \/api\/now\/sp\/rectangle\/dfa819256f8a0380903ed3b0be3ee400  user: xxxxxx\n...\n2021-06-22 08:10:44 (329) Default-thread-1 8F99C91F1BFC3850078D6397624BCB17 txid=409911131b78 EXCESSIVE \\*\\*\\* End #1405048 \/api\/now\/sp\/rectangle\/dfa819256f8a0380903ed3b0be3ee400  user: xxxxxx  **total time: 0:00:09.637**   processing time: 0:00:01.471  total wait: 0:00:08.166  **session wait: 0:00:08.166**  SQL time: 0:00:01.379 (count: 42)  source: 123.456.789.012   type:rest  method:POST  api_name:now\/sp  resource:now\/sp\/rectangle\/{rectangle}  version:Default  user_id:5935cf7a3776f200ba0694c543990e40  response_status:201\n\nWhilst these transactions execute the user's browser will essentially appear to have hung (i.e. the ServiceNow instance is completely unresponsive) - this does not give a good end-user experience.\n\nIn these cases  it may be necessary to understand why a particular widget is reloading such that this behaviour can be avoided. Reloads are generally triggered by the client script calling a function such as 'server.update()' or 'server.refresh()' - these functions cause the widget server-side script to execute (which in turn refreshes the contents of the widget). Such functions tend to be called (and therefore widget reloads are triggered):\n\n* Periodically due to some kind of interval\/timeout in the client-side script\n* On-demand due to a record watcher being triggered\n\nBoth methods will be discussed further below.\n\n**Periodic widget reloads**\n\nA widget can be reloaded on a periodic basis via code such as the following in the widgets client script:\n\n$interval(function(){\nc.server.refresh($scope);\n}  5000);\n\nThis code means that when the widget is loaded a timer is instantiated in the browser's run time environment. \n\nEach time the timer expires (in this case after 5000ms) the corresponding code (server.refresh()) will be executed. As described above this will cause the widgets server-side script to execute again to update the contents of the widget:\n\n\\[app136164.ytz3:\/home\/users\/james.ford\\]$ tail -f \/glide\/nodes\/empjfordparis001_16502\/logs\/localhost_log.2021-06-23.txt \\| grep FABB35231B387C90A8ADA9F36B4BCB3F \\| grep Start \\| grep rectangle\n2021-06-23 06:03:49 (192) Default-thread-186 FABB35231B387C90A8ADA9F36B4BCB3F txid=0d2686231bf4 \\*\\*\\* Start #33871 \/api\/now\/sp\/rectangle\/37cf5aeadbf96450c100e16e13961972  user: admin\n2021-06-23 06:03:54 (192) Default-thread-189 FABB35231B387C90A8ADA9F36B4BCB3F txid=9a2606eb1b38 \\*\\*\\* Start #33872 \/api\/now\/sp\/rectangle\/37cf5aeadbf96450c100e16e13961972  user: admin\n2021-06-23 06:03:59 (190) Default-thread-186 FABB35231B387C90A8ADA9F36B4BCB3F txid=2f268eeb1b38 \\*\\*\\* Start #33873 \/api\/now\/sp\/rectangle\/37cf5aeadbf96450c100e16e13961972  user: admin\n2021-06-23 06:04:04 (188) Default-thread-189 FABB35231B387C90A8ADA9F36B4BCB3F txid=b836ceef1bf4 \\*\\*\\* Start #33874 \/api\/now\/sp\/rectangle\/37cf5aeadbf96450c100e16e13961972  user: admin\n2021-06-23 06:04:09 (202) Default-thread-186 FABB35231B387C90A8ADA9F36B4BCB3F txid=8a368e2f1bb4 \\*\\*\\* Start #33875 \/api\/now\/sp\/rectangle\/37cf5aeadbf96450c100e16e13961972  user: admin\n...\n\nNote  however  that the above code is not optimal - as the timer is instantiated in the browser run time environment it may not be destroyed even if the user navigates away from the page holding the widget - as a result  the timer continues to fire which can send the constant widget to reload (\/api\/now\/sp\/rectangle) transactions to the corresponding application node. In addition  if the user navigates back to the page holding the widget it's possible for a second timer to be instantiated - now the timer will fire  on average  every 2.5s. In certain cases  this behaviour can quickly spiral out of control!\n\nTo avoid this if a periodic refresh is being used it should create a **'promise'** which is then destroyed when the user navigates away from the page  i.e.:\n\nvar intervalPromise = $interval(function(){\nc.server.refresh($scope);\n}  5000);\n$scope.$on('$destroy'  function() {\n$interval.cancel(intervalPromise);\n});\n\nFundamentally periodically refreshing a widget can work well however it means that users will trigger constant \/API\/now\/sp\/rectangle transactions even if the underlying data displayed in their widget has not changed. Due to this ServiceNow generally recommend refreshing widgets**'on demand'** via a record watcher (discussed below).\n\n**Using a record watcher to trigger widget reloads**\n\nThe client script of a widget can instantiate a record watcher against a given set of records (using an encoded query). If anything to do with that set of records changes (i.e. records are added\/removed or the details of one or more records are modified) the record watcher will be 'fired' at which point some arbitrary code is executed. In the case of a Service Portal widget this code generally just reloads the widget. For example the following code:\n\nSets data.user to the current user sys_id in the server-side script:\n\ndata.user = gs.getUserID();\n\nThe data object can then be accessed in the client side script via the variable 'c' after which a record watcher is instantiated to look at 'sc_request' records where 'requested_for' is set to the current user:\n\nvar c = this;\n...\nspUtil.recordWatch($scope  \"sc_request\"  \"requested_for=\" + c.data.user  function() {\nc.server.refresh();\n});\n\nIf details of this set of records change the record watcher will 'fire' and cause server.refresh() to be executed which then reloads the widget (via a corresponding \/API\/now\/sp\/rectangle transaction).\n\nWhen using record watchers it's necessary to ensure that the encoded query\/filter used in the record watcher is 'well formed' (i.e. it should match a relatively small set of records which do not frequently change and are somewhat 'user specific'). To demonstrate why this is necessary the following was seen in one customer implementation:\n\n* Within their Service Portal header  the customer had instantiated a record watcher which looked for records in the task table and was designated as a 'major incident' (due to a particular boolean field being set to true)\n* The header existed on every single page and  as a result  every single user of the Service Portal had the record watcher instantiated in their browser regardless of the page they were viewing\n* As soon as a task record was designated a 'major incident' the record watcher in the browser of every user of the Service Portal suddenly fired triggering a corresponding \/API\/now\/sp\/rectangle transaction to be sent to application nodes\n* This caused instant (but relatively short-lived) semaphore exhaustion across all application nodes which effectively caused a loss of service\n\nLikewise  even if a record watcher only looks at 'user specific' records if that set of records is large \/ frequently modified it could cause the record watcher to fire far more frequently than expected and cause similar issues.\n\n\n\nIn most cases  if there is evidence of excessive \/API\/now\/sp\/rectangle transactions for a given widget instance its relatively straight forwards to review the corresponding widget  determine how reloads are triggered (i.e. fixed interval or record watcher)  and make recommendations to improve (i.e. in the case of a record watcher 'tighten' the filter by adding additional clauses to the encoded query). In some cases  however  it might not be clear which record watcher is to blame for the reloads. For example  a widget may contain code such as the following which can instantiate multiple record watchers when the widget is loaded:\n\n...\n\/\/ Get list of record watchers\nvar record_watchers = \\[\\];\nif ($scope.data.menu.items) {\nfor(var i in $scope.data.menu.items) {\nvar item = $scope.data.menu.items\\[i\\];\nif (item.type == 'scripted') {\nif (item.scriptedItems.record_watchers)\nrecord_watchers = record_watchers.concat(item.scriptedItems.record_watchers);\n}\nif (item.type == 'filtered') {\nrecord_watchers.push({'table':item.table 'filter':item.filter});\n}\n}\n}\n\/\/ Init record watchers\nfor (var y in record_watchers){\nvar watcher = record_watchers\\[y\\];\nspUtil.recordWatch($scope  watcher.table  watcher.filter);\n}\n...\n\nTo understand which record watchers are instantiated we would need to look at the server side script to understand how 'data.menu.items' is populated  i.e.:\n\n...\nvar menu_id = $sp.getValue('sys_id'); \/\/ instance sys_id\nvar gr = new GlideRecord('sp_instance_menu');\ngr.get(menu_id);\ndata.menu.items = $sp.getMenuItems(menu_id);\n...\n\nThe above code queries essentially query the sp_rectangle_menu_item table looking for records where 'sp_rectangle_menu' is set to the current widget instances sys_id. For example lets say that the above widget instances sys_id is 'dfa819256f8a0380903ed3b0be3ee400' - this would return 6 records from sp_rectangle_menu_item:\n\nselect count(\\*) from sp_rectangle_menu_item where sp_rectangle_menu = 'dfa819256f8a0380903ed3b0be3ee400' and active = 1\n+------+----------+\n\\| Port \\| count(\\*) \\|\n+------+----------+\n\\| 3400 \\| 6 \\|\n+------+----------+\n\nIn the client-side code we look for which of these items have a type of either 'scripted' or 'filtered' - in this case  this returns 4 records:\n\nselect label  type from sp_rectangle_menu_item where sp_rectangle_menu = 'dfa819256f8a0380903ed3b0be3ee400' and type in ('scripted'  'filtered') and active = 1\n+------+-------------+----------+\n\\| Port \\| label \\| type \\|\n+------+-------------+----------+\n\\| 3400 \\| Demands \\| scripted \\|\n\\| 3400 \\| My Requests \\| scripted \\|\n\\| 3400 \\| My Work \\| scripted \\|\n\\| 3400 \\| Incidents \\| scripted \\|\n+------+-------------+----------+\n\nThe code uses the definition of each menu item to instantiate record watchers - for example if we look at the script of the **'My Requests'** menu item we see this instantiates record watchers as follows:\n\nselect record_script from sp_rectangle_menu_item where sp_rectangle_menu = 'dfa819256f8a0380903ed3b0be3ee400' and label = 'My Requests'\n...\nt.record_watchers = \\[\\];\nt.record_watchers.push({'table':'dmn_demand' 'filter':'active=true\\^opened_by=' + u});\nt.record_watchers.push({'table':'rm_story' 'filter':'active=true\\^opened_by=' + u});\nt.record_watchers.push({'table':'sc_req_item' 'filter':'active=true\\^request.requested_for=' + u});\nt.record_watchers.push({'table':'sc_request' 'filter':'active=true\\^requested_for=' + u});\n...\n\nBy working through each menu item (paying attention to conditions under which that menu item is loaded) we can determine which \/ how many record watchers the widget would instantiate. In some cases (such as the above)  however  there can be many record watchers instantiated so how can we work out which record watcher is triggering the widget reloads?\n\nTo answer this let's step back for a second:\n\n* We've already discussed that records watchers 'fire' when there are changes made to the set of records that they 'watch'\n* We've also discussed that the set of records a record watcher watches is defined via a table and encoded query\/filter - for example  the final record watcher above will watch records in the 'sc_request' table which are active and have 'requested_for' set to the current user\n* At a very high level  the way in which this works is that when the record watcher has instantiated the instance will create an AMB channel corresponding to the table \/ encoded query then the client will subscribe to this channel\n* Following this  should there be any modification made to the set of records  a notification will be published on the AMB channel and then delivered to the browser of every client subscribed to the channel - this then causes the record watcher will fire\n\nNote that:\n\n* Record watcher AMB channels are named as follows: \/amb\/rw\/default\/\\[TABLE NAME\\]\/\\[BASE64 ENCODED QUERY\\]\n* An example of a record watcher AMB channel might therefore be: \/amb\/rw\/default\/sc_request\/YWN0aXZlPXRydWVecmVxdWVzdGVkX2Zvcj01OTM1Y2Y3YTM3NzZmMjAwYmEwNjk0YzU0Mzk5MGU0MA--\n* We can immediately see that the channel corresponds to records in the 'sc_request' table and  by decoding the encoded query  can see that it matches records which are active and where 'requested_for' is equal to a given sys_id  i.e.:\n\n% echo \"YWN0aXZlPXRydWVecmVxdWVzdGVkX2Zvcj01OTM1Y2Y3YTM3NzZmMjAwYmEwNjk0YzU0Mzk5MGU0MA--\" \\| base64 -D\nactive=true\\^requested_for=5935cf7a3776f200ba0694c543990e40\n\n* We can then tell that this AMB channel corresponds to the last of the 4 record watchers listed above.\n\n\"\n\n",
        "QUERY": " 'how to troubleshoot portal widget code'",
        "GROUND_TRUTH": "To troubleshoot portal widget code, you need to understand the reasons behind the frequent reloads. In ServiceNow instances, transactions starting with \/API\/now\/sp\/rectangle indicate that a particular Service Portal widget is reloading or refreshing. \n\n1. Identify the Widget: The sys_id in the transaction URL indicates the widget instance which is reloading. You can use this sys_id to query the sp_instance table and identify the underlying Service Portal widget being used.\n\n2. Understand the Widget Operation: The sp_widget table contains definitions around how the widget operates. It includes CSS data, an HTML template, server-side JavaScript, and client-side JavaScript. \n\n3. Identify the Cause of Reloads: Reloads are generally triggered by the client script calling a function such as 'server.update()' or 'server.refresh()'. These functions cause the widget server-side script to execute, which in turn refreshes the contents of the widget. \n\n4. Periodic Widget Reloads: A widget can be reloaded on a periodic basis via code in the widgets client script. If this happens too frequently, it can cause various issues. To avoid this, you should create a 'promise' which is then destroyed when the user navigates away from the page.\n\n5. Using a Record Watcher to Trigger Widget Reloads: The client script of a widget can instantiate a record watcher against a given set of records. If anything to do with that set of records changes, the record watcher will be 'fired' at which point some arbitrary code is executed. \n\n6. Review the Widget: If there is evidence of excessive \/API\/now\/sp\/rectangle transactions for a given widget instance, review the corresponding widget, determine how reloads are triggered, and make recommendations to improve.\n\n7. Identify the Record Watcher: If it's not clear which record watcher is to blame for the reloads, you can determine which \/ how many record watchers the widget would instantiate by looking at the server-side script."
    },
    {
        "id": 116,
        "CONTEXT": "\"## ServiceNow BCM PDF Generation Installation Steps\n\n**Installation**\n\n1. The following steps should be replicated by a **ServiceNow Admin** in any instance ServiceNow BCM is installed\n2. A ServiceNow BCM Technician or Partner will supply you with a folder which contains 5 Update Sets\n3. Navigate to System Definition: Plugins\n4. Search for Webkit HTML to PDF and make sure it is installed\n\n5. Navigate to Retrieved Update Sets and select the related link Import Update Set from XML\n6. Import the 5 Update sets\n7. Preview and commit the 5 update sets in the following order  \n   1. BCM PDF Global (Note: this update set is in global)\n   2. BCM PDF\n   3. BCM Update PDF UI\n   4. BCM Update PDF UI 2\n   5. BCM PDF Image Update\n\n**Testing**\n\n1. Navigate to All Plans in the Business Continuity application\n2. Open a plan record and select View Plan in the header of the record\n3. In the top right of the Business Continuity UI select Generate PDF\n\n4. A new tab will open  and a progress bar will appear\n5. When the progress bar is completed a pdf will be downloaded which can be opened in the bottom left of the screen\n\n\n\n## Testing Email Issues Using an Employee Instance\n\n### Setup a Test User To Receive Email Notifications\n\n* User Administration-\\>Users-\\>New. Create a new user  e.g john.smith and set its email address to your ServiceNow email  e.g [john.smith@servicenow.com](mailto:john.smith@servicenow.com)\n* Set a password\n* Add at least the itil role to the new user (optionally give it admin as that generally makes changing instance settings when troubleshooting easier)\n* Add the new user to the Service Desk group (so that this user can be assigned incidents  changes etc.)\n* Set user's timezone as desired (e.g your local TZ)\n\n### **Testing Outbound Email**\n\n#### **Notification Test Incident**\n\nSetup an Incident that can be used to test notifications  i.e where an update to worknotes or additional comments triggers an email notification to your user:\n\n* Create Incident with Short Description \"Notification Test Incident\"  Assignment group *Service Desk*  Assigned to \\<the user you created\\>\n* Change OOB Notification *Incident commented for ITIL* to enable *Send to event creator*(switch the view to Advanced when you have the email Notification open)\n* Add a new Additional Comment to the Incident. Check System Logs-\\>Emails and ensure a notification email was generated\n* (Optional) Install the ServiceNow mobile app (Now Mobile or My ServiceNow)  login to the instance on the app  verify that the a push notification is sent out\n\n#### Enabling Email Sending\n\nSystem Properties-\\>Email Properties. Set 'Send all email to this test address...' to your ServiceNow email  e.g [john.smith@servicenow.com](mailto:john.smith@servicenow.com).\n\nTick Yes on Email Sending\n\nWait 2-5 minutes and you should see the emails that were previously in Type=send-ready change to Type=sent and appear in your Outlook mailbox.\n\nYou might want to untick Yes on Email Sending after a while as automatic reports and Scheduled Jobs in the demo data on your instance will regularly send you emails.\n\n### Testing Inbound Email\n\n#### Creating a New Record Using Inbound Actions\n\nSystem Properties-\\>Email Properties. Enable Email Sending and Email Receiving.\n\nSend an email from your Outlook to [instancename@sevice-now.com](mailto:instancename@sevice-now.com) (e.g [emptedwardsv@service-now.com](mailto:emptedwardsv@service-now.com)) with subject whatever you want to be in the subject of the new Incident  and body whatever text you want. Verify it created a new Incident:\n\n#### Updating an Existing Record Using Inbound Actions\n\nReply to the email you got from doing the steps in 'Notification Test Incident' above. It should update the additional comments of your test Incident:\n\n#### Using Inbound Email Triggers in Flow Designer\n\nFollow [Create a flow with an inbound email trigger](https:\/\/docs.servicenow.com\/csh?topicname=create-inbound-email-flow.html&version=latest \"Create a flow with an inbound email trigger\")\n\nFor example:\n\n* Flow name 'Test Inbound Email Incident'\n* Add Trigger Inbound Email  with Receive Type is Reply  Reply Record Type is Incident\n* Add an Action 'Update Record'. For Record drag-drop the Trigger-Inbound Email-\\>Incident Record Data Pill. Table Incident. Fields Additional Comments  drag-drop Trigger-Inbound Email-\\>Body Text Data Pill\n* Save and Activate\n* Reply to the email you got from doing the steps in 'Notification Test Incident' above  putting whatever text in the email body that you want to be added as an Additional Comment to the Incident. Then check Executions in Flow Designer and also confirm that the Incident's Additional Comments were updated.\n\n\n\n## Install Subscription Management v2\n\n[Administer the Now Platform](..\/..\/..\/administer\/general\/concept\/intro-now-platform-landing.html \"As a platform administrator  you have the power of the Now Platform at your fingertips. The Now Platform is an application platform as a service that automates business processes across the enterprise.\") \\> [Getting started on the Now Platform](..\/..\/..\/administer\/general\/concept\/get-started-now-platform.html \"Now Platform is the exclusive ServiceNow platform. It provides a range of options to improve and automate your business processes.\") \\> [Subscription Management v2](..\/..\/..\/administer\/subscription-management\/reference\/subscription-management-landing-page-v2.html \"Subscription Management enables you to proactively manage your subscriptions and monitor subscription usage on your instances.\") \\>\n\n# Install Subscription Management v2 {#ariaid-title1}\n\nIf you have the admin role  you can install the Subscription Management v2 application (com.snc.usage_admin.base).\n\n* Ensure that the application and all of its associated ServiceNow Store applications have valid ServiceNow entitlements. For more information  see [Get entitlement for a ServiceNow\n  product or application](https:\/\/store.servicenow.com\/$appstore.do#!\/store\/help?article=KB0030186).\n* Review the [Subscription Management](https:\/\/store.servicenow.com\/sn_appstore_store.do) application listing in the ServiceNow Store for information on dependencies  licensing or subscription requirements  and release compatibility.\n* Consider enabling the glide.ui.polaris.use system property to optimize the Subscription Management user experience if you've turned off the Next Experience.\n\n{#install-subscription-management__ul_wlc_d2m_5xb}\n\nRole required: admin\nThe following items are installed with Subscription Management v2:\n\n* Plugins\n* Roles\n* Tables\n{#install-subscription-management__ul_ylc_d2m_5xb}\n\nFor more information  see [Components installed with Subscription Management](..\/reference\/installed-with-subscription-management.html \"Several components are installed with activation of the Subscription Management plugin (com.glide.subscription_management)  including user roles and tables.\").\n\n1. Navigate to All \\> System Applications \\> All Available Applications \\> All.\n2. Find the Subscription Management v2 application (com.glide.subscription_management) using the filter criteria and search bar.  \n   You can search for the application by its name or ID. If you can't find the application  you might have to request it from the ServiceNow Store.  \n   In the list next to the Install button  the versions that are available to you are displayed.\n3. Select a version from the list and select Install.  \n   In the Install dialog box that is displayed  any dependencies that are installed along with your application are listed.\n4. If you're prompted  follow the links to the ServiceNow Store to get any additional entitlements for dependencies.\n5. **Optional:** If demo data is available and you want to install it  select the Load demo data check box.  \n   Demo data comprises the sample records that describe application features for the common use cases. Load the demo data when you first install the application on a development or test instance.  \n   Important: If you don't load the demo data during installation  it's unavailable to load later.\n6. Select Install.\n{#install-subscription-management__steps_zlc_d2m_5xb}  \n**Parent Topic:** [Configuring Subscription Management](..\/..\/..\/administer\/subscription-management\/concept\/configuring-subscription-management-v2.html \"Set up and configure Subscription Management.\")\n\n\"\n\n",
        "QUERY": " 'how to test the generated pdf in a ServiceNow BCM?'",
        "GROUND_TRUTH": "To test the generated PDF in a ServiceNow BCM, follow these steps:\n\n1. Navigate to All Plans in the Business Continuity application.\n2. Open a plan record and select View Plan in the header of the record.\n3. In the top right of the Business Continuity UI, select Generate PDF.\n4. A new tab will open and a progress bar will appear.\n5. When the progress bar is completed, a PDF will be downloaded which can be opened in the bottom left of the screen."
    },
    {
        "id": 117,
        "CONTEXT": "\"## Submit an idea\n\n# Submit an idea {#ariaid-title1}\n\nSubmit an idea for a product  feature  enhancement  or change in the Idea Portal for others to vote on. Select categories relevant to the idea and  if useful  attach files to add details.\nRole required: None\nNew ideas are created in the Submitted state. You can edit your submitted idea in the Idea Portal after it has been submitted until the state is changed to Completed.\n\n1. Navigate to All \\> Ideas \\> Idea Portal \\> Create an Idea.\n2. Fill in the fields on the Create an Idea form.  \n   {#d3153276e102}\n   |    Field    |                                                                                                       Description                                                                                                       |\n   |-------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n   | Title       | Brief description of the idea. As you start typing the title for your idea  the Related Ideas section appears and displays existing ideas that potentially match your idea.                                             |\n   | Category    | The category to associate with your idea. You can select multiple categories for an idea. Categories are also visible to other users in the Idea Portal when viewing submitted ideas.                                   |\n   | Description | Detailed description of the idea. Consider including details such as why is it useful  who would benefit from it  and how it would work. You can use the formatting toolbar to format text and add images or web links. |\n   Table 1. Create an Idea form fields   {#submit-idea__table_jzp_4ry_phb}\n3. **Optional:** If you have attachments related to the idea click Add attachments and attach them.\n4. Click Create.  \n**Parent topic:** [Idea Portal](..\/..\/..\/product\/innovation-management\/concept\/idea-portal.html \"The Idea Portal is your central location to collect  curate  and promote ideas into demand  project  epic  or story. It enables integration of key feedback and requests into your product planning and development process.\")\n\n\n\n## \"Submit Idea\" catalog item is not available\n\n## Description\n\nAs per the below documentation  there should be a catalog item present with name \"Submit Idea\" under self-service \\> service catalog for users to submit the idea request but it is not available\n[https:\/\/docs.servicenow.com\/bundle\/tokyo-it-business-management\/page\/product\/planning-and-policy\/task\/submit-idea-from-self-service.html](https:\/\/docs.servicenow.com\/bundle\/tokyo-it-business-management\/page\/product\/planning-and-policy\/task\/submit-idea-from-self-service.html)\n\n## Steps to Reproduce\n\nLogin to OOB Instance (orsandiego)\nNavigate to Self-Service \\> Service Catalog.\nClick Can We Help You?\nClick Submit Idea.\n\nExpected: There should a Submit Idea catalog item available\n\nActual: There is no catalog item with name \"Submit Idea\" is available\n\n## Workaround\n\nThis issue is under review. To receive notifications when more information is available  subscribe to this Known Error article by clicking the Subscribe button at the top right of the article. If you are able to upgrade  review the Fixed In or Intended Fix Version fields to determine whether any versions have a planned or permanent fix.\n\n**Related Problem: PRB1629452**\n\n\n\n## Configure the filters for feedback or product idea overview page\n\n[Strategic Portfolio Management](..\/..\/..\/product\/it-business-management\/reference\/r_ITBusinessManagement.html \"Align work with business goals to deliver products and services in a way that supports your strategic priorities. ServiceNow Strategic Portfolio Management (SPM  formerly IT Business Management  enables you to use digital transformation to drive more customer value faster. Plan  deliver  and track value across different methodologies.\") \\> [Strategic Planning](..\/..\/..\/product\/alignment-planner-workspace\/concept\/alignment-planner-workspace-landing-page.html \"ServiceNow Strategic Planning (formerly known as Lenses for Alignment Planner Workspace) helps product and portfolio managers align work with strategy across the organization  while facilitating Agile transformation by combining traditional and agile work streams.\") \\> [Configuring Strategic Planning Workspace](..\/..\/..\/product\/alignment-planner-workspace\/concept\/setting-up-alignment-planner-workspace.html \"Learn about the process required to set up Strategic Planning to enable your product and portfolio managers start aligning their work with business goals and strategy.\") \\>\n\n# Configure the filters for feedback or product idea overview page {#ariaid-title1}\n\nConfigure the filters for the feedback or product idea overview page to display information based on the preferences of your users. Use filters to find and engage with feedback and product ideas that are the most relevant to their interests.\nRole required: admin\n\n1. Navigate to All \\> System properties \\> All properties.\n2. Open the sn_apw_advanced.product_feedback_product_idea_filters property.\n3. Edit the property by selecting here.\n4. In the value field  enter the field name in the query.  \n   You can add field names in the JSON query. JSON has column objects for each filter. Each column object has two properties:\n   * fieldname: The field name on the feedback or product idea table.\n   * encodedQuery: The filter on the reference table.\n   {#configure-feedback-or-idea-overview-page__ul_izf_rvt_bzb} You can only add reference or choice type fields on the feedback or product idea tables as filters. You can either add a new filter or add a condition to the existing filter.\n5. Select Update.  \n**Parent Topic:** [Configuring Product Feedback in Strategic Planning](..\/..\/..\/product\/alignment-planner-workspace\/concept\/configuring-product-feedback-in-sp.html \"Configure Product Feedback with other applications to submit your feedback.\")\n\n\"\n\n",
        "QUERY": " 'how to submit an idea'",
        "GROUND_TRUTH": "To submit an idea, follow these steps:\n\n1. Navigate to All > Ideas > Idea Portal > Create an Idea.\n2. Fill in the fields on the Create an Idea form. The fields include:\n   - Title: A brief description of the idea. As you start typing the title for your idea, the Related Ideas section appears and displays existing ideas that potentially match your idea.\n   - Category: The category to associate with your idea. You can select multiple categories for an idea. Categories are also visible to other users in the Idea Portal when viewing submitted ideas.\n   - Description: A detailed description of the idea. Consider including details such as why is it useful, who would benefit from it, and how it would work. You can use the formatting toolbar to format text and add images or web links.\n3. Optionally, if you have attachments related to the idea, click Add attachments and attach them.\n4. Click Create.\n\nAfter submission, your idea will be in the Submitted state and you can edit it in the Idea Portal until the state is changed to Completed."
    },
    {
        "id": 118,
        "CONTEXT": "\"## How to impersonate a customer in NowSupport  (HI)\n\nGo to:\n\n[https:\/\/support.servicenow.com\/](https:\/\/support.servicenow.com\/)\n\nIn the navigator type: 'Grant'\n\nClick on 'Grant\/Revoke read-only Impersonator'\n\nClick on 'Grant Role'\n\nOn top right corner select 'Log out'\n\nClick 'Sign-In'\n\nLog in to NowSupport\n\nOn top right corner select 'Impersonate User'\n\nType name of User\n\nOn top right corner select 'Switch to NOW Support Portal'\n\n**How to stop impersonation**:\n\nOn top right corner select 'Switch to Advance'\n\nOn top right corner select 'End Impersonation'\n\nSelect 'Grant\/Revoke read-only Impersonator'\n\nClick on 'Revoke Role'\n\nOn top right corner select 'Log out'\n\nClick 'Sign-In'\n\nLog in to NowSupport\n\n\n\n## Unable to view the Service Catalog \"Activate Plugin\"\n\n1. Customer tried to submit a request to Activate Plugin following [KB0695388](https:\/\/support.servicenow.com\/kb?id=kb_article_view&sys_kb_id=9b673beb47993d504a78e825126d43ff).\n2. Following the link to [Activate Plugin](https:\/\/support.servicenow.com\/now?id=ns_automation_store&catalog_sys_id=891f088e465667e234a3cb52ffa1d299) is not working for customer. Getting the below error:\n\n3. Impersonated customer to check the Service Catalog. Activate Plugin is not available.\n\n4. User has customer_admin (NS Admin) in OMNI and should not have the \"Customer Limited Catalog\" in HI as the role is restricting the user from submitting requests in Now Support Portal.\n\nNote: User has \"customer limited catalog\" role assigned in HI but user is not having NS member role in OMNI.\n\n.\n5. Engaged with NowSupport Administration via task to remove the \"Customer Limited Catalog\" role from the user profile and re-sync user roles.\n\n\\*\\* Incident Resolution Details: SFSTRY0093889 and CHG48312623 is deployed to fix the issue.\n\n\\*\\* Reference: CSTASK642776\n\n6. Impersonated the user and the Service Catalog \"Activate Plugin\" is now visible.\n\n\n\n## Impersonate option not available in Agent workspace  if turn off the next experience\n\nGiven admin and impersonator roles  a user has the impersonate option in HR agent workspace if 'Next experience' is active. But if the Next Experience  is deactivated  then impersonation is not available in HR agent workspace (but its available in native\/platform\/core view).\n\nThis is as expected as UI16\/native\/platform\/core view and the Next Experience handle the showing of the Impersonate option. However  only Next Experience can show a workspace within  and thus able to offer impersonation.\n\nWhen Next Experience is turned off -- and a workspace is opened -- the workspace has full control of the page. UI16 does not contain it and cannot add its features.\n\n\"\n\n",
        "QUERY": " 'How to impersonate a customer in NowSupport (HI)'",
        "GROUND_TRUTH": "To impersonate a customer in NowSupport (HI), follow these steps:\n\n1. Go to [https:\/\/support.servicenow.com\/](https:\/\/support.servicenow.com\/)\n2. In the navigator, type 'Grant'\n3. Click on 'Grant\/Revoke read-only Impersonator'\n4. Click on 'Grant Role'\n5. On the top right corner, select 'Log out'\n6. Click 'Sign-In'\n7. Log in to NowSupport\n8. On the top right corner, select 'Impersonate User'\n9. Type the name of the User\n10. On the top right corner, select 'Switch to NOW Support Portal'\n\nTo stop impersonation:\n\n1. On the top right corner, select 'Switch to Advance'\n2. On the top right corner, select 'End Impersonation'\n3. Select 'Grant\/Revoke read-only Impersonator'\n4. Click on 'Revoke Role'\n5. On the top right corner, select 'Log out'\n6. Click 'Sign-In'\n7. Log in to NowSupport"
    },
    {
        "id": 119,
        "CONTEXT": "\"## Script Debugger - ITOM\n\nThis article covers Script Debugger topics specific to ITOM products. For non product specific information regarding the Script Debugger  see [Script Debugger](https:\/\/support.servicenow.com\/kb?id=kb_article_view&sysparm_article=KB0815530 \"Script Debugger\").\n\n### Using Script Debugger breakpoints with Discovery inputs {#mcetoc_1hc2hel1l2v}\n\nIn order to use the script debugger  you need to be able to process the payload on the same session as the debugger. The business rule which processes the ECC queue record of interest is reviewed below  with the actual code which processes the payload.\n\nFor Discovery  the business rule calls \"SchedulePriorityECCJob\" and passes \"var job = new DiscoverySensorJob(); job.process();\" as the script. Reviewing \"DiscoverySensorJob.process()\" shows that SncSensorProcessor.process() processes the payload.\n\nThe following will work when debugging script includes  business rules  and pre post scripts (sa_pattern_prepost_script) called by sensors. Sensors do not trigger the debugger directly. Therefore  the business rules and script includes used show that we can process the payload as follows:\n\n1. Get the sys id of the ecc_queue record  \n2. Go to \"System Definition \\> Scripts - Background\"  \n3. Run the following script replacing \\<ecc_queue_sys_id\\> with the correct sys ID\n\nvar eccRecord = new GlideRecord('ecc_queue');\neccRecord.get(\u0091<ecc_queue_sys_id>');\nvar sp = new SncSensorProcessor(eccRecord);\nsp.process();\u00a0\nThe Related link \"Run Again (Debug)\" on the ecc_queue input form can avoid you needing to run this script. It does the same thing  but with one click.\n\n### Discovery Sensor {#mcetoc_1hc2hel1l30}\n\nNot all scripts work with script debugger  discovery_sensor scripts is an example. Furthermore  the debugger is for the current user session only. Therefore scripts running in a different user session will not be debugged. The debugger is used often with Business Rules and Script Includes. As a workaround  create a script include and call it from the code which needs to be debugged. In the following example a script include DebugSensor was created to be called by a discovery sensor in order for it to be caught by the Script Debugger  and thus allow us to see what is in memory via the Script Debugger.\n\nDebugSensor\n\n### Post Sensor\n\nSingle page inputs will trigger the post sensor script. However  multipage inputs will not (except for the last page). For multipage scripts only the last page processed will trigger the post sensor. The Horizontal Discovery Sensor calls resultHandler.isPagingComplete() to determine whether to call the post sensor script. To 'trick' this test to pass  the 'Opt Counter' value in table discovery_multi_page_optimistic_lock can be adjusted.\n\n1. Find the output probe on table discovery_multi_page_optimistic_lock via field 'Output Probe ID'\n2. Set 'Opt counter' value to \\<number of inputs\\> - 1\n3. Reprocess the input via script background or 'Run Again (Debug)' link in the input record\n\n\n\n## Run a Quick Discovery\n\nTo choose the MID Server  supply either the <var class=\"keyword varname\">sys_id<\/var> or name of the MID Server as the argument.  \n   If you do not name a MID Server  the system attempts to find a valid one automatically. A valid MID Server has a status of Up and can discover the given IP address. If the system finds a valid MID Server and runs a Discovery  the discoveryFromIP method returns the <var class=\"keyword varname\">sys_id<\/var> of the Discovery status record. If no MID Server can discover this IP address  the method returns the value undefined.\n\nIf you manually specify the TARGET_MIDSERVER  the system validates the given value and ensures that the MID Server table contains the specified MID Server record. If the validation passes  the discoveryFromIP method returns the sys_id of the discovery status record. If the validation fails  the method return the value undefined.\n\n## Validate discovery results {#ariaid-title4}\n\nValidate the results of your discovery by accessing the ECC queue  analyzing the XML payload  and checking the Discovery log.\nRole required: discovery_admin\nInitial discoveries often reveal unexpected results  such as previously unknown devices and processes or failed authentication. Results should also accurately identify known devices and update the CMDB appropriately. Become familiar with the network that is being discovered and the types of data returned for the different types of discoveries. Use the Discovery Log and the ECC Queue to monitor the Discovery process as data is returned from probes or pattern operations.\n\n1. To view the actual payload of a probe  click the XML icon in a record in the ECC Queue.  \n   Figure 4. ECC Queue ![ECC Queue](..\/image\/DiscoveryECCQueueView.png)\n2. To view the actual payload of a probe  click the XML icon in a record in the ECC Queue.\n3. Use the Discovery Log form for a quick look at how the probes are doing.  \n   To display the Discovery Log  navigate to Discovery \\> Discovery Log.  \n   Figure 5. Discovery Log ![The Discovery log](..\/image\/DiscoveryLog.png)  \n   The Discovery Log provides this information:  \n   {#d256081e1501}\n   |     Column      |                                                                           Information                                                                           |\n   |-----------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------|\n   | Created         | Displays the timestamp for the probe launched. Click this link to view the record for the probe launched in this list.                                          |\n   | Level           | Displays the type of data returned by this probe. The possible levels are: * Debug * Error * Information * Warning {#t_ValidateDiscoveryResults__ul_pq4_bjx_wp} |\n   | Message         | Message describing the action taken on the information returned by the probe.                                                                                   |\n   | ECC queue input | Displays the ECC queue name associated with the log message.                                                                                                    |\n   | CI              | The CI discovered. Click this link to display the record from the CMDB for this CI.                                                                             |\n   | Source          | Displays the probe name that generated the log message.                                                                                                         |\n   | Device          | Displays the IP address explored by the probe. Click this link to examine all the log entries for the action taken on this IP address by this Discovery.        |\n   {#t_ValidateDiscoveryResults__table_gq5_q3x_wp}\nNote: If you cancel an active discovery  note the following information:\n   * Existing sensor jobs that have started processing are immediately terminated.\n   * The existing sensor jobs that are in a Ready state  but have not started processing  are deleted from the system.\n   {#t_ValidateDiscoveryResults__ul_t3h_wzr_ybb}\n4. View the [Discovery Home\n   page](..\/concept\/discovery-home-page.html \"The Discovery Home page provides a summary of discoveries that were triggered by cloud and non-cloud schedules. You can view any errors that occurred during a Discovery and find remediation suggestions. Use the Home page view to examine the cloud resources discovered for the service accounts you selected in a cloud schedule.\") for details about all schedules  cloud resources (virtual machines)  discovered devices  and related errors that might have occurred.  \n[Error details](..\/concept\/discovery-home-page.html#view-ci-discovery-schedule-errors \"From the ServiceNow Home page  you can view the Discovery errors that occurred during a Discovery and get suggestions for resolving these errors. You can view the errors for all schedules or for a single schedule.\") include possible remediation steps.  \n\n## MID Server selection sequence for Discovery schedules {#ariaid-title5}\n\nThe Discovery application follows this sequence to find a MID Server.\n\n### MID Server auto-selection\n\nDiscovery follows this sequence when you select Auto-Select MID Server for the MID Server selection method on the Discovery Schedule form.\nNote: MID Server auto-select is not supported with IPv6.\n\n1. Discovery looks for a MID Server that also has an appropriate IP range configured.\n2. \n\n## MID Server selection sequence for Discovery schedules\n\nIf the system finds a valid MID Server and runs a Discovery  the discoveryFromIP method returns the <var class=\"keyword varname\">sys_id<\/var> of the Discovery status record. If no MID Server can discover this IP address  the method returns the value undefined.\n\nIf you manually specify the TARGET_MIDSERVER  the system validates the given value and ensures that the MID Server table contains the specified MID Server record. If the validation passes  the discoveryFromIP method returns the sys_id of the discovery status record. If the validation fails  the method return the value undefined.\n\n## Validate discovery results {#ariaid-title4}\n\nValidate the results of your discovery by accessing the ECC queue  analyzing the XML payload  and checking the Discovery log.\nRole required: discovery_admin\nInitial discoveries often reveal unexpected results  such as previously unknown devices and processes or failed authentication. Results should also accurately identify known devices and update the CMDB appropriately. Become familiar with the network that is being discovered and the types of data returned for the different types of discoveries. Use the Discovery Log and the ECC Queue to monitor the Discovery process as data is returned from probes or pattern operations.\n\n1. To view the actual payload of a probe  click the XML icon in a record in the ECC Queue.  \n   Figure 4. ECC Queue ![ECC Queue](..\/image\/DiscoveryECCQueueView.png)\n2. To view the actual payload of a probe  click the XML icon in a record in the ECC Queue.\n3. Use the Discovery Log form for a quick look at how the probes are doing. To display the Discovery Log  navigate to Discovery \\> Discovery Log.  \n   Figure 5. Discovery Log ![The Discovery log](..\/image\/DiscoveryLog.png)  \n   The Discovery Log provides this information:  \n   {#d280650e1459}\n   |     Column      |                                                                           Information                                                                           |\n   |-----------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------|\n   | Created         | Displays the timestamp for the probe launched. Click this link to view the record for the probe launched in this list.                                          |\n   | Level           | Displays the type of data returned by this probe. The possible levels are: * Debug * Error * Information * Warning {#t_ValidateDiscoveryResults__ul_pq4_bjx_wp} |\n   | Message         | Message describing the action taken on the information returned by the probe.                                                                                   |\n   | ECC queue input | Displays the ECC queue name associated with the log message.                                                                                                    |\n   | CI              | The CI discovered. Click this link to display the record from the CMDB for this CI.                                                                             |\n   | Source          | Displays the probe name that generated the log message.                                                                                                         |\n   | Device          | Displays the IP address explored by the probe. Click this link to examine all the log entries for the action taken on this IP address by this Discovery.        |\n   {#t_ValidateDiscoveryResults__table_gq5_q3x_wp}\nNote: If you cancel an active discovery  note the following information:\n   * Existing sensor jobs that have started processing are immediately terminated.\n   * The existing sensor jobs that are in a Ready state  but have not started processing  are deleted from the system.\n   {#t_ValidateDiscoveryResults__ul_t3h_wzr_ybb}\n4. View the [Discovery Home page](..\/concept\/discovery-home-page.html \"The Discovery Home page provides a summary of discoveries that were triggered by cloud and non-cloud schedules. You can view any errors that occurred during a Discovery and find remediation suggestions. Use the Home page view to examine the cloud resources discovered for the service accounts you selected in a cloud schedule.\") for details about all schedules  cloud resources (virtual machines)  discovered devices  and related errors that might have occurred.  \n[Error details](..\/concept\/discovery-home-page.html#view-ci-discovery-schedule-errors \"From the ServiceNow Home page  you can view the Discovery errors that occurred during a Discovery and get suggestions for resolving these errors. You can view the errors for all schedules or for a single schedule.\") include possible remediation steps.  \n\n## MID Server selection sequence for Discovery schedules {#ariaid-title5}\n\nThe Discovery application follows this sequence to find a MID Server.\n\n### MID Server auto-selection\n\nDiscovery follows this sequence when you select Auto-Select MID Server for the MID Server selection method on the Discovery Schedule form.\n\n1. Discovery looks for a MID Server that also has an appropriate IP range configured.\n2. If no MID Servers meet these criteria  it looks for a MID Server that has the ALL application that also has an appropriate IP range configured.\n3. If more than one MID Servers meet the criteria  Discovery chooses the first MID Server with the status of Up. If more than one MID Servers are up  it randomly picks one.\n4. If none are up  it uses the default MID Server specified for the Discovery application  assuming it is up.\n5. \"\n\n",
        "QUERY": " 'how to handle external payload in ecc queue'",
        "GROUND_TRUTH": "To handle external payload in the ECC queue, you need to follow these steps:\n\n1. Get the sys id of the ecc_queue record.\n2. Go to \"System Definition > Scripts - Background\".\n3. Run the following script replacing <ecc_queue_sys_id> with the correct sys ID:\n\n```javascript\nvar eccRecord = new GlideRecord('ecc_queue');\neccRecord.get('<ecc_queue_sys_id>');\nvar sp = new SncSensorProcessor(eccRecord);\nsp.process();\n```\nThis script will process the payload of the ECC queue record. The business rule which processes the ECC queue record of interest is \"SchedulePriorityECCJob\" and it passes \"var job = new DiscoverySensorJob(); job.process();\" as the script. The \"DiscoverySensorJob.process()\" shows that SncSensorProcessor.process() processes the payload.\n\nAdditionally, the \"Run Again (Debug)\" link on the ecc_queue input form can be used to avoid running this script manually. It performs the same action with a single click."
    },
    {
        "id": 120,
        "CONTEXT": "\"## Read replicas: Troubleshooting  statistics and configuring secondary databases\n\nRead Replicas \\| Troubleshooting  statistics and configuring secondary databases\n\n# Overview\n\nFor a ServiceNow deployment  a potential performance bottleneck is traffic to the database. To alleviate this bottleneck and improve performance  you can set up secondary replica databases and configure database connection pools for accessing them. In this configuration  the primary database still performs all write operations  and the secondary databases can run queries that are less sensitive to replication lag. Thus  you off-load traffic from the main database  leaving it with more resources to complete time-critical operations.\n\n# Secondary database categories and pools\n\nTo view the configuration of secondary database categories and pools:\n\n1. Hop in to the instance  and go to **System Maintenance** :  \n      Preview Image\n\n**Secondary Database Pools:** shows the lag  replicas  and current state.\n\n**Secondary Database Categories** : shows per category replication lag thresholds and which read replicas are configured (categories are hard set).\n\n{#body_html_insert_image_dialog}\n\n# Reviewing replica statistics\n\nTo review replicas statistics:\n\n1. Hop in to the instance  and go to**System diagnostics \\> table iostats \\> personal list** (add **pool name** field).\n2. Sort by selects  filter out **glide ** and check if the SELECTs are incrementing against the replicas.\n\n\n\n# Troubleshooting read replicas\n\nTo troubleshoot read replicas:\n\n1. Log in to the primary database server and review a MySQL show processlist to confirm if their are any active **select** queries being run. This should not be the case if read replicas are active.\n\nConfirm if the MySQL read replicas are lagging behind the master. If the seconds behind master exceeds what is defined in the category (see Secondary Database Categories above)  this read replica stops receiving traffic.\n\nReview the replica query statistics per \"Reviewing replica statistics\" and ensure the counters are incrementing.\n\nUse the MySQL client on each of the replicas and perform the show processlist.\n\nIf there are very long running UPDATE  DELETE  INSERT or OPTIMIZE statements being run on the master  this could cause the read replicas to lag behind. If they lag behind longer than the counters set within **Secondary Database Categories**   this removes the replica from the pool and causes the related READ queries to be diverted to the master.5. If you see no queries on the read-replicas: ensure the table 'sys_db_pool' has up to date\/consistent configuration i.e the (replica_for field) is pointing to the correct replicas: (i.e has it been recently reseeded and configuration missed? )\n\nmysql\\>use \\<database_name\\>;\nmysql\\> select \\* from sys_db_pool\n\n**Previous issues:**\n\nPost upgrade  an optimize table is run on ts_c% tables. If these tables are very large  they could cause significant lag on each of the replicas. If there is lag on the read replicas that exceeds what is set within the **Secondary Database Categories**  traffic is diverted away from the read replicas. This then causes the load on the primary database to increase exponentially.\n\nOne option is to kill the optimize queries to provide relief to the customer  but you should contact Customer Support and SysAdmin before proceeding.\n\nRead Replica reseeds have been executed  without the sys_db_pool being updated  (5.) above.\n\n# Configuring secondary databases\n\nYou can configure secondary databases to perform these types of queries:\n\n* Reports\n* Homepages\n* Auto-complete\n* Text search\n* Lists\n* Embedded lists\n* Related lists\n* Logs\n* ODBC driver\n* CMDB relationships\n\n## Setting up secondary databases\n\nTo use secondary database pools  set up secondary database servers and enable replication of the primary database. Ensure that your hardware configuration meets these requirements:\n\n* Database replication uses a supported replication technology  such as Tungsten Replicator  MySQL binary logging  Shareplex for Oracle  or Oracle Dataguard. For an example  see the Deploy MySQL Replication section.\n* The secondary database servers are in read-only mode. The secondary database pools feature does not support multiple primary databases.\n* All secondary database names are the same as the primary database name.\n* The primary and secondary database server hard drives have sufficient disk space to accommodate replication logs.\n* \\[Recommended\\] The primary and secondary database servers are located in the same datacenter. Having the secondary databases in the same datacenter improves performance and security.  \n\n|----------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| ![](\/Note_25x.pngx \"Note\") | **Note:** Secondary database pools require that the primary database is always available. ServiceNow recommends enabling high availability and disaster recovery to ensure that a primary database is always available. |\n## Configuring database pools\n\nWhen you have completed the physical set up of secondary databases with replication  you are ready to set up ServiceNow to route queries to the secondary databases:\n\n* Activate the plugin.\n* Define a pool record for each connection pool.\n* Route categories of queries to specific connection pools.  \n\n## Activating the plugin\n\nTo install this feature  activate the Secondary Database Pools plugin. See [ServiceNow instructions for activating a plugin](https:\/\/docs.servicenow.com\/csh?topicname=t_ActivateAPlugin.html&version=latest \"ServiceNow instructions for activating a plugin\").\n\n## Defining database pools\n\nTo define a pool record for each connection pool on every secondary database:\n\n1. Navigate to **System Maintenance \\> Secondary Database Pools**.\n2. Define the pool by completing the below form:  \n   * **Field:** Description\n   * Enter a name for the pool record.\n   * **Database Name:** Enter the database name. **Note:** All secondary database names must be the same as the primary database name.\n   * **Type:** Select the database management system on the secondary database (MySQL  Oracle  or SQLServer).\n   * **Active:** Select the check box to make the connection pool available.\n   * **URL:** Enter a valid JDBC connection URL for the secondary database. For example: jdbc:mysql:\/\/172.16.241.133\/ \n   * **User:** Enter the user name for the database service account.\n   * **Password:** Enter the password for the database service account.\n   * **Min connections:** Define the minimum number of concurrent database connections.\n\n\n   * **Max connections:** Define the maximum number of concurrent database connections. **Note:** Consider using the same minimum and maximum connection settings as you use for your primary database  but do not exceed the connection limit of your database management system. Factor in the number of application servers as well as the number of pools associated with each secondary database. For example  you define two connection pools for a secondary database and set the maximum connections for each pool to 20. If you have two application servers  then you have configured a maximum of 80 total connections for the database (20 to each pool for each application server).\n3. Click **Submit**.\n4. Open the record and click **Test Database Connectivity** to verify that the database connection is defined correctly. If necessary  modify the pool record or database setup to establish connectivity. This example shows an issue with database settings for the maximum allowed packet size.   \n\n## Routing queries to pools\n\nNote: If there are any expensive OOB queries related to TextSeach  Report  PA  etc.  then please log a PRB to Dev-Persistence. They can redirect the PRB to the teams to tag their code to reroute the query to be issued. Setting the query category from the call sites that issues the query is a much safer way than doing it by query pattern because it is hard for a person to understand where a query pattern is used.\n\nNote: In general  tables in sys_metadata family(it includes any table that is extended from sys_metadata) should not be routed to Read-Replica. If there is a good reason to re-route these queries  then please validate with Dev team(Dev-Persistence) before re-routing the query. PRB1352069 fix will try to exclusion list additional tables to the above list so that people don't accidentally route a wrong table. exclusion listing the tables doesn't prevent a new query pattern or new tables that show up in the slow query module.\n\nCAVEAT: It has been requested by development that we open an Case task with the Dev - Persistence team to validate whether we can reroute a query to the read replicas  as we have seen some unexpected results (such as PRB1174630: Infinite loop querying sys_db_category). If Dev does agree to reroute the query  then we should test a restart of a standby node after implementing the reroute to verify that nodes will not fail to restart after implementing the query category. If the node does not restart  then backout the query category routing.\n\nUse categories to route types of queries to one or more specific secondary database connection pools. The categories below are available:\n\n\u0095 Reports (includes scheduled reports and exports)\n\u0095 Homepages\n\u0095 Auto-complete\n\u0095 Text search\n\u0095 Lists (includes exports)\n\u0095 Embedded lists\n\u0095 Related lists\n\u0095 Logs\n\u0095 ODBC driver\n\u0095 CMDB relationships\n\nTo configure categories settings to route queries to pools:\n\n1. Navigate to **System Maintenance \\> Secondary Database Categories**.\n2. Select the category of queries to route to secondary databases (for example  auto-complete).\n3. Configure the category settings by completing the form:  \n   * **Field:** Description\n   * **Name:** This identifies the type of queries you are routing to secondary databases (auto-complete  embedded_list  homepage  list  log  odbc  related_list  reporting  or text search).   \n     (*Note:* Do not change this field value or create a new category.)\n   * **Pool selection:** When multiple connection pools are configured for a category  select the method for determining where to route a query.  \n     * **Round-Robin:** Randomly select but evenly distribute queries among connection pools.\n     * **Priority:** Select a connection pool in a predictable order (determined by the **Order** field. See step 4). Uses the lowest order pool that is available.  \n       *(Note:* When all secondary database pools are unavailable  queries are always routed to the primary database.)\n   * **Active:** Select the check box to route queries to secondary database pools. Query routing is disabled for all categories by default.\n   * **Latency sensitivity:** Select the method for determining whether replication is too far behind to use a secondary database pool:  \n     * **None:** Disregard replication lag and always route this category of queries to an available pool.\n     * **Threshold:** Only route queries to a pool when replication lag is less than the replication lag threshold.\n     * **Threshold \\& users updates:** Only route queries to a pool when the replication lag is less than the threshold and less than the time elapsed since the last update on the queried table during the current session. \"\n\n",
        "QUERY": " 'how to get the secondary db'",
        "GROUND_TRUTH": "To get the secondary database in a ServiceNow deployment, follow these steps:\n\n1. Set up secondary database servers and enable replication of the primary database. Ensure that your hardware configuration meets the requirements mentioned in the article, such as using a supported replication technology, having the secondary database servers in read-only mode, having all secondary database names the same as the primary database name, and ensuring sufficient disk space for replication logs.\n\n2. Once you have completed the physical set up of secondary databases with replication, you are ready to set up ServiceNow to route queries to the secondary databases. This involves activating the Secondary Database Pools plugin.\n\n3. Define a pool record for each connection pool on every secondary database. Navigate to System Maintenance > Secondary Database Pools and define the pool by completing the form provided in the article. Test the database connectivity to verify that the database connection is defined correctly.\n\n4. Route categories of queries to specific connection pools. Navigate to System Maintenance > Secondary Database Categories and select the category of queries to route to secondary databases. Configure the category settings by completing the form provided in the article.\n\nRemember, secondary database pools require that the primary database is always available. ServiceNow recommends enabling high availability and disaster recovery to ensure that a primary database is always available."
    },
    {
        "id": 121,
        "CONTEXT": "\"## How to generate and upload a certificate for on-prem store app | Troubleshooting common issues or er\n\nWhen selecting an app for download on the Store  this will generate an encrypted format for a specific instance. Encryption is accomplished by using a PKCS12 key pair certificate. This process can be managed from a computer with internet access  which can then download the encrypted application and generate a certificate which will allow to finalize the app installation on the self-hosted instance.\n\nThis process will need to be done for each single instance. The same certificate can be used on all instances under the same customer account  but it will need to be uploaded once on each instance (i.e. prod  sub-prod  dev  test).\n\nA store application file has an expiry date upon download. Administrators will have two weeks time to install it  after which they will need to download it again.\n\nCertificates are valid for one year  after which they will be marked as invalid  and the administrator will need to generate a new one and remove the old one.\n\n### How to generate a new certificate?\n\nAfter requesting an application in the store for their on-premise instance  the customer will be requested to upload an existing certificate or generate a new certificate.\n\nIf they do not have a valid existing certificate  they can proceed as follows:\n\n**NOTE:** ***Role required: admin***\n\n1. Go to System Definition \\> Certificates\n2. Click Generate a new certificate\n3. Enter a password to generate a new one  \n4. The certificate is generated\n5. Click on Download certificate to save it for future use\n6. Click on Download application\n\n### Uploading a certificate to the instance\n\n1. Go to System Definition\\> Certificates\n2. Click New\n3. Add Name\n4. Add Type: select PKCS12\n5. Add Key store password: enter the same password chosen when generating the certificate\n6. Click the attachment icon to upload the certificate file (.p12) previously downloaded\n7. Click on the link to Validate Stores\/Certificates\n8. A banner confirming the valid certificate will display\n9. Right click on top grey header and select 'Save'\n\n\n\n#### Common issues related to certificates\n\n**Reasons for receiving an error for invalid certificate**\n\n1. If a certificate was never uploaded in the instance  but the customer proceeds with uploading the application directly  an error ' No valid certificate found to process application upload' displays.\n2. There is a mismatch between the certificate that was uploaded when downloading the app in the Store and the certificate that was already uploaded in the instance on a previous occasion. When uploading the application in the instance  the error displays.\n\n? Ensure only one valid certificate with the same Type is uploaded in the instance. This should match the one uploaded when requesting the app in the store.\n\n3. The certificate record was not created correctly in the instance or the .p12 file was not added as an attachment.\n\n? Customer can go to System Definition\\> Certificates and validate all active records by clicking on **Validate Store\/Certificate** . If result is invalid  the reason will be highlighted. The customer can just fix the record or  in case of obsolete records  deactivate them in favor of new ones  by unflagging the **Active** checkbox.\n\n4. The incorrect type of certificate was selected in the certificate form when uploading the certificate on the instance.\n\n? The selected Type should be PKCS12 Key Store\n\n5. The instance ID displayed in the selected instance when requesting an app in the Store does not match the current instance ID in the instance stats.\n\n?Customer will need to ensure the instance ID for their instance matches the one in their stats. If not  they can update their instance information with [Manage On-Premise Instance - Now Support Service Catalog](https:\/\/support.servicenow.com\/kb?id=kb_article_view&sysparm_article=KB0551693 \"Manage On-Premise Instance - Now Support Service Catalog\")\n\n6. The App Store Certificate got generated using the latest Java version but the On-Premise customer instance is using an older Java version.\n\nIn this case  these are the possible 3 workarounds (The certificate must be in PKCS12 format. The store app will not accept a PEM certificate.)\n\n1. Update the customer instance to at least JDK 1.8.0_342\/1.8.0_345 or 11.0.16. Ref. [OpenJDK 11 Installation for Red Hat Systems (RHEL\/CentOS) - Self-hosted](https:\/\/support.servicenow.com\/kb?id=kb_article_view&sysparm_article=KB0963329 \"OpenJDK 11 Installation for Red Hat Systems (RHEL\/CentOS) - Self-hosted\")\n2. Alternatively customer can try converting the pkcs12 certificate to the legacy\/weaker format  using a newer JDK with the **'keystore.pkcs12.legacy'** option\n3. Customer can use the below example to convert the certificate using **keytool** and then upload this newly generated certificate to the instance before doing the application upload:\n\nkeytool -importkeystore -srckeystore certname.p12 -srcstorepass **changeit** -srcstoretype PKCS12 -deststoretype PKCS12 -deststorepass **changeit** -destkeystore convertedcertname.p12 -J-Dkeystore.pkcs12.legacy\n\n*(where 'changeit' must be replaced with the password set by the customer when generating the certificate).*\n\n\n\n## Generate a Java KeyStore file\n\n# Generate a Java KeyStore file {#ariaid-title1}\n\nConvert the public certificate that you had generated to a Java KeyStore file on your machine because your ServiceNow instance supports a Java KeyStore file. After generating  you must upload the file to your ServiceNow instance.\nRole required: admin\n\n1. Open the command prompt on your machine.\n2. Use the cd command to change the directory to the location where you had generated the public certificate.\n3. On the command prompt  execute the command openssl pkcs12 -export -in publickey.cer -inkey private.key -out \\<your-file-name\\>.p12 -name \"\\<your-custom-name\\>\".\n4. In the Enter Export Password prompt  create an export password.\n5. In the Verifying - Enter Export Password prompt  reenter the export password you had created.  \n   The P12 file is created.\n6. Copy the P12 file to the respective JDK\/bin folder of your machine.  \n   Note: Depending on the OS of your machine  the location of the JDK\/bin folder might vary.\n7. Copy the path to the JDK\/bin folder.\n8. In the command prompt  use the cd command to change the current directory to the JDK\/bin folder.\n9. To convert the P12 file to a Java KeyStore file  execute the command keytool -importkeystore -srckeystore \\<P12-file-name\\>.p12 -srcstoretype pkcs12 -destkeystore \\<Java-KeyStore-filename\\>.jks.\n10. In the Enter destination keystore password prompt  create a password.\n11. In the Re-enter new password prompt  reenter the destination keystore password you had created.\n12. In the Enter source keystore password prompt  enter the same password that you had created to generate the P12 file.  \n    The Java KeyStore file is created in the JDK\/bin location.\n\n\"\n\n",
        "QUERY": " 'how to generate and upload a certificate for on-prem store app'",
        "GROUND_TRUTH": "To generate and upload a certificate for an on-prem store app, follow these steps:\n\nTo generate a new certificate:\n1. Go to System Definition > Certificates.\n2. Click Generate a new certificate.\n3. Enter a password to generate a new one.\n4. The certificate is generated.\n5. Click on Download certificate to save it for future use.\n6. Click on Download application.\n\nTo upload a certificate to the instance:\n1. Go to System Definition > Certificates.\n2. Click New.\n3. Add Name.\n4. Add Type: select PKCS12.\n5. Add Key store password: enter the same password chosen when generating the certificate.\n6. Click the attachment icon to upload the certificate file (.p12) previously downloaded.\n7. Click on the link to Validate Stores\/Certificates.\n8. A banner confirming the valid certificate will display.\n9. Right click on top grey header and select 'Save'.\n\nRemember, certificates are valid for one year, after which they will be marked as invalid, and the administrator will need to generate a new one and remove the old one."
    },
    {
        "id": 122,
        "CONTEXT": "\"## Bug in OOB Script (Schedule Job : Auto Close Case) - HRSD\n\n## Description\n\nIssue Definition: Bug in OOB Script (Schedule Job: Auto Close Case) - HRSD\ngrContext.query(); is not being used  after adding queries.\n\nObserved in: Orlando Patch 4  Orlando Patch 7\n\n## Steps to Reproduce\n\nSteps to Reproduce:\n\n1. On any OOB instance  open the below OOB scheduled script:  \n\/sysauto_script.do?sys_id=8d5f5dfb53032300c128ddeeff7b12bd\\&sysparm_record_target=sysauto\\&sysparm_record_row=1\\&sysparm_record_rows=6\\&sysparm_record_list=nameCONTAINScase%5EORDERBYname\n2. In the script block  after line 56 :  \n\"grContext.query();\" is missing  \n\n## Workaround\n\n1. Make changes to the auto close case: ( the scheduled job with sys_id - 8d5f5dfb53032300c128ddeeff7b12bd) in line number 53 - 56\n\n`if(grCase.hr_service.le_type) {`\n`if(new sn_hr_core.hr_CaseUtils()._getActivitySetContextCases(caseId))`\n`return;`\n`}`\n\n2. Make changes to the script include hr_CaseUtils (with sys_id 24c782869f202200d9011977677fcf89) as this :  \nadd a method -\n\n_getActivitySetContextCases: function(caseId){\nvar grContext = new GlideRecord('sn_hr_le_activity_set_context');\ngrContext.addQuery('hr_case' caseId);\ngrContext.addQuery('state' 'awaiting_trigger');\ngrContext.query();\nreturn grContext.hasNext();\n} \n\n3. Run the following background script:   \n\nnew sn_hr_core.hr_CaseUtils()._getActivitySetContextCases('123');\n\nThis should generate an RCA. Make sure there are two RCAs - one for the script include  and one for the scheduled script.\n\n**Related Problem: PRB1442036**\n\n\n\n## Internal FAQ & Case Handling Instructions for KB1553688\n\nServiceNow is providing a script designed to help our customers conduct an assessment of their instances with potentially misconfigured access control lists (ACLs). This is outlined in [https:\/\/support.servicenow.com\/kb?id=kb_article_view\\&sysparm_article=KB1561609](https:\/\/support.servicenow.com\/kb?id=kb_article_view&sysparm_article=KB1561609)\n\nThis Security Assessment Script simulates guest user access (unauthenticated users)  enabling customers to have a better understanding of data that may have been displayed to unauthenticated users prior to maintenance that ServiceNow recently performed.\n\n**Is there a plan to change the public access on widgets for all customer instances?**\nFor Support only. Do not copy-paste to customers.\nThis will be reviewed however ServiceNow is not performing this action as part of the current maintenance plan. For now this should be done by the customer where possible. If they need us to restrict public access on widgets they don't have access to do themselves  then have a CS SME create and perform the change as needed through a change request.\n\n**My team has updated 'X' amount of widgets but we don't have access to modify the remaining widgets as it requires \"Maint\" access. Can ServiceNow modify the remaining 'Y' amount of widgets?**\nFor Support only. Do not copy-paste to customers.\nYes. The case should be assigned moved from CS -- Mass Outage to a CS SME to gain approval from the customer and create and perform the change as needed.\n\n**Questions asking for RCA -- How long has ServiceNow Known about this misconfiguration? Why was this not addressed previously through patching? Etc.**\nPlease see KB1553688 and KB1555339 for information regarding this misconfiguration issue. ServiceNow is continuing to investigate this issue and will provide relevant information to customers as appropriate.\n\n**What is ServiceNow's response plan?**\nServiceNow is aware of the recent publications describing a potential misconfiguration issue that could result in unintended access and is actively investigating the reports that we have observed in various online resources.\nOut of an abundance of caution  ServiceNow has taken steps to try to mitigate this issue across all customer instances.\n\n* On October 18  ServiceNow performed proactive maintenance on customer instances to update ACLs that were configured empty -- ones that contained no role  no condition  and no script -- to require that a user be explicitly logged in to access the underlying data.\n* On October 20  ServiceNow applied additional proactive maintenance on customer instances to update ACLs with a similar configuration that may have been evaluated as empty.\n* On October 25 and 28  ServiceNow applied additional proactive maintenance on customer instances to update specific OOB widgets and further secure the sys_user table.  \nAdditional Information:\n* [KB1581507](https:\/\/support.servicenow.com\/kb?id=kb_article_view&sysparm_article=KB1581507) (Customer-Facing - WIP) Provides a short summary of the previously applied maintenance updates\n* [KB1555339](https:\/\/support.servicenow.com\/kb?id=kb_article_view&sysparm_article=KB1555339) (Customer Facing - Published) Details the proactive maintenance that ServiceNow has applied to enhance the security of customer instances by date. It also includes FAQs and recommended actions.  \n**There are some ACLs in my instance that have no role  no condition  and no script after the maintenance was performed. Was the maintenance complete?**  \nThe maintenance that ServiceNow applied on 10\/18 and 10\/20 did not adjust non-record ACLs. Additionally  the maintenance did not adjust inactive ACLs.\n\n**After the Maintenance - Customers experience issues with a report that uses a Function Field and may see the error - \"Access to this content denied based on report_view field ACLs\"**\n\nPlease note that PRB1675406 is not a PRB  but designed behaviour. Customers may see this issue because the write audit changed existing read ACLs.\n\nIn short  to read from a function field a user has to pass both:\n\n- read access to the function field.  \n- read access to all of the contributing fields used in the function.\n\nTo report on a function field a user has to pass all:\n\n- report_view access to the function field.   \n- report_view access to each of the contributing fields.   \n- role-only read ACL.   \n- role-only read read ACL for all contributing fields.\n\nUse the wording below when responding to the case after validating it matches the criteria.\n\n\"ServiceNow applied proactive maintenance to improve the security of your instance by updating some ACLs meeting specific criteria  as summarized in KB1555339. Following this update  some users may encounter access issues when attempting to access reports using function fields that are associated to a table for which one of the ACLs that had been updated by the maintenance is present.\n\nIf you encounter this scenario  please consider creating an appropriate ACL to expressly grant access to the intended users. Please refer to the documentation for more details: [https:\/\/docs.servicenow.com\/csh?topicname=acl-function-fields.html\\&version=latest](https:\/\/docs.servicenow.com\/csh?topicname=acl-function-fields.html&version=latest)\"\n\n\n\n## 6.0\n\n## Description\n\nServiceNow has identified a defect that can result in Clone failures of instances configured with gateway shards. A Module Access Policy included in San Diego for gateway and secondary database pool plugins caused a regression in our Cloud Automation Infrastructure.\n\nCustomers can face this defect on gateway shard instances if at least one of the Source or Target instances of the Clone are on San Diego.\n\n## Steps to Reproduce\n\n1. Ensure the Source instance to be used for Cloning is configured with gateway shards.\n2. Upgrade Target instance to be used for Cloning to San Diego\n3. Initiate a shard over shard Clone from the source instance to the target instance on San Diego.\n\n## Workaround\n\nServiceNow fixed this issue in San Diego Patch 2  and we recommend updating your instances to this version to prevent any impact caused by this defect. If either source or target version is on San Diego and on a version before San Diego Patch 2  below workaround can be applied. The below steps need to be performed on the Target instance of the Clone that is already on San Diego. They need to be performed on the Source instance as well if it is also upgraded to San Diego.\n\n### **Steps to apply the workaround on the instance(s) on San Diego:**\n\n#### **Part 1**\n\n* Login to the Instance as an 'admin' user.\n* Run the below script from '**Scripts - Background'.**This is will deactivate the Module Access Policy included in San Diego that's configured to reject Script Access of password.\n\nvar gr = new GlideRecord(\u0093sys_kmf_crypto_caller_policy\u0094);\ngr.get(\u0093fcf89b91eb4330100fcee5b26b5228c8\u0094);\ngr.setValue(\u0093active\u0094  \u00930\u0094);\ngr.update();\n\n#### **Part 2**\n\n* Create a new Module Access Policy as below. This requires user to have sn_kmf.cryptographic_manager or sn_kmf.admin role\n* Navigate to Key Management \\> Module Access Policies \\> All\n* Click on 'New' to create a new Module Access Policy with below fields:  \n  * Policy name - Allow Maint to access gateway module\n  * Crypto Module - com_snc_da_gateway_glideencrypter (See Note Below if this Crypto Module is not found)\n  * Type = Role\n  * Target Role = maint\n  * Result = Track\n  * Active Flag selected\n* Click 'Submit' and save the new Module Access Policy\n\n### **Data Preserver on source instance to protect the workaround**\n\nData preserver should be defined in the source instance to protect the above created 'Module Access Policy' on the target instance from being overwritten during a clone. Data Preservers are defined in the source instance but preserve records in the target instance.\n\n1. Login to the Source Instance as an 'admin' user.\n2. On the source instance  navigate to **System Clone** \\> **Preserve Data**.\n3. Click on 'Column Options' next to any Column Name or Right-Click on any Column Name and click on 'Import XML' menu from the drop-down.\n4. Select the attached XML (clone_data_preserver_28c9ba1d1b6641105edf32ebdc4bcb44.xml) and Upload.\n5. 'sys_kmf_crypto_caller_policy' Clone Data Preserver will be imported. This preserver is used to preserve the modifications made to the Module Access Policies mentioned in Part 1 and Part 2 above.\n6. Verify that the Clone Data Preserver imported is configured for Module Access Policy (sys_kmf_crypto_caller_policy) table.\n7. Verify that 2 conditions are configured with Or Condition as below:  \n   * Policy name is 'Allow Maint to access gateway module'\n   * Sys ID is 'fcf89b91eb4330100fcee5b26b5228c8'\n\nIf the data preserver is not configured correctly on the source instance  the workaround implemented on a target instance can be lost after a clone has been completed successfully. Therefore it is suggested to check if the workaround is present or not always before requesting for a new shard-over-shard clone on to a target instance on San Diego.\n\nBelow script can be run from '**Scripts - Background'**as an 'admin' user:\n\nvar gr = new GlideRecord(\u0093sys_kmf_crypto_caller_policy\u0094);\n\ngr.get(\u0093fcf89b91eb4330100fcee5b26b5228c8\u0094);\n\ngs.print(\"OOB MAP Status is \" + gr.active + '');\nvar queryStr = \u0093policy_name=Allow Maint to access gateway module\u0094;\ngr.initialize();\ngr.addEncodedQuery(queryStr);\ngr.query();\nif (gr.next())\n{\ngs.print(\"Custom Created MAP Status is \" + gr.active + '');\ngs.print(gr.getDisplayValue(\u0091type\u0092));\ngs.print(gr.getDisplayValue(\u0091target_role\u0092));\ngs.print(gr.getDisplayValue(\u0091crypto_module\u0092));\ngs.print(gr.sys_id);\n}\nOutput returned should be like below to confirm that out of the box included Module Access Policy (MAP) is not active and customer created MAP is active and configured so that with 'maint' role can access the gateway crypto module\n\n\\*\\*\\* Script: OOB MAP Status is false\n\\*\\*\\* Script: Custom Created MAP Status is true\n\\*\\*\\* Script: Role\n\\*\\*\\* Script: maint\n\\*\\*\\* Script: com_snc_da_gateway_glideencrypter\n\\*\\*\\* Script: \\<SYS ID\\>\n\n\"\n\n",
        "QUERY": " 'How to generate a RCA in OOB script?'",
        "GROUND_TRUTH": "To generate a RCA in OOB script, follow these steps:\n\n1. Make changes to the auto close case: (the scheduled job with sys_id - 8d5f5dfb53032300c128ddeeff7b12bd) in line number 53 - 56\n\n`if(grCase.hr_service.le_type) {`\n`if(new sn_hr_core.hr_CaseUtils()._getActivitySetContextCases(caseId))`\n`return;`\n`}`\n\n2. Make changes to the script include hr_CaseUtils (with sys_id 24c782869f202200d9011977677fcf89) as this :  \nadd a method -\n\n_getActivitySetContextCases: function(caseId){\nvar grContext = new GlideRecord('sn_hr_le_activity_set_context');\ngrContext.addQuery('hr_case' caseId);\ngrContext.addQuery('state' 'awaiting_trigger');\ngrContext.query();\nreturn grContext.hasNext();\n} \n\n3. Run the following background script:   \n\nnew sn_hr_core.hr_CaseUtils()._getActivitySetContextCases('123');\n\nThis should generate an RCA. Make sure there are two RCAs - one for the script include  and one for the scheduled script."
    },
    {
        "id": 123,
        "CONTEXT": "\"## Work Instruction | How to perform H-AHA transfers\/failovers: appstoreprod and DATACENTER instances\n\n|---------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------|\n| ![Warning](sys_attachment.do?sys_id=e688295d476abd10f93138ce536d4304 \"Warning\") | **Warning**: The work instructions below are for use by SRE  SRE-DevOps  and SA staff only. |\n# Overview\n\nThis article is intended for all non-AHA failovers using the fail-live.sh script for all instances listed below. For all other instances  please utilize the standard AHA transfer article [KB0529754](.\/kb?id=kb_article_view&sysparm_article=KB0529754 \"KB0529754\").\n\n|---------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------|\n| ![Warning](sys_attachment.do?sys_id=e688295d476abd10f93138ce536d4304 \"Warning\") | **Important Notice**: All hostnames used for servers should be fully qualified domain names. For example  foo.service-now.com. |\n**From Version 1.3**  all hostnames will be checked if they are resolvable and fully qualified. If not you will get the following error:\n\n**\\<hostname\\> is not a valid server\/hostname**\n\n## Non-AHA instances\n\n|             INSTANCE             |                    Non-AHA reason                    |\n|----------------------------------|------------------------------------------------------|\n| appstoreprod                     |                                                      |\n| DATACENTER                       | Should not AHA DATACENTER with DATACENTER Automation |\n| dcil4 (see KB20000207 in HIWAVE) | Should not AHA dcil4 with dcil4 Automation           |\n# Risks \\& Cautions {#RISKS_&_CAUTIONS}\n\nN\/A\n\n# Dependencies {#DEPENDENCIES}\n\n## Monitor the AHA2.0 Validation Test and Monitoring Results Report:\n\nTo avoid any surprises or delays  it's a good idea to monitor the results daily by checking the AHA2.0 Validation Test and Monitoring Results report for two weeks before the scheduled transfer to leave time to investigate any failed audits.\n\nThe AHA2.0 Validation Test and Monitoring Results report is emailed twice daily. It contains the latest AHA 2.0 critical and non-critical AHA 2.0 audit results for DATACENTER  datacenterdev  and datacentertest. Check the latest report and confirm that the critical tests pass for the instance to be transferred. Investigate any non-critical failures.\n\nThe recipient list for the 'AHA2.0 Validation Test and Monitoring Results' report can be found in DATACENTER in the 'cid.validation.daily.report.receipt.list' in the u_cid_configuration_parameters table 'cid.validation.daily.report.receipt.list' in the u_cid_configuration_parameters table\n\n**Note:** For DATACENTER  datacenterdev  and datacentertest  the following audits are skipped intentionally:\n\n**AHA Audit Failed or Skipped checks for DATACENTER  datacenterdev  and datacentertest:**\n\nFAILED:\n\n* Primary F5 BigIP Pool Exists - This is expected and due to CNS change [CHG46781814](https:\/\/support.servicenow.com\/nav_to.do?uri=%2Fchange_request.do%3Fsys_id%3Dc8d46e75479b295011eaf24c736d4392%26sysparm_stack%3D%26sysparm_view%3D \"https:\/\/support.servicenow.com\/nav_to.do?uri=%2fchange_request.do%3fsys_id%3dc8d46e75479b295011eaf24c736d4392%26sysparm_stack%3d%26sysparm_view%3d\")\n* Standby F5 BigIP Pool Exists - This is expected and due to CNS change [CHG46781814](https:\/\/support.servicenow.com\/nav_to.do?uri=%2Fchange_request.do%3Fsys_id%3Dc8d46e75479b295011eaf24c736d4392%26sysparm_stack%3D%26sysparm_view%3D \"https:\/\/support.servicenow.com\/nav_to.do?uri=%2fchange_request.do%3fsys_id%3dc8d46e75479b295011eaf24c736d4392%26sysparm_stack%3d%26sysparm_view%3d\")\n* DNS Entry Points To VIP Of Primary Datacenter - This is expected and due to CNS change [CHG46781814](https:\/\/support.servicenow.com\/nav_to.do?uri=%2Fchange_request.do%3Fsys_id%3Dc8d46e75479b295011eaf24c736d4392%26sysparm_stack%3D%26sysparm_view%3D \"https:\/\/support.servicenow.com\/nav_to.do?uri=%2fchange_request.do%3fsys_id%3dc8d46e75479b295011eaf24c736d4392%26sysparm_stack%3d%26sysparm_view%3d\")\n\nSKIPPED:\n\n* DNS Propagation Check - Skipped because the instance is an internal vip instances\n* Application instance is AHA 3 ready - Skipped because the instance is not configured for AHA 3.0 (For AHA 3.0  the instance must be ADCv2  and all nodes must be fast_failover_support capable).\n* Application instance has correct AHA 3 configuration - Skipped because the instance is not configured for AHA 3.0 (For AHA 3.0  the instance must be ADCv2  and all nodes must be fast_failover_support capable).\n* ADC Pool Members Count Symmetry - Skipped because the instance is using F5 not ADCv2\n* ADC Pool Count Symmetry - Skipped because the instance is using F5 not ADCv2\n\nSplunk errors shown in red in the report should be reported to the GCO - Service Apps team\n\n## Reschedule Move Changes Scheduled Within the Same Window\n\n**SRE-DevOps Completes this step:**\n\n1. Log in to DATACENTER and navigate to the Instance Move Contexts table \\[u_instance_move_context\\] and filter for records with a Move Cutover Start Time between the one hour before the Planned Start Date\/Time for the H-AHA change with Global Pause and the Planned End Date\/Time.\n2. When reviewing the records  keep in mind that H-AHA changes typically are completed within one hour  so move changes with a cutover time one hour before and one hour after the Planned Start Date\/Time for the H-AHA change should be rescheduled  and the rest should be noted in the H-AHA change work notes in case the change runs long.\n3. To reschedule move changes  see [KB1116804](.\/kb?id=kb_article_view&sysparm_article=KB1116804 \"Reference | How to reschedule a dcPROD (Instance Automation) Move?\") - Reference \\| How to reschedule a dcPROD Move?\n\n## Obtain CAB Approval\n\n**SRE-DevOps Completes this step (ideally two weeks before the planned start date):**\n\n1. Submit the change to the Commercial Cab meeting a week before the planned start date and represent the change at the meeting. Obtain approval.\n\n## Identify and Block the Time for the H-AHA Transfer\n\n**SRE-DevOps  CAB Lead to complete these steps (ideally three weeks before the planned start date) The change needs to be approved before the time will be blocked:**\n\n1. Log in to DATACENTER and navigate to the [Automation Schedule](https:\/\/datacenter.service-now.com\/nav_to.do?uri=%2Fsys_report.do%3Fsys_id%3D4a419bfedb9ce8907527d7c8f49619f0)report  and identify a four-hour window.\n2. Email Trent Laster  CAB Lead to create the entry in the blackout calendar.\n\n\n\n\/\/should return 0 \n\nif not  run\n\n\"set sql_log_bin=0;\" truncate table sys_mutex;\n\n# Verification Steps\n\n1. Go to https:\/\/\\<instance name\\>.service-now.com\/xmstats.do?include=database cluster\n2. Verify that the primary database is now in the new location.\n3. Verify as well that the nodes all have the correct schedulers.\n\n## Deactivate Global Pause\n\n#### To Resume:\n\n1. Open u_automation_group_pause_request_list.do and open the record you created during pause creation.\n2. Click on the resume button.\n   In the same page log:\n   * First you will see \"Resume requested  changing active to false and setting resume time as NOW.\"\n   * In between 2 to 5 minutes you will see \"All pause requests for this group pause are resumed. Changing state to resumed.\"\n   * The State field will change to Resumed in the automation pause request record.\n\n#### In the automation pause request record:\n\n* The State field will change to **Resumed.**\n\n#### Post resume validation request:\n\nPlease click the following URL and you will notice 100's of ecc_queue records.\n\n[https:\/\/datacenter.service-now.com\/ecc_queue_list.do?sysparm_query=sys_created_on%3E%3Djavascript%3Ags.beginningOfLastMinute()%5EagentNOT%20LIKE_disco_%5Estate!%3Dprocessed%5EORstate%3D\\&sysparm_view=](https:\/\/datacenter.service-now.com\/ecc_queue_list.do?sysparm_query=sys_created_onRELATIVEGT%40minute%40ago%401&sysparm_view= \"https:\/\/datacenter.service-now.com\/ecc_queue_list.do?sysparm_query=sys_created_onRELATIVEGT%40minute%40ago%401&sysparm_view=\")\n\n# Disable the Notification Banner\n\nSRE-DevOps will disable the [banner](https:\/\/datacenter.service-now.com\/nav_to.do?uri=%2Fsystem_properties_ui.do%3Fsysparm_title%3DBanner%2BNotification%2BProperties%26sysparm_nostack%3Dtrue%26sysparm_category%3DBanner%2BNotification)by unchecking the On\/Off checkbox.\n\n# Rollback\n\nThe script does not have auto rollback features. The rollback will need to be done by hand.\n\nPerform the following steps:\n\n1. Shut down all app nodes.\n2. Set the new target mysql\/mariadb to read-only.\n3. Set the old target mysql\/mariadb to read-write.\n4. Change the glide.db.properties on all nodes to point back to the old dbi (e.g.  jdbc url).\n5. Start the nodes.\n6. Change the DNS back to the old VIP.\n\n# Escalation\n\n* **Primary Contact:** SRE [sitereliability@servicenow.com](mailto:siterelibility@servicenow.com \"sitereliability@servicenow.com\")\n* **Alternate Contact:** SRE DevOps [sredevopsglobal@servicenow.com](mailto:sredevopsglobal@servicenow.com \"sredevopsglobal@servicenow.com\")\n* **Alternate Contact:** System Administrator [sysadmin@servicenow.com](mailto:sysadmin@servicenow.com \"sysadmin@servicenow.com\")\n\n# Additional Resources\n\n[KB1115822](.\/kb?id=kb_article_view&sysparm_article=KB1115822) Team Page \\| SRE DevOps\n\nRevision Log... **(Last updated: 19-Dec-2023)**\n\n|  Version  |  Published  |                                                                                                                              Summary of Changes                                                                                                                               |\n|-----------|-------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 1.0       | 08-Jan-2014 | Initial version                                                                                                                                                                                                                                                               |\n| 2.0 - 3.0 | --          | Outdated                                                                                                                                                                                                                                                                      |\n| 4.0       | 22-Jan-2021 | Update the help output for the script. Added a recommendation to use the -V option for DATACENTER H-AHA.                                                                                                                                                                      |\n| 5.0       | 6-Jul-2021  | Updated the commands to use snow APIs. Added the option to run the script using bssh. Added a rollback plan. Removed partner portal; it can now use the standard AHA automation.                                                                                              |\n| 6.0       | 14-Jan-2022 | Removed bssh  fast-aha and cmdb dependency. Only prompts for username and password.                                                                                                                                                                                           |\n| 7.0       | 28-Mar-2022 | Added an important note to the **Overview** section. Added dependencies under the **Dependencies** section. Updated the **Instructions** section and added parameters and troubleshooting steps. Added the primary and alternative contacts under the **Escalation** section. |\n| 8.0       | 28-Mar-2023 | Added a dependency under the **Dependencies** section to validate that the AHA 2.0 audits pass before proceeding.                                                                                                                                                             |\n| 9.0       | 24-Apr-2023 | Updated the **Dependencies**section to confirm that we can disregard failing non-critical audit check 'The application instance is AHA 3 ready'.                                                                                                                              |\n| 10.0      | 08-May-2023 | Updated the **Change Plan Instructions** to add a step for SRE-DevOps to obtain Firefighter access before initiating global pause  and added detailed steps for global pause.                                                                                                 |\n| 11.0      | 08-May-2023 | Updated **Change Plan Instructions**to include a link for obtaining Firefighter access.                                                                                                                                                                                       |\n| 12.0      | 19-May-2023 | Corrected SRE-DevOps's team name  Referenced DATACENTER as all caps                                                                                                                                                                                                           |\n| 13.0      | 23-May-2023 | Updated **Change Plan Instructions** to correct datacenter links where they were either broken or displaying the datacentertest instance name.                                                                                                                                |\n| 14.0      | 14-Nov-2023 | Updated the **Dependencies** section: -to include a note about expected AHA audit failures due to CNS changes. -to include steps for running response time tests if the database version will change after the transfer.                                                      |\n| 15.0      | 19-Dec-2023 | Formatting only. No content changes. Link name was KBKB1115822 corrected to KB1115822.                                                                                                                                                                                        |\n\n\n## Guideline | Schedule Time Conflict Detection (STCD) troubleshooting guide\n\n|---------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------|\n| ![Note](sys_attachment.do?sys_id=45349492db3d9f00fac7f4621f9619f1 \"Note\") | **Note**: The guideline below is intended for readers in the Instance Automation SE  SRE and SRE DevOps staff. |\n## Table of Contents\n\n* [Overview](#mcetoc_1hgim7tom6o)\n  * [STCD Requests](#mcetoc_1hgj1mtj735)\n  * [STCD Onboarded Operation](#mcetoc_1hgj0mn188)\n  * [STCD Onboarded Operation Profile Override](#mcetoc_1hgj0mn189)\n  * [Looking Up STCD Requests](#mcetoc_1hgj0mn18a)\n* [STCD Errors \\& Troubleshooting](#mcetoc_1hgim7tom6r)\n  * [CONFLICT_WITH_BLACKOUT_PERIOD_OF_THE_DATACENTER](#mcetoc_1hgj1mtj736)\n  * [CONFLICT_WITH_AUTOMATION_RESTRICTION](#mcetoc_1hgj1mtj737)\n  * [CONFLICT_WITH_NONCORE_HOURS_OF_THE_DATACENTER](#mcetoc_1hgj1mtj736)\n  * [CONFLICT_WITH_WEEKEND_SCHEDULING_HOURS_OF_THE_DATACENTER](#mcetoc_1hgj1mtj736)\n  * [NON_CORE_HOURS_DETAILS_ERROR](#mcetoc_1hgj1mtj736)\n  * [DATACENTER_IS_MISSING](#mcetoc_1hgj1mtj736)\n  * [MAINTENANCE_WINDOW_IS_NOT_ROUNDED_PER_CONFIG](#mcetoc_1hgj1mtj736)\n* [Additional Resources](#mcetoc_1hgim7tom6t)\n\n# Overview {#mcetoc_1hgim7tom6o}\n\nThe purpose of this article is to describe commonly encountered STCD errors and suggest potential resolutions to these errors. This article can be used by SREs  engineers and automation owners who use STCD APIs and service catalogs to schedule maintenance windows for cloud automations. This article serves as a self-help guide using which the audiences can understand and resolve commonly encountered STCD errors.\n\n## STCD Requests {#mcetoc_1hgj1mtj735}\n\n**Table name:** u_conflict_detection_scheduling_request\n\nThis table persists all the scheduling requests received and processed by STCD APIs. This table retains data of last thirty days at any time.\n\n**Key table fields to remember:**\n\n* **Onboarded operation ID:** The automation requesting STCD reservation (references u_onboarded_operations table)\n* **Request Type:** The type of STCD API (Propose\/Confirm\/Schedule\/Cancel\/Release)\n* **Scheduling Request Json:** Contains all the required inputs provided by the user and determined by the STCD APIs for making STCD reservations. Please see the below section for an explanation of key STCD user inputs.\n* **Scheduling Response Json:** The response provided by STCD APIs including the status of the STCD reservation.\n* **Error Details:** The error message returned by STCD APIs when there is failure.\n\nNote: Information contained in 'User Request Json' field of an STCD request record can be ignored as the 'Scheduling Request Json' field contains all the information.\n\n**Key user inputs to remember:**\n\nThe below inputs are provided by users as part of STCD API requests and can be found in the 'Scheduling Request Json' field (described above) of an STCD request record.\n\n* **MaintenanceStartDateTime(UTC):** User provided maintenance start time for STCD reservation\n* **MaintenanceEndDateTime(UTC):** User provided maintenance end time for STCD reservation\n* **DatacenterForNonCoreCheck:** User provided datacenter sysid for performing datacenter noncore hours and\/or weekend scheduling related checks\n* **DatacentersForRestrictivePeriodCheck:** User provided datacenter sysid(s) for performing restrictive period or blackout window related checks\n\n## STCD Onboarded Operation {#mcetoc_1hgj0mn188}\n\n**Table name:** u_onboarded_operations\nAn onboarded operation provides all the necessary details about an automation required for STCD scheduling. The onboarded operation record is referenced in the \"Onboarded Operation ID\" field of an STCD request record described above.\n\nBelow is a list of options a user can specify for an onboarded operation. These options are taken into consideration when STCD APIs process user scheduling requests.\n\n* **Blackout Applicable** - True\/False\n* **Schedule Ahead** - True\/False\n* **Schedule Ahead Days** - No. of schedule ahead days (if 'Schedule Ahead' field is set to true)\n* **NonCore Hours Applicable** - True\/False\n* **Schedule Weekends Only** - True\/False\n* **Additional Conflict Check Required** - True\/False\n* **Round Off** - True\/False\n* **Round Off Interval In Minutes** - Round-off interval value in minutes (if 'Round Off' field is set to true)\n* **Throttling Applicable** - True\/False\n\n## STCD Onboarded Operation Profile Override {#mcetoc_1hgj0mn189}\n\nSTCD Override APIs provide an option to dynamically override values for some of the onboarded operation profile options described above as part of individual STCD scheduling requests (Refer to [Conflict Detection and Scheduling API - User guide](https:\/\/servicenow-my.sharepoint.com\/:w:\/p\/pat_casey\/EaiSESJ36FxNle9GXjHBWj0BBZ7p4o9cQC9bP3Sjkw-Y6g?e=z6diXj) for instructions on how to use the STCD Override APIs and for a detailed description of the different override parameters listed below).\nBelow is a list of different override parameters that can be configured as part of individual STCD Override API requests to override the corresponding onboarded operation profile options listed above:\n\n* BLACKOUT_OVERRIDE\n* AUTOMATION_RESTRICTION_CHECK_OVERRIDE\n* ADDITIONAL_CONFLICT_CHECK_OVERRIDE\n* NONCORE_OVERRIDE\n* THROTTLING_OVERRIDE\n* WEEKEND_ONLY_SCHEDULING_OVERRIDE\n* THROTTLING_RESOURCE_OVERRIDE\n* RELATED_CI_OVERRIDE\n* SCHEDULE_AHEAD_DAYS_OVERRIDE\n\n## Looking Up STCD Requests {#mcetoc_1hgj0mn18a}\n\nTo lookup an STCD request  navigate to the 'u_conflict_detection_scheduling_request' table and search for a combination of the onboarded operation and request type and find your request by identifying the inputs in the 'User Request Json' field. To quickly identify your request  you can perform a 'CONTAINS (\\*)' search for the input CI sysid in the 'User Request Json' field. Alternatively  if you provided a CHG number as part of the STCD request  you can perform a 'CONTAINS (\\*)' search for the CHG number in the 'User Request Json' field. It is recommended to scope your search to a date range if known for faster responses.\n\n\"\n\n",
        "QUERY": " 'how to cancel automation change'",
        "GROUND_TRUTH": "no answer found"
    },
    {
        "id": 124,
        "CONTEXT": "\"## Assign KMF roles\n\n# Key Management Framework roles {#ariaid-title1}\n\nThe Key Management Framework (KMF) introduces specific roles for cryptographic module and key management-related configurations.\nImportant:\n\nTo assign the KMF admin role  you must have the admin  security_admin  and sn_kmf_admin roles.\n\nUse the KMF admin role to assign other KMF roles.\n\nKMF roles\n{#d133392e90}\n\n|           Role            |            Role Name            |                                                                                                                                                                          Description                                                                                                                                                                           |\n|---------------------------|---------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| KMF admin                 | sn_kmf.admin                    | Only users with the? KMF admin? role? can? assign? KMF roles  and in addition can perform all capabilities of the KMF cryptographic manager.                                                                                                                                                                                                                   |\n| KMF cryptographic manager | sn_kmf.cryptographic_manager    | Assigned to a user by a KMF admin. KMF cryptographic managers can perform create  read  and update operations on cryptographic modules (association of keys to cryptographic usage and algorithm configurations) and module access policies. Also  KMF cryptographic managers can perform key management (generate  rotate  revoke) and life cycle operations. |\n| KMF auditor               | sn_kmf.cryptographic_auditor    | Assigned to a user by a KMF admin. KMF cryptographic auditors can view cryptographic module information  key metadata  and life cycle-related details  as well as module access policy information.                                                                                                                                                            |\n| KMF integrator            | sn_kmf.cryptographic_integrator | Assigned to a user by a KMF admin. KMF integrators can access and utilize (integrate) cryptographic modules in third party applications.                                                                                                                                                                                                                       |\n{#kmf-roles__table_c1b_hds_4mb}\n\n## Assign KMF roles {#ariaid-title2}\n\nAdministrators with the security_admin role can assign KMF admins  who in turn can assign other KMF roles.\nRole required: admin and security_admin\n\nYou must elevate to the security_admin role before assigning the KMF admin role. For instructions  see [Elevate to a privileged\nrole](..\/..\/security\/task\/t_ElevateToAPrivilegedRole.html \"The base system admin can elevate to a privileged role to have access to the features of High Security Settings.\")\n\n1. Elevate to the security admin role.\n2. Navigate to User Administration \\> Users and select the user you want to be the KMF admin.\n3. Verify that the user already has the admin and security_admin roles. If not  click Edit under the Roles related list and add admin and security _admin.\n4. Navigate to System Security \\> Key Management Administration.\n5. Select the user you want to be KMF admin in the Available Users column and move them to the Selected User(s) column.  \n   ![KMF admin role](..\/..\/encryption\/image\/kmf-admin-select-user.png)\n6. Click Save.\n7. Navigate to User Administration \\> Users and select the user you just gave the KMF admin role to.  \n   That user now has the sn_kmf.admin role in the Roles related list. That user can now assign other KMF roles.\n      KMF admin role\n\nIf you have the KMF admin role  follow these steps for assigning other KMF roles:\n\n1. Navigate to User Administration \\> Users and select the user you want to have another KMF role  such as KMF Cryptographic Manager.\n2. In the Roles related list  click Edit and select the KMF roles you want to assign the users. All KMF roles start with `sn_kmf`.\n      Assigning other KMF roles      {#assign-kmf-roles__ol_lp3_zfh_m4b}\n\n\n\n## 6.0\n\n## Description\n\nServiceNow has identified a defect that can result in Clone failures of instances configured with gateway shards. A Module Access Policy included in San Diego for gateway and secondary database pool plugins caused a regression in our Cloud Automation Infrastructure.\n\nCustomers can face this defect on gateway shard instances if at least one of the Source or Target instances of the Clone are on San Diego.\n\n## Steps to Reproduce\n\n1. Ensure the Source instance to be used for Cloning is configured with gateway shards.\n2. Upgrade Target instance to be used for Cloning to San Diego\n3. Initiate a shard over shard Clone from the source instance to the target instance on San Diego.\n\n## Workaround\n\nServiceNow fixed this issue in San Diego Patch 2  and we recommend updating your instances to this version to prevent any impact caused by this defect. If either source or target version is on San Diego and on a version before San Diego Patch 2  below workaround can be applied. The below steps need to be performed on the Target instance of the Clone that is already on San Diego. They need to be performed on the Source instance as well if it is also upgraded to San Diego.\n\n### **Steps to apply the workaround on the instance(s) on San Diego:**\n\n#### **Part 1**\n\n* Login to the Instance as an 'admin' user.\n* Run the below script from '**Scripts - Background'.**This is will deactivate the Module Access Policy included in San Diego that's configured to reject Script Access of password.\n\nvar gr = new GlideRecord(\u0093sys_kmf_crypto_caller_policy\u0094);\ngr.get(\u0093fcf89b91eb4330100fcee5b26b5228c8\u0094);\ngr.setValue(\u0093active\u0094  \u00930\u0094);\ngr.update();\n\n#### **Part 2**\n\n* Create a new Module Access Policy as below. This requires user to have sn_kmf.cryptographic_manager or sn_kmf.admin role\n* Navigate to Key Management \\> Module Access Policies \\> All\n* Click on 'New' to create a new Module Access Policy with below fields:  \n  * Policy name - Allow Maint to access gateway module\n  * Crypto Module - com_snc_da_gateway_glideencrypter (See Note Below if this Crypto Module is not found)\n  * Type = Role\n  * Target Role = maint\n  * Result = Track\n  * Active Flag selected\n* Click 'Submit' and save the new Module Access Policy\n\n### **Data Preserver on source instance to protect the workaround**\n\nData preserver should be defined in the source instance to protect the above created 'Module Access Policy' on the target instance from being overwritten during a clone. Data Preservers are defined in the source instance but preserve records in the target instance.\n\n1. Login to the Source Instance as an 'admin' user.\n2. On the source instance  navigate to **System Clone** \\> **Preserve Data**.\n3. Click on 'Column Options' next to any Column Name or Right-Click on any Column Name and click on 'Import XML' menu from the drop-down.\n4. Select the attached XML (clone_data_preserver_28c9ba1d1b6641105edf32ebdc4bcb44.xml) and Upload.\n5. 'sys_kmf_crypto_caller_policy' Clone Data Preserver will be imported. This preserver is used to preserve the modifications made to the Module Access Policies mentioned in Part 1 and Part 2 above.\n6. Verify that the Clone Data Preserver imported is configured for Module Access Policy (sys_kmf_crypto_caller_policy) table.\n7. Verify that 2 conditions are configured with Or Condition as below:  \n   * Policy name is 'Allow Maint to access gateway module'\n   * Sys ID is 'fcf89b91eb4330100fcee5b26b5228c8'\n\nIf the data preserver is not configured correctly on the source instance  the workaround implemented on a target instance can be lost after a clone has been completed successfully. Therefore it is suggested to check if the workaround is present or not always before requesting for a new shard-over-shard clone on to a target instance on San Diego.\n\nBelow script can be run from '**Scripts - Background'**as an 'admin' user:\n\nvar gr = new GlideRecord(\u0093sys_kmf_crypto_caller_policy\u0094);\n\ngr.get(\u0093fcf89b91eb4330100fcee5b26b5228c8\u0094);\n\ngs.print(\"OOB MAP Status is \" + gr.active + '');\nvar queryStr = \u0093policy_name=Allow Maint to access gateway module\u0094;\ngr.initialize();\ngr.addEncodedQuery(queryStr);\ngr.query();\nif (gr.next())\n{\ngs.print(\"Custom Created MAP Status is \" + gr.active + '');\ngs.print(gr.getDisplayValue(\u0091type\u0092));\ngs.print(gr.getDisplayValue(\u0091target_role\u0092));\ngs.print(gr.getDisplayValue(\u0091crypto_module\u0092));\ngs.print(gr.sys_id);\n}\nOutput returned should be like below to confirm that out of the box included Module Access Policy (MAP) is not active and customer created MAP is active and configured so that with 'maint' role can access the gateway crypto module\n\n\\*\\*\\* Script: OOB MAP Status is false\n\\*\\*\\* Script: Custom Created MAP Status is true\n\\*\\*\\* Script: Role\n\\*\\*\\* Script: maint\n\\*\\*\\* Script: com_snc_da_gateway_glideencrypter\n\\*\\*\\* Script: \\<SYS ID\\>\n\n\n\n## Enabling KMF for On-Premise Instances\n\n   **IMPORTANT: for the kmf.file.keystore.path do not set an absolute path to the keystore location  it should be relative to ${glide.home.dist}.**\n\n   # vi \/glide\/nodes\/\\<Node_name\\>\/conf\/overrides.d\/glide.kmf.keystore.properties and change the file owner\/group to servicenow.\n\n   * kmf.file.keystore.enabled = true\n   * kmf.file.keystore.path = conf\/overrides.d\n   * kmf.file.keystore.name = \\<Keystore_name\\>.bcfks\n   * kmf.file.keystore.type = BCFKS\n   * kmf.file.keystore.password = \\<Password_youput_while_Creating_Root_Instance_Key\\>\n   * kmf.file.keystore.imk.alias = 256bitkey\n   For example:\n3. Reload the properties.  \n   Note: if KMF is already installed and the instance running  a re-load of system properties in memory is needed after populating the system properties in-file.\n   Logon to your instance \\> Click to 'All' \\> In the Search bar \\> Under System Definition \\> Scripts -Background \\> put the below mentioned KMF properties to reload properties \\> Run Script 'Global' \\> hit Enter\n\n   * GlideProperties.reload();  \n4. Re-checking the status of KMF.  \n   Logon to your instance \\> Click to 'All' \\> In the Search bar \\> Under System Definition \\> Scripts -Background \\> put the below mentioned KMF properties to reload properties \\> Run Script 'Global'.\nYou need to put the below entries in 'Script-Background' to verify it.   gs.info(\"kmf.file.keystore.enabled: \" + gs.getProperty('kmf.file.keystore.enabled'));\ngs.info(\"kmf.file.keystore.path: \" + gs.getProperty('kmf.file.keystore.path'));\ngs.info(\"kmf.file.keystore.name: \" + gs.getProperty('kmf.file.keystore.name'));\ngs.info(\"kmf.file.keystore.type: \" + gs.getProperty('kmf.file.keystore.type'));\ngs.info(\"kmf.file.keystore.imk.alias: \" + gs.getProperty('kmf.file.keystore.imk.alias'));\n   Now you would see details about the properties configured as per above. For example:\n\n\n# Validation and Test On-Premise KMF\n\nThis section describes a few ways you can verify\/test KMF on the on-premises instance.\n\n**1. KMF Health Page**\nReleased in San Diego  the KMF Health Page provides current state information. In order to access this page  you need to have the role assigned (can be seen by maint).\n\nLogon to your instance \\> Click to 'All' \\> In the Search bar \\> Key Management \\> Diagnostics\n\nFor on-premises instances  the KMF Health Page should indicate the following:\n\n* Key Secure \\> Disabled\n* File Key Store \\> Operational\n* Glide Encrypter \\> Operational\n* Instance Key Encryption \\> Operational\n* Instance HMAC Key \\> Operational\n* Vault PKI \\> Disabled (Disabled for all sub-items)\n* EJBCA PKI \\> Disabled (Disabled for all sub-items)\n* Instance PKI \\> Malfunction (Malfunction for all sub-items)  \n    Snapshot example follows:\n\n**2. KMF CryptoOperation API - Wrap \/ Unwrap Test**\n\nLogon to your instance \\> Click to 'All' \\> In the Search bar \\> Under System Definition \\> Scripts -Background \\> put below data \\> Run Script 'Global'\n\n\/\/ Symmetric key wrapping\nvar op1 = new sn_kmf_ns.KMFCryptoOperation(\"instance_level_glide_encrypter\" \"SYMMETRIC_WRAPPING\").withAlgorithm(\"AES\").withInputFormat(\"KMFNone\");\nvar wrappedKey = op1.doOperation(\"myplainkey\");\n\n\/\/ Symmetric key unwrapping\nvar op2 = new sn_kmf_ns.KMFCryptoOperation(\"instance_level_glide_encrypter\" \"SYMMETRIC_UNWRAPPING\").withInputFormat(\"FORMATTED\").withAlgorithm(\"AES\");\nvar unwrappedKey = GlideStringUtil.base64Decode(op2.doOperation(wrappedKey));\ngs.info(\"Unwrapped: \" + unwrappedKey);\n\nYou would expect to see below output (example):\n\nIf you repeat above step  you would below output which is expected.\n\n**![](sys_attachment.do?sys_id=ee4d571747e7a1903b05ff48436d43b8)**\n\n# Misc. Troubleshooting\n\n1. First and foremost  per the KMF Health Page check  if the page reports the presence of demo data  execute the following command in background scripts in global scope. For clearing demo data  you need to run the below script.   \n\nLogon to your instance \\> Click to 'All' \\> In the Search bar \\> Under System Definition \\> Scripts -Background \\> put below data \\> Run Script 'Global'\n\n* new SNC.KMFAutomationInterface().cleanUpKMFDemoData();\n\n2. Per the KMF Health Page indicates missing Instance Key Encryption Key and\/or Instance HMAC Key and missing components in the Glide Encrypter  execute the following command in background scripts in global scope:\n\nLogon to your instance \\> Click to 'All' \\> In the Search bar \\> Under System Definition \\> Scripts -Background \\> put below data \\> Run Script 'Global'\n\n* (new SNC.KMFGlideEncrypterAPI()).generateGlideEncrypterModule();\n\n3. Reload the properties in the background.\n\nLogon to your instance \\> Click to 'All' \\> In the Search bar \\> Under System Definition \\> Scripts -Background \\> put the below mentioned KMF properties to reload properties \\> Run Script 'Global' \\> hit Enter\n\n* GlideProperties.reload();\n\n4. In most cases  it's more straight forward to generate the keystore from scratch as described above. However  alternative to creating the keystore from scratch is to convert an existing JKS\/PKCS12 keystore to a BCFKS keystore. Below would be the notional command  with an example.\n\n* Re keystore  you may try to convert keystore JKS  PKCS12 to BCFKS. Below is the notional command.  \n  # keytool -importkeystore -v -srckeystore \\<PATH_OF_KEYSTORE\\>\/\\<KEYSTORE\\> -srcstoretype \\<CURRENT_FORMAT_TYPE_OF_KEYSTORE\\> -providername BCFIPS -providerclass org.bouncycastle.jcajce.provider.BouncyCastleFipsProvider -providerpath \\<PATH_OF_bc-fips_USUALLY_UNDER \/glide\/nodes\/\\<node\\>\/lib\/jsw\/bc-fips-1.0.2.jar\\> -destkeystore \/glide\/nodes\/\\<NODE\\>\/conf\/overrides.d\/\\<NAME_OF_KEYSTORE\\>.bcfks -deststoretype BCFKS\n\n  An example of the above command is provided below - it assumes you are at location: \/glide\/nodes\/\\<NODE\\>\/conf\/overrides.d\/:\n\n  # keytool -importkeystore -v -srckeystore \/glide\/java\/jre\/lib\/security\/cacerts -srcstoretype JKS -providername BCFIPS -providerclass org.bouncycastle.jcajce.provider.BouncyCastleFipsProvider -providerpath .\"\n\n",
        "QUERY": " 'how to add Kmf roles to user'",
        "GROUND_TRUTH": "To add KMF roles to a user, follow these steps:\n\nAssigning the KMF admin role:\n1. Elevate to the security admin role.\n2. Navigate to User Administration > Users and select the user you want to be the KMF admin.\n3. Verify that the user already has the admin and security_admin roles. If not, click Edit under the Roles related list and add admin and security_admin.\n4. Navigate to System Security > Key Management Administration.\n5. Select the user you want to be KMF admin in the Available Users column and move them to the Selected User(s) column.\n6. Click Save.\n7. Navigate to User Administration > Users and select the user you just gave the KMF admin role to. That user now has the sn_kmf.admin role in the Roles related list. That user can now assign other KMF roles.\n\nAssigning other KMF roles:\n1. Navigate to User Administration > Users and select the user you want to have another KMF role, such as KMF Cryptographic Manager.\n2. In the Roles related list, click Edit and select the KMF roles you want to assign the users. All KMF roles start with `sn_kmf`.\n\nPlease note that to assign the KMF admin role, you must have the admin, security_admin, and sn_kmf_admin roles."
    },
    {
        "id": 125,
        "CONTEXT": "\"## AHA Transfer Communications | FAQ\n\n## Table of Contents\n\n* [What is an Advanced High Availability (AHA) Transfer?](#mcetoc_1f65732gu1j)\n* [What is the impact on a customer when ServiceNow conducts an AHA \\& When do we conduct these AHAs?](#mcetoc_1f65732gu1k)\n* [How are communications changing for customers?](#mcetoc_1f65732gu1l)\n* [What if a customer reports an outage?](#mcetoc_1f65732gu1m)\n* [What information can I share if the customer has questions about why we did not notify them?](#mcetoc_1f65732gu1n)\n* [Will there be any customers that continue to receive these notifications?](#mcetoc_1f65732gu1o)\n* [How can a customer request to receive CHGs with 14-day advanced notification?](#mcetoc_1f65732gu1p)\n\n### What is an Advanced High Availability (AHA) Transfer? {#mcetoc_1f65732gu1j}\n\nServiceNow deploys a mirrored architecture for all customer production instances  ensuring customer's instances are always up and available during a maintenance activity or service outage. An Advanced High Availability (AHA) transfer is the moving of the customer's production instance from the existing primary location to the other mirrored location. An example is our SJC-IAD mirror pair  where ServiceNow transfers the primary location from SJC to IAD.\n\n### What is the impact on a customer when ServiceNow conducts an AHA \\& When do we conduct these AHAs? {#mcetoc_1f65732gu1k}\n\nThe impact of an AHA transfer is minimal (typically under 3 minutes) and occurs during off business hours in the location of the instance (i.e. Saturday at 11 pm PST).\n\n### How are communications changing for customers? {#mcetoc_1f65732gu1l}\n\nIn the past  customers received a 14-day change notification of the AHA transfer. Customers will no longer be receiving these notifications. ServiceNow believes AHA Transfers are routine maintenance activities of short duration that have minimal or no impact on production instances conducted outside of business hours per local data center region. Emergency changes are not notified.\n\nEffective 28-FEB-2017  all customer communications related to planned and emergency transfers will be handled per the following rules:\n\n* Customers that are on an exception list (see below) will always receive a CHG\n* Customers that are not on an exception list will only get notified if the transfer is scheduled outside weekend hours based on the local data center.\n\n### What if a customer reports an outage? {#mcetoc_1f65732gu1m}\n\nTypically  customers should experience a very brief outage (less than 3 minutes outside of business hours). There will be situations where customers may report an outage. If a Case is related to AHA transfer  then follow the P1case handling process with a few specific differences.\n\n1. When service is restored  use this [template](\/sys_template.do?sysparm_tiny=7d6c1a5ddb87ae800e3dfb651f96197e \"template\").\n2. Wrap-up activities specific to AHA transfers where there was not customer notification.  \n   * Outage Type: Unplanned\n   * Impact Start Time: Start time of the transfer when the instance became unavailable\n   * Impact End time: When the instance became accessible to the customer\n   * Case Category: AHA \\*Caused by Change: CHG for AHA transfer\n   * Update Short Description to: Instance unavailable due to AHA transfer\n   * Complete remaining normal P1 wrap-up steps\n3. To see if there is an active change underway for this instance  you can use the ServiceNow [RUCKUS](https:\/\/gitlab.servicenow.net\/opsdev\/ruckus\/blob\/master\/README.md \"RUCKUS\") tool.\n\n### What information can I share if the customer has questions about why we did not notify them? {#mcetoc_1f65732gu1n}\n\nWe encourage Support staff to discuss with their manager who should be handling these communications. We have guidelines for our maintenance notification policy in the [Subscription Service Guide](http:\/\/www.servicenow.com\/content\/dam\/servicenow\/other-documents\/schedules\/ServiceNow_Subscription_Service_Guide_111214.pdf \"Subscription Service Guide\") under the Upgrade Policy  Section 2 -- Notice; Maintenance Downtime.\n\n### Will there be any customers that continue to receive these notifications? {#mcetoc_1f65732gu1o}\n\nFor large multinational customers or for customers who have regulatory requirements  there will be a need to continue these communications. Following are the customer groups that will continue to receive these notifications:\n\n* Customers with a Support Account Manager\n* IQOQ regulatory group\n* MSP and Global Alliance customers\n* Escalated accounts\n\n### How can a customer request to receive CHGs with a 14-day advanced notification? {#mcetoc_1f65732gu1p}\n\nCustomers can request to receive 14-day advanced CHG notifications. The request will be routed to the Review Board (from Customer Communication Team) to determine if they will be added to an exception list. The following exception process should be followed (see process flow diagram below):\n\n1. If customer requests to be notified prior to future AHA transfers create a CSTask with the following characteristics:  \n   1. Assign the CSTask to: Patching-CHG group.\n   2. Short description: \"AHA Transfer Notification Exception Request\".\n   3. Description: Summarize the reason the customer is requesting an exception.\n2. On a weekly basis  members of the Review Board will assess the requests.\n\n\n## Work Instruction | How to perform H-AHA transfers\/failovers: appstoreprod and DATACENTER instances\n\n|---------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------|\n| ![Warning](sys_attachment.do?sys_id=e688295d476abd10f93138ce536d4304 \"Warning\") | **Warning**: The work instructions below are for use by SRE  SRE-DevOps  and SA staff only. |\n# Overview\n\nThis article is intended for all non-AHA failovers using the fail-live.sh script for all instances listed below. For all other instances  please utilize the standard AHA transfer article [KB0529754](.\/kb?id=kb_article_view&sysparm_article=KB0529754 \"KB0529754\").\n\n|---------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------|\n| ![Warning](sys_attachment.do?sys_id=e688295d476abd10f93138ce536d4304 \"Warning\") | **Important Notice**: All hostnames used for servers should be fully qualified domain names. For example  foo.service-now.com. |\n**From Version 1.3**  all hostnames will be checked if they are resolvable and fully qualified. If not you will get the following error:\n\n**\\<hostname\\> is not a valid server\/hostname**\n\n## Non-AHA instances\n\n|             INSTANCE             |                    Non-AHA reason                    |\n|----------------------------------|------------------------------------------------------|\n| appstoreprod                     |                                                      |\n| DATACENTER                       | Should not AHA DATACENTER with DATACENTER Automation |\n| dcil4 (see KB20000207 in HIWAVE) | Should not AHA dcil4 with dcil4 Automation           |\n# Risks \\& Cautions {#RISKS_&_CAUTIONS}\n\nN\/A\n\n# Dependencies {#DEPENDENCIES}\n\n## Monitor the AHA2.0 Validation Test and Monitoring Results Report:\n\nTo avoid any surprises or delays  it's a good idea to monitor the results daily by checking the AHA2.0 Validation Test and Monitoring Results report for two weeks before the scheduled transfer to leave time to investigate any failed audits.\n\nThe AHA2.0 Validation Test and Monitoring Results report is emailed twice daily. It contains the latest AHA 2.0 critical and non-critical AHA 2.0 audit results for DATACENTER  datacenterdev  and datacentertest. Check the latest report and confirm that the critical tests pass for the instance to be transferred. Investigate any non-critical failures.\n\nThe recipient list for the 'AHA2.0 Validation Test and Monitoring Results' report can be found in DATACENTER in the 'cid.validation.daily.report.receipt.list' in the u_cid_configuration_parameters table 'cid.validation.daily.report.receipt.list' in the u_cid_configuration_parameters table\n\n**Note:** For DATACENTER  datacenterdev  and datacentertest  the following audits are skipped intentionally:\n\n**AHA Audit Failed or Skipped checks for DATACENTER  datacenterdev  and datacentertest:**\n\nFAILED:\n\n* Primary F5 BigIP Pool Exists - This is expected and due to CNS change [CHG46781814](https:\/\/support.servicenow.com\/nav_to.do?uri=%2Fchange_request.do%3Fsys_id%3Dc8d46e75479b295011eaf24c736d4392%26sysparm_stack%3D%26sysparm_view%3D \"https:\/\/support.servicenow.com\/nav_to.do?uri=%2fchange_request.do%3fsys_id%3dc8d46e75479b295011eaf24c736d4392%26sysparm_stack%3d%26sysparm_view%3d\")\n* Standby F5 BigIP Pool Exists - This is expected and due to CNS change [CHG46781814](https:\/\/support.servicenow.com\/nav_to.do?uri=%2Fchange_request.do%3Fsys_id%3Dc8d46e75479b295011eaf24c736d4392%26sysparm_stack%3D%26sysparm_view%3D \"https:\/\/support.servicenow.com\/nav_to.do?uri=%2fchange_request.do%3fsys_id%3dc8d46e75479b295011eaf24c736d4392%26sysparm_stack%3d%26sysparm_view%3d\")\n* DNS Entry Points To VIP Of Primary Datacenter - This is expected and due to CNS change [CHG46781814](https:\/\/support.servicenow.com\/nav_to.do?uri=%2Fchange_request.do%3Fsys_id%3Dc8d46e75479b295011eaf24c736d4392%26sysparm_stack%3D%26sysparm_view%3D \"https:\/\/support.servicenow.com\/nav_to.do?uri=%2fchange_request.do%3fsys_id%3dc8d46e75479b295011eaf24c736d4392%26sysparm_stack%3d%26sysparm_view%3d\")\n\nSKIPPED:\n\n* DNS Propagation Check - Skipped because the instance is an internal vip instances\n* Application instance is AHA 3 ready - Skipped because the instance is not configured for AHA 3.0 (For AHA 3.0  the instance must be ADCv2  and all nodes must be fast_failover_support capable).\n* Application instance has correct AHA 3 configuration - Skipped because the instance is not configured for AHA 3.0 (For AHA 3.0  the instance must be ADCv2  and all nodes must be fast_failover_support capable).\n* ADC Pool Members Count Symmetry - Skipped because the instance is using F5 not ADCv2\n* ADC Pool Count Symmetry - Skipped because the instance is using F5 not ADCv2\n\nSplunk errors shown in red in the report should be reported to the GCO - Service Apps team\n\n## Reschedule Move Changes Scheduled Within the Same Window\n\n**SRE-DevOps Completes this step:**\n\n1. Log in to DATACENTER and navigate to the Instance Move Contexts table \\[u_instance_move_context\\] and filter for records with a Move Cutover Start Time between the one hour before the Planned Start Date\/Time for the H-AHA change with Global Pause and the Planned End Date\/Time.\n2. When reviewing the records  keep in mind that H-AHA changes typically are completed within one hour  so move changes with a cutover time one hour before and one hour after the Planned Start Date\/Time for the H-AHA change should be rescheduled  and the rest should be noted in the H-AHA change work notes in case the change runs long.\n3. To reschedule move changes  see [KB1116804](.\/kb?id=kb_article_view&sysparm_article=KB1116804 \"Reference | How to reschedule a dcPROD (Instance Automation) Move?\") - Reference \\| How to reschedule a dcPROD Move?\n\n## Obtain CAB Approval\n\n**SRE-DevOps Completes this step (ideally two weeks before the planned start date):**\n\n1. Submit the change to the Commercial Cab meeting a week before the planned start date and represent the change at the meeting. Obtain approval.\n\n## Identify and Block the Time for the H-AHA Transfer\n\n**SRE-DevOps  CAB Lead to complete these steps (ideally three weeks before the planned start date) The change needs to be approved before the time will be blocked:**\n\n1. Log in to DATACENTER and navigate to the [Automation Schedule](https:\/\/datacenter.service-now.com\/nav_to.do?uri=%2Fsys_report.do%3Fsys_id%3D4a419bfedb9ce8907527d7c8f49619f0)report  and identify a four-hour window.\n2. Email Trent Laster  CAB Lead to create the entry in the blackout calendar.\n\n\n\n**Example email sent for a previous change:**\n\nTo: Trent Laster\n\nSubject line: Requesting for Blackout calendar - AHA transfer of DC instance\n\nHi Trent \n\nWe are planning to do AHA transfer of Datacenter instance on May 9th at 11 AM to 3PM PST.\n\nRef change: CHG45753304\n\nCan you please help with creating blackout calendar for the specified date?\n\nThank You \n\nDarshak\n\n## Request that Notification be Sent for the H-AHA Transfer\n\n**SRE-DevOps and the Release Team complete these steps:**\n\n1. Email Srinavas Yalakanti from the Release Team to request that two notifications be sent. The first should be sent one week before the schedule change planned start date  and the second notification should be sent the morning of the change planned start date.\n\n**Example email sent for a previous change:**\n\nTo: Srinivas Yalakanti\n\nSubject line: Requesting notifications for AHA transfer of DC instance\n\nHi Srinivas \n\nYour team helped to send broader DC users communication emails. We are planning to do a planned AHA transfer of the datacenter instance on May 9th between 11:00 to 15:00 PDT.\n\nThis change is for the Commercial datacenter  and the expected audience is Commercial Prod users.\n\nPlease send pre-notification on May 5th and final notification on May 8th.\n\nHere is the change: CHG45753304\n\nCAB was approved on: May 2nd\n\nPlanned start\/end date time: 2023-05-09 11:00:00 PDT \/ 2023-05-09 15:00:00\n\nReason: To support patching activity.\n\nTeams engaged: Secore  SRE DevOps Global  TDO \\& SRE team.\n\nService impact: Automations that are related to workflow orchestration will be PAUSED and discovery will NOT be available from the datacenter instance for 30 to 45 minutes during the change window and expected instance downtime between 2 to 5 minutes.\n\nThank you \n\nTeri\n\n## Create a Zoom Meeting Request For Respective Parties:\n\n**SRE-DevOps Complete this step**\n\n1. Send a Zoom meeting request to start 30 minutes before the planned change start time to the respective parties: TDO  SRE  SA  SE CORE\n\n## Create a Maintenance Notification Banner:\n\n**SRE-DevOps Complete this step**\n\n1. 72 hours before the planned start time  create a [banner](https:\/\/datacenter.service-now.com\/nav_to.do?uri=%2Fsystem_properties_ui.do%3Fsysparm_title%3DBanner%2BNotification%2BProperties%26sysparm_nostack%3Dtrue%26sysparm_category%3DBanner%2BNotification)so that everyone will be informed of the global pause. Disable after the maintenance is complete.\n\n## Before the Transfer Confirm that the AHA 2.0 Audits pass  or are in the expected state:\n\n**Steps to manually validate that the AHA 2.0 Audits pass for the instance:**\n\n1. Log in to DATACENTER and navigate to the cmdb_ci_service_now table and filter for the instance record and open it.\n2. Scroll down to the AHA Transfer\/Failover tab.\n3. Click on the Validate Now link\n4. Leave all the checkboxes checked  then click the Validate Now button.\n5. Investigate any failures before proceeding.\n\n## Determine if the Instance has Worker Nodes\n\nThe script has no way of determining which nodes need a custom node type. All it recognizes are the generic.primary and generic.standby. It will set the nodes as such (all primary nodes as generic.primary and all standby are set as generic.standby). Please work with customer support to reinstate the correct node type accordingly. Look up the application instance CI in DATACENTER and find the **Special Settings** tab. If **Has Worker Nodes** is checked  **STOP** and please work with customer support to reinstate the correct node type accordingly.\n\n## Run response time validation tests if the database version will change after the transfer:\n\nInclude the output in the work notes of the change  or attach it as a text file to the change.\n\n**Queries to run and compare on primary  standby  and read-replicas:**\n\nSELECT u_chg_sync0.\\`sys_id\\` FROM (u_chg_sync u_chg_sync0 INNER JOIN sys_import_set_row sys_import_set_row0 ON u_chg_sync0.\\`sys_id\\` = sys_import_set_row0.\\`sys_id\\` ) WHERE u_chg_sync0.\\`u_change_sysid\\` = 'd397bb9a478af918dc5a08e6926d43e8' AND u_chg_sync0.\\`u_is_stale_data\\` = 0 ORDER BY sys_import_set_row0.\\`sys_created_on\\` DESC  u_chg_sync0.\\`u_change_number\\` limit 0 50;\n\nSELECT sys_dictionary0.\\`calculation\\`  sys_metadata0.\\`sys_replace_on_upgrade\\`  sys_dictionary0.\\`dynamic_ref_qual\\`  sys_dictionary0.\\`choice_field\\`  sys_dictionary0.\\`function_field\\`  sys_metadata0.\\`sys_updated_on\\`  sys_dictionary0.\\`spell_check\\`  sys_dictionary0.\\`reference_cascade_rule\\`  sys_dictionary0.\\`reference\\`  sys_metadata0.\\`sys_updated_by\\`  sys_dictionary0.\\`read_only\\`  sys_metadata0.\\`sys_created_on\\`  sys_dictionary0.\\`element_reference\\`  sys_dictionary0.\\`array_denormalized\\`  sys_metadata0.\\`sys_name\\`  sys_dictionary0.\\`reference_key\\`  sys_dictionary0.\\`reference_qual_condition\\`  sys_dictionary0.\\`xml_view\\`  sys_dictionary0.\\`dependent\\`  sys_dictionary0.\\`internal_type\\`  sys_metadata0.\\`sys_created_by\\`  var_dictionary0.\\`order\\`  sys_dictionary0.\\`element\\`  sys_dictionary0.\\`max_length\\`  sys_dictionary0.\\`use_dependent_field\\`  sys_dictionary0.\\`delete_roles\\`  sys_dictionary0.\\`virtual_type\\`  var_dictionary0.\\`u_server_name\\`  sys_dictionary0.\\`active\\`  sys_dictionary0.\\`choice_table\\`  var_dictionary0.\\`model_table\\`  sys_dictionary0.\\`foreign_database\\`  sys_metadata0.\\`sys_update_name\\`  var_dictionary0.\\`hint\\`  sys_dictionary0.\\`unique\\`  sys_dictionary0.\\`name\\`  sys_dictionary0.\\`dependent_on_field\\`  var_dictionary0.\\`u_table_name_2\\`  var_dictionary0.\\`u_table_name_1\\`  sys_dictionary0.\\`dynamic_creation\\`  sys_dictionary0.\\`primary\\`  sys_metadata0.\\`sys_policy\\`  sys_dictionary0.\\`next_element\\`  sys_dictionary0.\\`virtual\\`  sys_dictionary0.\\`widget\\`  sys_dictionary0.\\`use_dynamic_default\\`  sys_dictionary0.\\`sizeclass\\`  sys_dictionary0.\\`mandatory\\`  sys_metadata0.\\`sys_class_name\\`  sys_dictionary0.\\`dynamic_default_value\\`  sys_metadata0.\\`sys_id\\`  sys_dictionary0.\\`write_roles\\`  sys_dictionary0.\\`array\\`  sys_dictionary0.\\`audit\\`  sys_dictionary0.\\`read_roles\\`  sys_metadata0.\\`sys_scope\\`  sys_dictionary0.\\`dynamic_creation_script\\`  sys_dictionary0.\\`create_roles\\`  sys_dictionary0.\\`defaultsort\\`  sys_dictionary0.\\`comments\\`  sys_dictionary0.\\`column_label\\`  sys_dictionary0.\\`use_reference_qualifier\\`  sys_dictionary0.\\`reference_floats\\`  sys_dictionary0.\\`display\\`  sys_metadata0.\\`sys_mod_count\\`  sys_dictionary0.\\`default_value\\`  var_dictionary0.\"\n\n",
        "QUERY": " 'How frequent should a customer do an AHA'",
        "GROUND_TRUTH": "no answer found"
    },
    {
        "id": 126,
        "CONTEXT": "\"## Manage Unclassed Hardware CIs\n\n|----------------------------------------------------------------------------------------------------------------|\n| Product Success Playbook Manage Unclassed Hardware CIs A step-by-step guide to analyze and remediate CMDB Data |\n**Table of Contents**\n\n[Summary](#_Toc43211764)\n\n[Goal of this Playbook](#_Toc43211765)\n\n[Audience](#_Toc43211766)\n\n[Problem Overview](#_Toc43211767)\n\n[Executive Summary](#_Toc43211768)\n\n[How this playbook can help you achieve business goals](#_Toc43211769)\n\n[How this playbook is structured](#_Toc43211770)\n\n[Problem Analysis](#_Toc43211771)\n\n[Upstream Causes](#_Toc43211772)\n\n[Downstream Consequences](#_Toc43211773)\n\n[Impact on Your Business](#_Toc43211774)\n\n[Engagement Questions](#_Toc43211775)\n\n[Remediation Plays](#_Toc43211776)\n\n[Summary](#_Toc43211777)\n\n[Play 1: Review your data](#_Toc43211778)\n\n[Play 2: Analyze your CI records](#_Toc43211779)\n\n[Play 3: Fix Play](#_Toc43211780)\n\n[Play 4: Fix Play](#_Toc43211780)\n\n[Data Governance](#_Toc43211781)\n\n[References\\<optional\\>](#_Toc43211782)\n\n{#_Toc43211764}**Summary**\n\n{#_Toc43211765}**Goal of this Playbook**\n\nThis playbook helps in reducing CIs in the Unclassed Hardware class. CIs in \"Unclassed Hardware\" class has limited information and purpose\n\nDetails about this playbook\n\n|------------------------------------|---|------------------------------------------------------------------------------|\n| **Author**                         |   | Bibu Elias Punnachalil                                                       |\n| **Reviewer**                       |   | Ravi Kumar Kanukollu                                                         |\n| **Date**                           |   | 12\/04\/2022                                                                   |\n| **Addresses HSD #**                |   | HSD0011887                                                                   |\n| **Applicable ServiceNow Releases** |   | All Releases                                                                 |\n| **Prerequisites**                  |   |                                                                              |\n| **Time Required**                  |   | Approximately 30 to 40 hours per integration (depending on your environment) |\n{#_Toc43211766}**Audience**\n\n* Vulnerability Administrator  Vulnerability Analysts  Remediation teams\n* Configuration Manager or Configuration Management team\n* ServiceNow Administrator or Discovery Administrator\n\n{#_Toc43211767}**Problem Overview**\n\nWhen data is imported from a third-party integration  Vulnerability Response automatically uses host data to search for matches in the Configuration Management Database (CMDB). It does this using CI Lookup Rules. If CI Lookup Rules fail to match on anything in the CMDB then the Vulnerability Integration framework may create a new record in \"Unclassed Hardware\". Prioritization and assignment of VIs may be impacted if the VIs are unmatched and discovered item is getting created into Unclassed Hardware. More records on Unclassed Hardware also affects the CMDB reporting and performance\n\n{#_Toc43211768}**Executive Summary**\n\n{#_Toc43211769}**How this playbook can help you achieve business goals**\n\nThis playbook recognizes the need to reduce the records in Unclassed Hardware table in order to get best value from a VR implementation It will help you reclassify the unclassed and put in place processes to reduce the recurrence of this issue. This will ensure that the maximum number of vulnerabilities are communicated to the remediation teams in a timely manner for effective treatment. This  is turn  will contribute to improving the vulnerability profile of you organization.\n\n{#_Toc43211770}**How this playbook is structured**\n\nThis Playbook will guide you through 4 plays.\n\n* Play 1 (a review data play) helps you review CI records in Unclassed Hardware class\n* Play 2 (a remediation play) provides remediation steps required to fix VI records without remediation target\n* Play 3 (a remediation play) provides remediation steps required to fix issues which might result in creation of VI without remediation target\n* Play 4 (a remediation play) lists the guidelines and processes for continuing to have Vis with remediation target\n* Play 5 (a Data Governance play) provides remediation steps required to fix issues which might result in creation of VI without remediation target\n\n{#_Toc43211771}**Problem Analysis**\n\n{#_Toc43211772}**Upstream Causes**\n\nThe drivers of a low DIs-to-CI matching rate are (by decreasing order of business impact):\n\n* Unprepared CMDB:\n\n- The CMDB is not VR-ready  it has not been aligned to the VR strategy\n- The network ranges used to discover assets and populate the CMDB are not aligned to the network ranges used by the third party vulnerability scanner\n- CIs do not exist in the CMDB to match the DIs created by the VR application\n\n* Unsuitable CI Lookup Rules\n\n- OOTB rules are not fully suited to specific environment and need to be tailored\n- Rules were created after all data is imported from the scanner (sub-optimal implementation methodology)\n- CI lookup rules in incorrect order\n- IRE CI Identifiers needs to be adjusted\n\n{#_Toc43211773}**Downstream Consequences**\n\n**Data Consequence**\n\n* Duplicate configuration item records\n\n**Operation Consequence**\n\n* Support teams are unable to lookup CIs quickly\n\n**App Consequence**\n\n* Event Management not able to bind the CI\n* Performance\n\n{#_Toc43211774}**Impact on Your Business**\n\nMore CIs in Unclassed Hardware will negatively impact the effectiveness and efficiency of your Vulnerability Remediation teams  vulnerability teams and audits. Also these records\n\n* Security MTTR\n  * Delay in vulnerability identification\n  * Slower response to vulnerability remediation\n* Audit\/Compliance  \n  * Incomplete DI data\n\n{#_Toc43211775}**Engagement Questions:**\n\nConsider the answers to these questions:\n\n* What is the current proportion of unmatched DIs?\n* Does the VR strategy explicitly address CI Lookup rules?\n* Did the VR implementation follow the \"crawl-walk-run\" approach?\n* Did the CI Lookup Rules strategy use the OOTB rules first?\n\n\n* Is there an established relationship between the VR team and the CMDB team?\n\n{#_Toc43211776}**Remediation Plays**\n\n{#_Toc43211777}**Summary** The table below lists and summarizes each of the remediation plays in the playbook. Details are included later.\n\n|-------------------------------------|-------------------------|-------------------------------------------------------------------------------|\n| **Play Name**                       |                         |                                                                               |\n| Review your data                    | What this play is about | Review the data present in unclassed CI                                       |\n| Review your data                    | Required tasks          | List the CIs in Unclassed Hardware class                                      |\n| Fix Play: Update discovered items   | What this play is about | This play helps you analyze the discovered items and assign the correct class |\n| Fix Play: Update discovered items   | Required tasks          | List the discovered items and manually select the appropriate class           |\n| Fix Play -- Reclassify CIs manually | What this play is about | List the CIs which needs to be reclassified                                   |\n| Fix Play -- Reclassify CIs manually | Required tasks          | Reclassify identified CIs                                                     |\n| Fix Play -- Retire unused CIs       | What this play is about | Retire the unused CIs in Unclassed hardware items class                       |\n| Fix Play -- Retire unused CIs       | Required tasks          | Create and run the schedule job                                               |\n| Data Governance                     | What this play is about | Monitor and maintain CIs in Unclassed hardware items class                    |\n| Data Governance                     | Required tasks          | Review the data and fix appropriately                                         |\n{#_Toc43211778}**Play 1 - Review your data**\n\n**What this Play is about**\n\nThis play helps in listing all the CIs in Unclassed Hardware\n\n**Required tasks**\n\n1. On the Navigator type cmdb_ci_unclassed_hardware.list. This will list the unclassified hardware CIs\n2. Group by Discovery Source to help identify from where these CIs originated\n3. Work iteratively with CMDB team to ensure that CMDB has the CIs for which VIs are getting discovered. Fix discovery issues like \n   1. Discovery credential issues\n   2. Issues related to discovery IP ranges\n   3. Availability of MID Servers for all the target servers\n\n{#_Toc43211779}**Play 2 -- Update discovered items to the correct class**\n\n**What this Play is about**\n\nThis play helps you analyze the discovered items and assign the correct class\n\nRepeat this process until all you have left is items that are in the \"it will never match\" category.\n\n**Required tasks**\n\n1. Adjust the columns on the Discovered Items module to also include: Host tag  NetBIOS  Operating system  DNS Name  Fully Qualified domain name\n2. Now  group by Operating system. Investigate this list by looking at that OSs with the largest count down to the smallest.\n3. Look to solve the biggest patterns first  to navigation search box  type cmdb_ci_unclassed_hardware.list. to display the CIs\n4. Ensure that the Class field is displayed in the list.\n5. If you do not see this attribute  personalize the list to add the Class field.\n6. Double-click the Class value for the CI  and select a new class.\n7. Click the green check box to confirm your selection.\n\n{#_Toc43211780}**Play 3 - Fix Play Reclassify CIs manually**\n\n**What this Play is about**\n\nReclassify CIs manually\n\n**Required tasks**\n\n1. Locate the CI that you want to reclassify and display it in a list view.\n2. You can use the application navigator. Or for example  if the CI is a server  then in the navigation search box  type cmdb_ci_unclassed_hardware.list. to display the CIs\n3. Ensure that the Class field is displayed in the list.\n4. If you do not see this attribute  personalize the list to add the Class field.\n5. Double-click the Class value for the CI  and select a new class.\n6. Click the green check box to confirm your selection.\n\n**Play 4 - Fix Play : Fix Play : Retire Unmatched CIs**\n\n**What this Play is about**\n\nCreate a scheduled job that looks at all non \"retired\" Unclassed Hardware items to see if they are still in use. What I mean is  I would see if there is a reference to the Unclassed Hardware item in the Discovered Items module. If there is not a reference in the Discovered Items module to that CI  then set that CI Status to \"Retired\".\n\n**Required tasks**\n\n1. Download \\& upload the [xml file](sys_attachment.do?sys_id=607b9fa01bf39190acdc54e56b4bcb7c)\n2. Go to scheduled job and review the job \"Retire UnMatched CIs\"\n3. Make the job active\n\n{#_Toc43211781}**Data Governance**\n\n**What this Play is about**\n\nMonitor and maintain data in Unclassed hardware\n\n**Required tasks**\n\n1. Regularly execute play 1 to review CIs in Unclassed Hardware\n2. \n\n## Unclassed hardware\n\n# Unclassed hardware {#ariaid-title1}\n\nIf an asset does not find a match in the Configuration Management Database (CMDB) by CI lookup rules  the asset is termed as an unclassed hardware.\n\nWhen data is imported from ServiceNow built integrations  Vulnerability Response automatically uses host data to search for matches in the CMDB. The CI lookup rules are used to identify configuration items (CIs) and add them to the vulnerable item record to aid in remediation. If the assets are not found in the CMDB  the Identification and Reconciliation engine (IRE) runs its own identification rules defined on the hardware class and all its children and creates a CI under an Unclassed Hardware class. When Discovery finds this asset  instead of reclassifying the existing unclassed hardware  it creates a CI. This is because the identification rule relies on the name of the hardware CI.\n\nIf the Identification and Reconciliation engine (IRE) is activated  the reclassify option from discovered items is not supported.\nThe following extracts show the IRE payload for creating CI in Unclassed Hardware via Vulnerability Response and Discovery respectively.\n\nIn this case  the existing Unclassed Hardware CI is not reclassified to Windows server  and instead a new CI under Windows server is created. The reason being the identification rule relies on the name of the Hardware CI. If you notice the payloads  the name in the payload from Vulnerability Response is the same as either FQDN\/NETBIOS  but it is just the host name for the payload from Discovery.\n\n\"\n\n",
        "QUERY": " 'How does records on unclassed hardware affect CMDB?'",
        "GROUND_TRUTH": "Records on unclassed hardware affect the CMDB in several ways. When data is imported from a third-party integration, Vulnerability Response uses host data to search for matches in the CMDB using CI Lookup Rules. If these rules fail to match anything in the CMDB, the Vulnerability Integration framework may create a new record in \"Unclassed Hardware\". This can impact the prioritization and assignment of Vulnerability Items (VIs) if they are unmatched and a discovered item is created in Unclassed Hardware. \n\nFurthermore, having more records in Unclassed Hardware can affect CMDB reporting and performance. The upstream causes of this issue can be an unprepared CMDB or unsuitable CI Lookup Rules. The downstream consequences include duplicate configuration item records, support teams being unable to lookup CIs quickly, Event Management not being able to bind the CI, and performance issues.\n\nThe impact on the business is significant. More CIs in Unclassed Hardware can negatively impact the effectiveness and efficiency of Vulnerability Remediation teams, vulnerability teams, and audits. It can cause a delay in vulnerability identification, slower response to vulnerability remediation, and incomplete Discovered Item (DI) data, affecting security Mean Time To Repair (MTTR) and Audit\/Compliance."
    },
    {
        "id": 127,
        "CONTEXT": "\"## Container Vulnerability Response\n\n# Container Vulnerability Response {#ariaid-title1}\n\nThe ServiceNow\u00ae Container Vulnerability Response application imports container vulnerable items (CVITs) and according to the rules enables you to remediate container vulnerabilities. Vulnerability data is pulled from internal and external sources  such as the National Vulnerability Database (NVD) or third-party integrations.\nStarting with version 18.0 of Vulnerability Response  you can monitor and remediate CVITs in Vulnerability Manager Workspace and IT Remediation Workspace respectively. For more information  see [Explore the Vulnerability Manager Workspace](..\/..\/vr-vulnerability-manager-workspace\/concept\/exploring-vuln-mgr-workspace.html \"As a vulnerability manager and analyst  get a unified view of all the vulnerability types in the Vulnerability Manager Workspace. You can see host  application  and container vulnerabilities and configuration issues  create watch topics across these vulnerabilities and misconfigurations  and create remediation tasks.\") and [Explore the IT Remediation Workspace](..\/..\/vulnerability-response\/task\/vr-ws-itro-wkspce.html \"The IT Remediation Workspace is intended for IT remediation owners and IT groups. It is composed of home and list views as well as data visualizations that you can click that let you see the remediation tasks you've been assigned and how many records assigned to you have solutions.\").\n\n## Request apps on the Store {#cvr-landing__section_vxk_psm_1jb}\n\nVisit the [ServiceNow Store](https:\/\/store.servicenow.com\/sn_appstore_store.do#!\/store\/home) website to view all the available apps and for information about submitting requests to the store. For cumulative release notes information for all released apps  see the [ServiceNow Store version history release\nnotes](https:\/\/docs.servicenow.com\/bundle\/store-release-notes\/page\/release-notes\/store\/sn-store-release-notes.html).{#cvr-landing__inline-send-to-store}\n\n|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| [Explore ![](..\/..\/..\/reuse\/icons\/brand-icons\/bus-explore.png) Learn about Container Vulnerability Response concepts and features.](..\/..\/secops-integration-vr\/prisma\/concept\/exploring-cvr.html \"The Container Vulnerability Response application imports container vulnerable items (CVITs). According to the rules  the feature enables you to remediate the container vulnerabilities. Container Vulnerability Response is available through a separate subscription.\") | [Configure ![](..\/..\/..\/reuse\/icons\/brand-icons\/bus-sdlc.png) Configure Container Vulnerability Response.](..\/concept\/configure-cvr.html \"Before you run Container Vulnerability Response in your Now Platform instance  you must configure it.\")                                                                                                                                                                                                          | [Integrate ![](..\/..\/..\/reuse\/icons\/brand-icons\/bus-integration-and-apis.png) Extend Container Vulnerability Response by integrating with other applications.](..\/concept\/integrate-cvr.html \"Extend the capabilities of Container Vulnerability Response by integrating with other applications.\")                                 |\n| [Remediate ![](..\/..\/..\/reuse\/icons\/brand-icons\/bus-optimize-manage.png) Learn how to remediate container vulnerabilities.](..\/concept\/remediate-cvr.html \"Monitoring remediation is a process that begins with reviewing status and ends with closing container vulnerable items (CVITs). Container Vulnerability Response offers tools and procedures to make that process more productive and efficient.\")                                                                | [Analytics \\& Reporting ![](..\/..\/..\/reuse\/icons\/brand-icons\/bus-infographic.png) View Container Vulnerability Response analytics](..\/..\/..\/use\/application-content-packs\/concept\/cvr-content-pack.html \"This Platform Analytics Solution contains prepackaged Platform Analytics content for use with other Now Platform products. This Platform Analytics Solution provides an exposure on how vulnerabilities are managed through the various charts.\") | [Reference ![](..\/..\/..\/reuse\/icons\/brand-icons\/bus-learn.png) Get details about Container Vulnerability Response components like fields  tables and properties.](..\/concept\/cvr-reference.html \"Reference topics containing information about tables  roles  and properties installed with the Container Vulnerability Response.\") |\nTable 1. Container Vulnerability Response{#cvr-landing__table_myz_gt3_b5c}\n\n## Benefits {#cvr-landing__id_ntb_vq1_fxb}\n\nThe Container Vulnerability Response application provides the following benefits:\n\n* Integrates with third-party container security products  like Prisma Cloud Compute from Palo Alto Networks.\n* Imports vulnerability data for the images that are deployed to runtime  and enriches the vulnerability data with runtime contextual information (hosts  Kubernetes clusters  services  and namespaces).\n* Provides a list of the references created from vulnerabilities to the relevant Kubernetes entities in the Configuration Management Database (CMDB) using ServiceNow Kubernetes Discovery.\n* Offers a comprehensive reporting dashboard  providing insights into the vulnerability and remediation trends.\n{#cvr-landing__ul_amv_tbl_5xb}  \n\n## Key features {#cvr-landing__section_mzl_kr1_fxb}\n\nThe Container Vulnerability Response application manages vulnerabilities detected in the containers. It provides the following features:\n\n* Point to source Docker Image from CVITs instead of running containers.\n* Configure granularity of CVITs to track at image  Kubernetes cluster  namespace  or service level.\n* Track new image versions to identify fixed vulnerabilities. Any vulnerabilities reported in older versions are automatically resolved in ServiceNow when new image versions are deployed at runtime.\n* Track CVITs in Base images separately from Application images to enable independent remediation.\n* Raise exception requests or false positive requests  which can be reviewed through a multi-level approver process.\n* Define exception rules to defer CVITs automatically.\n{#cvr-landing__ul_dmt_xqm_vsb}  \n\n## Use cases {#cvr-landing__section_gq5_p2d_dxb}\n\nContainers are a great way of deploying and scaling applications on multiple environments with less overhead and increased portability. \n\n## Vulnerability Crisis Management\n\n# Vulnerability Crisis Management {#ariaid-title1}\n\nCreate and track critical vulnerability events through the Vulnerability Crisis Management workflow. Create vulnerability assessment records  record key attributes of the vulnerability to calculate risk  perform assessment to identify exposure level  and engage stakeholders for a coordinated and swift response to vulnerabilities.\n\n## Managing vulnerability crisis events {#vulnerability-crisis-management__section_mqb_zj1_hzb}\n\nVulnerability Crisis Management is a complete workflow to handle vulnerability crisis events with the following capabilities.\n\n* Efficiently identify vulnerable configuration items by correlating critical vulnerabilities with software installation inventory from Software Asset Management and Software Bill of Materials (SBOM) inventory  scanner-reported vulnerabilities  and Configuration Management Database (CMDB).\n* Convert assessment results into vulnerable items for remediation.\n* Initiate a major security incident  to ensure a swift and coordinated response to the threat.\n* Engage and collaborate with teams across the organization  facilitating a unified response to vulnerabilities.\n* Provide regular status reports to cross-functional stakeholders and involved teams to maintain transparency and communication throughout the crisis.\n{#vulnerability-crisis-management__ul_obs_1k1_hzb}  \n\n## Vulnerability Crisis Management using the Vulnerability Assessment Workspace {#vulnerability-crisis-management__section_nth_wyz_gzb}\n\nOn identifying a vulnerability of interest  the vulnerability event manager creates an vulnerability assessment record with information on the threat intelligence source  vulnerability characteristics  and affected products. This information is used for further impact and exposure analysis.\n\nAfter the record for a vulnerability of interest has been created  a risk assessment is performed. This assessment comprises structured risk scoring  reviewing the record  and the observations of the analyst performing the task. The initial risk score for a vulnerability of interest is calculated using the attributes available at the time of event creation. The risk score for the assessment may change as additional intelligence becomes available. Use the risk score to determine the potential impact of exploitation and establish response priorities.\n\nAfter the assessment for a vulnerability of interest has been created and determined to present risk to the organization's infrastructure  you can analyze the threat further by updating the risk assessment with an in-depth exposure assessment with the software installation inventory from Software Asset Management  Software Bill of Materials (SBOM) inventory  scanner-reported vulnerabilities  and Configuration Management Database (CMDB). Impacted Configuration Items and Applications are automatically identified through assessment. Additional impacted items can be added manually.\n\nOnce the assessment is completed  Vulnerable Items or Application Vulnerable Items can be created for the exposure results that do not already have a associated vulnerable Item. Risk score calculator of vulnerable items can be leveraged\/configured to adjust risk score for vulnerable items linked to vulnerability assessment records. The Vulnerability Assessment record can be assigned exposure level and event priority. Based on the event priority  the Vulnerability Event Manager can choose to propose  promote or link the vulnerability assessment to a Major Security Incident.\n\nUse Major Security Incident Management to track and manage remediation activity  link ongoing security incidents  create ad-hoc tasks  engage affected teams  send status reports and collaborate using collaboration integrations available in Major Security Incident Management.\n\n## ServiceNow\u00ae\n\nSoftware Asset Management and Software Bill of Materials (SBOM) assessment- processing logic {#vulnerability-crisis-management__section_zb2_btk_gzb}\n\nUsing the Software Asset Management data  the CPEs coming from NVD for the CVE  and then the discovery models are fetched using the string-matching logic. After fetching the discovery models  a scan for related installations is run  the related configuration items are fetched  and the Affected Configuration Item table is populated. You can provide further details like Publisher  Product  Version and Edition. Based on these  all the matching discovery models and the software installations for the record are fetched. Subsequently  the related configuration items are fetched and the Affected Configuration Item table is repopulated.For SBOM  the associated software for the CVE from the related (m2m) table (between CVE and Software) is fetched. After pulling in the software details  the related SBOM components are identified by matching the product and version from the SBOM component to the product and version of the software identified.\n\nAfter the associated components are found  the entities related to the components are fetched. The product model from the entities and the related CI (if found) are fetched and the configuration item is saved in the Affected Configuration Item table. If the configuration item is without vulnerable items you can use it to create the vulnerable item. If a configuration item is not found  the product model is saved in the Affected Software Model table and can be used to create application vulnerable items.\n\n\n\nAfter the associated components are found  the entities related to the components are fetched. The product model from the entities and the related CI (if found) are fetched and the configuration item is saved in the Affected Configuration Item table. If the configuration item is without vulnerable items you can use it to create the vulnerable item. If a configuration item is not found  the product model is saved in the Affected Software Model table and can be used to create application vulnerable items.\n\nFor more information on using the Vulnerability Crisis Management workflow  see [Using the Vulnerability Assessment workspace](vr-ws-vuln-assessment.html \"The Vulnerability Assessment Workspace is designed for the Vulnerability Event Manager to create a vulnerability event and to perform vulnerability assessment  especially during the zero-day vulnerability analysis.\").\n**Parent Topic:** [Explore the Vulnerability Response application](..\/..\/..\/product\/vulnerability-response\/concept\/c_VulnerabilityResponse.html \"The ServiceNow Vulnerability Response application imports and automatically groups vulnerable items according to group rules allowing you to remediate vulnerabilities quickly. Vulnerability data is pulled from internal and external sources  such as the National Vulnerability Database (NVD) or third-party integrations and processed with applications developed by ServiceNow.\")\n**Related concepts**\n\n* [Installation of Vulnerability Response and supported applications](..\/..\/..\/product\/vulnerability-response\/concept\/cj-vr-setup.html \"The Vulnerability Response application is available from the ServiceNow Store. The application supports other ServiceNow applications and third-party integrations that you also download from the ServiceNow Store. More options also are available to extend the basic setup.\")\n* [Vulnerability Response personas and granular roles](..\/..\/..\/product\/vulnerability-response\/concept\/vr-persona-overview.html \"Before you can successfully remediate vulnerabilities with the Vulnerability Response application  you must assign personas and roles to your users and groups in Setup Assistant.\")\n* [Vulnerability Response assignment rules overview](..\/..\/..\/product\/vulnerability-response\/concept\/vr-assignment-rules.html \"Define the criteria by which vulnerable items (VIs) are automatically assigned to an assignment group for remediation.\")\n* [Vulnerability Response remediation tasks and task rules overview](..\/..\/..\/product\/vulnerability-response\/concept\/vulnerability-groups.html \"Configure remediation tasks (VULs) to help analysts and remediation specialists organize vulnerable items (VI) and analyze them in bulk. The criteria by which groups are formed is configured so that you do not have to manually assign vulnerable items into groups. Using remediation tasks  you can monitor progress and drive the remediation process more efficiently.\")\n* [Vulnerability Response remediation target rules](..\/..\/..\/product\/vulnerability-response\/concept\/time-to-remediate-rules.html \"Remediation target rules define the expected time frame for remediating a vulnerable items (VI)  much like SLAs provide a time frame for remediating the vulnerability itself. For example  if an asset contains PCI data (credit card data) then the vulnerability on that item needs to be fixed within 30 days according to PCI DSS.\")\n* [Vulnerability classification groups and rules](..\/..\/..\/product\/vulnerability-response\/concept\/vulnerability-classification-rules.html \"Classification groups automate classification of values of a table based on the classification rules defined in the group. The condition for each rule is evaluated in order  and the first matching rule is used.\")\n* [Machine Learning solutions for Vulnerability Response](..\/..\/..\/product\/vulnerability-response\/concept\/ml_vr.html \"Vulnerability Assignment Recommendations uses ServiceNow Predictive Intelligence and machine learning to recommend assignment groups for vulnerable items (VIs) and remediation tasks (VULs  or RTs). You can reduce the time that you spend on identifying the owners for unassigned or incorrectly assigned vulnerability findings. Also  you can see a system-generated confidence score that evaluates if the recommended assignment group is suited to resolve the vulnerability.\")\n* [CI Lookup Rules for identifying configuration items from Vulnerability Response third-party vulnerability integrations](..\/..\/..\/product\/security-operations-common\/concept\/ci-identifier-rules.html \"When data is imported from a third-party integration  Vulnerability Response automatically uses host data to search for matches in the Configuration Management Database (CMDB). It does this using CI Lookup Rules. These rules are used to identify configuration items (CIs) and add them to the vulnerable item record to aid in remediation.\")\n* [Creating CIs for Vulnerability Response using the Identification and Reconciliation engine](..\/..\/..\/product\/vulnerability-response\/concept\/ci-creation-using-IRE.html \"You can create configuration items (CIs) in the Configuration Management Database (CMDB) using the Identification and Reconciliation engine (IRE) API. By using the IRE API to create CIs  you can prevent duplicate CIs from being created and you can reconcile CI attributes by allowing only authoritative data sources to write to CMDB.\")\n* [Discovered Items](..\/..\/..\/product\/vulnerability-response\/concept\/cj-discovered-items.html \"Assets are automatically matched to configuration items (CIs) in the Configuration Management Database (CMDB) when they are imported using CI Lookup Rules. \"\n\n",
        "QUERY": " 'How are the vulnerabilities detected in CMDB?'",
        "GROUND_TRUTH": "Vulnerabilities in the Configuration Management Database (CMDB) are detected using the ServiceNow\u00ae Container Vulnerability Response application and the Vulnerability Crisis Management workflow. The Container Vulnerability Response application imports container vulnerable items (CVITs) from internal and external sources such as the National Vulnerability Database (NVD) or third-party integrations. It integrates with third-party container security products and imports vulnerability data for the images that are deployed to runtime. It enriches the vulnerability data with runtime contextual information (hosts, Kubernetes clusters, services, and namespaces). \n\nIn the Vulnerability Crisis Management workflow, vulnerable configuration items are identified by correlating critical vulnerabilities with software installation inventory from Software Asset Management and Software Bill of Materials (SBOM) inventory, scanner-reported vulnerabilities, and the CMDB. The workflow converts assessment results into vulnerable items for remediation. \n\nFor Software Asset Management and SBOM assessment, the CPEs coming from NVD for the CVE are used, and then the discovery models are fetched using the string-matching logic. After fetching the discovery models, a scan for related installations is run, the related configuration items are fetched, and the Affected Configuration Item table is populated. For SBOM, the associated software for the CVE from the related (m2m) table (between CVE and Software) is fetched. After pulling in the software details, the related SBOM components are identified by matching the product and version from the SBOM component to the product and version of the software identified. The product model from the entities and the related CI (if found) are fetched and the configuration item is saved in the Affected Configuration Item table."
    },
    {
        "id": 128,
        "CONTEXT": "\"## Layout_overview_premium or \"Layout for premium overview pages\" does not render correctly on Homepage\n\n## Description\n\nThis layout choice does not render correctly for Homepages but works fine for Performance Analytics Dashboards. This issue is observed since the Kingston release.\n\n## Steps to Reproduce\n\n1. Login a Kingston instance  \n2. Create a new homepage or open an existing homepage  \n3. Change the layout to \"**Layout for premium overview pages** \" and click 'Change'  \n\nNo reports render on the homepage. The following error appears:\n\"**Unable to find table: homepage_grid** \".\n\n## Workaround\n\nThis is expected behaviour. The \"**homepage_grid** \" table name mentioned in the error does not belong to the home page system  nor to the dashboards system. The homepage layout \"**Layout for premium overview pages**\" does not use that table name and does not load any data.\n\nChange the layout of the homepage to one that renders and works correctly. If you are unable to modify the layout of that homepage  follow the steps below:\n\n1. Navigate to sys_portal_page table and find the homepage that is having issues\n2. Ensuring that the layout column exist on the list  double-click on the reference field which is currently set to \"layout_overview_premium\"\n3. Change the value to one of your choosing. E.G. \"layout_3_across_catalog\"\n\n**Related Problem: PRB1280438**\n\n\n\n## Vancouver security and notable fixes\n\nlist contains an ID in parentheses after the first and last names  the avatar initials are incorrect.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | 1. Log in to a Utah instance. 2. Ensure that Next Experience is enabled. 3. Navigate to sys_user.list. 4. Find a user without a picture. 5. Open the entry. 6. Add some number in parenthesis (for example  (987654321)) to the Last name field. 7. Save the entry. 8. Add the Name field to the form. 9. Impersonate the user. 10. Repeat the steps on San Diego and Tokyo releases. {#vancouver-security-notables__ol_h3n_zd1_2yb} Verify that the initials are displayed correctly in San Diego and Tokyo  but not Utah.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n| Now Experience UI Builder PRB1547162                                                                                             | Translations don't work for group actions in UI Builder                                                                                                                                                         | The translation prefix should be on the drop-down for group actions  but isn't.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n| Now Experience UI Builder PRB1599538                                                                                             | Event mappings that are mapped to the client script 'Execute - Client Script' aren't copied to the newly duplicated variant                                                                                     |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | 1. Navigate to a UIB page that has event mappings in the client script. 2. Create a duplicate of the page. {#vancouver-security-notables__ol_mjn_zd1_2yb} Notice that event mappings mapped to the client script 'Execute - Client Script' are not copied to the newly duplicated variant.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n| Now Support Administration - Upgrades PRB1630363 [KB1224143](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB1224143)    | Large standby DBI replication lags are caused by multiple IndexCreator jobs                                                                                                                                     | This is triggered on large CMDB tables via a fix script that was introduced after upgrading to Tokyo.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | Refer to the listed KB article for details.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n| Now User Experience PRB1640986 [KB1279248](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB1279248)                      | After upgrading to Utah  the icons in the top-right of the Next Experience Unified Navigation become un-clickable                                                                                               | This affects the 'globe'  'help'  'Notifications menu'  and 'User menu' icons.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | Refer to the listed KB article for details.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n| Now User Experience PRB1641340                                                                                                   | The ux_data_broker ACL for Enabled Plugins has an incorrect condition set                                                                                                                                       | The incorrect condition causes OpenFrameIT to fail.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | 1. Open the latest track. 2. Try to run OpenFrameIT. {#vancouver-security-notables__ol_zjn_zd1_2yb} Expected behavior: OpenFrameIT should not fail. Actual behavior: OpenFrameIT fails and an error message displays.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |\n| Performance Analytics Dashboards PRB1616064                                                                                      | The Homepage Deprecation Help Tool does not work as expected when domain separation is enabled in an instance                                                                                                   | There are several issues that occur.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | 1. Provision a base instance with demo data and domain-separation installed. 2. Adjust some of the demo Homepages from the 'global' domain to another domain (for example  TOP\/Cisco). 3. Try to generate the migration records and convert those homepages to dashboards. {#vancouver-security-notables__ol_jkn_zd1_2yb} Observe that the Homepage Deprecation Help Tool does not work as expected.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |\n| Performance Analytics Dashboards PRB1641996 [KB1321867](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB1321867)         | The breakdown source name displays unicode for the '\/' character on a breakdown source selector list                                                                                                            |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | Refer to the listed KB article for details.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n| Performance Analytics Dashboards PRB1642052 [KB1343166](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB1343166)         | Date\/time fields in dashboard reports display incorrectly as the 'YYYY-MM-DD HH:MM:SSYYYY-MM-DD HH:MMdays ago' format                                                                                           | Date fields should display in the user preference's set format.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | Refer to the listed KB article for details.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n| Performance Analytics Dashboards PRB1669003                                                                                      | After a Utah upgrade  the interactive filters and single score report widgets added to dashboards have a scrollbar                                                                                              | After upgrading to the Utah version  the existing dashboards have been affected where most interactive filters added to dashboard now are showing scrollbars.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | 1. On a Tokyo version instance  create a dashboard. 2. Add a few interactive filters for some reference\/date\/cascade types 3. Resize them and add them to left column. 4. Add a few more reports on the right side of the layout. 5. Reload and ensure the dashboard does not show any scroll bars on any filter widgets. 6. Now upgrade the instance Utah. {#vancouver-security-notables__ol_zkn_zd1_2yb} After the upgrade  observe how the same dashboard now has resized widgets. The widgets should not be resized by the upgrade.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |\n| Performance Analytics PRB1560880                                                                                                 | The homepage reloads infinitely when 'my_home_navigation_page' is set to an empty string                                                                                                                        |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | 1. Log in to a San Diego instance as System Admin. 2. Set the user preference 'my_home_navigation_page' for the System Admin user with the value empty string (''). 3. Log out of the instance and log back in. {#vancouver-security-notables__ol_rln_zd1_2yb} Expected behavior: The user is navigated toward the homepages (or dashboards if the deprecate_homepages preference is set to true). Actual behavior: The page is blank and the request to home.do gets infinitely called.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n\n\n## Homepage deprecation - Troubleshooting and Guidelines\n\n## Summary\n\nThe [classic homepage feature](https:\/\/docs.servicenow.com\/bundle\/quebec-platform-user-interface\/page\/administer\/homepage-administration\/concept\/c_HomepageAdministration.html \"classic homepage feature\") is being deprecated  as the preferred [responsive dashboards](https:\/\/docs.servicenow.com\/bundle\/quebec-now-intelligence\/page\/use\/dashboards\/concept\/c_ResponsiveDashboards.html \"responsive dashboards\") have been available for numerous releases.\n\n* As of Rome  new and upgrading instances will have the option to [disable the editing and or creation of Homepages.](https:\/\/staging-docs-servicenow.zoominsoftware.io\/bundle\/rome-platform-user-interface\/page\/use\/dashboards\/task\/disable-homepages.html \"disable the editing and or creation of Homepages.\")  \n* **In San Diego  the homepage feature will not be enabled on net new instances.**\n  * The property 'com.glideapp.home.deprecate_homepages'**:**   \n    * For net new customers is set to TRUE from San Diego\n    * Upgrading customers have the value set to FALSE.\n    * If upgrading customers enable the Next Experience UI  the value will be set to TRUE.  \n* **Upgrading customers will be able to migrate manually existing homepages to dashboards via 3 options:**   \n  * System admins can migrate in bulk all the homepages to the dashboards using the List action on the \"sys_portal_page\" table.\n  * Users can only migrate the Homepages they have created from the UI action on the Homepage itself.\n  * Using the new Store application \"Homepage deprecation help tool\" released on 23rd March 2022.  \n* **Starting Utah  while upgrading customers will be able to see existing homepages  they will not be able to create new or edit homepages.**\n  * The property 'com.glideapp.home.deprecate_homepages' will be set to TRUE on the upgrade to Utah.  \n* **Customers who have not upgraded to Utah+ releases are not affected by the mandatory deprecation.**\n\n## FAQs\n\n#### *Which features from homepages are deprecated if the property is TRUE?*\n\n* Homepage administration menus will not be available for net new customers.\n* System cogwheel for net new customers will not display anymore the option to set your home default to homepages\n* The add to the homepage from the Report Designer feature will be deprecated.\n* 'Create new'  'Add content'  and 'Change layout' buttons on the homepages will not be available.\n\n#### *How can I convert a homepage to the dashboard?*\n\n* Follow the steps in this doc: [convert homepage to dashboard](https:\/\/docs.servicenow.com\/bundle\/quebec-now-intelligence\/page\/use\/dashboards\/task\/clo...",
        "QUERY": " 'Homepage grid layout choice not rendered correctly'",
        "GROUND_TRUTH": "The \"Layout for premium overview pages\" does not render correctly on Homepages but works fine for Performance Analytics Dashboards. This issue has been observed since the Kingston release. When you try to change the layout to \"Layout for premium overview pages\" and click 'Change', no reports render on the homepage and the error \"Unable to find table: homepage_grid\" appears. This is because the \"homepage_grid\" table name mentioned in the error does not belong to the home page system nor to the dashboards system. The homepage layout \"Layout for premium overview pages\" does not use that table name and does not load any data. \n\nAs a workaround, you can change the layout of the homepage to one that renders and works correctly. If you are unable to modify the layout of that homepage, you can follow these steps:\n\n1. Navigate to the sys_portal_page table and find the homepage that is having issues.\n2. Ensuring that the layout column exists on the list, double-click on the reference field which is currently set to \"layout_overview_premium\".\n3. Change the value to one of your choosing. For example, \"layout_3_across_catalog\"."
    },
    {
        "id": 129,
        "CONTEXT": "\"## CMDB Health Dashboard - Duplicate Health Metric needs to filter out CIs with Status **Retired**\n\n**CMDB Health Dashboard**\n\nLooking for an alternative to filtering by CI Status Field.\nThe Duplicate health metric report is showing duplicates from CI's with Status as Retired.\nWould like to filter out any CI's that do not have a Status of Retired from appearing in the **CMDB Health Duplicate** report\n\n\n\nThere is a feature called **\"identification inclusion rules**\"  With this  it can narrow the scope of CIs that are included in the identification process. The same identification rules are automatically used by the CMDB Health duplicate metric also. For example  this scenario user can include CIs whose state is not \"Retired\"  so Retired CI's are not considered during the duplicate CI identification process and during metrics as well.\n\nPlease keep in mind that as these rules are considered during duplicate identification process  duplicate CI's may get created even if there is a Retired CI in the system.\n\nHere is the doc link which I am referring above [https:\/\/docs.servicenow.com\/csh?topicname=create-id-inclusion-rule.html\\&version=latest](https:\/\/docs.servicenow.com\/csh?topicname=create-id-inclusion-rule.html&version=latest)\n\n**Steps to be taken :**\n\n1. Add the Identification inclusion rule:\n   \/nav_to.do?uri=cmdb_ie_active_config.do?sys_id=-1\n2. Remove the records that are created in **cmdb_health_result and reconcile_duplicate_task table.**   \n   **Note:** Please use table cleaner to clean the records [https:\/\/docs.servicenow.com\/csh?topicname=c_DataManagement.html\\&version=latest](https:\/\/docs.servicenow.com\/csh?topicname=c_DataManagement.html&version=latest).Refer to table cleaner sections\n3. Make the CI **discovery_source** to \"\" or anything else other than \"Duplicate\" and \"unknown\" and the Identification engine should run against the identification rule and filter out the retired CIs from the health result table.\n4. Re-run the **Health Job** to re-populate the data\n5. Review the results once the job is completed\n\n\n\nTo hide the CIs with retired status from other Health Metric  please refer to docs below\n\n[https:\/\/docs.servicenow.com\/csh?topicname=create-health-inclusion-rule.html\\&version=latest](https:\/\/docs.servicenow.com\/csh?topicname=create-health-inclusion-rule.html&version=latest)\n\n\"\n\n",
        "QUERY": " 'Health inclusion rules for duplicate metric not working as expected for non-operational\/retired CIs'",
        "GROUND_TRUTH": "If the health inclusion rules for the duplicate metric are not working as expected for non-operational\/retired CIs, you can use the \"identification inclusion rules\" feature to narrow the scope of CIs included in the identification process. This will also be used by the CMDB Health duplicate metric. \n\nHere are the steps to be taken:\n\n1. Add the Identification inclusion rule by navigating to: \/nav_to.do?uri=cmdb_ie_active_config.do?sys_id=-1\n2. Remove the records that are created in the cmdb_health_result and reconcile_duplicate_task table. Use the table cleaner to clean the records. Refer to the table cleaner sections in the provided documentation.\n3. Make the CI discovery_source to \"\" or anything else other than \"Duplicate\" and \"unknown\". The Identification engine should run against the identification rule and filter out the retired CIs from the health result table.\n4. Re-run the Health Job to re-populate the data.\n5. Review the results once the job is completed.\n\nTo hide the CIs with retired status from other Health Metric, refer to the provided documentation on creating health inclusion rules."
    },
    {
        "id": 130,
        "CONTEXT": "\"## AIS | AI Search general troubleshooting and debugging\n\n# Description {#DESCRIPTION}\n\nThis article talks about the basic troubleshooting and debugging steps for working on AIS issues\n\n# Troubleshooting and Debugging AI Search {#LOGICAL_ARCHITECTURE}\n\n1. In order to turn on the logs  From the filter navigator **System Diagnostics \\> Debug AI Search** You can see similar debug messages below  \n![](sys_attachment.do?sys_id=41eb4dc14732bd502c31b98a436d4392)\n2. You could check for logs by navigating to **AI Search \\> All** or **AI Search \\> Ingestion Warnings \\& Errors.** Refer [AI Search logging and debugging](https:\/\/docs.servicenow.com\/bundle\/quebec-platform-administration\/page\/administer\/ai-search\/concept\/logging-debugging-ais.html \"AI Search logging and debugging\")\n3. Use [Search Preview UI](https:\/\/docs.servicenow.com\/bundle\/rome-platform-administration\/page\/use\/dashboards\/application-content-packs\/ai-search-preview.html#ais-search-preview-admin-tools \"Search Preview UI\") by installing the application [Advanced AI Search Management Tools](https:\/\/docs.servicenow.com\/bundle\/quebec-platform-administration\/page\/use\/dashboards\/application-content-packs\/install-adv-ais-mgmt-tools.html \"Advanced AI Search Management Tools\")and once the app is installed Navigate to **Ai Serch \\> Preview \\> Search Preview**\n3. a. Choose the search profile on the dropdown on the top left\n\n3.b. In the field which says 'Search here' enter the search term \\> Hit 'Search' button\n\n4. check localhost logs as well.\n5. If you would like to test the search query for an NLU model  you could search from \"Scripts - background' and check the results and debug logs\n\n\/\/Enter the solution name you are testing in the line #1 like ml_x_snc_sn_km_mr_global_ais_gra_b8bc699a67621010b3d782f45685efa5\n\nvar geniusSearchSolutionName = 'enter the solution name';\n\nvar searchQuery = new sn_ml.GeniusSearchQuery(\"abraham lincoln\");\n\nvar searchContext = new sn_ml.GeniusSearchContext({\"session\" : \"\"});\n\nvar geniusSearchSolution = sn_ml.GeniusSearchSolutionStore.get(geniusSearchSolutionName)\n\ngeniusSearchSolution = sn_ml.GeniusSearchSolutionStore.get(geniusSearchSolutionName);\n\nvar result = geniusSearchSolution.search(searchQuery  searchContext  {});\n\ngs.print(JSON.stringify(JSON.parse(result)  false  4));\n\n6. Along with the Search Preview UI  you can use the below script to debug the results with search profile or narrow down the search issue. **The AISASearchUtil API below is unsupported and should only be used for debugging purposes.**\n\n\/\/Make changes to the searchParams based on your requirements.\n\/\/Just need to change searchContextConfigId and searchTerm to get started.\n\nsearchParams = {\nsearchContextConfigId: '00731b9d5b231010d9a5ce1a8581c7dd'     \/\/sys_id of the Search Application configuration\nsys_search_context_config record that you want to use for searching.\nsearchTerm: '***'               \/\/ The search term *** returns all records that have been indexed.\npaginationToken: ''              \/\/ Leaving paginationToken empty returns the data for the first page. To get the paginationToken for 2nd page  click the next page button on the portal after doing the search and check the URL for paginationToken.\ndisableSpellCheck: false \nfacetFilters:  \nsearchFilters:  \nrequestedFields: {} \nrpSysId: 'test'                    \/\/ Just a dummy value that is needed for the script to work.\n};\n\nvar aiSearchUtil = new AISASearchUtil();\nvar result = aiSearchUtil.search(searchParams);\n\nvar searchResult = result.data.search.searchResults;\n\ngs.info(searchResult.length);\n\nfor (var i=0; i<searchResult.length; i++){\ngs.info(searchResult\ni\n.title + \" | \" + searchResult\ni\n.text);\n}\n\ngs.info(JSON.stringify(result));\n\n\/\/Note: Ensure that search sources have been indexed before trying to search.\n\n# Common spot checks to perform if AI Search does not display search results: {#PHYSICAL_ARCHITECTURE}\n\n1. Check if AI Search is enabled '[glide.ais.enabled](https:\/\/aitraining.service-now.com\/sys_properties.do?sys_id=a3f1bb94ff031010b7b67677d53bf1a0&sysparm_record_target=sys_properties&sysparm_record_row=3&sysparm_record_rows=39&sysparm_record_list=nameCONTAINSais%5EORDERBYname)' should be set to true. and also check **AI Search \\> Connection \\> Test Connection**\n\nIf there are any connection issues  please reach out to Tech Support\n2. If the Genius results are not being returned check if '[glide.ais.genius_result.enabled](https:\/\/aitraining.service-now.com\/sys_properties.do?sys_id=e6bcb3831b0b7010704084c2604bcbf5&sysparm_record_target=sys_properties&sysparm_record_row=1&sysparm_record_rows=1&sysparm_record_list=nameCONTAINSgenius%5EORDERBYname)' is set to '**true** ' \n\n2.a. Check if facet is clicked and then genius result is not displayed then check 'glide.ais.genius_result.enabled_with_facet_filter' to false\n\n2.b. If for any non-English language Genius results are not working  because it's not supported. Genius results is supported only for English even though AI Search supports several languages Refer [Internationalization support for AI Search](https:\/\/docs.servicenow.com\/bundle\/rome-platform-administration\/page\/administer\/ai-search\/concept\/international-language-support-ais.html#international-language-support-ais \"Internationalization support for AI Search\")\n\n2.c. If the Genius result is not working check if there is an EVAM configuration is added for the corresponding Genius result. For example Peoplefind Genius result will not work unless you add one manually in **sys_ux_composite_data_template_predicate_bundle**   See [EVAM Configuration](https:\/\/docs.servicenow.com\/bundle\/rome-servicenow-platform\/page\/administer\/evam\/task\/define-view-configuration-bundle.html \"EVAM Configuration\")\n\n3. If the expected results are not displayed \\> then check the following\n\n3.a. Check if the correct '**Search Profile**' is being linked\n\n3.b. Check if the '**Search Profile** ' is '**Published**' if not publish again and retest\n\n4. If the expected results are not present  check the '**Search Source** ' filter condition. For example  if a 'search source' filters out certain records  then those records would never show up in the results. Other reasons would be **ACL**   **Domain** **Separation**   **User** **Criteria and Before Query Business Rules**.\n5. If expected number of results are not displayed  go to the **Search application configuration** (sys_search_context_config) and check the **search** **results** **limit**\n\n# Other issues with AI Search: {#DESCRIPTION}\n\n1. Check if Edge encryption  Encryption support is enabled since this is not supported as of Quebec\n\n\n## Exclude a search profile from the search query parameter evaluation framework\n\nSearch Event Signal Provider is the only supported value.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |\nTable 3. evaluation_parameter### A\/B Testing Evaluation Parameter Result table {#srch-qry-parm-eval-fw-tables-ais__section_q2g_ylj_yxb}\n\nThe A\/B Testing Evaluation Parameter Result \\[evaluation_parameter_result\\] table stores calculation results for individual search query parameters considered in A\/B testing evaluations for live search traffic.\n{#d166076e893}\n\n|        Column        |                                                          Description                                                           |\n|----------------------|--------------------------------------------------------------------------------------------------------------------------------|\n| Evaluation Execution | Reference to the record for the evaluation execution on the A\/B Testing Evaluation Execution \\[evaluation_execution\\] table.   |\n| Parameter Evaluation | Reference to the record for the search query parameter on the A\/B Testing Evaluation Parameter \\[evaluation_parameter\\] table. |\n| Best Value           | Best value for the search query parameter  as determined by the Winning Score.                                                 |\n| Winning Score        | Numeric score for the search query parameter  determined using the Score Calculation Type.                                     |\n| Score Metadata       | Metadata from the score computation for the search query parameter.                                                            |\nTable 4. evaluation_parameter_result## Exclude a search profile from the search query parameter evaluation framework {#ariaid-title3}\n\nExclude a search profile from A\/B testing evaluations of live AI Search traffic. This procedure prevents AI Search from using A\/B testing evaluation results when publishing the search profile's search result relevancy model and its Q\\&A Genius Result answer validation model.\nRole required: ais_admin\nThe search query parameter evaluation framework performs A\/B testing evaluations of search configuration settings using live search traffic. By default  AI Search evaluates configuration settings for all search profiles.\nSearch administrators can exclude individual search profiles from the search query parameter evaluation framework. Excluding a search profile from the framework prevents AI Search from performing A\/B testing evaluations for live search traffic that uses the excluded search profile.\nNote: When you exclude a search profile from evaluations  AI Search no longer considers evaluation results when updating the machine learning relevancy and machine reading comprehension (MRC) models for that search profile. As a result  relevancy scoring settings and Q\\&A Genius Result answer filtering settings for the search profile may be less reflective of your search traffic. For more information on how AI Search uses A\/B evaluation testing results when publishing these models  see [Machine learning relevancy in AI Search](machine-learning-relevancy-ais.html \"AI Search displays the most relevant search results for a query first. Machine learning automatically tunes search result relevancy scoring for search experiences based on aggregated user interactions.\") and [Default Genius Result configurations](default-genius-result-configs-ais.html \"Pre-configured AI Search Genius Result configurations display actionable top results for Catalog Item  user  and Knowledge search queries.\").\n\n1. Navigate to All \\> AI Search \\> Search Experience \\> Search Profiles.\n2. Open the record for the search profile that you want to exclude from A\/B testing evaluations of live search traffic.\n3. If the Search Profile form does not already display the Exclude from evaluation field  configure the form layout to make the field visible.  \n   For details on configuring a form layout to make fields visible  see [Configuring the form layout](..\/..\/form-administration\/concept\/configure-form-layout.html \"Administrators or users with the personalize_form role can configure the form and related list layout.\").\n4. Select the Exclude from evaluation option.\n5. Select Update.\n\nAI Search no longer performs A\/B testing evaluations for traffic that uses the excluded search profile. Machine learning relevancy no longer updates the relevancy model for the search profile.\n\n\n\n## Search query parameter evaluation framework\n\nSearch Event Signal Provider is the only supported value.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |\nTable 3. evaluation_parameter### A\/B Testing Evaluation Parameter Result table {#srch-qry-parm-eval-fw-tables-ais__section_q2g_ylj_yxb}\n\nThe A\/B Testing Evaluation Parameter Result \\[evaluation_parameter_result\\] table stores calculation results for individual search query parameters considered in A\/B testing evaluations for live search traffic.\n{#d166076e893}\n\n|        Column        |                                                          Description                                                           |\n|----------------------|--------------------------------------------------------------------------------------------------------------------------------|\n| Evaluation Execution | Reference to the record for the evaluation execution on the A\/B Testing Evaluation Execution \\[evaluation_execution\\] table.   |\n| Parameter Evaluation | Reference to the record for the search query parameter on the A\/B Testing Evaluation Parameter \\[evaluation_parameter\\] table. |\n| Best Value           | Best value for the search query parameter  as determined by the Winning Score.                                                 |\n| Winning Score        | Numeric score for the search query parameter  determined using the Score Calculation Type.                                     |\n| Score Metadata       | Metadata from the score computation for the search query parameter.                                                            |\nTable 4. evaluation_parameter_result## Exclude a search profile from the search query parameter evaluation framework {#ariaid-title3}\n\nExclude a search profile from A\/B testing evaluations of live AI Search traffic. This procedure prevents AI Search from using A\/B testing evaluation results when publishing the search profile's search result relevancy model and its Q\\&A Genius Result answer validation model.\nRole required: ais_admin\nThe search query parameter evaluation framework performs A\/B testing evaluations of search configuration settings using live search traffic. By default  AI Search evaluates configuration settings for all search profiles.\nSearch administrators can exclude individual search profiles from the search query parameter evaluation framework. Excluding a search profile from the framework prevents AI Search from performing A\/B testing evaluations for live search traffic that uses the excluded search profile.\nNote: When you exclude a search profile from evaluations  AI Search no longer considers evaluation results when updating the machine learning relevancy and machine reading comprehension (MRC) models for that search profile. As a result  relevancy scoring settings and Q\\&A Genius Result answer filtering settings for the search profile may be less reflective of your search traffic. For more information on how AI Search uses A\/B evaluation testing results when publishing these models  see [Machine learning relevancy in AI Search](machine-learning-relevancy-ais.html \"AI Search displays the most relevant search results for a query first. Machine learning automatically tunes search result relevancy scoring for search experiences based on aggregated user interactions.\") and [Default Genius Result configurations](default-genius-result-configs-ais.html \"Pre-configured AI Search Genius Result configurations display actionable top results for Catalog Item  user  and Knowledge search queries.\").\n\n1. Navigate to All \\> AI Search \\> Search Experience \\> Search Profiles.\n2. Open the record for the search profile that you want to exclude from A\/B testing evaluations of live search traffic.\n3. If the Search Profile form does not already display the Exclude from evaluation field  configure the form layout to make the field visible.  \n   For details on configuring a form layout to make fields visible  see [Configuring the form layout](..\/..\/form-administration\/concept\/configure-form-layout.html \"Administrators or users with the personalize_form role can configure the form and related list layout.\").\n4. Select the Exclude from evaluation option.\n5. Select Update.\n\nAI Search no longer performs A\/B testing evaluations for traffic that uses the excluded search profile. Machine learning relevancy no longer updates the relevancy model for the search profile.\n\n\"\n\n",
        "QUERY": " 'Genius result not displayed when using AI search'",
        "GROUND_TRUTH": "If the Genius results are not being displayed when using AI Search, there are several checks you can perform according to KB12345:\n\n1. Check if the property '[glide.ais.genius_result.enabled](https:\/\/aitraining.service-now.com\/sys_properties.do?sys_id=e6bcb3831b0b7010704084c2604bcbf5&sysparm_record_target=sys_properties&sysparm_record_row=1&sysparm_record_rows=1&sysparm_record_list=nameCONTAINSgenius%5EORDERBYname)' is set to 'true'. \n\n2. If the facet is clicked and then the Genius result is not displayed, check 'glide.ais.genius_result.enabled_with_facet_filter' to false.\n\n3. If Genius results are not working for any non-English language, it's because Genius results are supported only for English even though AI Search supports several languages.\n\n4. If the Genius result is not working, check if there is an EVAM configuration added for the corresponding Genius result. For example, the Peoplefind Genius result will not work unless you add one manually in 'sys_ux_composite_data_template_predicate_bundle'.\n\nRemember to reach out to Tech Support if there are any connection issues."
    },
    {
        "id": 131,
        "CONTEXT": "\"## Reset AD user password \/ query AD action shows success when there is an auth failure\n\n## Description\n\nThe flow status of the action shows success but there is an error message says Authentication failure with the user ...\n\n? The handle error step itself has a success state in spite of the errors. That is to say  the window_error_code is empty which as design makes the action assume it was successful\n\n? In general  many systems use the error message as informational and only when combines with error code it is identified really an 'error'. Flow designer ad spoke cannot assume it is an error as there is no error code\n\n## Steps to Reproduce\n\n1. Configure the Reset AD user password with the AD Spoke installed  \n2. Use the Reset AD user password action  \n3. Put wrong credentials  \n4. The system will say in the step two Authentication failure with the user ... but the status of query AD action is success  \n5. More info in screenshoot\n\n## Workaround\n\nWith admin privileges  import the attached XML because if you import the xml first and then update the plugin in case you have to do it for any other requirement  it will override the changes of the imported file.\n\n**Related Problem: PRB1475449**\n\n\n\n## Description\n\nThe flow status of the action shows success but there is an error message says Authentication failure with the user ...\n\n? The handle error step itself has a success state in spite of the errors. That is to say  the window_error_code is empty which as design makes the action assume it was successful\n\n? In general  many systems use the error message as informational and only when combines with error code it is identified really an 'error'. Flow designer ad spoke cannot assume it is an error as there is no error code\n\n## Steps to Reproduce\n\n1. Configure the Reset AD user password with the AD Spoke installed  \n2. Use the Reset AD user password action  \n3. Put wrong credentials  \n4. The system will say in the step two Authentication failure with the user ... but the status of query AD action is success  \n5. More info in screenshoot\n\n## Workaround\n\nWith admin privileges  import the attached XML because if you import the xml first and then update the plugin in case you have to do it for any other requirement  it will override the changes of the imported file.\n\n**Related Problem: PRB1475449**\n\n\n\n## Playbook for Possible Password Spray\n\n[Security Operations](..\/..\/..\/product\/security-operations\/concept\/security-operations-intro.html \"ServiceNow Security Operations brings incident data from your security tools into a structured response engine that uses intelligent workflows  automation  and a deep connection with IT to prioritize and resolve threats based on the impact they pose to your organization.\") \\> [Security Incident Response](..\/..\/..\/product\/security-incident-response\/reference\/sir-landing-page.html \"The ServiceNow Security Incident Response application tracks the progress of security incidents from discovery and initial analysis  through containment  eradication  and recovery  and into the final post incident review  knowledge base article creation  and closure.\") \\> [Playbook Resources](..\/..\/..\/product\/security-incident-response\/concept\/cj-sir-flow-library.html \"Security Incident Response provides a rich set of playbook resources that include a comprehensive library of playbooks  subflows  and actions. You can create or configure playbooks quickly and easily without writing complicated code. You can use these playbooks to resolve security threats in a step-by-step manner.\") \\> [Security Incident Response playbooks](..\/..\/..\/product\/security-incident-response\/concept\/cj-sir-about-flows.html \"Using the Flow Designer  security administrators and flow design authors can more easily transition from manual or undocumented playbooks to automated and repeatable playbooks. The drag-and-drop feature provides flexibility in moving objects  condition checks  parallel branching  decision tables  and more.\") \\>\n\n# Playbook for Possible Password Spray {#ariaid-title1}\n\nThis playbook provides systematic remediation steps to investigate password spray alerts triggered by multiple failed logins (too many authentication failures from more than one IP address for the same user).\n\nThe workflow is created based on an existing playbook  which provides a consistent and efficient approach for incident investigation. Each decision point in the playbook has been converted into an outcome driven task and flow changes direction based on the outcome of such tasks.\n\n## Prerequisites {#playbook-possible-password-spray__section_qqy_xkw_fzb}\n\nMake sure you have the following roles:\n\n* sn_si.admin\n* flow_designer\n{#playbook-possible-password-spray__ul_w1v_zkw_fzb}\n\nMake sure you have installed Security Operations Spoke (sn_sec_spoke).\n\n## Getting started with the Possible Password Spray playbook {#playbook-possible-password-spray__section_nkx_2tw_fzb}\n\n1. Login as a user with sn_si.user and flow_designer roles.\n2. Navigate to All \\> Flow Designer and select the Possible Password Spray playbook.\n3. (Optional) You can create a copy of the Possible Password Spray playbook flow and make the necessary modifications. To create a copy of the playbook's flow  click the ![More actions menu](..\/image\/more-action-menu.png) icon and select Copy flow. Perform this step only if you plan to customize or make specific changes to the flow.  \n   Figure 1. Possible Password Spray playbook ![Overview of the Possible Password Spray playbook](..\/image\/playbook-password-spray.png)\n4. Activate the playbooks.\n   * Activate the main flow to use the playbook available in the base system.\n   * Activate the copied flows after making any modifications according to your requirements.\n   {#playbook-possible-password-spray__ul_qps_ktw_fzb}\n{#playbook-possible-password-spray__ol_ops_ktw_fzb}  \nTrigger Condition: This playbook is triggered and associated with the security incident when the following conditions are met:\n\n* Category is Unauthorized access.\n* Sub-category is Brute force password cracking attempts\n{#playbook-possible-password-spray__ul_mm5_xhv_tkb}\n\nTrigger condition for Possible Password Spray playbook.\n\n* **[Using the Possible Password Spray playbook](..\/..\/..\/product\/security-incident-response\/task\/playbook-using-possible-password-spray.html)**   \n  The following steps give you a walkthrough of the actions  tasks  and subflows that are available in the Possible Password Spray playbook.\n\n**Parent Topic:** [Flow-based Playbooks](..\/..\/..\/product\/security-incident-response\/concept\/playbook-flow-based.html \"Using the Flow Designer  security administrators and flow design authors can more easily transition from manual or undocumented playbooks to automated and repeatable playbooks. The drag-and-drop feature provides flexibility in moving objects  condition checks  parallel branching  decision tables  and more.\")\n\n\"\n\n",
        "QUERY": " 'Flow status is incorrect when there is an Authentication failure during AD password reset'",
        "GROUND_TRUTH": "The issue of incorrect flow status during an Authentication failure during AD password reset arises because the handle error step itself has a success state despite the errors. This happens because the window_error_code is empty, which by design makes the action assume it was successful. Many systems use the error message as informational and only when combined with an error code is it identified as an 'error'. The Flow designer AD spoke cannot assume it is an error as there is no error code. \n\nTo reproduce this issue, you need to configure the Reset AD user password with the AD Spoke installed. Then use the Reset AD user password action and put wrong credentials. The system will say in the step two Authentication failure with the user, but the status of query AD action is success.\n\nThe workaround for this issue is to import the attached XML with admin privileges. However, you should import the XML first and then update the plugin in case you have to do it for any other requirement, as it will override the changes of the imported file."
    },
    {
        "id": 132,
        "CONTEXT": "\"## Repair SLA failing on HR Case\n\n## Description\n\nSLA not getting attached to the sn_hr_core_task if the Short Description field is used as start condition on the SLA.\n\nAlso  when doing the repair for the affected HR task \\& HR case where the SLA is not attached  getting below error message.\nSLARepair.recreateByFilter: No matching Tasks found\n\n## Steps to Reproduce\n\nGo to an HR Case and click on Repair\n\nError: SLARepair.recreateByFilter: No matching Tasks found\n\n## Workaround\n\n1. Navigate to \"Application Restricted Caller Access\"  \n2. Sort by \"Status\"  \n3. Update the relevant records with source \"Script Include:SLARepair\" \\& \"Script Include:SLAConditionBase\" from status \"Requested\" to \"Allowed\"  \n\n**Related Problem: PRB1481877**\n\n\n\n## Create  edit  and delete Application Vulnerability Response remediation task rules\n\n# Create  edit  and delete Application Vulnerability Response remediation task rules {#ariaid-title1}\n\nYou can create rules to automatically group application vulnerable items (AVI) into remediation tasks (AVUL) based on filter conditions. These rules automatically group AVIs as they're imported or manually created.\nIf you create a new rule  it doesn't apply to existing data. After you submit it  it's run against new imports.\n\nRole required: App-Sec Manager\n\n1. Navigate to All \\> Application Vulnerability Response \\> Administration \\> Remediation Task Rules.\n2. Open the rule or select New.\n3. Fill in the fields on the form or edit them.  \n   {#d93845e112}\n   |                  Field                  |                                                                                                                                                                                                                                                                                                                               Description                                                                                                                                                                                                                                                                                                                                |\n   |-----------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n   | Name                                    | Name of the task rule.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |\n   | Active                                  | Indicates whether the task is active.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |\n   | Description                             | Description of the rule.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |\n   | Case sensitive                          | Determines whether a condition is case sensitive or not. Note: The default value is case insensitive.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |\n   | Condition                               | Optional filter conditions for the rule. By default  (Case sensitive check box not selected)  the search text you enter in the condition builder on task rules records and forms isn't case-sensitive. You have the option to enable case-sensitive searches on task records and forms. An example condition is Vulnerability \\> is \\> VULNENT123451 (a known imported vulnerability). Any AVIs that have this vulnerability match this condition.                                                                                                                                                                                                                       |\n   | Group by (up to six condition sets are available)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 ||\n   | Group application vulnerable items from | The table the rule uses to group AVIs. Select the elements from the tree: * Application Vulnerable Item \\[sn_vuln_vulnerable_item\\] * Application Vulnerable Item Configuration Item \\[cmdb_ci\\] * Application Vulnerable Item Application Vulnerability \\[sn_vul_third_party_entry\\] * Application Vulnerable Item Product Model {#avm-create-rt-rule__ul_yhb_ffv_rkb} Note: If you choose an extended table  the Using field is applied only for application vulnerable items that use that extended table.                                                                                                                                                            |\n   | Using field                             | Field on the table that the rule uses to group AVIs. Select conditions from the tree. Note: You can create group by conditions so that not all the AVIs that share data are assigned to the same remediation task. For example  if you select Assignment group and IP Address as the Using fields  and multiple AVIs match the initial filtering condition Vulnerability  these AVIs can be assigned to distinct assignment groups for remediation using the many options available in the Group by section. The Group by conditions give you the flexibility to create distinct remediation tasks that can be assigned to different group even if they share some data. |\n   | Assignment                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        ||\n   | Assign remediation tasks by             | When automatically assigning remediation tasks  the Assignment choice is used in addition to the Group By choices to group the vulnerable items. New tasks are created  as needed  so that each AVI is placed in a task with a matching assignment group set. To automate the assignment of tasks created based on this rule  choose one of the options available. * Group by field: If you selected any user group field from the Using field values in the Group bysection  they appear in the drop-down menu. * User Group: Use the lookup list to select a static user group. {#avm-create-rt-rule__ul_j2b_tmn_rkb}                                                  |\n   Table 1. Remediation Task Rule   {#avm-create-rt-rule__table_g2b_tmn_rkb}\n   if you delete a rule from either the form or list view  you have the option to delete all Open remediation tasks created by that rule. Groups not in the Open state are excluded.\n\n4. Select Submit for new rules.  \n   After you select Submit  your rule is displayed on the Remediation Task Rules list \\[sn_vul_grouping_rule\\]. The following situations initiate your rule.\n   * When new AVIs are created.\n   * When you select Reapply. You select Reapply to evaluate task rules on existing remediation tasks only.\n   * When AVIs are updated  either by you or by the system.  \n     If an AVI is updated  task rules are evaluated for matches to existing remediation tasks. Some common updates that initiate a rules check are:\n     * When the State changes from Closed to Open  or from Closed to Under Investigation.\n     * The configuration item (CI) is changed.\n     * The vulnerability is changed.\n     {#avm-create-rt-rule__ul_ylb_dth_1zb}\n   {#avm-create-rt-rule__ul_xwb_fsh_1zb}\n   The system checks all the task rules for matches to the updated AVI. If the conditions of a rule match the conditions of a remediation task  matching AVIs are assigned to it.\n\n   If no match is found  a new remediation task is created.\n\n\n\n# Create  edit  and delete Application Vulnerability Response remediation task rules {#ariaid-title1}\n\nYou can create rules to automatically group application vulnerable items (AVI) into remediation tasks (AVUL) based on filter conditions. These rules automatically group AVIs as they're imported or manually created.\nIf you create a new rule  it doesn't apply to existing data. After you submit it  it's run against new imports.\n\nRole required: App-Sec Manager\n\n1. Navigate to All \\> Application Vulnerability Response \\> Administration \\> Remediation Task Rules.\n2. Open the rule or select New.\n3. Fill in the fields on the form or edit them.  \n   {#d92830e108}\n   |                  Field                  |                                                                                                                                                                                                                                                                                                                               Description                                                                                                                                                                                                                                                                                                                                |\n   |-----------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n   | Name                                    | Name of the task rule.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |\n   | Active                                  | Indicates whether the task is active.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |\n   | Description                             | Description of the rule.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |\n   | Case sensitive                          | Determines whether a condition is case sensitive or not. Note: The default value is case insensitive.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |\n   | Condition                               | Optional filter conditions for the rule. By default  (Case sensitive check box not selected)  the search text you enter in the condition builder on task rules records and forms isn't case-sensitive. You have the option to enable case-sensitive searches on task records and forms. An example condition is Vulnerability \\> is \\> VULNENT123451 (a known imported vulnerability). Any AVIs that have this vulnerability match this condition.                                                                                                                                                                                                                       |\n   | Group by (up to six condition sets are available)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 ||\n   | Group application vulnerable items from | The table the rule uses to group AVIs. Select the elements from the tree: * Application Vulnerable Item \\[sn_vuln_vulnerable_item\\] * Application Vulnerable Item Configuration Item \\[cmdb_ci\\] * Application Vulnerable Item Application Vulnerability \\[sn_vul_third_party_entry\\] * Application Vulnerable Item Product Model {#avm-create-rt-rule__ul_yhb_ffv_rkb} Note: If you choose an extended table  the Using field is applied only for application vulnerable items that use that extended table.                                                                                                                                                            |\n   | Using field                             | Field on the table that the rule uses to group AVIs. Select conditions from the tree. Note: You can create group by conditions so that not all the AVIs that share data are assigned to the same remediation task. For example  if you select Assignment group and IP Address as the Using fields  and multiple AVIs match the initial filtering condition Vulnerability  these AVIs can be assigned to distinct assignment groups for remediation using the many options available in the Group by section. The Group by conditions give you the flexibility to create distinct remediation tasks that can be assigned to different group even if they share some data. |\n   | Assignment                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        ||\n   | Assign remediation tasks by             | When automatically assigning remediation tasks  the Assignment choice is used in addition to the Group By choices to group the vulnerable items. New tasks are created  as needed  so that each AVI is placed in a task with a matching assignment group set. To automate the assignment of tasks created based on this rule  choose one of the options available. * Group by field: If you selected any user group field from the Using field values in the Group bysection  they appear in the drop-down menu. * User Group: Use the lookup list to select a static user group. {#avm-create-rt-rule__ul_j2b_tmn_rkb}                                                  |\n   Table 1. Remediation Task Rule   {#avm-create-rt-rule__table_g2b_tmn_rkb}\n   if you delete a rule from either the form or list view  you have the option to delete all Open remediation tasks created by that rule. Groups not in the Open state are excluded.\n\n4. Select Submit for new rules.  \n   After you select Submit  your rule is displayed on the Remediation Task Rules list \\[sn_vul_grouping_rule\\]. The following situations initiate your rule.\n   * When new AVIs are created.\n   * When you select Reapply. You select Reapply to evaluate task rules on existing remediation tasks only.\n   * When AVIs are updated  either by you or by the system.  \n     If an AVI is updated  task rules are evaluated for matches to existing remediation tasks. Some common updates that initiate a rules check are:\n     * When the State changes from Closed to Open  or from Closed to Under Investigation.\n     * The configuration item (CI) is changed.\n     * The vulnerability is changed.\n     {#avm-create-rt-rule__ul_ylb_dth_1zb}\n   {#avm-create-rt-rule__ul_xwb_fsh_1zb}\n   The system checks all the task rules for matches to the updated AVI. If the conditions of a rule match the conditions of a remediation task  matching AVIs are assigned to it.\n\n   If no match is found  a new remediation task is created.\n\n\"\n\n",
        "QUERY": " 'Error: SLARepair.recreateByFilter: No matching Tasks found'",
        "GROUND_TRUTH": "The error \"SLARepair.recreateByFilter: No matching Tasks found\" occurs when the SLA is not getting attached to the sn_hr_core_task if the Short Description field is used as start condition on the SLA. This error can be seen when you go to an HR Case and click on Repair. To resolve this issue, you can follow the workaround provided in the article. \n\n1. Navigate to \"Application Restricted Caller Access\".\n2. Sort by \"Status\".\n3. Update the relevant records with source \"Script Include:SLARepair\" & \"Script Include:SLAConditionBase\" from status \"Requested\" to \"Allowed\".\n\nThis should help in resolving the error and attaching the SLA to the sn_hr_core_task."
    },
    {
        "id": 133,
        "CONTEXT": "\"## \"Grandchild\" schedules (children of children schedules) are not recognized in the overall schedule\n\n## Description\n\nA schedule can have a child schedule  and that child schedule's entries are included as part of the overall schedule. For example  the base system instance has a \"Holidays\" child schedule; however  the \"grandchild\" schedules (a child of a child schedule) are not taken into account.\n\n## Steps to Reproduce\n\n1. Ensure that a SLA is attached to an incident to ensure proper functionality of the demo instance:\n   1. Create a new incident with impact of 1 and urgency of 3 (this results in a priority of 3). Click **Save** and check the task SLA at the bottom.\n   2. Adjust the related list to display the field **Planned end time**.\n   3. Note the planned end time for later comparison.\n2. Modify the SLA and add the schedule:\n   1. Go to **Schedules** and create a new schedule (for example  NEW test schedule).\n   2. Add a schedule entry with the following parameters: (not listed fields stay with their default value)   \n      **Name:** Workday 8-17   \n      **Show as:** Busy  \n      **When:** Use the given dates but change first time to **08:00:00** and the second one to **17:00:00** .  \n      **Repeats:** \"Every Weekday (Mon-Fri)\"\n   3. Click **Submit**.\n   4. Go to **SLA Definitions** and open **Priority 3 resolution (5 day)**.\n   5. Adjust the field **Schedule** to use the just-created schedule (NEW test schedule) and update the record.\n3. Test the first adjustments:\n   1. Create a new incident with a priority of 3 (see 1)  save it. Check the related list task SLA at the bottom of the form.\n   2. Compare the planned end time of this incident to the first one and note the difference.\n   3. Note that this time it reflects the applied schedule of weekdays 8-17.\n4. Add a holiday to the schedule and check with a new incident:\n   1. Go to **Schedules** . Open the NEW test schedule record and add another schedule entry.   \n      Parameters for this entry: (not listed fields stay with their default value)   \n      **Name:** holiday  \n      **Type:** Excluded   \n      **Show as:** Free  \n      **When:** Check the box for **All day**. Select a date which is 2-3 days in the future and is a weekday (Mon-Fri).\n   2. Click **Submit** to save the new entry.\n   3. Create a new incident with a priority of 3 (see step 1). Save it and check the planned end time of the active SLA within the related list Task SLA at the bottom of the form.\n   4. Compare with the previous ticket  and note that it is reflecting the holiday as the date now one day in the future.\n5. Remove the holiday and recreate as the child schedule:\n   1. Go to **Schedules** . Open **NEW test schedule** and delete the schedule entry **holiday**.\n   2. Switch to the related list **Child Schedules** and click **New**.\n   3. Name this **NEW child schedule** and save the record (not submit in order to stay on the form).\n   4. Create a new entry within the related list **Scheduled entries** : (not listed fields stay with their default value)   \n      - Name: **holiday**   \n      - Type **Excluded**   \n      - Show as: **Free**   \n      - When: Check the box for **All day**. Select a date which is 2-3 days in the future and is a weekday (Mon-Fri).\n   5. Click **Submit** to save the new entry.\n   6. Create a new incident with a priority of 3 (see step 1). Save it and check the planned end time of the active SLA within the related list Task SLA at the bottom of the form.\n   7. Compare with the previous ticket and note that it is also reflecting the holiday and the date is the same.\n6. Remove the holiday from child schedule and recreate in next recursion level:\n   1. Go to **Schedules** . Open **NEW child schedule** and delete the schedule entry **holiday**.\n   2. Switch to the related list**Child Schedules** and click **New**.\n   3. Name it **NEW childs' child schedule** and save the record (not submit in order to stay on the form).\n   4. Create a new entry within the related list Scheduled entries: (not listed fields stay with their default value)   \n      **Name:** holiday  \n      **Type:** Excluded   \n      **Show as:** Free   \n      **When:** Check the box for **All day**. Select a date which is 2-3 days in the future and is a weekday (Mon-Fri).\n   5. Hit **Submit** to save the new entry.\n   6. Create a new incident with a priority of 3 (see step 1). Save it and check the planned end time of the active SLA within the related list Task SLA at the bottom of the form.\n   7. \n\nCompare with the previous ticket and note that it is now one day earlier than the previous two tests.\n   8. Note that the holiday  even though it is configured properly  is not reflected by the SLA calculation.\n\n## Workaround\n\nThere is currently no workaround for this issue. As per Using Schedules in product documentation:\n\n\"Note: Schedules only include schedule entries from a parent and its direct child schedules. Schedule entries from a child schedule of another child schedule are not included in a parent schedule. For example  if schedules B and C both have schedule A as their parent schedule  then the schedule entries for both schedules B and C are included in schedule A. However  if the parent schedule of schedule C is schedule B  the schedule entries for schedule C are excluded from schedule A.\"\n\nThis means  the system is behaving as expected. That is  duration is always calculated based on the parent and child and not to arbitrary levels.\n\n**Related Problem: PRB581690**\n\n\n\n## Grandfathered and exempted tables\n\n[Administer the Now Platform](..\/..\/..\/administer\/general\/concept\/intro-now-platform-landing.html \"As a platform administrator  you have the power of the Now Platform at your fingertips. The Now Platform is an application platform as a service that automates business processes across the enterprise.\") \\> [Getting started on the Now Platform](..\/..\/..\/administer\/general\/concept\/get-started-now-platform.html \"Now Platform is the exclusive ServiceNow platform. It provides a range of options to improve and automate your business processes.\") \\> [Subscription Management v2](..\/..\/..\/administer\/subscription-management\/reference\/subscription-management-landing-page-v2.html \"Subscription Management enables you to proactively manage your subscriptions and monitor subscription usage on your instances.\") \\> [Using Subscription Management](..\/..\/..\/administer\/subscription-management\/concept\/using-subscription-management-v2.html \"Use Subscription Management to manage how purchased subscriptions are used on your instances.\") \\>\n\n# Grandfathered and exempted tables {#ariaid-title1}\n\nGrandfathered and exempted custom tables are treated differently than other custom tables on your instance. Please refer to your organization's ServiceNow\u00ae contract for additional information.\n\n## Grandfathered tables {#grandfathered-and-exempt-tables-v2__section_hhx_zt1_3nb}\n\nDepending on your organization's legacy subscription entitlements and contractual migration  grandfathered tables may be allotted to your production instance. To preserve an organization's previous custom table entitlements through the license migration  a grandfathered table entitlement may be provided to the production instance. This one-time process ensures that custom tables that currently exist in a production instance don't count against your new custom table entitlements.\n\nGrandfathered custom tables may not be transferred  reused  or otherwise classified as another custom table type. If you delete a grandfathered custom table  you can't classify any other custom table as a grandfathered custom table to take its place. Once a custom table has been associated with a grandfathered table subscription  it remains in the Custom Table Inventory even if the table is deleted from the instance.\n\nTo view your grandfathered tables  enter ua_custom_table_inventory.list in the filter navigator  and then add a filter with the condition \\[Allotment type\\] \\[is\\] \\[Grandfather\\].\n\n## Grandfathered tables on an instance {#grandfathered-and-exempt-tables-v2__example_prf_ky1_3nb}\n\nScenario: Your organization purchases three new subscriptions with entitlements for a total of 55 App Engine Starter custom tables (formerly Bundled Custom Tables). To ensure your organization doesn't lose the previous custom table entitlements  a grandfather subscription with an entitlement for 25 grandfathered tables is allotted to your instance. This subscription requires the admin to identify and map the existing 25 tables. These subscriptions result in a total custom table entitlement of 80. Once a custom table has been assigned to the grandfather subscription there is no option to revert this.\n{#d139091e96}\n\n|        Subscription        | Number of custom table entitlements | Number of mappable tables |\n|----------------------------|-------------------------------------|---------------------------|\n| Grandfathered subscription | 25                                  | 25                        |\n| New Subscription 1         | 25                                  | 25                        |\n| New Subscription 2         | 25                                  | 25                        |\n| New Subscription 3         | 5                                   | 5                         |\nTable 1. Multiple subscriptions with grandfathered and custom table entitlements{#grandfathered-and-exempt-tables-v2__table_qrf_ky1_3nb}\n\n## Exempted tables {#grandfathered-and-exempt-tables-v2__section_jfz_ky1_3nb}\n\nExempted tables are custom tables that do not count towards your subscription entitlements. These tables consist mostly of custom extensions of certain system-related tables  such as sys_portal_page and sys_user_preference  that you extend for operational improvements in your production instance.\n\nTo view a list of exempted tables  enter ua_exempted_table_inventory.list in the filter navigator.\nThe [Custom Table Guide legal schedule](https:\/\/www.servicenow.com\/content\/dam\/servicenow-assets\/public\/en-us\/doc-type\/legal\/custom-table-guide.pdf) provides the complete definition of which tables are considered exempt.\nNote: Legal schedules are enforced based on when the contract is executed.To find past versions of the Custom Table Guide  see the [Legal Schedules Custom Table Guide](https:\/\/www.servicenow.com\/schedules-archived.html).\n**Parent Topic:** [Managing custom tables and applications in Subscription Management](..\/..\/..\/administer\/subscription-management\/concept\/allocating-custom-tables-subscr-apps-v2.html \"Manage and track custom table entitlements on your instances using Subscription Management.\")\n\n\"\n\n",
        "QUERY": " 'Entries from grandchild schedules are not considered'",
        "GROUND_TRUTH": "According to the KB12345 article, entries from grandchild schedules are not considered in the overall schedule. This is because the system only includes schedule entries from a parent and its direct child schedules. Schedule entries from a child schedule of another child schedule are not included in a parent schedule. This means that the duration is always calculated based on the parent and child and not to arbitrary levels. The article also confirms that there is currently no workaround for this issue."
    },
    {
        "id": 134,
        "CONTEXT": "\"## Scheduled job \"SAM - Subscription Maintenance\" can't run properly in SAM Foundation because SAMPRese\n\n## Description\n\nScheduled job \"SAM - Subscription Maintenance\" can't run properly in SAM Foundation so that it fails to update entitlement status\n\n## Steps to Reproduce\n\nGo to the scheduled job \"SAM - Subscription Maintenance\". Click \"Execute Now\".\n\n## Go to System logs and observe that there is warning as below:\n\norg.mozilla.javascript.EcmaError: \"SAMPReserveEntitlementUtil\" is not defined.\nCaused by error in \\<refname\\> at line 10\n\n## 7: }\n\n8:\n9: function sampUpdateStateStatus() {\n==\\> 10: SAMPReserveEntitlementUtil.processExpiredReserveEntitlements();\n11:\n12: var today = gs.daysAgo(0);\n13: var grEnt = new GlideRecord('alm_license');\n\n## Workaround\n\nComment out the line below under the function sampUpdateStateStatus() of the \"SAM - Subscription Maintenance\" Scheduled Script Execution:\n\nSAMPReserveEntitlementUtil.processExpiredReserveEntitlements();\n\n**Related Problem: PRB1475316**\n\n\n\n## SAMP Reconciliation fails with error 'TypeError: Cannot read property \"install_condition\" from null'\n\n## Description\n\n* The SAMP reconciliation fails after 100% progress due to a race condition when the Import User Subscriptions job is also running at the same time.\n* The progress summary shows:\n\nTypeError: Cannot read property \"install_condition\" from null\n\nat sys_script_include.444d8294c32222006081face81d3aebf.script:514 (anonymous)\nat sys_script_include.444d8294c32222006081face81d3aebf.script:266 (anonymous)\nat sys_script_include.444d8294c32222006081face81d3aebf.script:247 (anonymous)\nat sys_script_include.30bbdf9587f52300923aa75fe5cb0b97.script:413 (anonymous)\nat sys_script_include.30bbdf9587f52300923aa75fe5cb0b97.script:400 (anonymous)\nat sys_script_include.6761b0dd0b1232001a17650d37673a77.script:868 (anonymous)\nat sys_script_include.6761b0dd0b1232001a17650d37673a77.script:824 (anonymous)\nat sys_script_include.6761b0dd0b1232001a17650d37673a77.script:54 (anonymous)\nat sys_trigger.f7006c6e471e59d0e1ce8a12736d4378:1\n\nPossibly related problems:\n[Reconciliation errors in the form 'Cannot read property \\<someSysId\\> from undefined' caused by SamNewPerCoreForMicrosoftLicenseCalculator loading all optimization records and Per Core VM allocations missing relationships](https:\/\/support.servicenow.com\/kb_view.do?sysparm_article=KB1227294 \"Reconciliation errors in the form 'Cannot read property <someSysId> from undefined' caused by SamNewPerCoreForMicrosoftLicenseCalculator loading all optimization records and Per Core VM allocations missing relationships\")\n[Workday reconciliation is failing if subscriptions are pulled from SSO or custom integration profile](https:\/\/support.servicenow.com\/kb_view.do?sysparm_article=KB1316426 \"Workday reconciliation is failing if subscriptions are pulled from SSO or custom integration profile\")\n\n## Steps to Reproduce\n\n1. Schedule the Reconciliation job to run at the same time as the SAM - Import User Subscriptions job.  \n   2. Check Recon Results for recon with status 'Failed' and Progress summary 'Reconciliation Complete'.  \n   3. Check the progress summary for the TypeError: cannot read property 'install_condition' from null.\n\n## Workaround\n\n- Use Case 1 -  \nMake sure the Import User Subscription and Software Reconciliation jobs are never scheduled to run at the same time.\n- Use Case 2 -\n\nIf you have verified both Import User Subscription and Software Reconciliation are running at different times  then consider changing the \"**SamAllocationSuiteEngine** \" Script Include ( \/sys_script_include.do?sys_id=444d8294c32222006081face81d3aebf ) to add the code below line at 435  so that the final code looks like this:\n\ngetSubscriptionSuitesOrComponents: function(publisherId  getSuites) {\n\/\/ First get all the licensable software models of this publisher\nvar subscriptionModels = \\[\\];\nvar subscriptionGa = new SampAggregate('samp_sw_subscription');\nsubscriptionGa.addQuery('software_model.manufacturer'  publisherId);\nsubscriptionGa.addNotNullQuery('user');\nsubscriptionGa.addNotNullQuery('licensable_software_model'); --- We are filtering record where licensable_software_model is empty.\nsubscriptionGa.groupBy('licensable_software_model');\nsubscriptionGa.query();\n\nNOTE: The reconciliation job processes the valid record where you already have a valid licensable_software_model value and show the data. This same job also populates the licensable_software_model value for the records where licensable_software_model is empty. Thus the records skipped in the current run will be automatically processed in the next run.\n\nif you see any record with licensable_software_model as empty and publisher as empty  then those records will not be processed. Those do not have any direct impact on the job execution. This needs to be investigated separately.\n\n**Related Problem: PRB1610760**\n\n\n\n## Existing reclamation rule of Office 365\/M365 and Adobe doesnt is not updated with M2M products\n\n## Description\n\nIn vancouver release   we have new products of Adobe Creative Cloud and M365 subscriptions added to reclamation rule M2M productsto create optimization candidates based on these products usage. On upgrade from utah \\[For Adobe  Microsoft Integration\\] or tokyo \\[For Microsoft Integration\\] release to higher release  these products need to be added automatically to existing reclamation rule 'Creative Cloud' and 'M365' \\[ Office 365 changes to M365 name\\] which is not happening.\n\n## Steps to Reproduce\n\n1. Have an instance on utah release with Microsoft + Adobe integration setup.  \n2. Execute import subscriptions job which auto creates reclamation rules and pulls subscriptions.  \n3. Verify in samp_m2m_rule_product table for Reclamation Rule 'Creative Cloud' and Rule 'Office 365' only products which are supported in utah release exists.  \n4. Now upgrade the instance to Vancouver release . Verify above reclamation rules and its associated products in M2M should contain all Creative Cloud + its component products and the rule name changes from Office 365 to M365 should contain additional products of M365 as well. \\[Which doesn't happen\\]  \n\nCurrent behaviour - Creative Cloud components are not added. Additional M365 products are also not added.\n\nExpected - Creative Cloud components should be added. Additional M365 products are also should be added.\n\n## Workaround\n\n1. Either apply the updateset attached or make changes manually as mentioned below since the updateset attached is compatible with Vancouver Patch 1.\n\nSearch for the function **getExistingReclamationRule** in script include **'SAMSaasFixScriptUtil' (https:\/\/\\<INSTANCENAME\\>.service-now.com\/sys_script_include.do?sys_id=e38b3023532011107f77ddeeff7b12a9)**and replace it as mentioned below:\n\nIn the existing function definition **getExistingReclamationRule**change the higlighted text:\n\nm2mReclamationRuleGr.addQuery(\n'reclamation_rule.applies_to' \n**'subscription_software'**\n);\n\nReplace text **'subscription_software'** with**'Subscription Software'**\n\n2. Once the updateset\/changes are made. Please run the Fix Scripts manually which corrects the reclamation rule and adds additional products for reclamation rule 'Creative Cloud' and 'Microsoft 365'.\n\nFix scripts to run - a. **SAM - Fix Adobe CC Reclamations. b. SAM - Fix Microsoft 365 reclamations.**\n\n3. Once done please validate the reclamation candidates are created as expected.  \nNote: Please note that applying the Workaround is treated as a customization and corresponding record ( Name: sys_script_include_e38b3023532011107f77ddeeff7b12a9 ) is created in the Customer Update `[sys_update_xml]` table where the Replace on upgrade field is set to false.\n\nPlease refer to the [product documentation](https:\/\/docs.servicenow.com\/en-US\/bundle\/utah-application-development\/page\/build\/system-update-sets\/task\/t_OverwriteCustomizsDuringUpgrades.html) to overwrite this customization when upgraded to a fixed version later. During the upgrade process the skipped upgrades should be reviewed and reverted for any records in the \"sys_script_include\" table related to the **\"Software Asset Management - SaaS License Management (sn_sam_saas_int)\"** . Not overwriting this customization can prevent from getting any latest updates made on the corresponding record in later releases.\n\n**Related Problem: PRB1715174**\n\n\"\n\n",
        "QUERY": " 'EcmaError while running SAM subscription maintenance job'",
        "GROUND_TRUTH": "The 'EcmaError while running SAM subscription maintenance job' occurs when the scheduled job \"SAM - Subscription Maintenance\" can't run properly in SAM Foundation, causing it to fail to update entitlement status. The error message you're seeing, \"org.mozilla.javascript.EcmaError: \"SAMPReserveEntitlementUtil\" is not defined\", is caused by an issue in the script at line 10. \n\nTo resolve this issue, you can apply the workaround provided in the article. You need to comment out the line \"SAMPReserveEntitlementUtil.processExpiredReserveEntitlements();\" under the function sampUpdateStateStatus() of the \"SAM - Subscription Maintenance\" Scheduled Script Execution. This should allow the job to run properly. \n\nPlease note that this is a workaround and not a permanent solution. The related problem PRB1475316 is linked to this issue, and you may want to follow up on that for a more permanent resolution."
    },
    {
        "id": 135,
        "CONTEXT": "\"## Downloaded attachment with encrypted text redirects to page  \"Requested attachment does not exist\"\n\n## Description\n\nAfter impersonating from the admin to a user role  an attempt to export an attachment that has encryption text as xls\/pdf for incident or any other table results in a successful download. However  when opening the downloaded file  the error \"Requested attachment does not exist\" occurs.\n\n## Steps to Reproduce\n\nLog in to an instance as an admin.\n\nActivate the Encryption plugins.\n\nCreate an Encryption context and assign to the admin role.\n\nFor more information  see the product documentation topic [Encryption support](https:\/\/docs.servicenow.com\/csh?topicname=c_EncryptionSupport.html&version=latest).4.\n\nGo to the incident table configuration and make a file (for example  Description) an Encrypted Text field.\n\nImpersonate a user who has an access to a table list such as the incident table.\n\nGo to incident-list.do.\n\nRight-click in the banner  then choose **Export \\> Excel**.\n\nAfter the download is complete  click **Download** to download the file.\n\nCheck the sys_attachment table.\n\nNote that the attachment exists (the attachment is against the sys_poll table).\n\n## Workaround\n\nYou can choose from various workarounds:\n\nDon't use impersonation when exporting data with encrypted-text column. Instead  log in as the user.\n\nSet the **glide.encryption.export_encrypted_data.allowed** property to false\n\nCreate a special decrypt_attachment role that includes the Encrypted Export Attachment context  and add that role to the impersonating user.\n\n**Note** -- In order to be able to see and add that context  disable the \"Hide system contexts\" business rule on sys_encryption_context first.\n\n**Related Problem: PRB1268370**\n\n\n\n## San Diego Patch 7b: Known Errors\n\nSan Diego Patch 7b : Known Errors\n\nThis article presents notable known errors in San Diego Patch 7b  grouped by severity and organized by category. Not all known errors appear on this page. Click the link in the Article column to view details about each known error. To view other families and versions  see the [Known Error Portal](\/kb_view.do?sysparm_article=KB0597477 \"Known Error Portal\").\n\n**Note**: The severity level does not indicate the order in which issues will be fixed  which is determined by a combination of variables.\n\nTo view the release notes for this release family  see San Release Notes( \"San\u00a0 Release Notes\").\n\n# Severity 1\n\nThe following table lists notable Severity 1 known errors in this patch.\n\n|                      Article                       |  Problem   |                 Category                 |                                                                                      Title                                                                                       |\n|----------------------------------------------------|------------|------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| [KB0861651](\/kb_view.do?sysparm_article=KB0861651) | PRB1434839 | Microsoft SMS\/SCCM Integration           | SCCM 2016\/2012 v2\/2007 populates cmdb_ci_pc_hardware instead of cmdb_ci_computer  which differs from Discovery                                                                   |\n| [KB0965271](\/kb_view.do?sysparm_article=KB0965271) | PRB1505867 | PDF Generation                           | The font cache is occupying memory in the heap  which causes memory contention                                                                                                   |\n| [KB0996495](\/kb_view.do?sysparm_article=KB0996495) | PRB1512640 | Language and Translations                | Customised sys_ui_messages are overwritten after upgrade                                                                                                                         |\n| [KB0996745](\/kb_view.do?sysparm_article=KB0996745) | PRB1523413 | PDF Generation                           | HTML2PDF API generated PDF documents and Security Incident PIR reports are removed by scheduled job \"Clean up converted documents genereated by PDF Generation Utilities plugin\" |\n| [KB1208278](\/kb_view.do?sysparm_article=KB1208278) | PRB1547401 | Persistence                              | Gateway database pool can't be instantiated at node startup                                                                                                                      |\n| [KB1156965](\/kb_view.do?sysparm_article=KB1156965) | PRB1596539 | Import \/ Export                          | The import set API fails with a '403 - Failed API level ACL Validation' error if the role 'import_admin' isn't assigned                                                          |\n| [KB1168974](\/kb_view.do?sysparm_article=KB1168974) | PRB1596558 | Antivirus Scanning                       | Invalid requests to antivirus servers impact attachment functionality                                                                                                            |\n| [KB1182003](\/kb_view.do?sysparm_article=KB1182003) | PRB1611377 | Configuration Management Database (CMDB) | CMDB Baseline creation job causing app node out of memory when a CI has a huge number of records referencing it                                                                  |\n| [KB1194249](\/kb_view.do?sysparm_article=KB1194249) | PRB1615702 | Orchestration                            | Orchestration: The Tibco JMS test connection doesn't work when using a custom connectionFactory name                                                                             |\n| [KB1218131](\/kb_view.do?sysparm_article=KB1218131) | PRB1632463 |                                          | batchedGlideAjax can set sysparm_aggregation_size wrongly  and when it ends up a huge number can cause AJAXXMLHttpAggregator to run the app node out-of-memory                   |\n# Severity 2\n\nThe following table lists notable Severity 2 known errors in this patch.\n\n|                      Article                       |  Problem   |                 Category                 |                                                                                                           Title                                                                                                           |\n|----------------------------------------------------|------------|------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| [KB0686716](\/kb_view.do?sysparm_article=KB0686716) | PRB1268370 | Platform Security                        | Downloaded attachment redirects to page \"Requested attachment does not exist\" after impersonating to a user for the attachment which has encryption context                                                               |\n| [KB0870572](\/kb_view.do?sysparm_article=KB0870572) | PRB1437726 | Human Resources Service Management       | WCAG 4.1.2 Campaign Content Library - when the value is announced in VoiceOver  the role is needed for users to understand the context                                                                                    |\n| [KB0870579](\/kb_view.do?sysparm_article=KB0870579) | PRB1457469 | Appsee - Seismic                         | Seismic VA dashboard - 'Conversation' table not shows data after sorting by a topic                                                                                                                                       |\n| [KB0870585](\/kb_view.do?sysparm_article=KB0870585) | PRB1458043 | Application Navigator \\& Banner Frame    | When added to the visible UI  both the current Application and the current Update Set select lists have no accessible label                                                                                               |\n| [KB0870590](\/kb_view.do?sysparm_article=KB0870590) | PRB1458929 | Performance Analytics                    | \\[High chart upgrade 8.2.2\\] Heatmap and geomap visualization are broken                                                                                                                                                  |\n| [KB0870896](\/kb_view.do?sysparm_article=KB0870896) | PRB1459399 | Agent Workspace                          | Process Automation Designer (PAD) - Condition Builder fields are not populated in Modify Condition Widget in PAD Application                                                                                              |\n| [KB0966593](\/kb_view.do?sysparm_article=KB0966593) | PRB1477967 | UI Builder                               | Client Scripts do not save when using UIB in Firefox                                                                                                                                                                      |\n| [KB0962393](\/kb_view.do?sysparm_article=KB0962393) | PRB1483305 | Discovery                                | 'HorizontalDiscoveryProbe' has multiple mutex locks                                                                                                                                                                       |\n| [KB1064739](\/kb_view.do?sysparm_article=KB1064739) | PRB1483528 | Scheduled Job Processing                 | Job scheduler is clogged by interactive jobs because of leaked child jobs                                                                                                                                                 |\n| [KB0966596](\/kb_view.do?sysparm_article=KB0966596) | PRB1485591 | Lists                                    | Simple - List component is not available in the Dashboard Builder                                                                                                                                                         |\n| [KB0959468](\/kb_view.do?sysparm_article=KB0959468) | PRB1489024 | MID Server                               | In Quebec : MID Server upgrade stops if PowerShell tests in Pre-upgrade check failed  PowerConsole remains busy  and closing the PowerShell session fails                                                                 |\n| [KB0961414](\/kb_view.do?sysparm_article=KB0961414) | PRB1490701 | VA-Designer                              | The default custom chat experience in Virtual Agent is set to inactive after upgrading                                                                                                                                    |\n| [KB0966598](\/kb_view.do?sysparm_article=KB0966598) | PRB1492041 | Mobile Platform                          | Mobile Task button actions on WOT form page do not refresh the form once the Task has been accepted\/rejected                                                                                                              |\n| [KB0993213](\/kb_view.do?sysparm_article=KB0993213) | PRB1492353 | Platform Licensing                       |\n\n## NowAttachmentUploadConfiguration init(tableName: String  recordSysId: SysID  fileName: String  conte\n\n# NowAttachmentUploadConfiguration structure - iOS {#ariaid-title1}\n\nThe NowAttachmentUploadConfiguration structure enables you to define the configuration information for an attachment that you are uploading to your ServiceNow instance.\n{#d468535e70}\n\n|       Name        |  Type  |                                                                                                            Description                                                                                                            |\n|-------------------|--------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| contentType       | String | HTTP data content type.                                                                                                                                                                                                           |\n| encryptionContext | String | Optional. Sys_id of an encryption context record. Specify this parameter to allow only users with the specified encryption context to access the attachment. Default: Attached file is not encrypted with any encryption context. |\n| fileName          | String | Name of the attachment file.                                                                                                                                                                                                      |\n| recordSysId       | String | Sys_id of the record on the ServiceNow instance to associate the attachment with.                                                                                                                                                 |\n| tableName         | String | Name of the table on the ServiceNow instance that contains the record specified in the recordSysid parameter.                                                                                                                     |\nTable 1. Properties{#NowAttachUploadConfigiOSStruct__table_vx2_klw_5pb}\n**Parent Topic:** [Mobile SDK API reference - iOS](..\/..\/..\/..\/..\/..\/app-store\/dev_portal\/API_reference\/MobileSDKiOS\/concept\/MobileSDKiOSAPI.html \"The Mobile SDK for iOS provides the classes necessary to interface iOS-based devices with the ServiceNow platform.\")\n\n## NowAttachmentUploadConfiguration init(tableName: String  recordSysId: SysID  fileName:\n\nString  contentType: String  encryptionContext: String?) {#ariaid-title2}\n\nSets the upload configuration values for the specified attachment.\n{#d468535e274}\n\n|       Name        |  Type  |                                                                                                            Description                                                                                                            |\n|-------------------|--------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| tableName         | String | Name of the table on the ServiceNow instance that contains the record specified in the recordSysid parameter.                                                                                                                     |\n| recordSysId       | String | Sys_id of the record on the ServiceNow instance to associate the attachment with.                                                                                                                                                 |\n| fileName          | String | Name of the attachment file.                                                                                                                                                                                                      |\n| contentType       | String | HTTP data content type.                                                                                                                                                                                                           |\n| encryptionContext | String | Optional. Sys_id of an encryption context record. Specify this parameter to allow only users with the specified encryption context to access the attachment. Default: Attached file is not encrypted with any encryption context. |\nTable 2. Parameters{#NAUC-init_S_S_S_S_S__table_shs_cgn_mqb}\n{#d468535e373}\n\n| Type | Description |\n|------|-------------|\n| None |             |\nTable 3. Returns{#NAUC-init_S_S_S_S_S__table_ths_cgn_mqb}\nThe following code example shows how to call this function.\n\n\"\n\n",
        "QUERY": " 'Downloaded attachment redirects to page \"Requested attachment does not exist\" after impersonating to a user for the attachment which has encryption context'",
        "GROUND_TRUTH": "The issue of a downloaded attachment redirecting to the page \"Requested attachment does not exist\" after impersonating a user for the attachment which has an encryption context occurs when an admin impersonates a user role and attempts to export an attachment that has encryption text as xls\/pdf for an incident or any other table. The download is successful, but when opening the downloaded file, the error \"Requested attachment does not exist\" occurs. \n\nTo workaround this issue, you can choose from the following options:\n\n1. Don't use impersonation when exporting data with encrypted-text column. Instead, log in as the user.\n2. Set the \"glide.encryption.export_encrypted_data.allowed\" property to false.\n3. Create a special decrypt_attachment role that includes the Encrypted Export Attachment context, and add that role to the impersonating user. Note that in order to be able to see and add that context, you need to disable the \"Hide system contexts\" business rule on sys_encryption_context first.\n\nThis issue is also listed as a known error (PRB1268370) in the San Diego Patch 7b."
    },
    {
        "id": 136,
        "CONTEXT": "\"## NowAnalyticsService - deleteCurrentUserData(completion: @escaping ((_ success: Bool) -> Void))\n\n# NowAnalyticsService protocol - iOS {#ariaid-title1}\n\nThe NowAnalyticsService protocol provides functions that enable you to configure analytics properties  user settings  and events for managing the collection of user analytics data.\n\nAn object conforming to this protocol is returned by `sharedAnalyticsService`. Use it in your application to perform API calls.\n{#d522418e73}\n\n|      Name       |  Type   |                                                                                                                                                                                                                 Description                                                                                                                                                                                                                  |\n|-----------------|---------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| trackingConsent | Boolean | Flag that indicates whether the user has consented to analytics tracking for the associated device. By default  devices are opted-out. Explicitly setting this value to `false` will immediately finish any ongoing session and delete the locally recorded data. Valid values: * true: User consented to anlaytics data being tracked on this device. * false: User denied data tracking. {#NowAnalyticsServiceiOSProtocol__ul_flc_ngx_5pb} |\nTable 1. Requirements{#NowAnalyticsServiceiOSProtocol__table_vx2_klw_5pb}\n**Parent Topic:** [Mobile SDK API reference - iOS](..\/..\/..\/..\/..\/..\/app-store\/dev_portal\/API_reference\/MobileSDKiOS\/concept\/MobileSDKiOSAPI.html \"The Mobile SDK for iOS provides the classes necessary to interface iOS-based devices with the ServiceNow platform.\")\n\n## NowAnalyticsService - addEvent(named eventName: String  with properties: \\[String:\n\nAny\\]?) {#ariaid-title2}\n\nAdds an application event  such as a user reaching a specific level or screen  and enables the setting of custom properties on the event. These events appear on the dashboard in the order that they occurred.\nThese events appear in your analytics dashboard.\n{#d522418e234}\n\n|      Name       |  Type  |                                                                                                                                                                                                                   Description                                                                                                                                                                                                                    |\n|-----------------|--------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| named eventName | String | Name of the event to add.                                                                                                                                                                                                                                                                                                                                                                                                                        |\n| with properties | Array  | Optional. Custom property key-value pairs for the event. Property keys may not contain the dot ('.') or dollar ('$') signs. They will be trimmed. Supported value types: * Date * NSNull * NSNumber * Strings * URL {#NAnalyServiOs-addEvent_S_A__ul_g41_kxy_spb} Note: The total size of the eventName  properties key and value should not exceed 300 bytes (per event). Strings are UTF-8 encoded. Events that exceed this limit are ignored. |\nTable 2. Parameters{#NAnalyServiOs-addEvent_S_A__table_hwl_5vy_spb}\n{#d522418e311}\n\n| Type | Description |\n|------|-------------|\n| None |             |\nTable 3. Returns{#NAnalyServiOs-addEvent_S_A__table_iwl_5vy_spb}\nThe following example shows how to add a \"Successful Login\" event and an \"Open Case\" event with properties.\n\n## NowAnalyticsService - addScreenAction(named actionName: String) {#ariaid-title3}\n\nAdds a custom action to the current screen. These actions appear in the user dashboard as part of the session data and describes a screen change in an application.\n{#d522418e397}\n\n|       Name       |  Type  |                                           Description                                           |\n|------------------|--------|-------------------------------------------------------------------------------------------------|\n| named actionName | String | Name of the action to add to the screen  such as MyButtonClick. Maximum length: 256 UTF-8 bytes |\nTable 4. Parameters{#NAnalyServiOS-addScreenAction_S__table_lxc_hzy_spb}\n{#d522418e442}\n\n| Type | Description |\n|------|-------------|\n| None |             |\nTable 5. Returns{#NAnalyServiOS-addScreenAction_S__table_mxc_hzy_spb}\n\n## NowAnalyticsService - appendToUserProperty(named propertyName: String  listItem:\n\nString) {#ariaid-title4}\n\nAppends the specified item to the specified user property list.\n{#d522418e532}\n\n|        Name        |  Type  |                        Description                        |\n|--------------------|--------|-----------------------------------------------------------|\n| named propertyName | String | Name of the user property list to append the listItem to. |\n| listItem           | String | List item to append to the property.                      |\nTable 6. Parameters{#NAnalyServiOS-appendToUserProp_S_S__table_uhf_jd1_tpb}\n{#d522418e587}\n\n| Type | Description |\n|------|-------------|\n| None |             |\nTable 7. Returns{#NAnalyServiOS-appendToUserProp_S_S__table_vhf_jd1_tpb}\n\n## NowAnalyticsService - deleteCurrentUserData(completion: @escaping ((_ success: Bool) -\\>\n\nVoid)) {#ariaid-title5}\n\nDeletes all analytics data associated with the current user. This method also unsets the current active user and opts this device out of future tracking.\nTo set the current user  use the [setUserId()](NowAnalyticsServiceiOSProtocol.html#NAnalyServiOS-setUserId_S \"Sets the application's user ID for the current user. Pass nil to log out the current user.\") method.\n{#d522418e683}{#NAnalyServiOS-deleteCurrentUserData_O__mobilesdkiOS-completion_data_dataerror-row}\n\n|    Name    |                  Type                  |                                                                                                                                                                              Description                                                                                                                                                                              |\n|------------|----------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| completion | @escaping ((_ success: Bool) -\\> Void) | Completion handler to execute after the analytics data is deleted. Return values for the completion handler: * Success: Returns a Boolean value of true. * Failure: Void - Failure may occur if the Appsee servers cannot be reached  as when there is no connectivity. If failure occurs  retry the method. {#NAnalyServiOS-deleteCurrentUserData_O__ul_uzx_thj_ppb} |\nTable 8. Parameters{#NAnalyServiOS-deleteCurrentUserData_O__table_akt_yry_spb}\n{#d522418e733}\n\n| Type | Description |\n|------|-------------|\n| None |             |\nTable 9. Returns{#NAnalyServiOS-deleteCurrentUserData_O__table_bkt_yry_spb}\n\n## NowAnalyticsService - incUserProperty(named propertyName: String  by value: Int) {#ariaid-title6}\n\nIncrements or decrements the value of the specified numeric property by the specified value.\n{#d522418e820}\n\n|        Name        |  Type   |                                    Description                                     |\n|--------------------|---------|------------------------------------------------------------------------------------|\n| named propertyName | String  | Name of the user property to increment.                                            |\n| by value           | Integer | Value to increment the property by. Enter a negative value to decrement the value. |\nTable 10. Parameters{#NAnalyServiOS-incUserProperty_S_I__table_fkc_chz_spb}\n{#d522418e872}\n\n| Type | Description |\n|------|-------------|\n| None |             |\nTable 11. Returns{#NAnalyServiOS-incUserProperty_S_I__table_gkc_chz_spb}\n\n\n\n## NowAnalyticsService - removeUserProperty(named propertyName: String)\n\n# NowAnalyticsService protocol - iOS {#ariaid-title1}\n\nThe NowAnalyticsService protocol provides functions that enable you to configure analytics properties  user settings  and events for managing the collection of user analytics data.\n\nAn object conforming to this protocol is returned by `sharedAnalyticsService`. Use it in your application to perform API calls.\n{#d522418e73}\n\n|      Name       |  Type   |                                                                                                                                                                                                                 Description                                                                                                                                                                                                                  |\n|-----------------|---------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| trackingConsent | Boolean | Flag that indicates whether the user has consented to analytics tracking for the associated device. By default  devices are opted-out. Explicitly setting this value to `false` will immediately finish any ongoing session and delete the locally recorded data. Valid values: * true: User consented to anlaytics data being tracked on this device. * false: User denied data tracking. {#NowAnalyticsServiceiOSProtocol__ul_flc_ngx_5pb} |\nTable 1. Requirements{#NowAnalyticsServiceiOSProtocol__table_vx2_klw_5pb}\n**Parent Topic:** [Mobile SDK API reference - iOS](..\/..\/..\/..\/..\/..\/app-store\/dev_portal\/API_reference\/MobileSDKiOS\/concept\/MobileSDKiOSAPI.html \"The Mobile SDK for iOS provides the classes necessary to interface iOS-based devices with the ServiceNow platform.\")\n\n## NowAnalyticsService - addEvent(named eventName: String  with properties: \\[String:\n\nAny\\]?) {#ariaid-title2}\n\nAdds an application event  such as a user reaching a specific level or screen  and enables the setting of custom properties on the event. These events appear on the dashboard in the order that they occurred.\nThese events appear in your analytics dashboard.\n{#d522418e234}\n\n|      Name       |  Type  |                                                                                                                                                                                                                   Description                                                                                                                                                                                                                    |\n|-----------------|--------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| named eventName | String | Name of the event to add.                                                                                                                                                                                                                                                                                                                                                                                                                        |\n| with properties | Array  | Optional. Custom property key-value pairs for the event. Property keys may not contain the dot ('.') or dollar ('$') signs. They will be trimmed. Supported value types: * Date * NSNull * NSNumber * Strings * URL {#NAnalyServiOs-addEvent_S_A__ul_g41_kxy_spb} Note: The total size of the eventName  properties key and value should not exceed 300 bytes (per event). Strings are UTF-8 encoded. Events that exceed this limit are ignored. |\nTable 2. Parameters{#NAnalyServiOs-addEvent_S_A__table_hwl_5vy_spb}\n{#d522418e311}\n\n| Type | Description |\n|------|-------------|\n| None |             |\nTable 3. Returns{#NAnalyServiOs-addEvent_S_A__table_iwl_5vy_spb}\nThe following example shows how to add a \"Successful Login\" event and an \"Open Case\" event with properties.\n\n## NowAnalyticsService - addScreenAction(named actionName: String) {#ariaid-title3}\n\nAdds a custom action to the current screen. These actions appear in the user dashboard as part of the session data and describes a screen change in an application.\n{#d522418e397}\n\n|       Name       |  Type  |                                           Description                                           |\n|------------------|--------|-------------------------------------------------------------------------------------------------|\n| named actionName | String | Name of the action to add to the screen  such as MyButtonClick. Maximum length: 256 UTF-8 bytes |\nTable 4. Parameters{#NAnalyServiOS-addScreenAction_S__table_lxc_hzy_spb}\n{#d522418e442}\n\n| Type | Description |\n|------|-------------|\n| None |             |\nTable 5. Returns{#NAnalyServiOS-addScreenAction_S__table_mxc_hzy_spb}\n\n## NowAnalyticsService - appendToUserProperty(named propertyName: String  listItem:\n\nString) {#ariaid-title4}\n\nAppends the specified item to the specified user property list.\n{#d522418e532}\n\n|        Name        |  Type  |                        Description                        |\n|--------------------|--------|-----------------------------------------------------------|\n| named propertyName | String | Name of the user property list to append the listItem to. |\n| listItem           | String | List item to append to the property.                      |\nTable 6. Parameters{#NAnalyServiOS-appendToUserProp_S_S__table_uhf_jd1_tpb}\n{#d522418e587}\n\n| Type | Description |\n|------|-------------|\n| None |             |\nTable 7. Returns{#NAnalyServiOS-appendToUserProp_S_S__table_vhf_jd1_tpb}\n\n## NowAnalyticsService - deleteCurrentUserData(completion: @escaping ((_ success: Bool) -\\>\n\nVoid)) {#ariaid-title5}\n\nDeletes all analytics data associated with the current user. This method also unsets the current active user and opts this device out of future tracking.\nTo set the current user  use the [setUserId()](NowAnalyticsServiceiOSProtocol.html#NAnalyServiOS-setUserId_S \"Sets the application's user ID for the current user. Pass nil to log out the current user.\") method.\n{#d522418e683}{#NAnalyServiOS-deleteCurrentUserData_O__mobilesdkiOS-completion_data_dataerror-row}\n\n|    Name    |                  Type                  |                                                                                                                                                                              Description                                                                                                                                                                              |\n|------------|----------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| completion | @escaping ((_ success: Bool) -\\> Void) | Completion handler to execute after the analytics data is deleted. Return values for the completion handler: * Success: Returns a Boolean value of true. * Failure: Void - Failure may occur if the Appsee servers cannot be reached  as when there is no connectivity. If failure occurs  retry the method. {#NAnalyServiOS-deleteCurrentUserData_O__ul_uzx_thj_ppb} |\nTable 8. Parameters{#NAnalyServiOS-deleteCurrentUserData_O__table_akt_yry_spb}\n{#d522418e733}\n\n| Type | Description |\n|------|-------------|\n| None |             |\nTable 9. Returns{#NAnalyServiOS-deleteCurrentUserData_O__table_bkt_yry_spb}\n\n## NowAnalyticsService - incUserProperty(named propertyName: String  by value: Int) {#ariaid-title6}\n\nIncrements or decrements the value of the specified numeric property by the specified value.\n{#d522418e820}\n\n|        Name        |  Type   |                                    Description                                     |\n|--------------------|---------|------------------------------------------------------------------------------------|\n| named propertyName | String  | Name of the user property to increment.                                            |\n| by value           | Integer | Value to increment the property by. Enter a negative value to decrement the value. |\nTable 10. Parameters{#NAnalyServiOS-incUserProperty_S_I__table_fkc_chz_spb}\n{#d522418e872}\n\n| Type | Description |\n|------|-------------|\n| None |             |\nTable 11. Returns{#NAnalyServiOS-incUserProperty_S_I__table_gkc_chz_spb}\n\n\n\n## NowAnalyticsSDK - deleteCurrentUserData()\n\n# NowAnalyticsSDK interface - Android {#ariaid-title1}\n\nThe NowAnalyticsSDK interface provides functions that enable you to configure analytics properties  user settings  and events for managing a collection of user analytics data.\n{#d306977e68}\n\n|     Name      |                                                                                                                   Type                                                                                                                   |                 Description                  |\n|---------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------|\n| configuration | [NowServiceConfiguration](..\/..\/NowServiceConfiguration\/concept\/NowServiceConfigurationAndroidAPI.html \"The NowServiceConfiguration class enables you to configure the ServiceNow instance URL and package name for a feature service.\") | Configuration to associate with the service. |\nTable 1. Properties{#NowAnalyticsAndroidInterface__table_vx2_klw_5pb}\n**Parent Topic:** [Mobile SDK API reference - Android](..\/..\/..\/..\/..\/..\/app-store\/dev_portal\/API_reference\/MobileSDKAndroid\/concept\/MobileSDKAndroidAPI.html \"The Mobile SDK for Android provides the classes necessary to interface Android devices with the ServiceNow platform.\")\n\n## NowAnalyticsSDK - addEvent(eventName: String) {#ariaid-title2}\n\nAdds an application event  such as a user reaching a specific level or screen. These events appear on the dashboard in the order that they occurred.\n{#d306977e230}\n\n|      Name       |  Type  |        Description        |\n|-----------------|--------|---------------------------|\n| named eventName | String | Name of the event to add. |\nTable 2. Parameters{#NAnaly-addEvent_S__table_ikl_dhm_tpb}\n{#d306977e270}\n\n| Type | Description |\n|------|-------------|\n| None |             |\nTable 3. Returns{#NAnaly-addEvent_S__table_jkl_dhm_tpb}\nThe following code example shows how to call this function to add an application event.\n\n## NowAnalyticsSDK - addEvent(eventName: String  props: MutableMap\\<String  Any\\>) {#ariaid-title3}\n\nAdds an application event  such as a user reaching a specific level or screen  and enables the setting of custom properties on the event. These events appear on the dashboard in the order that they occurred.\n{#d306977e356}\n\n|   Name    |                                            Type                                             |                                                                                                                                                                                                                  Description                                                                                                                                                                                                                  |\n|-----------|---------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| eventName | String                                                                                      | Name of the event to add.                                                                                                                                                                                                                                                                                                                                                                                                                     |\n| props     | [MutableMap](https:\/\/kotlinlang.org\/api\/latest\/jvm\/stdlib\/kotlin.collections\/-mutable-map\/) | Custom property key-value pairs for the event. The total size of the event name  property key and value cannot exceed 300 bytes per property. Properties that exceed this limit are ignored. Property keys may not contain dot ('.') or dollar ('$') signs  if they do  they are trimmed. Strings are UTF-8 encoded. Supported value types: * Integer * Double * Float * String * Url * Boolean * Date {#NAnaly-addEvent_S_S__ul_g41_kxy_spb} |\nTable 4. Parameters{#NAnaly-addEvent_S_S__table_ps4_j3m_tpb}\n{#d306977e438}\n\n| Type | Description |\n|------|-------------|\n| None |             |\nTable 5. Returns{#NAnaly-addEvent_S_S__table_qs4_j3m_tpb}\nThe following code example shows how to call this function to add an application event that includes custom properties for the event.\n\n## NowAnalyticsSDK - addScreenAction(actionName: String) {#ariaid-title4}\n\nAdds a custom action. These actions appear in the user dashboard as part of the session data and describes a screen change in an application.\n{#d306977e525}\n\n|    Name    |  Type  |                           Description                           |\n|------------|--------|-----------------------------------------------------------------|\n| actionName | String | Name of the action to add to the screen  such as MyButtonClick. |\nTable 6. Parameters{#NAnaly-addScreenAction_S__table_mcv_4km_tpb}\n{#d306977e568}\n\n| Type | Description |\n|------|-------------|\n| None |             |\nTable 7. Returns{#NAnaly-addScreenAction_S__table_ncv_4km_tpb}\nThe following code example shows how to call this function to add a custom action to the current screen.\n\n## NowAnalyticsSDK - appendToUserProperty(propertyName: String  item: String) {#ariaid-title5}\n\nAppends the specified item to the specified user property list.\n{#d306977e654}\n\n|     Name     |  Type  |                      Description                      |\n|--------------|--------|-------------------------------------------------------|\n| propertyName | String | Name of the user property to append the list item to. |\n| item         | String | List item to append to the property.                  |\nTable 8. Parameters{#NAnaly-appendToUserProperty_S_S__table_hnp_wgj_5pb}\n{#d306977e706}\n\n| Type | Description |\n|------|-------------|\n| None |             |\nTable 9. Returns{#NAnaly-appendToUserProperty_S_S__table_inp_wgj_5pb}\nThe following code example shows how to call this function to append a case to the specified list.\n\n## NowAnalyticsSDK - configure(instanceURL: URL  configureCallbacks: ConfigureCallbacks?) {#ariaid-title6}\n\nConfigures the specified ServiceNow instance URL and callbacks to NowAnalytics.\nOnce this method is called  the actual configuration is fetched from the associated ServiceNow instance and synced automatically.\n{#d306977e802}\n\n|        Name        |        Type        |                                                                                                Description                                                                                                 |\n|--------------------|--------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| instanceURL        | URL                | URL of the ServiceNow instance to associate with the NowAnalyticfs service.                                                                                                                                |\n| configureCallbacks | ConfigureCallbacks | Optional. Callback to execute whenever there is an error communicating to the ServiceNow instance. This callback can be called at any point in time and can be called multiple times for multiple reasons. |\nTable 10. Parameters{#NAnaly-configure_S_O__table_zhj_yhj_5pb}\n{#d306977e860}\n\n| Type | Description |\n|------|-------------|\n| None |             |\nTable 11. Returns{#NAnaly-configure_S_O__table_a3j_yhj_5pb}\nThe following code example shows how to call this function to configure the specified instance.\n\n## NowAnalyticsSDK - deleteCurrentUserData() {#ariaid-title7}\n\nDeletes all local and remote data for the current user.\nThis method also unsets the current active user and opts this device out of future analytics tracking.\nNote: This method performs a synchronous call to the SNAnalytics servers. \"\n\n",
        "QUERY": " 'disable user analytics'",
        "GROUND_TRUTH": "Based on the information in KB12345, you can disable user analytics by deleting all analytics data associated with the current user. This can be done using the `deleteCurrentUserData(completion: @escaping ((_ success: Bool) -> Void))` method in the NowAnalyticsService protocol for iOS or the `deleteCurrentUserData()` method in the NowAnalyticsSDK interface for Android. These methods not only delete all analytics data but also unset the current active user and opt the device out of future tracking. Please note that for the iOS method, a completion handler is executed after the analytics data is deleted, which returns a Boolean value of true upon success."
    },
    {
        "id": 137,
        "CONTEXT": "\"## How to troubleshoot ACL issues for the mobile applets?\n\n1. Login to the instance on Desktop\n2. Go to transaction logs\n3. Add the filter as URL \\| contains \\| \/sg\n4. Reproduce the issue on the Mobile app and identify the transaction that contains the document ID.  \n   Example transaction : \/api\/now\/sg\/document?DocumentId=5375bda8db552450c10d1681399619f8\\&api=api\n5. Open REST API Explorer from the Filter navigator\n6. Select the API name and the method as shown in the screenshot\n7. Under 'Request body'  click on 'Raw' tab\n8. Enter the follwing code and replace the DocumentId with appropriate value.  \n   Sample code:\n   {   \"SubmittedForm\": null    \"Context\": {   \"ContextType\": \"GLOBAL\"   }    \"DocumentId\": \"d2852ec567f82300b270c3105685ef10\"   }\n\n9. Click on 'Send'and click on 'OK' on the alert. Analyze the Response body.\n10. To execute a request for a particular user  launch the REST API Explorer in a distinct window.\n11. On the previous window  impersonate the affected user\n12. Run the request in the Rest API explorer\n13. Enable debug logs and run the API explorer code to check different log results for analysis.\n\n#### **NOTE:**\n\n1. Replace the DocumentID with the sys_id of the Applet that is found in the transaction logs\n2. When utilizing the sample code in the API explorer  ensure that you retype the double quotes; otherwise  running the code may result in errors.\n\n\n\n## Agent Chat & Advanced Work Assignment Troubleshooting Guide\n\nIf work item reject reason is 'No Access'  the issue is caused by ACLs or business rules as described in step 4 above.\n6. Consider the following potential root cases:\n   1. Check whether the WorkloadResponder was triggered for the specific interaction (see [*KB1220747*](https:\/\/support.servicenow.com\/kb?id=kb_article_view&sysparm_article=KB1220747)*- SECTION* *A9* on how to query using a specific interaction ID). In [CSTASK667020](https:\/\/support.servicenow.com\/nav_to.do?uri=sn_customerservice_task.do?sys_id=ddd0f6cac3e1f5d07268428dc001315a%26sysparm_view=case)  chats were not assigning to some agents due the WorkloadResponder onExit method not executing  resulting in issues with agent capacity (see *SCENARIO 3* for more details on resolving issues with agent capacity).\n   2. Troubleshoot AWA group caching. Due to certain jobs or transactions running as domain of ACL-constrained users  it may corrupt the cache. Check if setting the com.snc.awa.cache.enabled to false alleviates the issue.\n   3. ([KB0955191](https:\/\/support.servicenow.com\/kb?id=kb_article_view&sysparm_article=KB0955191)) If live agent routing is done via Virtual Agent context variables  troubleshoot setup.\n\n### SCENARIO 3: Agent capacity is not being updated correctly.\n\n1. Verify if behavior is an actual issue.\n   1. A work item counts towards an agent's capacity if it adheres to the utilization conditions of the channel.\n   2. ([KB0823303](https:\/\/support.servicenow.com\/kb?id=kb_article_view&sysparm_article=KB0823303)) Any custom business rules on awa_agent_capacity will not trigger.\n2. Check that the WorkloadResponder exists and is active. The WorkloadResponder is the mechanism that is responsible for updating agent capacity.\n3. ([KB0779869](https:\/\/support.servicenow.com\/kb?id=kb_article_view&sysparm_article=KB0779869)) Trigger agent capacity recalculation by running the 'AWA Daily Cleanup' job  which recalculates capacity for all agents.\n   1. ([PRB1644770](https:\/\/support.servicenow.com\/nav_to.do?uri=problem.do?sys_id=ae5fe80adb9da950d860e6be1396196b%26sysparm_view=prb_view)) Beginning in Vancouver  the job was configured to run every 5 minutes. For instances before Vancouver  consider changing the frequency of how often the job runs. If this customer also has fewer than 10 000 entries in awa_agent_presence_capacity database view  it is recommended to have the job run every 30 minutes. If more like 2 000 or less  it can be configured to run every 5 minutes.\n   2. Trigger recalculation for a specific agent by going to awa_agent_capacity and clearing out the value in the 'Capacity in use' field.\n4. ([KB0869459](https:\/\/support.servicenow.com\/kb?id=kb_article_view&sysparm_article=KB0869459)) Check if the record watcher (sys_rw_action) has the same condition as the corresponding service channel.\n   1. Record watcher responders provide a way for the ServiceNow platform to notice when records in a table are updated and take actions defined in java code.\n\n### SCENARIO 4: Cannot connect to live agent when instance has domain separation enabled.\n\n1. ([KB1517304](https:\/\/support.servicenow.com\/kb?id=kb_article_view&sysparm_article=KB1517304) -- Utah) Problems with connecting to a live agent on a domain-separated instance.\n\n### *SCENARIO 5: Records in chat-related tables are growing and consuming a lot of memory.*\n\n1. ([KB0954363](https:\/\/support.servicenow.com\/kb?id=kb_article_view&sysparm_article=KB0954363)) Use table cleanup (sys_auto_flush) to limit conversation data growth.\n\n### *SCENARIO 6: Customer wants to allow chats to get in queue when no agents are available.*\n\n1. ([KB1517701](https:\/\/support.servicenow.com\/kb?id=kb_article_view&sysparm_article=KB1517701)) Allow chats get into queue when no agents available by creating a dummy agent group and a dummy presence state.\n\n## FINAL STEPS\n\n*If you have exhausted all the resources listed in this KB  consider the following actions before filing a case task to our team.*\n\n### FINAL STEP 1: Check if Issue is a Known PRB\n\n*Refer to Appendix 1: Recommended Upgrades \\& Known Issues for a list of PRBs that have been fixed or are being fixed. The section also contains a list of versions where the development team backported significant changes and\/or improvements.*\n\n### FINAL STEP 2: Ask Customer to Enable Enhanced Logging\n\n*There are three types of logging configurations that can be enabled to help with debugging. Consider asking customers to enable one or more of these configurations depending on the issue.*\n\n1. ([DOCS](https:\/\/docs.servicenow.com\/bundle\/vancouver-servicenow-platform\/page\/administer\/advanced-work-assignment\/task\/awa-activate-logging.html)) Enable logging via `sys_logger_configuration`.\n   * Configures which level of logging (using the standard logging framework) shows up in the node logs.\n   * Using this can also be helpful to have logs that usually show up in Splunk also show up in syslog. This can be done by setting the destination to \"Database\".\n   * *TO ENABLE: Adding `com.snc.awa` and\/or `com.glide.cs`* *to `sys_logger_configuration.list`*\n2. ([KB0786263](https:\/\/support.servicenow.com\/kb?id=kb_article_view&sysparm_article=KB0786263)) Enable AWA Event logging in `awa_service_channel`.\n   * Collects a detailed picture of work item milestones  and the details about the environment at the time those milestones were reached (e.g.  when a work item is routed  what were the values in the interaction and the conditions on the queue that was selected)\n   * The AWA Event logs are generally meant to only be enabled temporarily when trying to answer certain questions like \"Why are these items routing to that queue?\" or \"Why are they offered to that agent?\".\n   * *TO ENABLE: Go to a specific service channel where the issue occurs and enable the 'Enable Logging' field. \n\n## AIS | AI Search general troubleshooting and debugging\n\n# Description {#DESCRIPTION}\n\nThis article talks about the basic troubleshooting and debugging steps for working on AIS issues\n\n# Troubleshooting and Debugging AI Search {#LOGICAL_ARCHITECTURE}\n\n1. In order to turn on the logs  From the filter navigator **System Diagnostics \\> Debug AI Search** You can see similar debug messages below  \n![](sys_attachment.do?sys_id=41eb4dc14732bd502c31b98a436d4392)\n2. You could check for logs by navigating to **AI Search \\> All** or **AI Search \\> Ingestion Warnings \\& Errors.** Refer [AI Search logging and debugging](https:\/\/docs.servicenow.com\/bundle\/quebec-platform-administration\/page\/administer\/ai-search\/concept\/logging-debugging-ais.html \"AI Search logging and debugging\")\n3. Use [Search Preview UI](https:\/\/docs.servicenow.com\/bundle\/rome-platform-administration\/page\/use\/dashboards\/application-content-packs\/ai-search-preview.html#ais-search-preview-admin-tools \"Search Preview UI\") by installing the application [Advanced AI Search Management Tools](https:\/\/docs.servicenow.com\/bundle\/quebec-platform-administration\/page\/use\/dashboards\/application-content-packs\/install-adv-ais-mgmt-tools.html \"Advanced AI Search Management Tools\")and once the app is installed Navigate to **Ai Serch \\> Preview \\> Search Preview**\n3. a. Choose the search profile on the dropdown on the top left\n\n3.b. In the field which says 'Search here' enter the search term \\> Hit 'Search' button\n\n4. check localhost logs as well.\n5. If you would like to test the search query for an NLU model  you could search from \"Scripts - background' and check the results and debug logs\n\n\/\/Enter the solution name you are testing in the line #1 like ml_x_snc_sn_km_mr_global_ais_gra_b8bc699a67621010b3d782f45685efa5\n\nvar geniusSearchSolutionName = 'enter the solution name';\n\nvar searchQuery = new sn_ml.GeniusSearchQuery(\"abraham lincoln\");\n\nvar searchContext = new sn_ml.GeniusSearchContext({\"session\" : \"\"});\n\nvar geniusSearchSolution = sn_ml.GeniusSearchSolutionStore.get(geniusSearchSolutionName)\n\ngeniusSearchSolution = sn_ml.GeniusSearchSolutionStore.get(geniusSearchSolutionName);\n\nvar result = geniusSearchSolution.search(searchQuery  searchContext  {});\n\ngs.print(JSON.stringify(JSON.parse(result)  false  4));\n\n6. Along with the Search Preview UI  you can use the below script to debug the results with search profile or narrow down the search issue. **The AISASearchUtil API below is unsupported and should only be used for debugging purposes.**\n\n\/\/Make changes to the searchParams based on your requirements.\n\/\/Just need to change searchContextConfigId and searchTerm to get started.\n\nsearchParams = {\nsearchContextConfigId: '00731b9d5b231010d9a5ce1a8581c7dd'     \/\/sys_id of the Search Application configuration\nsys_search_context_config record that you want to use for searching.\nsearchTerm: '***'               \/\/ The search term *** returns all records that have been indexed.\npaginationToken: ''              \/\/ Leaving paginationToken empty returns the data for the first page. To get the paginationToken for 2nd page  click the next page button on the portal after doing the search and check the URL for paginationToken.\ndisableSpellCheck: false \nfacetFilters:  \nsearchFilters:  \nrequestedFields: {} \nrpSysId: 'test'                    \/\/ Just a dummy value that is needed for the script to work.\n};\n\nvar aiSearchUtil = new AISASearchUtil();\nvar result = aiSearchUtil.search(searchParams);\n\nvar searchResult = result.data.search.searchResults;\n\ngs.info(searchResult.length);\n\nfor (var i=0; i<searchResult.length; i++){\ngs.info(searchResult\ni\n.title + \" | \" + searchResult\ni\n.text);\n}\n\ngs.info(JSON.stringify(result));\n\n\/\/Note: Ensure that search sources have been indexed before trying to search.\n\n# Common spot checks to perform if AI Search does not display search results: {#PHYSICAL_ARCHITECTURE}\n\n1. Check if AI Search is enabled '[glide.ais.enabled](https:\/\/aitraining.service-now.com\/sys_properties.do?sys_id=a3f1bb94ff031010b7b67677d53bf1a0&sysparm_record_target=sys_properties&sysparm_record_row=3&sysparm_record_rows=39&sysparm_record_list=nameCONTAINSais%5EORDERBYname)' should be set to true. and also check **AI Search \\> Connection \\> Test Connection**\n\nIf there are any connection issues  please reach out to Tech Support\n2. If the Genius results are not being returned check if '[glide.ais.genius_result.enabled](https:\/\/aitraining.service-now.com\/sys_properties.do?sys_id=e6bcb3831b0b7010704084c2604bcbf5&sysparm_record_target=sys_properties&sysparm_record_row=1&sysparm_record_rows=1&sysparm_record_list=nameCONTAINSgenius%5EORDERBYname)' is set to '**true** ' \n\n2.a. Check if facet is clicked and then genius result is not displayed then check 'glide.ais.genius_result.enabled_with_facet_filter' to false\n\n2.b. If for any non-English language Genius results are not working  because it's not supported. Genius results is supported only for English even though AI Search supports several languages Refer [Internationalization support for AI Search](https:\/\/docs.servicenow.com\/bundle\/rome-platform-administration\/page\/administer\/ai-search\/concept\/international-language-support-ais.html#international-language-support-ais \"Internationalization support for AI Search\")\n\n2.c. If the Genius result is not working check if there is an EVAM configuration is added for the corresponding Genius result. For example Peoplefind Genius result will not work unless you add one manually in **sys_ux_composite_data_template_predicate_bundle**   See [EVAM Configuration](https:\/\/docs.servicenow.com\/bundle\/rome-servicenow-platform\/page\/administer\/evam\/task\/define-view-configuration-bundle.html \"EVAM Configuration\")\n\n3. If the expected results are not displayed \\> then check the following\n\n3.a. Check if the correct '**Search Profile**' is being linked\n\n3.b. Check if the '**Search Profile** ' is '**Published**' if not publish again and retest\n\n4. If the expected results are not present  check the '**Search Source** ' filter condition. For example  if a 'search source' filters out certain records  then those records would never show up in the results. Other reasons would be **ACL**   **Domain** **Separation**   **User** **Criteria and Before Query Business Rules**.\n5. If expected number of results are not displayed  go to the **Search application configuration** (sys_search_context_config) and check the **search** **results** **limit**\n\n# Other issues with AI Search: {#DESCRIPTION}\n\n1. Check if Edge encryption  Encryption support is enabled since this is not supported as of Quebec\n\"\n\n",
        "QUERY": " 'debugging acl issue'",
        "GROUND_TRUTH": "To debug ACL issues, follow these steps as outlined in KB12345:\n\n1. Login to the instance on Desktop.\n2. Navigate to transaction logs.\n3. Add the filter as URL | contains | \/sg.\n4. Reproduce the issue on the Mobile app and identify the transaction that contains the document ID.  \n5. Open REST API Explorer from the Filter navigator.\n6. Select the API name and the method as shown in the screenshot.\n7. Under 'Request body' click on 'Raw' tab.\n8. Enter the following code and replace the DocumentId with the appropriate value.  \n9. Click on 'Send' and click on 'OK' on the alert. Analyze the Response body.\n10. To execute a request for a particular user, launch the REST API Explorer in a distinct window.\n11. On the previous window, impersonate the affected user.\n12. Run the request in the Rest API explorer.\n13. Enable debug logs and run the API explorer code to check different log results for analysis.\n\nNote: Replace the DocumentID with the sys_id of the Applet that is found in the transaction logs. When utilizing the sample code in the API explorer, ensure that you retype the double quotes; otherwise, running the code may result in errors."
    },
    {
        "id": 138,
        "CONTEXT": "\"## False error \"User <user id> with mid_server role not associated with a MID Server\" issues records  w\n\n## Description\n\nIf the upper\/lowercase of the Username of the MID Server's login user is not identical in the sys_user record  and config.xml file  then wrong MID Server Issues records are created. Usernames are not case sensitive  so this should not cause an issue  and the MID Server will otherwise be working fine.\n\nMID Server's MIDUserConnectivity scheduled job runs every 4 hours  and will create MID Server issues \\[ecc_agent_issue\\] records for:\n\nIssue source: MIDUserConnectivity\nShort description: User \\<user id\\> with mid_server role not associated with a MID Server. No login attempts within reporting period.\n\nWhen Event Management Self-Health Monitoring is enabled  this will also create false Events \\[em_event\\]  Alerts \\[em_alert\\] and Incidents  causing a considerable nuisance to customers.\nEvent \\[em_event\\] record:\n\nDescription: There is an error in the MID server: User \\<user id\\> with mid_server role not associated with a MID Server. No login attempts within reporting period.\\<\/description\\>\nMessage key: MID Server Metric Alert_MID Servers_undefined - MIDUserConnectivity\nMetric name: undefined - MIDUserConnectivity\nResource: MID Servers\nSource: EMSelfMonitoring\nType: SelfMonitoring\n\n## Steps to Reproduce\n\n1. Add a new instance user  giving it the mid_server role  and user name: MIDServerUser\n2. Set up a new mid server entering the username as: MidserverUSER\n3. (Optionally also enable enable Event Management Self-Health Monitoring)\n4. Run the MIDUserConnectivity scheduled job to run\n5. Check the ecc_agent_issue table (and em_event)\n\nTo get the script to run with debug  run with 2 in the MIDUserConnectivity function  to tell the constructor to turn on debug:\nnew MIDUserConnectivity(2).checkConnectivity(4 \\* (60 \\* 60 \\* 1000));\nOutput:\n...\n\\*\\*\\* Script: Log Object: MIDUserConnectivity: activeUserIDsWithMIDServerRole\n\\[0\\]: string = MIDServerUser\n\\*\\*\\* Script: MIDUserConnectivity: no MIDServers are down\n\\*\\*\\* Script: Log Object: configuredMIDServerUserIDs\n\\[0\\]: string = MidserverUSER\n\\*\\*\\* Script: Log Object: MIDUserConnectivity: activeUserIDsWithMIDServerRoleNotAssociatedWithConfiguredMIDServer\n\\[0\\]: string = MIDServerUser\n...\n\n**Expected behaviour:**\nServicenow user IDs are not case sensitive  so MIDServerUser and MidserverUSER are the same thing  and should not be triggering an issue\/event.\n\n**Actual behaviour:**\nThe MIDUserConnectivity script include is doing case sensitive comparisons  so is not matching up the users with mid_server role  with the ecc_agent.user_name field  which is derived from the config.xml parameter mid.instance.username  as it was entered when the MID Server was set up.\nThe MID Server logs in and works fine  but these fails alerts are caused.\n\n## Workaround\n\nThis problem is currently under review and targeted to be fixed in a future release. Subscribe to this Known Error article to receive notifications when more information will be available.\n\n**Related Problem: PRB1707499**\n\n\n\n## MID log flooded with warning - \"Affinity ID 'No Source' got converted via IPAddress to 'null'\"\n\nIt is an expected message as the customer is using CyberArk credentials and the IP address is Null\n\nBelow are the sample DEBUG logs from the MID Server\n\n2023-12-04 16:15:38 WARN (Worker-Standard:HorizontalDiscoveryProbe-58a12ed733b6f9985eb6001fad5c7b06) \\[StandardCredentialsProvider:232\\] Affinity ID 'No Source' got converted via IPAddress to 'null'\n2023-12-04 16:15:38 DEBUG (Worker-Standard:HorizontalDiscoveryProbe-94a12ed733b6f9985eb6001fad5c7bc8) \\[Events:113\\] Completed dispatching event: 'GenericCounterMetricEvent'\n2023-12-04 16:15:38 DEBUG (Worker-Standard:HorizontalDiscoveryProbe-58a12ed733b6f9985eb6001fad5c7b06) \\[StandardCredentialsProvider:241\\] Using a high-security credential: linux_discovery_infosys\/ssh\/null\n2023-12-04 16:15:38 DEBUG (Worker-Standard:HorizontalDiscoveryProbe-9ca16ed733b6f9985eb6001fad5c7b64) \\[ExecutionContextImpl:716\\] (162)ExecutionContextImpl - setAttribute(pattern_runtime_mode horizontal)\n2023-12-04 16:15:38 DEBUG (Worker-Standard:HorizontalDiscoveryProbe-1ca12ed733b6f9985eb6001fad5c7bc4) \\[Events:105\\] Dispatching event: 'GenericCounterMetricEvent' to 42 registered listeners\n2023-12-04 16:15:38 DEBUG (Worker-Standard:HorizontalDiscoveryProbe-9ca16ed733b6f9985eb6001fad5c7b64) \\[ExecutionContextImpl:716\\] (162)ExecutionContextImpl - setAttribute(pattern Azure - Storage Container(LP))\n2023-12-04 16:15:38 WARN (Worker-Standard:HorizontalDiscoveryProbe-58a12ed733b6f9985eb6001fad5c7b06) \\[StandardCredentialsProvider:232\\] Affinity ID 'No Source' got converted via IPAddress to 'null'\n2023-12-04 16:15:38 DEBUG (Worker-Standard:HorizontalDiscoveryProbe-94a12ed733b6f9985eb6001fad5c7bc8) \\[Events:105\\] Dispatching event: 'GenericCounterMetricEvent' to 42 registered listeners\n2023-12-04 16:15:38 DEBUG (Worker-Standard:HorizontalDiscoveryProbe-9ca16ed733b6f9985eb6001fad5c7b64) \\[CommandsRunner:69\\] (162)CommandsRunner - input: ; commands count: 1\n2023-12-04 16:15:38 DEBUG (Worker-Standard:HorizontalDiscoveryProbe-1ca12ed733b6f9985eb6001fad5c7bc4) \\[Events:113\\] Completed dispatching event: 'GenericCounterMetricEvent'\n2023-12-04 16:15:38 DEBUG (Worker-Standard:HorizontalDiscoveryProbe-9ca16ed733b6f9985eb6001fad5c7b64) \\[Events:105\\] Dispatching event: 'GenericCounterMetricEvent' to 42 registered listeners\n\n==\\> Below is the snippet of the code(backend java code) that is throwing the message.\n\n225 \/\/If this is a \"high-security\" cred  i.e. a credential ID is in record in lieu of username\/password\/sshkey\n226 if (credential.isUseHighSecurity()) {\n227 \/\/Wrap this guy up with the IP address info; user\/pass will come from CredentialResolver\n228 if (ipAddress == null) {\n229 \/\/If affinity ID is a hostname or dotted notation  this covers it:\n230 ipAddress = getIPAddressAsString(affinityId);\n231 if (StringUtil.nil(ipAddress))\n232 fLogger.warn(\"Affinity ID '\" + affinityId + \"' got converted via IPAddress to '\" + ipAddress + \"'\");\n233 }\n234\n235 if (fSerialized) {\n236 MIDServerSerializer serializer = new MIDServerSerializer();\n237 credential = new HighSecurityCredential(credential  ipAddress  serializer::format  serializer::unformat);\n\n\/\/If this is a \"high-security\" cred  i.e. a credential ID is in record in lieu of username\/password\/sshkey\n\n==\\> The above line means the credential ID in a record which is instead of username\/password\/sshkey then it is treated as high-security credentials and if you look at the If condition it is checking for IP address is Null.\n\n\n\n## MID Server Script Files get skipped in instance\/app upgrades  after Checksum values are automaticall\n\n## Description\n\nThe fix for a pair of security related product defects added in Utah (PRB1608860\/PRB1588253)  adds values to the checksum field of all MID Server Script File \\[ecc_agent_script_file\\] records where the record has no attachment and the script is in the record field.\nThis causes those records to be skipped in future upgrades as Customizations.\n\nThe impact of that is\n\n* customers don't get potentially important fixes and enhancements related to those scripts  which could break features  or leave vulnerable script versions in place.\n* customers may revert these to out-of-box  as is best practice after upgrades  removing the necessary checksum\n* this may add confusion and delay for customers and servicenow tech support when debugging issues with these applications after upgrade  where checking for skipped upgrades is an important part of that process.\n\nMany feature use MID Server Script Files of various types  including but not limited to:\n\n* Agent Client Collector Framework\n* Core Automation\n* Discovery - IP Based\n* Event Management\n* Microsoft SCCM Spoke\n* MID Server\n* Orchestration - Runtime\n* Pattern Designer\n* Security Support Orchestration\n* ServiceNow IntegrationHub Action Step - PowerShell\n\nSystem Center Configuration Manager (SCCM)\n\n## Steps to Reproduce\n\nTested in a Vancouver patch 2 instance\n\n1. Install any app or plugin that uses MID Server Script files   \n   e.g. Agent Client Collector Framework  which adds:  \n   GetAllIpsForWindows.ps1 \/ecc_agent_script_file.do?sys_id=f9e62be6ffb11010574f5897d53bf1ac  \n   GetAllIpsForLinux.sh \/ecc_agent_script_file.do?sys_id=0fd36f66ffb11010574f5897d53bf11c  \n   * already the 2 newly added script files are no longer on the out-of-box version \"Store Application: Agent Client Collector Framework\"  but a custom \"Update Set: Default\" version  created by system\n   * The sys_update_xml record to go with this is set replace_on_upgrade=false\n2. Upgrade the app  or upgrade the plugin by upgrading the instance. If there is nothing to upgrade  Repair the plugin instead because the same code runs.\n   * The files are Skipped as customisations. If the app had a newer version of th record  to add a feature or correct a defect  it would not be applied.\n   * Tracing the database update to the transction will take you to a \"Events process 0\" job  which will have been processing either:\n     * A plugin.upgraded event  by script action Generate Checksum on Plugin Upgrade\n     * A plugin.activated event by script action Generate Checksum on Plugin Activation\n   * These both do a normal GlideRecord .update().\n\n## Workaround\n\nThis problem is currently under review and targeted to be fixed in a future release. Subscribe to this Known Error article to receive notifications when more information will be available.\n\nThe workaround will ideally need doing before any app or instance upgrade that changes the contents of the script field of any MID Server Script File record:\n\n* Set any sys_update_xml records for out of boc ecc_agent_script_file records  where they are in the Default update set  updated by system  to Replace on upgrade = true\n\nThe alternative is to revert to the out-of-box version of the record after the upgrade  but this may leave the record without a checksum. In that case update the record  and a business rule adds it before update.\n\nWith both methods  you will still end up with a custom version  you will need to deal with in the next upgrade until this problem is fixed.\n\n**Related Problem: PRB1708634**\n\n\"\n\n",
        "QUERY": " 'Debug entries spamming the MID Server agent log'",
        "GROUND_TRUTH": "no answer found"
    },
    {
        "id": 139,
        "CONTEXT": "\"## When saving a read-only currency field its value may be converted to default US Dollars automaticall\n\n## Description\n\nWhen a read only currency field is being saved  its value may be automatically converted to US$  even though the user locale are set to a different currency.\n\n## Steps to Reproduce\n\n1. Login a multi-currency instance as administrator.\n2. Verify the currency values are displayed in the user locale chosen currency  i.e. GBP.\n3. Create a currency type field and put the field on the form.\n4. Change it the currency to be like pound.\n5. Make the field read only.\n6. Save the form.  \n   Notice the currency is changed to US$ instead of showing GB Pounds.\n\n## Workaround\n\nThis is expected behaviour  due to different settings across the [currency system properties](https:\/\/docs.servicenow.com\/csh?topicname=currency-properties.html&version=latest#currency-properties \"currency system properties\"). When a Currency field is set to read-only  the default currency on a new record will be the reference currency set in the [**glide.system.locale**](https:\/\/docs.servicenow.com\/csh?topicname=locales.html&version=latest \"glide.system.locale\") property. This property should be set before starting to use the instance and loading data. Setting the **glide.system.locale** system property to a different value from the initial instance startup requires clearing the instance cache and may cause previously stored data loss for all existing currency fields.\n\n**Related Problem: PRB1238857**\n\n\n\n## Vancouver security and notable fixes\n\n Refer to the listed KB article for details.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n| Next Experience Unified Navigation PRB1657497                                                                                    | 'Reset to Default' in the domain picker doesn't lead to the global domain in the Next Experience UI                                                                                                             | Global domain is missing for the user. When users click 'Reset to default'  it goes to the global domain in the UI16  but in the Next Experience UI  it goes to the ACME domain.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | 1. Activate the Domain Support - Domain Extensions Installer plugin (com.glide.domain.msp_extensions.installer) plugin. 2. Ensure that the property 'glide.ui.domain_reference_picker.enabled' is set to true. 3. Add the admin role to the acme.itil user who is in the ACME domain. 4. Create a group. 5. Provide domain visibility to all the domains. 6. Add the user acme.itil to the group. 7. Impersonate the user. 8. Change the domain in the domain picker. 9. Click Reset to default. 10. Verify that in the UI16  it leads to the global domain. 11. Verify that in Next Experience  it leads to the user's default domain: ACME. {#vancouver-security-notables__ol_p2n_zd1_2yb} Expected behavior: The user should be able to navigate to the global domain. Actual behavior: The user navigates to the default domain  which is ACME in this case. There's no way for the user to navigate to the global domain.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |\n| Next Experience Unified Navigation PRB1658346                                                                                    | An impersonation copies the favorites and history of an impersonated user                                                                                                                                       | In Utah  when the user has two tabs open  if they impersonate a user on the first one and then end the impersonation  the user's favorites are overtaken by the ones of the impersonated user.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | 1. Open a Utah instance in 2 tabs of the same browser. 2. Impersonate any user from the first tab. 3. Switch to the second tab. 4. Check the 'Favorites' and 'History' tabs. 5. End the impersonation. 6. Check the 'Favorites' and 'History' tabs. {#vancouver-security-notables__ol_sfn_zd1_2yb} An impersonation copies the favorites and history of an impersonated user.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n| Next Experience Unified Navigation PRB1661584 [KB1340471](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB1340471)       | Non-admin users are not triggering sidebar notifications for users that are following records                                                                                                                   | If a non-admin user updates an incident  the other users following the record will not get a notification. Alternatively  if an admin user updates an incident  the other users following the record will get a sidebar notification.                                                                                                                                                                                                                                                                                                                                                                                                                                                               | Refer to the listed KB article for details.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n| Next Experience Unified Navigation PRB1675610                                                                                    | The sys_auto flush to clean up the live_group_profile table is deleting knowledge entries                                                                                                                       | Some attachments added to the knowledge article's feedback is not displayed after an upgrade to Utah due to sys_auto_flush for the live_group_profile table.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n| NLU Workbench PRB1609001                                                                                                         | Once NLU model is deleted  its artifacts in tables ml_solution and ml_capability_definition_base are not deleted                                                                                                |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | 1. Log in to a San Diego or Tokyo instance. 2. Create a custom NLU model  then train it and publish it. 3. Delete the NLU model from the workbench. {#vancouver-security-notables__ol_ogn_zd1_2yb} Expected behavior: The NLU model artifacts in other tables such as ml_solution and ml_capability_definition_base are deleted.Current. Actual behavior: The NLU model artifacts in other tables such as ml_solution and ml_capability_definition_base are not deleted.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n| Now Experience Component Library PRB1642469                                                                                      | A column of type 'List' shows 'undefined' as form field value                                                                                                                                                   | When the user assigns sn_hr_er_case to themselves  the Collaborators field displays 'undefined'.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n| Now Experience Component Library PRB1651586 [KB1430005](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB1430005)         | In the Comments and the Work Notes fields  the user can't use the up and down arrows on the keyboard                                                                                                            | The user can only use the left\/right arrow keys.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | Refer to the listed KB article for details.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n| Now Experience Component Library PRB1652450                                                                                      | Read-only checkbox variables are not clearly visible in the Next Experience UI                                                                                                                                  |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | 1. Log into any San Diego  Tokyo  or Utah instance with Next Experience enabled. 2. Create a new checkbox variable in any catalog item. 3. Create a UI policy to set that checkbox variable to read-only. 4. Submit the catalog item with the newly created variable and notice that the checkbox variable is read-only. {#vancouver-security-notables__ol_phn_zd1_2yb} Expected behavior: The checkbox variable that is set to read-only should be easier to read. Actual behavior: The checkbox variable is hard to read which can lead to confusion.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |\n| Now Experience Framework PRB1656410                                                                                              | Profile avatar initials aren't populating correctly in Utah                                                                                                                                                     | When the Name field in sys_user.\n\n When processing the cache flush messages  any errors should be handled gracefully and the transaction should be processed. However  an exception is thrown and the transaction doesn't complete. This manifests as the users are unable to log in  because their transactions are trying to process the flush messages. On exception  they are unable to proceed.                                                                                                                                                                                                                                                                                                                                   | Refer to the listed KB article for details.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n| Currency Administration PRB1317349 [KB0866499](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB0866499)                  | In Service Portal list  price and currency values only show in the user's currency                                                                                                                              | Non-USD dollar currencies (for example  Euro) will show up as the equivalent USD currency on the Service Portal list.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n| Currency Administration PRB1458430                                                                                               | When updating a record  the reference amount updates based on the latest currency rate  even if the input\/session amount has not changed                                                                        | For instances configured with a multi-currency setup with a reference currency set to EUR  the reference amount for a currency field is updated based on the latest currency rate whenever a record is being updated. This occurs even if the update did not change the input currency amount or currency. For non-system currency (System Currency is EUR) when rate card is updated  this effects the existing reference currency instances and may add up over time  resulting in costs higher or lower than the intended amount.                                                                                                                                                                | 1. Create a currency field u_test_currency on the incident table. 2. Set the property glide.system.locale to nl.NL. Single currency mode is set to false by default. 3. In UI16  create an incident record or update an existing one and set the currency field value to 100 JPY (or GBP or CHF). 4. Identify the entry created in fx_currency_instance. Observe the reference amount value (in EUR). 5. Navigate to the fx_rate table and create a rate for the input currency used in step 3 or update value for the most recent rate. 6. Open the incident record opened at step 3 and update the short description. 7. Navigate back to fx_currency_instance and observe that th ereference amount has been updated based on latest currency rate for ALL. {#vancouver-security-notables__ol_jkm_zd1_2yb} Expected behavior: Once the currency is entered in the system for the record  the reference amount should not be updated to match the latest rate after the rate change in the system (if the field with currency was not touched by the user). Actual behavior: The reference amount for the currency field is updated to match the latest rate after the rate change in the system  even though nothing was updated for that field in the Form.                                                                                                                                                                                                                                                        |\n| Database Persistence - Data Access PRB1326267 [KB0745443](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB0745443)       | Database views with 'left joins' fail to retrieve data                                                                                                                                                          | When the left-join coercion is active on an instance  the query fails to retrieve data on database views with left-join.                                                                                                                ...",
        "QUERY": " 'currency changes to USD in a read-only currency field though the locale is set to a different currency'",
        "GROUND_TRUTH": "According to the KB12345 article, when a read-only currency field is saved, its value may be automatically converted to US dollars, even if the user's locale is set to a different currency. This is due to different settings across the currency system properties. When a Currency field is set to read-only, the default currency on a new record will be the reference currency set in the glide.system.locale property. This property should be set before starting to use the instance and loading data. Changing the glide.system.locale system property to a different value from the initial instance startup requires clearing the instance cache and may cause previously stored data loss for all existing currency fields. This is considered expected behavior."
    },
    {
        "id": 140,
        "CONTEXT": "\"## Create a many-to-many relationship\n\n[Building applications](..\/..\/..\/build\/custom-application\/concept\/build-applications.html \"Learn how to become an application developer using Now Platform low-code tools. Start with what you know and use a library of reusable components and published applications to modernize your legacy processes.\") \\> [Planning your application](..\/..\/..\/build\/custom-application\/concept\/planning-applications.html \"The application development process starts with planning. Consider how the application will work  who will use it  and how it improves user experience.\") \\>\n\n# Create a many-to-many relationship {#t_CreateAManyToManyRelationship__CreatingAManyToManyRelationship}\n\nMany-to-many relationships allow a list to point to a list of entries  rather than to single field.\nFor example  if a knowledge base article points to a list of related configuration items  it uses a related list. Not all lists can be used as related lists  however. For a list to be related to another list  a many-to-many relationship that refers to both tables must exist.\n\nThe Many to Many Definitions `[sys_m2m]` table allows administrators to create custom many-to-many relationships.\n\nMany-to-Many tables are not considered custom tables and don't count toward your custom table allotment.\n\n1. In the navigation filter  enter sys_m2m.list.\n2. Click New.\n3. In the From table field  specify a parent table.\n4. In the To table field  specify a child table.  \n   The Many-to-Many form automatically populates the other fields with suggested values.\n5. **Optional:** Edit other field values  if appropriate. Many-to-Many table names cannot exceed 30 characters.  \n\n* **[Reference default many-to-many relationships](..\/..\/..\/administer\/table-administration\/reference\/r_RefDefaultManyToManyRels.html)**   \n  Some many-to-many relationships are defined by default.\n\n**Parent Topic:** [Table administration](..\/..\/..\/administer\/table-administration\/concept\/c_TableAdministration.html \"A table is a collection of records in the database. Each record corresponds to a row in a table  and each field on a record corresponds to a column on that table.\")\n**Related reference**\n\n* [Reference default many-to-many relationships](..\/reference\/r_RefDefaultManyToManyRels.html \"Some many-to-many relationships are defined by default.\")\n\n\n\n## Add a CMDB group CI relationship\n\n[IT Operations Management](..\/..\/..\/product\/it-operations-management\/reference\/r_ITOMApplications.html \"Get better visibility into your infrastructure and services  prevent service outages  and expand your organization's operational agility with ServiceNow IT Operations Management.\") \\> [ITOM Health](..\/..\/..\/product\/it-operations-management\/reference\/itom-health-landing-page.html \"The ServiceNow ITOM Health product includes the ServiceNow Event Management and ServiceNow Metric Intelligence applications  which help you track and maintain the health of services in your organization.\") \\> [Event Management](..\/..\/..\/product\/event-management\/concept\/c_EM.html \"The ServiceNow Event Management application helps you to identify health issues across the datacenter on a single management console. It provides alert aggregation and root cause analysis (RCA) for discovered services  application services  and automated alert groups. Event Management is available as a separate subscription from the rest of the ServiceNow platform.\") \\> [Using Event Management](..\/..\/..\/product\/event-management\/concept\/using-event-management.html \"Event Management administrators administer events  manage and monitor alerts  aggregate alerts  and work review and monitor services' status with the Operator Workspace service monitor.\") \\> [Alert aggregation](..\/..\/..\/product\/event-management\/concept\/c_ServiceAnalyticsOverview.html \"The alert aggregation capability enhances Event Management with alert data analysis and alert aggregation. Alert aggregation helps organize incoming real-time alerts and reduce alert noise.\") \\>\n\n# Add a CMDB group CI relationship {#ariaid-title1}\n\nAdd a CMDB group CI relationship for a class. The list of CMDB group CI relationships are used for CMDB alert group generation for specific property configuration.\nRole required: evt_mgmt_admin\n\n1. Navigate to All \\> Event Management \\> Administration \\> CMDB Group CI Relations.\n   1. Click New.\n   2. Complete the form.  \n      {#d71650e109}\n      |      Field      |                                                                Description                                                                |\n      |-----------------|-------------------------------------------------------------------------------------------------------------------------------------------|\n      | Base class      | The base class in the relationship  which depending on the relationship type  is either the parent or the child in the relationship.      |\n      | Relationship    | Relationship type.                                                                                                                        |\n      | Dependent class | The dependent class in the relationship  which depending on the relationship type  is either the parent or the child in the relationship. |\n      Table 1. Suggested Relationship fields      {#add-suggested-relationship__table_ycc_qjs_5t}      {#add-suggested-relationship__substeps_cx4_1j5_5cb}\n2. Click Submit.\n\n{#add-suggested-relationship__steps_yfk_lm4_ryb}\n\n## CMDB Group CI relation {#add-suggested-relationship__example_zfk_lm4_ryb}\n\n{#add-suggested-relationship__simpletable_nxx_kj5_5cb}{#d71650e176}\n\n| Base Class | Relationship | Dependent\/Target Class |\n|------------|--------------|------------------------|\n| Oracle     | Is Hosted On | Linux Server           |\n| Oracle     | Is Hosted On | Solaris Server         |\nNote: The same parent class and relationship can appear more than once.\nYou may need to delete a CMDB group CI relationship  for example  to limit the choice of available relationships used in CMDB alert grouping. Removing a CMDB group CI relationship does not affect relationships that are created or updated by Discovery.\n**Parent Topic:** [CMDB alert groups](..\/..\/..\/product\/event-management\/concept\/cmdb-alert-groups.html \"Event Management alert aggregation aggregates alerts into CMDB alert groups based on CMDB relations. CMDB alert groups are displayed in the Express list in the Service Operations Workspace.\")\n**Related concepts**\n\n* [CMDB alert groups](..\/concept\/cmdb-alert-groups.html \"Event Management alert aggregation aggregates alerts into CMDB alert groups based on CMDB relations. CMDB alert groups are displayed in the Express list in the Service Operations Workspace.\")\n\n\n\n[IT Operations Management](..\/..\/..\/product\/it-operations-management\/reference\/r_ITOMApplications.html \"Get better visibility into your infrastructure and services  prevent service outages  and expand your organization's operational agility with ServiceNow IT Operations Management.\") \\> [ITOM Health](..\/..\/..\/product\/it-operations-management\/reference\/itom-health-landing-page.html \"The ServiceNow ITOM Health product includes the ServiceNow Event Management and ServiceNow Metric Intelligence applications  which help you track and maintain the health of services in your organization.\") \\> [Event Management](..\/..\/..\/product\/event-management\/concept\/c_EM.html \"The ServiceNow Event Management application helps you to identify health issues across the datacenter on a single management console. It provides alert aggregation and root cause analysis (RCA) for discovered services  application services  and automated alert groups. Event Management is available as a separate subscription from the rest of the ServiceNow platform.\") \\> [Configuring Event Management](..\/..\/..\/product\/event-management\/concept\/using-event-management.html \"Event Management administrators administer events  manage and monitor alerts  aggregate alerts  and work review and monitor services' status with the Operator Workspace service monitor.\") \\> [Alert aggregation](..\/..\/..\/product\/event-management\/concept\/c_ServiceAnalyticsOverview.html \"The alert aggregation capability enhances Event Management with alert data analysis and alert aggregation. Alert aggregation helps organize incoming real-time alerts and reduce alert noise.\") \\>\n\n# Add a CMDB group CI relationship {#ariaid-title1}\n\nAdd a CMDB group CI relationship for a class. The list of CMDB group CI relationships are used for CMDB alert group generation for specific property configuration.\nRole required: evt_mgmt_admin\n\n1. Navigate to All \\> Event Management \\> Administration \\> CMDB Group CI Relations.\n   1. Click New.\n   2. Complete the form.  \n      {#d46896e113}\n      |      Field      |                                                                Description                                                                |\n      |-----------------|-------------------------------------------------------------------------------------------------------------------------------------------|\n      | Base class      | The base class in the relationship  which depending on the relationship type  is either the parent or the child in the relationship.      |\n      | Relationship    | Relationship type.                                                                                                                        |\n      | Dependent class | The dependent class in the relationship  which depending on the relationship type  is either the parent or the child in the relationship. |\n      Table 1. Suggested Relationship fields      {#add-suggested-relationship__table_ycc_qjs_5t}      {#add-suggested-relationship__substeps_cx4_1j5_5cb}\n2. Click Submit.\n\n{#add-suggested-relationship__steps_yfk_lm4_ryb}\n\n## CMDB Group CI relation {#add-suggested-relationship__example_zfk_lm4_ryb}\n\n{#add-suggested-relationship__simpletable_nxx_kj5_5cb}{#d46896e180}\n\n| Base Class | Relationship | Dependent\/Target Class |\n|------------|--------------|------------------------|\n| Oracle     | Is Hosted On | Linux Server           |\n| Oracle     | Is Hosted On | Solaris Server         |\nNote: The same parent class and relationship can appear more than once.\nYou may need to delete a CMDB group CI relationship  for example  to limit the choice of available relationships used in CMDB alert grouping. Removing a CMDB group CI relationship does not affect relationships that are created or updated by Discovery.\n**Parent Topic:** [CMDB alert groups](..\/..\/..\/product\/event-management\/concept\/cmdb-alert-groups.html \"Event Management alert aggregation aggregates alerts into CMDB alert groups based on CMDB relations. CMDB alert groups are displayed in the Express list in the Service Operations Workspace.\")\n**Related concepts**\n\n* [CMDB alert groups](..\/concept\/cmdb-alert-groups.html \"Event Management alert aggregation aggregates alerts into CMDB alert groups based on CMDB relations. CMDB alert groups are displayed in the Express list in the Service Operations Workspace.\")\n\n\"\n\n",
        "QUERY": " 'Creating a Many-to-Many Relationship'",
        "GROUND_TRUTH": "To create a many-to-many relationship, follow these steps:\n\n1. In the navigation filter, enter sys_m2m.list.\n2. Click New.\n3. In the From table field, specify a parent table.\n4. In the To table field, specify a child table. The Many-to-Many form automatically populates the other fields with suggested values.\n5. Optionally, you can edit other field values if appropriate. Note that Many-to-Many table names cannot exceed 30 characters.\n\nThe Many to Many Definitions `[sys_m2m]` table allows administrators to create custom many-to-many relationships. Many-to-Many tables are not considered custom tables and don't count toward your custom table allotment. Some many-to-many relationships are defined by default."
    },
    {
        "id": 141,
        "CONTEXT": "\"## Syntax highlighting not honoured with KBs with Code snippets in Content view of a Knowledge article \n\n## Description\n\nSyntax highlighting not honoured with KBs with Code snippets in Content view of a Knowledge article  both in platform and service portal view\n\n## Steps to Reproduce\n\n1. Add 'codesample' in the property: glide.ui.html.editor.v4.toolbar.valid.buttons to make it one of the valid button  \n2. Add 'codesample' in the property glide.ui.html.editor.v4.toolbar.line1 to add in the toolbar  \n3. Open a kb_knowledge record  \n4. In the HTML field 'Article body'  navigate to the toolbars of TinyMCE. Notice a new button is available as {;}. Click on this  \n5. It will open an editor. Select any language. I chose javascript  and wrote below code:  \nvar test ='testing';  \nconsole.log(test);  \n6. Save the record. Note that var and log keywords are showing in color  \n7. Open the record in SP with the below URL  \nsp?id=kb_article\\&sys_id=\\<SYS_ID OF THE KB\\>  \n\nOR\nOpen in platforrm\n\/kb_view?sys_id=\\<SYS_ID OF THE KB\\>\n\n8. Note that the codes are not showing color (no highlight for syntax)  \n\nrefer the screenshots\n\n## Workaround\n\nThis is expected behavior and by design in all currently supported releases.\n\n**Related Problem: PRB1434350**\n\n\n\n## Vancouver security and notable fixes\n\n5. Attach any document. {#vancouver-security-notables__ol_eyn_zd1_2yb} Observe that the file is attached but the encrypt attachment modal doesn't appear.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |\n| Sidebar (Family Release) PRB1640158 [KB1264544](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB1264544)                 | A session times out when a user tries to search for a user after clicking the Sidebar Discuss button                                                                                                            | The issue occurs when a call hits the max execution time. From the end users' perspective  they see the session expiration.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | Refer to the listed KB article for details.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n| Software Asset Management PRB1393880                                                                                             | Downgrade rights are not copied onto entitlements with the generic agreement type                                                                                                                               |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n| Software Asset Management PRB1561507 [KB1219890](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB1219890)                | Opening Office 365 and its editions in the License Workbench dashboard takes approximately 10 seconds                                                                                                           |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | Refer to the listed KB article for details.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n| Software Asset Management PRB1597289                                                                                             | Installations discovered by ServiceNow are not deduped if there are already previously existing installations                                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n| Software Asset Management PRB1625502 [KB1220642](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB1220642)                | Recon job workers' threads are created on only one node                                                                                                                                                         | This causes spikes and issues in a user's instance.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | 1. Run reconciliation. 2. Observe if most of the time  the worker nodes are created on one node. {#vancouver-security-notables__ol_qzn_zd1_2yb}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n| Software Asset Management PRB1662889 [KB1316557](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB1316557)                | Adobe subscriptions aren't pulled by the 'SAM - Import User Subscriptions' scheduled job                                                                                                                        | 'Get groups and profiles' and 'product name' were modified to contain additional information  which breaks pulling subscriptions.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | Refer to the listed KB article for details.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n| Source Control Engine PRB1659580 [KB1307937](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB1307937)                    | Setting the default branch causes Source Control to fetch before using the MID server                                                                                                                           | This makes the user unable to commit.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | Refer to the listed KB article for details.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n| Source Control Menu for Dev Studio PRB1658786                                                                                    | An app packaging failure during inclusive commit  or when applying remote changes with disk comparison  causes the git repo directory to be left in an unusable staged state                                    | When users inclusive commit or do delta loading that involves disk comparison  rmAll() is called on the repository first. Then  the files are unloaded to the repo. In the case that the unload fails due to the duplicate names exception  the repo is left in the staged state where the repo app folder is removed.                                                                                                                                                                                                                                                                                                                                                                              | Inclusive commit: 1. Create a scoped app. 2. Link to Service Catalog. 3. Create two sys_script_include files (script1  script2). 4. Manually modify the sys_metadata for script2 so that its sys_metadata. sys_update_name is the same as that of script1 (create a duplicated update name scenario). 5. Inclusive commit on studio. {#vancouver-security-notables__ol_r14_zd1_2yb} Expected behavior: The commit fails with an error. The git repo is clean and the app directory under the repo directory is kept. Actual behavior: The commit fails with an error. The git repo directory is left in a staged state and can't be used. The app directory under the git repo directory is removed. Apply remote changes with disk comparison: 1. Create a scoped app. 2. Link to Service Catalog. 3. Create two sys_script_include files (script1  script2). 4. Manually modify the sys_metadata for script2 so that its sys_metadata.sys_update_name is the same as that of script1 (create a duplicated update name scenario). 5. Empty current_ref in sys_repo_config (force a disk comparison). 6. Apply remote changes. {#vancouver-security-notables__ol_s14_zd1_2yb} Expected behavior: Apply remote changes fails with an error. The git repo is clean and the app directory under the git repo directory is kept. Actual behavior: Apply remote changes fails with an error. The git repo directory is left in a staged state and can't be used. The app directory under the git repo directory is removed. |\n| Syntax Editor PRB1645468 [KB1306354](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB1306354)                            | Code editor\/Syntax editor isn't loading correctly when Next Experience is enabled                                                                                                                               | The script syntax isn't highlighted in colors. An error displays in the browser console: 'Uncaught SyntaxError: Invalid or unexpected token'.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | Refer to the listed KB article for details.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n| System Import Sets PRB1664632                                                                                                    | Excel files exported from ServiceNow data is corrupted after upgrade                                                                                                                                            |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n| System Update Sets PRB1598276 [KB1196555](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB1196555)                       | Progress worker status does not reflect the correct status after abrupt node\/instance restart                                                                                                                   | The progress worker of a transaction is stuck in the 'Running' state even though the corresponding transaction is not executing in the background. \n\n## Format a code snippet automatically in knowledge articles\n\n# Format a code snippet automatically in knowledge articles {#ariaid-title1}\n\nAdd syntax highlighting to a code snippet in knowledge articles using the HTML editor.\nOpen the knowledge article in which you want to use the HTML editor.\n\nRole required: knowledge\nYou can also use the HTML editor when creating or editing a knowledge article using the Knowledge Management application in the Now Platform interface or in Agent Workspace. To create or edit a knowledge article in the Now Platform interface  see [Create a knowledge article](create-knowledge-article.html \"Knowledge contributors can create and edit knowledge articles within a knowledge base to share information across your organization.\") or [Edit a knowledge article](edit-knowledge-article.html \"Edit knowledge articles within a knowledge base to update information shared across your organization.\"). To create or edit a knowledge in Agent Workspace  see [Create a knowledge article in Agent Workspace](create-article-workspace.html \"Create a knowledge article in Agent Workspace to share information across your organization.\") or [Edit a knowledge article in Agent Workspace](edit-article-workspace.html \"Edit knowledge articles within a knowledge base in Agent Workspace to update information shared across your organization.\").\n\n1. In the Article body section  place the cursor where you want to insert the code snippet.\n2. Click the insert\/edit code sample icon (![Insert\/Edit code sample icon](..\/image\/insert-code.png)) on the HTML toolbar.\n3. In the Insert\/Edit code sample dialog box  select a markup or programming language from the Language list.\n4. In the text box after the Language list  enter your code.  \n   Alternatively  you can copy the code from an external source and paste it.\n5. Click Ok.\n6. **Optional:** Edit the code by clicking the code snippet  and then clicking the insert\/edit code sample icon (![Insert\/Edit code sample icon](..\/image\/insert-code.png)).\n\n\"\n\n",
        "QUERY": " 'Code not highlighted for syntax in KBs with code snippet'",
        "GROUND_TRUTH": "The issue of code not being highlighted for syntax in KBs with code snippets is a known issue and is expected behavior by design in all currently supported releases. This issue can be reproduced by following these steps:\n\n1. Add 'codesample' in the property: glide.ui.html.editor.v4.toolbar.valid.buttons to make it one of the valid button.\n2. Add 'codesample' in the property glide.ui.html.editor.v4.toolbar.line1 to add in the toolbar.\n3. Open a kb_knowledge record.\n4. In the HTML field 'Article body' navigate to the toolbars of TinyMCE. Notice a new button is available as {;}. Click on this.\n5. It will open an editor. Select any language. For example, if you choose javascript and write a code, you will notice that var and log keywords are showing in color.\n6. Save the record.\n7. Open the record in SP with the URL sp?id=kb_article\\&sys_id=\\<SYS_ID OF THE KB\\> or open in platform \/kb_view?sys_id=\\<SYS_ID OF THE KB\\>.\n8. You will notice that the codes are not showing color (no highlight for syntax).\n\nThis behavior is related to Problem PRB1434350."
    },
    {
        "id": 142,
        "CONTEXT": "\"## It is not possible to archive all cmdb_rel_ci record when CI records are Archived  leading to Compac\n\n## Description\n\nOn an instance that has the Data Archiving plugin installed and in use for the CMDB tables  the CMDB Identification and Reconcilliation engine can throws a huge number of errors like this in the syslog table:\nCompactRelation: failed to get details of CI bc4fe864135b9bc0574c75276144b09f: no thrown error \\| syslog \\| 000009c213579700f6a7f107d144b0b3 \\| system \\| 2018-07-15 21:36:31 \\| \\| 2 \\| com.glide.ui.ServletErrorListener \\| service_cache_mgr\nAnd the app node localhost log:\n2018-07-26 10:35:54 (605) Default-thread-3 AAD39C29136797C04DD2F027D144B0B4 #10927362 \/ngbsmprocessor.do Parameters -------------------------\nactionType=loadBasic\nmapScriptID=\nserviceMode=false\ncmd=get\nid=8f59d4dce19f71c424893534899bf84e\ncacheKill=1532626554567\n2018-07-26 10:35:55 (694) Default-thread-3 AAD39C29136797C04DD2F027D144B0B4 SEVERE \\*\\*\\* ERROR \\*\\*\\* service_cache_mgr : CompactRelation: failed to get details of CI 0008b8644fe49ec06433ab99f110c7cf\n2018-07-26 10:35:55 (696) Default-thread-3 AAD39C29136797C04DD2F027D144B0B4 SEVERE \\*\\*\\* ERROR \\*\\*\\* service_cache_mgr : CompactRelation: failed to get details of CI 001ae60913d2ba006203b2d96144b0ad\n2018-07-26 10:35:55 (698) Default-thread-3 AAD39C29136797C04DD2F027D144B0B4 SEVERE \\*\\*\\* ERROR \\*\\*\\* service_cache_mgr : CompactRelation: failed to get details of CI 009d6cd81337fac0b23ffea2e144b057\n2018-07-26 10:35:55 (700) Default-thread-3 AAD39C29136797C04DD2F027D144B0B4 SEVERE \\*\\*\\* ERROR \\*\\*\\* service_cache_mgr : CompactRelation: failed to get details of CI 00fc04514f126a806433ab99f110c70d\n.......\nThis code runs when the Dependency View ngbsmprocessor needs to collect details of all related CIs.\nThis causes 2 main problems:\n\n* ServiceCacheManager.getInboundRelations and .getOutboundRelations won't work properly  even though the data is there in the instance.\n* On an instance using the CMDB and discovery sources heavily  this can add such a large number of entries to syslog tables to cause disk space and performance issues.\n\nUnlike the CI Form  which realizes the CI is archived and redirects to the archived record  this code does not check to see if a CI record is archived and instead just throws an error.\nNeither the Archiving nor CMDB plugins have any feature for 'cascade archiving' related records such as cmdb_rel_ci when a CI is archived that would prevent a relationship record remaining after a CI is archived.\n\n## Steps to Reproduce\n\nOn a clean Kingston instance with demo data:\n1\/ Install 'Data Archiving' plugin\n2\/ For the purposes of this test  create an archive rule for the Linux Server table to archive any CIs with condition: name starts with lnux  which will cover 'lnux100' and 'lnux101' CIs\n3\/ Activae the rule  then manually run the 'Archive' scheduled job to avoid waiting\n4\/ Open the form for the \"Client Services\" business service CI. This has relationships with those server\n5\/ Open the Dependency view (BSM Map) from the button ion the CI relations section (which you'll notice doesn't show the archived Linux servers)\n6\/ Check the syslog entries  and you will see a pair of errors  one for each linux server\n2018-07-27 18:18:28 Error service_cache_mgr : CompactRelation: failed to get details of CI 53958ff0c0a801640171ec76aa0c8f86: no thrown error com.glide.ui.ServletErrorListener\n2018-07-27 18:18:28 Error service_cache_mgr : CompactRelation: failed to get details of CI 539747cac0a801640163e60735fbbf6e: no thrown error com.glide.ui.ServletErrorListener\n\n## Workaround\n\nFor Data Archiving there is an option to archive related records and whenever a rule is created for CMDB CI archival  the corresponding relations should also be archived.\n\nHowever  cmdb_rel_ci (CMDB relationship) table has two references to cmdb_ci table (parent and child). When creating a \"Archive Related Records\" rule  you can only select 1 of those fields  and therefore **this workaround is only half a workaround  but is better than nothing**.\n\n1. In your CMDB related archive rules  add a new Archive Related Records entry\n2. Select \"Relationships\" in drop down  and it will pre-select \"parent\" element and there is no way to add the \"child\" element as well.\n\n**Related Problem: PRB1296280**\n\n\n\n## Vancouver security and notable fixes\n\n The job 'CMDB Health Dashboard - Relationship Score Calculation' has a slow query                                                                                                                               | The query doesn't have any conditions nor defined range  and performs a full table scan. This causes a load on the CPU.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | 1. Clone an instance. 2. Run the 'CMDB Health Dashboard - Relationship Score Calculation' job. {#vancouver-security-notables__ol_f3m_zd1_2yb}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n| Configuration Management Database (CMDB) PRB1591705 [KB1169983](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB1169983) | When users are using query builder and querying for more items  the Save and Run buttons at the top disappear                                                                                                   | When users access query builder and query for a decent number of items  the page shifts down. TheSave and Run query buttons are hidden                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | Refer to the listed KB article for details.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n| Configuration Management Database (CMDB) PRB1611377 [KB1182003](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB1182003) | The 'CMDB Baseline creation' job causes an 'app node out of memory (OOM)' error when a configuration item (CI) has a large number of records referencing it                                                     | The referencing records might be tasks or other table records with a reference field to a CI. When the baseline creation gets to the CI with many relations  it causes the app node to have an OOM error and restart. It usually re-runs the same job again with every restart. The symptom is poor performance for any users logged into that app node. The restart may cause any other transactions running at the time to stop. The baseline being created doesn't finish. Due to re-running many times  there's duplicate cmdb_baseline_entry records for the same CI sys_ids.                                                                                                                  | 1. Create a lot of task records all referencing the same CI. 2. Create a CMDB baseline for that CI. * Saving the cmdb_baseline record triggers the 'SNC Create Baseline' business rule  which creates a 'ASYNC: Script Job' scheduled job using SNC.CMDBUtil.baselineSchedule for the new record. {#vancouver-security-notables__ul_w3m_zd1_2yb} 3. Observe the memory usage. 4. Take a heap dump. {#vancouver-security-notables__ol_v3m_zd1_2yb} There's a large amount of memory used for the related task data.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n| Configuration Management Database (CMDB) PRB1615879 [KB1218266](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB1218266) | CMDB_CI Index  added as part of PRB1519942  isn't added to the CMDB partition tables on upgrades                                                                                                                | On upgrading to San Diego or later  the index CMDB (install_status  sys_created_on) isn't added to the CMDB partition tables. This results in a UI and GlideRecord.query errors. This could prevent a user from accessing some CMDB and child table lists. This could also prevent some GlideRecord.query() methods from returning the expected results.                                                                                                                                                                                                                                                                                                                                            | Refer to the listed KB article for details.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n| Configuration Management Database (CMDB) PRB1627051 [KB1211804](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB1211804) | An encoded query isn't properly displayed on a condition builder when editing a policy                                                                                                                          | When using the CMDB data manager policies  a condition builder isn't displaying the selected class name. Filters aren't saved even after it shows that the record was saved successfully.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | Refer to the listed KB article for details.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n| Configuration Management Database (CMDB) PRB1629419 [KB1220355](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB1220355) | Not all the sys_class_names for all records are validated                                                                                                                                                       | Not validating the sys_class_names records causes errors in Health Job processing.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | Refer to the listed KB article for details.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n| Core Platform PRB1381876                                                                                                         | If the user uses 'trend on' or 'after'  there's a getGlideObject() application scope error when notifications are triggered                                                                                     | There's an error: 'Function getGlideObject is not allowed in scope sn_customerservice'.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n| Core Platform PRB1401625 [KB0966877](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB0966877)                            | CSRF token rotation isn't happening during log in flows                                                                                                                                                         |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | Refer to the listed KB article for details.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n| Core Platform PRB1617296                                                                                                         | Next Experience UI record not found                                                                                                                                                                             | In the incident table before login displays 'record not found'  even after logging in.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n| Core Platform PRB1635023                                                                                                         | Exception in ExpiringCache.cleanup                                                                                                                                                                              | The cache cleanup sometimes generates an exception (ExpiringCache.java:120). This can happen randomly and to different threads that need to access files on the file system.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n| Core Platform PRB1635585 [KB1221999](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB1221999)                            | StaticContentServer caches the existence and non-existence of snc_node_disable.html                                                                                                                             | After an instance migration  some instances don't come online when nodes are re-pointed to a new database.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   ...",
        "QUERY": " 'CMDB Identification and Reconciliation engine throwing CompactRelation: failed to get details of CI error'",
        "GROUND_TRUTH": "The issue of the CMDB Identification and Reconciliation engine throwing a CompactRelation error occurs when the Data Archiving plugin is installed and in use for the CMDB tables. The engine fails to get details of Configuration Items (CI) when they are archived, leading to a large number of errors in the syslog table. This happens because the code does not check if a CI record is archived and instead throws an error. The Archiving and CMDB plugins do not have a feature for 'cascade archiving' related records such as cmdb_rel_ci when a CI is archived. This can cause disk space and performance issues due to a large number of entries in the syslog tables.\n\nTo reproduce this issue, you can follow the steps provided in the article which involve installing the 'Data Archiving' plugin on a clean Kingston instance with demo data, creating an archive rule for the Linux Server table, activating the rule, and manually running the 'Archive' scheduled job.\n\nThe workaround suggested in the article is to archive related records whenever a rule is created for CMDB CI archival. However, this is only a partial solution as the cmdb_rel_ci (CMDB relationship) table has two references to the cmdb_ci table (parent and child) and you can only select one of those fields when creating an \"Archive Related Records\" rule."
    },
    {
        "id": 143,
        "CONTEXT": "\"## Vancouver security and notable fixes\n\n The job 'CMDB Health Dashboard - Relationship Score Calculation' has a slow query                                                                                                                               | The query doesn't have any conditions nor defined range  and performs a full table scan. This causes a load on the CPU.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | 1. Clone an instance. 2. Run the 'CMDB Health Dashboard - Relationship Score Calculation' job. {#vancouver-security-notables__ol_f3m_zd1_2yb}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n| Configuration Management Database (CMDB) PRB1591705 [KB1169983](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB1169983) | When users are using query builder and querying for more items  the Save and Run buttons at the top disappear                                                                                                   | When users access query builder and query for a decent number of items  the page shifts down. TheSave and Run query buttons are hidden                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | Refer to the listed KB article for details.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n| Configuration Management Database (CMDB) PRB1611377 [KB1182003](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB1182003) | The 'CMDB Baseline creation' job causes an 'app node out of memory (OOM)' error when a configuration item (CI) has a large number of records referencing it                                                     | The referencing records might be tasks or other table records with a reference field to a CI. When the baseline creation gets to the CI with many relations  it causes the app node to have an OOM error and restart. It usually re-runs the same job again with every restart. The symptom is poor performance for any users logged into that app node. The restart may cause any other transactions running at the time to stop. The baseline being created doesn't finish. Due to re-running many times  there's duplicate cmdb_baseline_entry records for the same CI sys_ids.                                                                                                                  | 1. Create a lot of task records all referencing the same CI. 2. Create a CMDB baseline for that CI. * Saving the cmdb_baseline record triggers the 'SNC Create Baseline' business rule  which creates a 'ASYNC: Script Job' scheduled job using SNC.CMDBUtil.baselineSchedule for the new record. {#vancouver-security-notables__ul_w3m_zd1_2yb} 3. Observe the memory usage. 4. Take a heap dump. {#vancouver-security-notables__ol_v3m_zd1_2yb} There's a large amount of memory used for the related task data.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n| Configuration Management Database (CMDB) PRB1615879 [KB1218266](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB1218266) | CMDB_CI Index  added as part of PRB1519942  isn't added to the CMDB partition tables on upgrades                                                                                                                | On upgrading to San Diego or later  the index CMDB (install_status  sys_created_on) isn't added to the CMDB partition tables. This results in a UI and GlideRecord.query errors. This could prevent a user from accessing some CMDB and child table lists. This could also prevent some GlideRecord.query() methods from returning the expected results.                                                                                                                                                                                                                                                                                                                                            | Refer to the listed KB article for details.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n| Configuration Management Database (CMDB) PRB1627051 [KB1211804](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB1211804) | An encoded query isn't properly displayed on a condition builder when editing a policy                                                                                                                          | When using the CMDB data manager policies  a condition builder isn't displaying the selected class name. Filters aren't saved even after it shows that the record was saved successfully.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | Refer to the listed KB article for details.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n| Configuration Management Database (CMDB) PRB1629419 [KB1220355](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB1220355) | Not all the sys_class_names for all records are validated                                                                                                                                                       | Not validating the sys_class_names records causes errors in Health Job processing.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | Refer to the listed KB article for details.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n| Core Platform PRB1381876                                                                                                         | If the user uses 'trend on' or 'after'  there's a getGlideObject() application scope error when notifications are triggered                                                                                     | There's an error: 'Function getGlideObject is not allowed in scope sn_customerservice'.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n| Core Platform PRB1401625 [KB0966877](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB0966877)                            | CSRF token rotation isn't happening during log in flows                                                                                                                                                         |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | Refer to the listed KB article for details.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n| Core Platform PRB1617296                                                                                                         | Next Experience UI record not found                                                                                                                                                                             | In the incident table before login displays 'record not found'  even after logging in.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n| Core Platform PRB1635023                                                                                                         | Exception in ExpiringCache.cleanup                                                                                                                                                                              | The cache cleanup sometimes generates an exception (ExpiringCache.java:120). This can happen randomly and to different threads that need to access files on the file system.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n| Core Platform PRB1635585 [KB1221999](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB1221999)                            | StaticContentServer caches the existence and non-existence of snc_node_disable.html                                                                                                                             | After an instance migration  some instances don't come online when nodes are re-pointed to a new database.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n| Core Platform PRB1646526                                                                                                         | Unable to run SCCM load tests after upgrading to Utah                                                                                                                                                           | A CO transformation fails with an IllegalArgumentException  InvalidPathException  or Concurrent ModificationException when running concurrent transformer instances with segmented path caching logic enabled.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n| Core Platform PRB1663472 [KB1323602](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB1323602)                            | If a record is deleted before a cache flush can be processed  an null point exception (NPE) is thrown and it prevents further processing of sys_cache_flush records                                             |\n\n## CMDB Baseline life-cycle best practices and Diff Formatter troubleshooting\n\n* PRB1283806 CMDB Baseline: cmdb_baseline_entry **table can grow to a huge size** causing instance and database performance and disk space issues\n* PRB1611377 CMDB Baseline creation job causing **app node out of memory** when a CI has a huge number of records referencing it\n* PRB591082 CMDB Baselines allow baselining non CMDB tables even though the data cannot be displayed\n* PRB1476641 When Baseline has a large number of records to populate (compute)  it will slow down the loading of the ci's form\n\nApart from PRB1611377  none of these are likely to be fixed.\n\n\n\n## CMDB Baseline creation job causing app node out of memory when a CI has a huge number of records ref\n\n## Description\n\nCMDB Baseline creation job causing app node out of memory when a CI has a huge number of tasks referencing...",
        "QUERY": " 'cmdb baseline creation job causing app node out of memory when a ci has a huge number of records referencing it'",
        "GROUND_TRUTH": "The 'CMDB Baseline creation' job can cause an 'app node out of memory (OOM)' error when a configuration item (CI) has a large number of records referencing it. The referencing records might be tasks or other table records with a reference field to a CI. When the baseline creation gets to the CI with many relations, it causes the app node to have an OOM error and restart. It usually re-runs the same job again with every restart. The symptom is poor performance for any users logged into that app node. The restart may cause any other transactions running at the time to stop. The baseline being created doesn't finish. Due to re-running many times, there's duplicate cmdb_baseline_entry records for the same CI sys_ids. \n\nTo reproduce this issue, you can create a lot of task records all referencing the same CI and then create a CMDB baseline for that CI. Saving the cmdb_baseline record triggers the 'SNC Create Baseline' business rule, which creates a 'ASYNC: Script Job' scheduled job using SNC.CMDBUtil.baselineSchedule for the new record. You can then observe the memory usage and take a heap dump. There's a large amount of memory used for the related task data."
    },
    {
        "id": 144,
        "CONTEXT": "\"## MID Server Selection for CPG - Supported Application\n\n## Description\n\nTraditionally MID server selection is based on:\n\\> Supported Applications\n\\> IP Range\n\\> Capabilities\n\nWe have a Supported Application option that's called \"Cloud Management\".\nHowever Cloud Provisioning does not support this  but only respects \"Capabilities\".\n\n## Steps to Reproduce\n\n\\> Configure a single MID server on an instance  with Supported Applications \"Discovery\" only  and Capabilities \"All\"\n\\> Launch a stack from Cloud User Portal\n\\> Check ecc_queue table  the MID server is used to run APIProxyProbe\n\\> Provision works successfully even though the MID server doesn't have Supported Application \"Cloud Management\"\n\n## Workaround\n\nImport and commit attached update set.\n\n**Related Problem: PRB1641341**\n\n\n\n## Configure MID Server capabilities\n\n# Configure MID Server capabilities {#ariaid-title1}\n\nMID Server capabilities define the specific functions of a MID Server within an IP address range  allowing an application to select the most appropriate MID Server. Configure capabilities on MID Servers for applications like Orchestration  Cloud Management  and Service Mapping.\nRole required: admin or sm_admin\n\n|-------------------------------------------------------------------------------------|\n| ![Setup indicator for configuration phase](..\/reuse\/..\/image\/ProgressBarConfig.png) |\n{#t_ConfigureCapabilities__table_u1g_ts4_nhb}\nSeveral applications use capabilities  IP ranges  and [..\/concept\/c_MIDServerSelector.html](..\/concept\/c_MIDServerSelector.html) to narrow the pool of MID Servers the applications need.\nNote: At least one capability is required for each MID Server used by Orchestration. See [..\/..\/orchestration\/concept\/c_OrchestrationMID.html](..\/..\/orchestration\/concept\/c_OrchestrationMID.html) for more information.\nThe following capabilities are available by default with Discovery:\n\n|------------------|------------|-------------|\n| All              | IBM        | Resolve DNS |\n| Ansible          | JDBC       | REST        |\n| AWS              | NetApp     | SNMP        |\n| Azure            | Nmap       | SOAP        |\n| Chef             | OpenStack  | SSH         |\n| Cloud Management | PowerShell | VMware      |\n| Google           | RCA        | WMI         |\n{#t_ConfigureCapabilities__table_mqg_wkg_f3b}\n\n1. Navigate to All \\> MID Server \\> Capabilities.\n2. Select an existing capability or select ALL to include all capabilities.  \n   Note: Ensure that each IP address range has MID Servers with the necessary capabilities to complete the Orchestration activities on that network segment.\n3. Create a new capability:\n   1. Click New. {#t_ConfigureCapabilities__cmd_substepa}\n   2. Configure the value for a custom capability.  \n      An example is a capability for DOMAIN  with a value of service-now. {#t_ConfigureCapabilities__cmd_substepb}\n   3. Click Submit. {#t_ConfigureCapabilities__cmd_substepc}\n   {#t_ConfigureCapabilities__substeps_tlb_gf1_3v}\n4. Click Edit in the MID Servers related list to add MID Servers to the capability. {#t_ConfigureCapabilities__step_step4}\n{#t_ConfigureCapabilities__step_step4}\n5. Select one or more MID Servers for this capability from the Available list. {#t_ConfigureCapabilities__step_step5}\n{#t_ConfigureCapabilities__step_step5}\n6. Click Save.  \n   The capability defined here also appears in the primary record for this MID Server.\n   {#t_ConfigureCapabilities__step_step6}\n{#t_ConfigureCapabilities__step_step6}  \n**Parent Topic:** [Configuring MID Servers](product\/mid-server\/concept\/c_MIDServerConfiguration.html \"After installing and validating your MID Servers  ensure that they have access to sufficient system resources  probe the proper targets  and communicate with the instance as expected. Configure MID Server selection criteria  create clusters for failover protection  and set up MID Servers in different domains to protect data.\")  \n\n## MID Server capabilities {#ariaid-title2}\n\nMID Server capabilities define the specific functions of a MID Server within an IP address range.{#ph_short-desc}\n\n### Nmap capability {#r_MIDServerCapabilities__section_gfp_mwr_t1b}\n\nThe Nmap capability is only assigned to MID Servers for which the Network Mapper (Nmap) scanner has been installed for [credential-less Discovery](..\/product\/discovery\/concept\/nmap-credential-less-discovery.dita\/nmap-credential-less-discovery.html). This capability cannot be added to or removed from any MID Server manually. For instructions on installing or uninstalling Nmap  see [..\/task\/install-nmap-on-mid-server.html](..\/task\/install-nmap-on-mid-server.html)\n\n### MID Server capability values {#r_MIDServerCapabilities__section_xyd_n3v_s1b}\n\nCapabilities provided in the base system do not have a defined value string. The capability value can be blank  a single value  or a \\* (wildcard). A MID Server configured to use a capability that has no value can locate any device using that capability's protocol. If a capability has a defined value  the MID Server using that capability finds only those devices using that protocol that match the value string exactly. The exception to this is the Resolve DNS capability  which is configured to resolve any DNS name into an IP address using a partial string match. The \\* (wildcard) will match a request capability specifying any non-blank value.\n\nStarting with the Madrid release  the \\[capability name\\]:\\[value\\] combination appears in the slushbucket when you add a capability to a MID Server. This combination allows you to see all the capabilities that have different values  even if the capability name is the same. For example  if you are using the Cloud Management capability  and you use the value field to specify the us-west logical datacenter on one of the capability records  you can see the combination in the Collection list.\n\nCapability name and value combination\n\n### MID Server selection {#r_MIDServerCapabilities__section_jdr_znl_cmb}\n\nThe MID Server is selected using an algorithm based on the capabilities available. The selector produces a list of suitable MIDs using filters in the following order: server status \\> applications \\> IP range \\> capabilities. The algorithm searches the capabilities in the following sequence and  if a step finds at least one MID server  the remaining steps are skipped:\n\n1. Exact value match: Find each MID Server containing capabilities with name\/value pairs with an exact match to all requested capability name\/value pairs. Blank is a valid value. \\* (wildcard) values are ignored.\n2. \n\n# Configure MID Server capabilities {#ariaid-title1}\n\nMID Server capabilities define the specific functions of a MID Server within an IP address range  allowing an application to select the most appropriate MID Server. Configure capabilities on MID Servers for applications like Orchestration  Cloud Management  and Service Mapping.\nRole required: admin or sm_admin\n\n|-------------------------------------------------------------------------------------|\n| ![Setup indicator for configuration phase](..\/reuse\/..\/image\/ProgressBarConfig.png) |\n{#t_ConfigureCapabilities__table_u1g_ts4_nhb}\nSeveral applications use capabilities  IP ranges  and [..\/concept\/c_MIDServerSelector.html](..\/concept\/c_MIDServerSelector.html) to narrow the pool of MID Servers the applications need.\nNote: At least one capability is required for each MID Server used by Orchestration. See [..\/..\/orchestration\/concept\/c_OrchestrationMID.html](..\/..\/orchestration\/concept\/c_OrchestrationMID.html) for more information.\nThe following capabilities are available by default with Discovery:\n\n|------------------|------------|-------------|\n| All              | IBM        | Resolve DNS |\n| Ansible          | JDBC       | REST        |\n| AWS              | NetApp     | SNMP        |\n| Azure            | Nmap       | SOAP        |\n| Chef             | OpenStack  | SSH         |\n| Cloud Management | PowerShell | VMware      |\n| Google           | RCA        | WMI         |\n{#t_ConfigureCapabilities__table_mqg_wkg_f3b}\n\n1. Navigate to All \\> MID Server \\> Capabilities.\n2. Select an existing capability or select ALL to include all capabilities.  \n   Note: Ensure that each IP address range has MID Servers with the necessary capabilities to complete the Orchestration activities on that network segment.\n3. Create a new capability:\n   1. Click New. {#t_ConfigureCapabilities__cmd_substepa}\n   2. Configure the value for a custom capability.  \n      An example is a capability for DOMAIN  with a value of service-now. {#t_ConfigureCapabilities__cmd_substepb}\n   3. Click Submit. {#t_ConfigureCapabilities__cmd_substepc}\n   {#t_ConfigureCapabilities__substeps_tlb_gf1_3v}\n4. Click Edit in the MID Servers related list to add MID Servers to the capability. {#t_ConfigureCapabilities__step_step4}\n{#t_ConfigureCapabilities__step_step4}\n5. Select one or more MID Servers for this capability from the Available list. {#t_ConfigureCapabilities__step_step5}\n{#t_ConfigureCapabilities__step_step5}\n6. Click Save.  \n   The capability defined here also appears in the primary record for this MID Server.\n   {#t_ConfigureCapabilities__step_step6}\n{#t_ConfigureCapabilities__step_step6}  \n**Parent Topic:** [Configuring MID Servers](product\/mid-server\/concept\/c_MIDServerConfiguration.html \"After installing and validating your MID Servers  ensure that they have access to sufficient system resources  probe the proper targets  and communicate with the instance as expected. Configure MID Server selection criteria  create clusters for failover protection  and set up MID Servers in different domains to protect data.\")  \n\n## MID Server capabilities {#ariaid-title2}\n\nMID Server capabilities define the specific functions of a MID Server within an IP address range.{#ph_short-desc}\n\n### Nmap capability {#r_MIDServerCapabilities__section_gfp_mwr_t1b}\n\nThe Nmap capability is only assigned to MID Servers for which the Network Mapper (Nmap) scanner has been installed for [credential-less Discovery](..\/product\/discovery\/concept\/nmap-credential-less-discovery.dita\/nmap-credential-less-discovery.html). This capability cannot be added to or removed from any MID Server manually. For instructions on installing or uninstalling Nmap  see [..\/task\/install-nmap-on-mid-server.html](..\/task\/install-nmap-on-mid-server.html)\n\n### MID Server capability values {#r_MIDServerCapabilities__section_xyd_n3v_s1b}\n\nCapabilities provided in the base system do not have a defined value string. The capability value can be blank  a single value  or a \\* (wildcard). A MID Server configured to use a capability that has no value can locate any device using that capability's protocol. If a capability has a defined value  the MID Server using that capability finds only those devices using that protocol that match the value string exactly. The exception to this is the Resolve DNS capability  which is configured to resolve any DNS name into an IP address using a partial string match. The \\* (wildcard) will match a request capability specifying any non-blank value.\n\nStarting with the Madrid release  the \\[capability name\\]:\\[value\\] combination appears in the slushbucket when you add a capability to a MID Server. This combination allows you to see all the capabilities that have different values  even if the capability name is the same. For example  if you are using the Cloud Management capability  and you use the value field to specify the us-west logical datacenter on one of the capability records  you can see the combination in the Collection list.\n\nCapability name and value combination\n\n### MID Server selection {#r_MIDServerCapabilities__section_jdr_znl_cmb}\n\nThe MID Server is selected using an algorithm based on the capabilities available. The selector produces a list of suitable MIDs using filters in the following order: server status \\> applications \\> IP range \\> capabilities. The algorithm searches the capabilities in the following sequence and  if a step finds at least one MID server  the remaining steps are skipped:\n\n1. Exact value match: Find each MID Server containing capabilities with name\/value pairs with an exact match to all requested capability name\/value pairs. Blank is a valid value. \\* (wildcard) values are ignored.\n2. \"\n\n",
        "QUERY": " 'Cloud provisioning works though the MID server doesn't have Cloud management'",
        "GROUND_TRUTH": "According to KB12345, traditionally, MID server selection is based on Supported Applications, IP Range, and Capabilities. The Cloud Management is a Supported Application option. However, Cloud Provisioning does not support this but only respects \"Capabilities\". This means that even if the MID server doesn't have the Supported Application \"Cloud Management\", Cloud Provisioning can still work successfully as long as the MID server has the necessary \"Capabilities\". This is demonstrated in the \"Steps to Reproduce\" section where a single MID server with Supported Applications \"Discovery\" only and Capabilities \"All\" is used to successfully launch a stack from the Cloud User Portal and run APIProxyProbe."
    },
    {
        "id": 145,
        "CONTEXT": "\"## Field Level Write ACL Does not respect Condition when triggered by GlideRecordSecure\n\n## Description\n\nScripts are allowing record updates (PUT) when the request is sent via ServiceNow script by users that don't have met ACL conditions.\n\n## Steps to Reproduce\n\nSet up sc_task.comments write ACL requiring a custom role and a specific assignment group for the condition. Remove admin override.\n\nCreate an sc_task and assign to an assignment group other than the one set up in the ACL.\n\nCreate a user with the custom role and admin.\n\nGo to Script Background Run the following:\nvar currentUser = gs.getUserID();\nvar impUser = new GlideImpersonate();\nimpUser.impersonate(\\<users sys_id\\>);\n\nvar gr = new GlideRecordSecure('sc_task');\ngr.get(\\<sc_task sys_id\\>);\ngr.comments = 'test comment';\n\ngr.update();\n\nimpUser.impersonate(currentUser);\n\nResults: User is able to post the comment even though it should be stopped by the condition\n\nChange Role requirement on the sc_task.comments ACL to a role the user doesn't have. Remove the assignment group condition.\n\nRun script again.\n\nResults: Script fails due to authorization.\n\n## Workaround\n\nA workaround is not available.\n\n**Related Problem: PRB1533263**\n\n\n\n## Self Hosted ACL Maintenance\n\n### Overview\n\n**2023-10-28 Update:** ServiceNow has added an additional script  labeled \"KB1555340 ACL and Widget Maintenance Combined 10_28.txt\"  to this KB that is designed to enhance the security of your instance by applying an update to select Service Portal widgets. Please note this script contains additional changes as well as the changes present in \"KB1555340 Widget Maintenance.txt\"  \"KB1555340 Invalid Role ACL Update Script.txt\" and \"KB1555340 ACL Update Script.txt\". Refer to the \"Procedure to Apply ACL and Widget Combined Update\" section below for instructions to apply these changes.\n**2023-10-25 and 2023-10-26 Update:**Based on further investigation and customer feedback following the security updates that ServiceNow provided during the week of 2023-10-15 to 2023-10-21  ServiceNow has added an. additional script  labeled \"KB1555340 Widget Update.txt\"  to this KB that is designed to enhance the security of your instance by applying an update to select Service Portal widgets.\n**2023-10-20 Update:** If you have applied the update using the original script  please review the changes and also apply the new script  labeled \"KB1555340 Invalid Role ACL Update Script.txt\".\n\nThis article describes the steps to apply a security update for some of the existing ACLs that do not contain any roles  script  and conditions. As described in KB1553688  in certain circumstances  an ACL configured to have empty roles  scripts  and conditions can grant unintended access the data within the tables associated with the ACLs.\nSelf-hosted customers were notified to take action to apply a security update to their instance(s). Please follow the steps below to apply the update.\n**What is the purpose of this?**\nServiceNow has determined that updating existing ACLs that do not contain roles  scripts and conditions may be necessary to prevent unintended access. This KB provides guidance on how on-premise customers can apply this change.\n\n### Procedure to Apply ACL Maintenance\n\n1. Download the files attached to this KB  which are named:\n   1. \"KB1555340 ACL Update Script.txt\"\n   2. \"KB1555340 Invalid Role ACL Update Script.txt\"\n2. Request temporary \"maint\" access via Now Support  then log in to your instance as \"maint\".\n3. Perform the following steps for each script.\n4. Navigate to \"System Definition\" \\> \"Scripts -- Background\".\n5. Set the \"Record for rollback?\" checkbox  as this will provide an option to rollback your changes  if needed. For further details  please refer to the following documentation: [https:\/\/docs.servicenow.com\/csh?topicname=c_ScriptsBackground.html](https:\/\/docs.servicenow.com\/csh?topicname=c_ScriptsBackground.html)\n6. Copy the contents of the downloaded file into the textbox labeled \"Run script (JavaScript executed on server)\"\n7. Dry Run: Run the script in dry run mode by setting the variable dry_run_mode = true  near the top of the script. Click \"Run script\". Observe the output values in the JSON string. Please note  that an example output is provided below.\n   1. \"KB1555340 ACL Update Script.txt\":  \n   2. \"KB1555340 Invalid Role ACL Update Script.txt\":\n      1. The script will output similar content to the \"KB1555340 ACL Update Script.txt\"; however  it will also include ACLs that will be changed.\n8. Updating ACLs: Edit the script to set dry_run_mode = false and click \"Run script\" again. Observe the output values in the JSON string. Please note  that an example output is provided below.\n   1. \"KB1555340 ACL Update Script.txt\":  \n   2. \"KB1555340 Invalid Role ACL Update Script.txt\":\n      1. The script will output similar content to the \"KB1555340 ACL Update Script.txt\"; however  it will also include ACLs that were changed.  \n9. Repeat Dry Run: Repeat step 5. to run in Dry Run mode and confirm that the reported ACL counts show 0.\n   1. \"KB1555340 ACL Update Script.txt\"  \n   2. KB1555340 Invalid Role ACL Update Script.txt\n      1. The script will output similar content to the \"KB1555340 ACL Update Script.txt\"; if successful  you should see a count of 0 and no ACLs listed.\n\n### Procedure to Apply ACL and Widget Combined Update\n\n1. Download the attached script  named \"KB1555340 ACL and Widget Maintenance Combined 10_28.txt\".\n   * Please note  this script contains additional updates and as well as the updates that are present in \"KB1555340 Widget Maintenance.txt\"  \"KB1555340 Invalid Role ACL Update Script.txt\" and \"KB1555340 ACL Update Script.txt\".\n2. Request temporary \"maint\" access via Now Support  then log in to your instance as \"maint\".\n3. Navigate to \"System Definition\" \\> \"Scripts -- Background\".\n4. Set the \"Record for rollback?\" checkbox  as this will provide an option to rollback your changes  if needed. For further details  please refer to the following documentation: [https:\/\/docs.servicenow.com\/csh?topicname=c_ScriptsBackground.html](https:\/\/docs.servicenow.com\/csh?topicname=c_ScriptsBackground.html)\n5. Copy the contents of the downloaded file into the textbox labeled \"Run script (JavaScript executed on server)\"\n6. Click \"Run script\".\n7. Once the update has been applied  review the new System Properties `glide.service_portal.\n\n## What happens to a record in sys_attachment_doc when the associated record in sys_attachment is archi\n\nHave you ever wondered what happens to the record in the sys_attachment_doc table when its associated record in the sys_attachment table is archived? This article talks about this specific scenario as multiple customers are frequently reporting this as an issue.\n\n* By archiving a record in the sys_attachment table(along with the parent record of any table)  the related record in the sys_attachment_doc table will not get archived and it will be hidden in the instance(There will be no archived table created for the sys_attachment_doc table).\n* This behavior is due to the OOB READ ACL present on the sys_attachment_doc table. Below is the screenshot of OOB ACL for reference:  \n  [https:\/\/instancename.service-now.com\/sys_security_acl.do?sys_id=0b57466f0a0006d400deede3fb639b98](https:\/\/instancename.service-now.com\/sys_security_acl.do?sys_id=0b57466f0a0006d400deede3fb639b98)  \n* The record leftover in the sys_attachment_doc table is generally considered as an orphan record as its associated record in the sys_attachment table is removed \\& placed in the ar_sys_attachment table.\n* But  if you run the destroy rule on the parent table(with the destroy related records option checked)  the parent record along with its associated records in the sys_archive_log  ar_sys_attachment  sys_attachment_doc records will be deleted \\& thus no orphan records will exist in the platform.\n\n**How can we test this behavior in any OOB instance?**\n\n1. Create an archive rule on any table say incident\/change_request\n2. Create an archive-related rule on the sys_attachment table in the above parent rule.\n3. Create a record in the change_request(parent) table \\& add an attachment to the change record; Note the sys_ids of the records in the  \n   (a) change_request table (parent record)  \n   (b) sys_attachment table (associated attachment record) \\&  \n   (c) sys_attachment_doc table(for 1 attachment  there can be 1 or more records created in DOC table  so please note the sys_ids of all individual records related to the attachment of change_request record for reference)\n4. Run the archive rule on the change_request table.\n5. You will notice the change_request record along with the related sys_attachment record will be archived and will be moved to ar_change_request \\& ar_sys_attachment tables.\n6. Navigate to the sys_attachment_doc table LIST view \\& search with the sys_id of the sys_attachment_doc record you have noted in step 3(c).\n7. You will not find any record(This is due to the above ACL restricting access to view the records)\n8. Try accessing the record by using the below sample URL \\& you will find the record exists but it shows a blank page:  \n   [https:\/\/instancename.service-now.com\/sys_attachment_doc.do?sys_id=sys_id_of_doc_table_record_copied_in_step_3c](https:\/\/instancename.service-now.com\/sys_attachment_doc.do?sys_id=sys_id_of_doc_table_record_copied_in_step_3c)\n9. You can also validate the same by logging into the database and querying the sys_attachment_doc table by using the sys_ids(noted in step 3c)\n10. Now create an archive destroy rule on the ar_change_request table with the delete related records option checked \\& run the archive destroy rule.\n11. You will notice the associated records in all the tables - ar_change_request  sys_archive_log  ar_sys_attachment including sys_attachment_doc will be deleted.\n12. You can try accessing the sys_attachment_doc form link again and now you will see the \"Record not found\" message which means that the DOC record is deleted.  \n    [https:\/\/instancename.service-now.com\/sys_attachment_doc.do?sys_id=sys_id_of_doc_table_record_copied_in_step_3c](https:\/\/instancename.service-now.com\/sys_attachment_doc.do?sys_id=sys_id_of_doc_table_record_copied_in_step_3c)\n\nIn short - Due to the ACL restriction  the records in the \"sys_attachment_doc\" table will be hidden when the attachment record is archived but they still exist in the instance data.\n\n\"\n\n",
        "QUERY": " 'Can't write to Journal fields on the child table with write ACL failure on the parent table'",
        "GROUND_TRUTH": "no answer found"
    },
    {
        "id": 146,
        "CONTEXT": "\"## Calendar Type Report shows 0:00 when 'Calendar by' field is not date-time stamp\n\n## Description\n\nCalendar Type Report shows 0:00 when 'Calendar by' field is not date-time stamp:\n\n## Steps to Reproduce\n\n1. Navigate to **Reports \\> Create New**.\n2. Fill in the fields.\n3. In the **Calendar by** field  select one that is not a date-time stamp.  \n   Note that 0:00 is displayed as shown in the screenshot.\n\n## Workaround\n\nThere are two workarounds you can try:\n\nOption 1. [Disable the new calendar reporting](https:\/\/docs.servicenow.com\/csh?topicname=c_CustomizeCalendarReports.html&version=latest \"Disable the new calendar reporting\") as these timelines were introduced from Helsinki when the calendar reporting was reworked completely.\n\nOption 2. Add a new System Property:\n\nName: glide.report.new_calendar\nType: true\\|false\nValue: false\n\nBy adding this property and setting it to **false**   the instance will use the pre-Helsinki calendar report where the time stamp will be hidden.\n\n**Related Problem: PRB699940**\n\n\n\n## Generate labor costs\n\nFor example  the scheduled job runs at 10:15 p.m. London time if Europe\/London is the default system time zone for the instance  and you enter 22:15 in the Time field. * Actual time zone. For example  the scheduled job runs at 1:30 p.m. in the US Eastern time zone if you select US\/Eastern  and enter 13:30 in the Time field. {#scheduled-script-execution-form__ul_zxv_cnb_njb}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |\n| Day               | Day on which the scheduled job should run. * If Run is set to Weekly  select the day of the week. For example  select Wednesday. * If Run is set to Monthly  select the day of the month. For example  select 25 for the 25th day of the month. {#scheduled-script-execution-form__ul-day-sched-job} This field appears only if you select Monthly or Weekly in the Run field.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |\n| Repeat Interval   | Duration of the repeat interval for each scheduled job execution. Enter the duration in the number of days  hours  or minutes. For example: * To run the scheduled job every four days  enter 04 in the Days field. * To run it every 26 hours  enter 26 in the Hours field. * If it should repeat at an interval of 13:30:25  enter 13 in the Hours field  and then enter 30 and 25 in the two unlabeled fields after it. {#scheduled-script-execution-form__ul_bmk_m2b_njb} These fields appear only if you select Periodically in the Run field. Note: This setting does not account for Daylight Saving Time changes. For example  if you select a period of one day  the calculation adds 24 hours to the starting time of the job. If the start time is in a Daylight Saving Time (DST) period  the job runs with a one-hour offset when that time zone is not in DST.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n| Business Calendar | Business calendar entry that you are using to determine the business calendar start or end date for the scheduled job. This field appears only if you select Business Entry: Start Date or Business Entry: End Date in the Time field.{#scheduled-script-execution-form__business-calendar-sched-job}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n| Offset Type       | Type of time offset  if any  to apply to the business calendar that you selected for scheduling this job: * Past: Apply an offset factor to schedule the job to run before the start of the time span of the selected business calendar. * Future: Apply an offset factor to schedule the job to run after the end of the time span of the selected business calendar. * --None--: Do not apply a time offset when scheduling this job. {#scheduled-script-execution-form__ul_dfj_ptw_5lb} Adding an offset factor enables you to schedule the job to run before  or after  the formal time span that is defined in the business calendar for the following use cases: * Schedule a job at a certain time  outside of the time span for the selected business calendar. * Arrange multiple jobs to run in sequence  around the time span of the selected business calendar. {#scheduled-script-execution-form__ul_onj_13x_5lb} The Offset type and Offset fields appear only if you select Business Calendar: Entry Start or Business Calendar: Entry End in the Run field.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n| Offset            | Amount of time offset  expressed in days  hours  minutes  and seconds  to apply to the business calendar that you selected for scheduling this job. For example  if you want to schedule the job to start three days  14 hours  10 minutes  and 45 seconds before the business calendar start date  do the following actions: * Select Past in the Offset type field. * Enter 3 in the Days field. * Enter 14  10  and 45 in the Hours field. {#scheduled-script-execution-form__ul_aty_1ww_5lb} The Offset Days and Hours fields appear only if you select Business Calendar: Entry Start or Business Calendar: Entry End in the Run field  and Past or Future in the Offset type field.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n| Time              | Time of day at which the scheduled job should run  expressed in hours  minutes  and seconds on a 24-hour clock. The selection that you make in the Time zone field determines the time zone for this entry. Note: Time values are always saved in the Now Platform\u00ae in UTC time and then translated into the proper time. This translation depends on the selected Time zone and the entry in the Time field. This field appears only if you select Daily  Weekly  or Monthly in the Run field.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |\n| Starting          | Date and time of the first scheduled job generation. Select the calendar date and time. This field appears only if you select Periodically in the Run field.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n\n\n## Date and time format guidelines\n\nInstead  you can use next() by itself to move to the next record. The following example provides a comparison between GlideRecord and GlideRecordSecure.\n\n### Examples\n\nThese are two simple examples using GlideRecordSecure.\n\n## GlideSystem {#ariaid-title6}\n\nThe GlideSystem API provides methods for retrieving information.\n\nThe GlideSystem (referred to by the variable name '<var class=\"keyword varname\">gs<\/var>' in business rules) provides a number of convenient methods to get information about the system  the current logged in user  etc. For example  the method addInfoMessage() permits communication with the user.\n\nMany of the GlideSystem methods facilitate the easy inclusion of dates in query ranges and are most often used in filters and reporting.\n\nFor additional information  see [GlideSystem](..\/..\/..\/app-store\/dev_portal\/API_reference\/glideSystemScoped\/concept\/c_GlideSystemScopedAPI.html \"The scoped GlideSystem (referred to by the variable name 'gs' in any server-side JavaScript) API provides a number of convenient methods to get information about the system  the current logged in user  etc.\").\n\n## GlideDateTime {#ariaid-title7}\n\nThe GlideDateTime class provides methods for performing operations on GlideDateTime objects  such as instantiating GlideDateTime objects or working with <var class=\"keyword varname\">glide_date_time<\/var> fields.\n\nIn addition to the instantiation methods described below  a GlideDateTime object can be instantiated from a <var class=\"keyword varname\">glide_date_time<\/var> field using the getGlideObject() method (for example  `var gdt\n= gr.my_datetime_field.getGlideObject();`).\n\nSome methods use the Java Virtual Machine time zone when retrieving or modifying a date and time value. Using these methods may result in unexpected behavior. Use equivalent local time and UTC methods whenever possible.\n**Related concepts**\n\n* [GlideDate](..\/..\/..\/app-store\/dev_portal\/API_reference\/GlideDate\/concept\/GlideDateAPI.html \"The GlideDate class provides methods for performing operations on GlideDate objects  such as instantiating GlideDate objects or working with GlideDate fields.\")\n* [GlideDate](..\/..\/..\/app-store\/dev_portal\/API_reference\/glideDateScoped\/concept\/c_GlideDateScopedAPI.html \"The scoped GlideDate class provides methods for performing operations on GlideDate objects  such as instantiating GlideDate objects or working with GlideDate fields.\")\n* [GlideDateTime](..\/..\/..\/app-store\/dev_portal\/API_reference\/GlideDateTime\/concept\/c_GlideDateTimeAPI.html \"The GlideDateTime class provides methods for performing operations on GlideDateTime objects  such as instantiating GlideDateTime objects or working with glide_date_time fields.\")\n* [GlideDateTime](..\/..\/..\/app-store\/dev_portal\/API_reference\/glideDateTimeScoped\/concept\/c_GlideDateTimeScoped.html \"The scoped GlideDateTime class provides methods for performing operations on GlideDateTime objects  such as instantiating GlideDateTime objects or working with glide_date_time fields.\")\n* [GlideTime](..\/..\/..\/app-store\/dev_portal\/API_reference\/glideTimeScoped\/concept\/c_GlideTimeScopedAPI.html \"The scoped GlideTime class provides methods for performing operations on GlideTime objects  such as instantiating GlideTime objects or working with GlideTime fields.\")  \n**Related reference**   \n* [Modify a GlideDateTime field value](..\/..\/useful-scripts\/reference\/r_ModifyAGlideDateTimeFieldValue.html \"This example demonstrates how to modify a GlideDateTime field value using a server-side script.\")  \n\n### GlideDate and GlideDateTime examples {#ariaid-title8}\n\nThe GlideDate and GlideDateTime APIs are used to manipulate date and time values.\nNote: This functionality requires a knowledge of JavaScript.\n\nFor additional information  refer to [GlideDate](..\/..\/..\/app-store\/dev_portal\/API_reference\/glideDateScoped\/concept\/c_GlideDateScopedAPI.html \"The scoped GlideDate class provides methods for performing operations on GlideDate objects  such as instantiating GlideDate objects or working with GlideDate fields.\") API and [GlideDateTime](..\/..\/..\/app-store\/dev_portal\/API_reference\/glideDateTimeScoped\/concept\/c_GlideDateTimeScoped.html \"The scoped GlideDateTime class provides methods for performing operations on GlideDateTime objects  such as instantiating GlideDateTime objects or working with glide_date_time fields.\") API.\nYou can create a GlideDateTime object from a GlideDate object by passing in the GlideDate object as a parameter to the GlideDateTime constructor. By default  the GlideDateTime object is expressed in the internal format  yyyy-MM-dd HH:mm:ss and the system time zone UTC.\n\nOutput:\n\nSee also:\n\n* [Modify a GlideDateTime field value](..\/..\/useful-scripts\/reference\/r_ModifyAGlideDateTimeFieldValue.html \"This example demonstrates how to modify a GlideDateTime field value using a server-side script.\")\n* [GlideTime](..\/..\/..\/app-store\/dev_portal\/API_reference\/glideTimeScoped\/concept\/c_GlideTimeScopedAPI.html \"The scoped GlideTime class provides methods for performing operations on GlideTime objects  such as instantiating GlideTime objects or working with GlideTime fields.\")\n{#r_GlideDate_GlideDateTime_examples__ul_tbs_4sr_fzb}  \n\n### Set a duration field value in script {#ariaid-title9}\n\nExamples of JavaScript that can be used to set the value of a duration field.\nNote: Negative duration values are not supported.\n\n#### Using the GlideDateTime.subtract() method\n\nThe subtract(GlideDateTime start  GlideDateTime end) method in [GlideDateTime](..\/..\/..\/app-store\/dev_portal\/API_reference\/glideDateTimeScoped\/concept\/c_GlideDateTimeScoped.html \"The scoped GlideDateTime class provides methods for performing operations on GlideDateTime objects  such as instantiating GlideDateTime objects or working with glide_date_time fields.\") enables you to set the duration value using a given start date\/time and end date\/time. An example on how to set the duration for the time a task was opened is:\n\nIf you want to work with the value returned as a number to use in date or duration arithmetic  convert the return to milliseconds:\n\n\"\n\n",
        "QUERY": " 'Calendar reporting shows 0:00 when the Calender field is not a date-time stamp'",
        "GROUND_TRUTH": "The issue of 'Calendar reporting showing 0:00 when the Calendar field is not a date-time stamp' occurs when a non-date-time stamp field is selected in the 'Calendar by' field. This issue can be resolved by two workarounds as mentioned in the KB12345 article. \n\nOption 1: You can disable the new calendar reporting. This is because the timelines were introduced from Helsinki when the calendar reporting was reworked completely. You can follow the link provided in the article to disable the new calendar reporting.\n\nOption 2: You can add a new System Property with the following details:\n- Name: glide.report.new_calendar\n- Type: true\\|false\n- Value: false\n\nBy adding this property and setting it to false, the instance will use the pre-Helsinki calendar report where the time stamp will be hidden."
    },
    {
        "id": 147,
        "CONTEXT": "\"## In CMDB tables  Auto numbering field is not working properly when combined with pre-filled value fro\n\n## Description\n\nWhen we use list filter for a field that is using auto numbering (e.g. = javascript:global.getNextObjNumberPadded()); for example: cmdb_ci_business_app - Number field; and then click New  we prefill the field's value with list filter's value.\nThis is fine for normally editable fields like Company or Caller  but we should respect the form settings for non-editable fields like Number.\n\n## Steps to Reproduce\n\n1. Go to cmdb_ci_business_app table in list view.\n2. Filter the Number field using right-click and show matching.\n3. After the list is refreshed  click the New button.\n4. Observe that the Number field is prefilled with the same number\/string you selected.\n5. This field should use the next sequence unique number\/string instead of the prefilled value from step 4.\n\n## Workaround\n\nThis problem is currently under review. You can contact [ServiceNow Technical Support](http:\/\/www.servicenow.com\/support\/contact-support.html) or subscribe to this Known Error article by clicking the **Subscribe** button at the top right of this form to be notified when more information will become available.\n\nIf you can not wait until then  you can add attributes: \"ignore_filter_on_new=true\" in the sys_dictionary record of that Number field.\n\n**Related Problem: PRB1431792**\n\n\n\n## Auto numbering field is not working properly when combined with pre-filled value from List filter\n\n## Description\n\nWhen we use list filter for any field that is using auto numbering (e.g. = javascript:global.getNextObjNumberPadded()); and then click New  we prefill the field's value with list filter's value.\n\n## Steps to Reproduce\n\nPre-req;\n\n- Create a field (i.e. u_testfield) on any table (i.e. incident)  \n- Set the 'Use dynamic default' to true  \n- Set the 'Dynamic default value' to 'Get Next Padded Number'  \n- Create a record on the table (incident) and fill the created field (u_testfield). i.e. ASD001  \n\n1. Go to the list view of the table (incident.list)  \n2. Filter the list according the created field. (\/incident_list.do?sysparm_query=u_testfield%3DASD001)  \nor select 'Show matching' on the field's value.  \n3. Click New  \n\nActual behavior:\nObserve that the field (u_testfield) is prefilled with the same number\/string (ASD001) you selected.\nExpected behavior:\nThe field should use the next sequence unique number\/string instead of the prefilled value\n\n## Workaround\n\nAdd the following attribute in the sys_dictionary record of the required field. \"ignore_filter_on_new=true\"\n\n**Related Problem: PRB1537504**\n\n\n\n## Vancouver security and notable fixes\n\n{#vancouver-security-notables__ol_ywm_zd1_2yb} Notice that the following error is received:'MID Server reported error: java.sql.SQLException: java.sql.SQLException: \\[Teradata JDBC Driver\\] \\[TeraJDBC 17.10.00.17\\] \\[Error 1536\\] \\[SQLState HY000\\] Invalid connection parameter name allowLocalInfile'.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |\n| Key Management Framework (KMF) PRB1636242 [KB1263904](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB1263904)           | Password2 decryption fails during an upgrade when a new table schema hasn't been updated                                                                                                                        | An error is thrown.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | Refer to the listed KB article for details.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n| Key Management Framework (KMF) PRB1646533 [KB1291203](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB1291203)           | Code_signing_key_ release_publiccodesignver certificates are sending expiry warnings to instance administrators                                                                                                 | The user may receive a certificate expiration warning for a code_signing_key_ release_publiccodesignver certificate.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | Refer to the listed KB article for details.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n| Knowledge Management PRB1621103 [KB1206016](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB1206016)                     | AI Search does not index the tags when the user adds them to the Meta field of Info Message                                                                                                                     |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | Refer to the listed KB article for details.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n| List Administration PRB1289928 [KB1001697](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB1001697)                      | Slowness when displaying the slushbucket from the sys_user form                                                                                                                                                 | Occurs when the user navigates to Configure \\> For Layout on any sys_user record.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | Refer to the listed KB article for details.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n| List Administration PRB1587393                                                                                                   | The 'Group by' column within a 'sys_ux_list' record is not applied when accessing the list a second time                                                                                                        | 'Group by' is not retained when switching between lists on the same table in Configurable workspace.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n| List Administration PRB1614180 [KB1217354](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB1217354)                      | Switching quickly between filters in legacy Agent Workspace and HR Workspace can generate a 'Can't Display this List' error                                                                                     | When clicking frequently between lists in HR Agent Workspace  the user is faced with an error: 'Can't display this list. Try contacting the system administrator. Method invocation failure...'.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | Refer to the listed KB article for details.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n| List Administration PRB1630973 [KB1289138](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB1289138)                      | With Next Experience enabled  list columns and headers are misaligned when list actions are hidden                                                                                                              | Column headers are shifted to the right and no longer align with the column data.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | Refer to the listed KB article for details.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n| List Administration PRB1661329 [KB1314498](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB1314498)                      | When using the 'edit' feature in the list view  the Impact and Urgency fields default to '1 - High'                                                                                                             | These two fields are blank prior to Utah.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | Refer to the listed KB article for details.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n| List Editor PRB1646683 [KB1318238](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB1318238)                              | A list widget doesn't highlight multi-selected records                                                                                                                                                          | All selected cells during multi-select should be highlighted to indicate the selection.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | Refer to the listed KB article for details.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n| List Filters PRB1629748                                                                                                          | Group sort options in Report builder does not work                                                                                                                                                              | Group sort options on list report is not functional. The button is present but no list appears.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | 1. On a Tokyo instance  open the Report Builder (Reports \\> Create New). 2. Create a list type report on a table (for example  Incident). 3. Group the report under the configure tab based on a field (for example  Caller). 4. Click the 'Group Sort Options' button new to Tokyo (no list appears). {#vancouver-security-notables__ol_aym_zd1_2yb} If the user does the same thing on a list view (incident_list.do -\\> group by caller)  the 'Group Sort Options' button will contain a drop down menu with options.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n| Major Incident Management PRB1608095                                                                                             | The Work notes field has hard-coded color                                                                                                                                                                       | The background-color is from a stylesheet that is not part of Next Experience.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | 1. Switch the instance theme to dark mode. 2. Navigate to the incident communication plans. 3. Create a new incident communication plan. {#vancouver-security-notables__ol_tym_zd1_2yb} Notice that the work notes field is filled with a very bright yellow color.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     ...",
        "QUERY": " 'Auto numbering field is not working properly when combined with pre-filled value from List filter'",
        "GROUND_TRUTH": "The issue arises when a list filter is used for a field that uses auto numbering. When you click 'New', the field's value is pre-filled with the list filter's value. This is not a problem for normally editable fields, but for non-editable fields like 'Number', the form settings should be respected. \n\nTo reproduce the issue, you can follow these steps:\n1. Go to the cmdb_ci_business_app table in list view.\n2. Filter the Number field using right-click and show matching.\n3. After the list is refreshed, click the New button.\n4. Observe that the Number field is pre-filled with the same number\/string you selected.\n5. This field should use the next sequence unique number\/string instead of the prefilled value from step 4.\n\nThe workaround for this issue is to add the attribute \"ignore_filter_on_new=true\" in the sys_dictionary record of the Number field. This problem is currently under review and you can contact ServiceNow Technical Support or subscribe to this Known Error article to be notified when more information will become available."
    },
    {
        "id": 148,
        "CONTEXT": "\"## Clone automation restarts nodes while scheduler set to 'specified'\n\n## Description\n\nPost clone  application nodes may not have the correct scheduled job configuration.\n\nSpecifically jobs with parent record set to 'Active Nodes' fail to be created per application node preventing them from running\n\nThis will impact several areas of the platform including event processing\n\n## Steps to Reproduce\n\nReview a recently cloned instance for which application nodes have not restarted post clone\n\n## Workaround\n\nIf an instance has been impacted then no jobs set to 'Active Nodes' will run. You have two options to fix this issue:\n\n1. Restart the instance - This can be requested from ServiceNow by raising a new Case via the NowSupport Portal\n2. Manually trigger the creation of the jobs. This is a straight forward process that can be actioned as follows:\n   1. Navigate to sys_trigger.list\n   2. Filter for all jobs where 'System ID' = 'Active Nodes'\n   3. For each record perform the following:  \n      1. Open the record\n      2. Update the 'Next Action' time and increase by 1 second\n      3. Save the record\n\nThis will generate all the child records for these jobs and restart processing of events and other functions\n\n**Related Problem: PRB1576973**\n\n\n\n## Description\n\nAfter cloning  application nodes may not have the correct scheduled job configuration.\n\nSpecifically jobs with parent record set to 'Active Nodes' fail to be created per application node preventing them from running\n\nThis will impact several areas of the platform including event processing\n\n## Steps to Reproduce\n\nReview a recently cloned instance for which application nodes have not restarted post clone\n\nA way to determine if your instance is affected post clone is to search the sys_trigger table for the number of 'events process 0' records. You should have 1 + 1 each for every application node. So a standard 2 node sub-production instance should have 3. If only 1 'events process 0' record exists then the instance is affected.\n\n## Workaround\n\nIf an instance has been impacted then no jobs set to 'Active Nodes' will run. You have two options to fix this issue:\n\n1. Manually trigger the creation of the jobs. This is a straight forward process that can be actioned as follows:\n   1. Navigate to sys_trigger.list\n   2. Filter for all jobs where 'System ID' = 'Active Nodes'\n   3. For each record perform the following:  \n      1. Open the record\n      2. Update the 'Next Action' time and increase by 1 second\n      3. Save the record\n\nThis will generate all the child records for these jobs and restart processing of events and other functions\n\n2. Restart the instance - This can be requested from ServiceNow by raising a new Case via the NowSupport Portal  \n\n**Related Problem: PRB1576973**\n\n\n\n## [Application node (JVM)] App node throwing java.io.IOException: Too many open files due to IDR\/Kafka\n\n## Description\n\nApp nodes throw errors that makes instance unavailable for access and the error is \"java.io.IOException: Too many open files\"\n\n## Steps to Reproduce\n\n\\[Customer Escalation\\]\nN\/A\n\n\\[Customers impacted\\]\nInstance sensedev\n\n\\[Actual result\\]\nN\/A\n\n\\[Bug\/Enhancement\/New Feature\\]\nN\/A\n\n\\[Expected Result\\]\nN\/A\n\n## Workaround\n\nIn most cases  it was either a Clone of Hermes-enabled source to NON Hermes-enabled target that also spun up a \"Enable Hermes\" CHG during the clone.\n\nSRE provided relief by restarting the app-nodes  so a \"Rolling Restart\" of the app-nodes\/instance should provide relief. This can be done by the CS team as well as they have access to the specific Service Catalog.\n\nPS: this might be a temporary relief as the PRB owner team needs to check the issue and confirm the nodes won't hit the open files limit again and result to another outage.\n\n**Related Problem: PRB1721309**\n\n\"\n\n",
        "QUERY": " 'Application nodes are not restarting post the clone'",
        "GROUND_TRUTH": "After cloning, application nodes may not have the correct scheduled job configuration. Specifically, jobs with the parent record set to 'Active Nodes' fail to be created per application node, preventing them from running. This can impact several areas of the platform, including event processing. \n\nTo determine if your instance is affected post clone, you can search the sys_trigger table for the number of 'events process 0' records. You should have 1 + 1 each for every application node. So a standard 2 node sub-production instance should have 3. If only 1 'events process 0' record exists then the instance is affected.\n\nIf an instance has been impacted then no jobs set to 'Active Nodes' will run. You have two options to fix this issue:\n\n1. Manually trigger the creation of the jobs. This is a straight forward process that can be actioned as follows:\n   1. Navigate to sys_trigger.list\n   2. Filter for all jobs where 'System ID' = 'Active Nodes'\n   3. For each record perform the following:  \n      1. Open the record\n      2. Update the 'Next Action' time and increase by 1 second\n      3. Save the record\n\nThis will generate all the child records for these jobs and restart processing of events and other functions\n\n2. Restart the instance - This can be requested from ServiceNow by raising a new Case via the NowSupport Portal."
    },
    {
        "id": 149,
        "CONTEXT": "\"## The Agile Story field is blank for stories which are created from the triage board.\n\n## Description\n\nWhen adding any incident to the backlog from the triage board  the story is created as expected. However  the 'Agile Story' field is blank in respective story.\n\nThe 'Agile Story' field is populated correctly when the story is created from:\n\n* **Agile Board \\> Create story**\n* or\n* rm_story table (rm_list.do \\> New)  \n\n## Steps to Reproduce\n\nOn an affected instance:\n\n1. Create an Agile Board.\n2. Create a Triage Board on incidents in that Agile board.\n3. Add any Incident to Backlog.\n4. Observe that New story created for that incident.\n5. Observe that the new story's Agile field is empty.\n\n## Workaround\n\nThis is working as expected for the following reason:\n\nFor Task derivatives like incidents and problems  when they get added to the backlog  we populate the agile story field in the incident or problems table  but for wrapper story which gets created for the incident which was added in the backlog we only set the original task field pointing to the incident  the agile story is not populated for wrapper story. This is as per design.\n\n**Related Problem: PRB1457284**\n\n\n\n## Polaris: Toggle Domain Scope + Reload Form does NOT work when opening form in Next Experience UI\n\n## Description\n\nToggle Domain Scope + Reload Form does NOT work when opening form in Next Experience UI\n\n## Steps to Reproduce\n\n1. Create a \"Parent\" domain as TOP\/Parent  \n2. Create a Company (core_company) named \"Parent Company\" that is in the \"TOP\/Parent\" domain  \n3. Create a Group (sys_user_group) named \"Parent Group\" who's company is \"Parent Company\"  \n4. Create a User (sys_user) id \"parent.user\" who's company is \"Parent Company\"  \n5. Add \"parent.user\" user to \"Parent Group\".  \n6. Grant \"domain_expand_scope\" and \"itil\" roles to \"parent.user\" user.  \n7. Create a \"Child\" domain as TOP\/Parent\/Child  \n8. Create a Company (core_company) named \"Child Company\" that is in the \"TOP\/Parent\/Child\" domain  \n9. Create a Group (sys_user_group) named \"Child Group\" who's company is \"Child Company\"  \n10. Create a User (sys_user) id \"child.user\" who's company is \"Child Company\"  \n11. Add \"child.user\" user to \"Child Group\".  \n12. Grant \"itil\" role to \"child.user\" user.  \n13. Create a new incident record.  \n14. Set incident record domain to TOP\/Parent\/Child domain by setting Caller field to \"child.user\".  \n15. Set assignment_group to \"Parent Group\".  \n16. Set assignment_to to \"parent.user\".  \n17. Impersonate \"parent.user\" (who is in Parent domain)  \n18. Using the UI16 user interface  navigate to the newly created incident record.  \nYou will see the 'Assignment Group' and 'Assigned to' fields being empty.  \n19. Via the form context menu  click 'Toggle Domain Scope'  then reload the form.  \nYou will then see the 'Assignment Group' and 'Assigned to' fields displaying their set values  'Parent Group' and 'Parent User' respectively.  \n20. Make sure \"glide.ui.polaris.experience\" sys_properties value is \"true\" and enable the Next Experience user interface.  \n21. Navigate to the same incident record.  \n22. Again  you will see the 'Assignment Group' and 'Assigned to' fields being empty.  \n23. Via the form context menu  click 'Toggle Domain Scope'  then reload the form.\n\nExpected behaviour: You will then see the 'Assignment Group' and 'Assigned to' fields displaying their set values  'Parent Group' and 'Parent User' respectively.\nActual behaviour: The fields do not display the expected values.\n\n## Workaround\n\nThis problem is currently under review and has no workaround. To receive notifications when more information will become available  subscribe to this Known Error article by clicking the **Subscribe** button at the top of this article. If you are able to upgrade  review the **Fixed In** or **Intended Fix Version** fields to determine whether any versions have a planned or permanent fix.\n\n**Related Problem: PRB1576627**\n\n\n\n## San Diego Patch 10\n\nIf the 'content_active' field for a lifecycle is updated  the corresponding 'Active' field should also be updated in the Software Product Lifecycles \\[sam_sw_product_lifecycle\\] table.                                                                                                                                                                                                                                                                                                                                               | Refer to the listed KB article for details. |\n| UI Policies PRB1562733                                                                                            | Issue with reference qualifier script implementation in the new seismic modal form                                                                                | When a user updates the assignment group and clicks in the assigned-to reference field  the 'getEncodedRecord' function in the field.js should receive the correct value for 'isNewRecord' as specified in the configuration of the form. Instead  the value of 'isNewRecord' is undefined.                                                                                                                                                                                                                                                                                                                                                     |                                             |\n{#sandiego-patch-10__notable-fixes}\n\n## All other fixes {#sandiego-patch-10__section_d3c_hbd_fwb}\n\n{#d120127e431}\n\n|                                                        Problem                                                         |                                                                                               Short description                                                                                               |                                                                                                                                                                                                                                                                                           Description                                                                                                                                                                                                                                                                                           |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Steps to reproduce                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |\n|------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Access Control PRB1581854                                                                                              | Archive table ACLs can slow down other form loads when there's a significant number of archived tables in the instance                                                                                        | The performance problem takes place when the property glide.security.enable_archive_table_acls is set to true.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | 1. Ensure glide.security.enable_archive_table_acls is set to true. 2. Open an instance with many archived tables. 3. Notice the form shows slowness on load. {#sandiego-patch-10__ol_x2r_n4s_gwb}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n| Agent Chat PRB1625505                                                                                                  | The Agent Chat window is empty due to a failure in processing a picker control that doesn't have a matched value                                                                                              | Messages should be shown in the chat window but it appears blank.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | 1. Have a picker control in a topic. 2. Have an agent available in one browser. 3. Run the topic from the web client and force the value of one of the options to be something different in Chrome Inspect. 4. Select that option. 5. Notice that the topic fails and requester is redirected to Live agent. 6. Accept the work item from the live agent. Notice that the chat window is blank. {#sandiego-patch-10__ol_yg3_4jz_fwb}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |\n| Agent Workspace PRB1604316                                                                                             | A 'Choice' field value clears on Agent Workspace when using a client script  but works fine on platform                                                                                                       | The value for 'Tax Years' clears on clicking anywhere in the form.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n| AI Search Glide PRB1553655                                                                                             | When a KB is updated with a new version  search suggestions results in \/sp continue to point to the old version                                                                                               |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n| API Access Policies PRB1614023                                                                                         | OAuth token expiration period and PA\\&Reporting performance issues                                                                                                                                            | When a user schedules multiple exports close in time  all exports have the same token that has an expiration time. Not all the requests from the same user have the same timeout  as the expiration time is fixed when the token is created. User incidents have some export requests not succeed because the token expired while waiting in the queue to export.                                                                                                                                                                                                                               |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n| Authentication PRB1595695                                                                                              | KMFInstanceTypeDetector improperly detects non-prod DC instances                                                                                                                                              | Sometimes  datacenter instances do not have the glide.installation.production property set to true  which causes KMFInstanceTypeDetector to improperly classify them as unknown.                                                                                                                                                                                                                                                                                                                                                                                                                | 1. Provision a DC instance. 2. Set this property to false: glide.installation.production. 3. Generate a key pair using ensureAll. {#sandiego-patch-10__ol_oh3_4jz_fwb} Expected behavior: The Key Pair uses an external generator (for example  Vault). Actual behavior: The Key Pair is generated internally by KMF. Certificates don't contain 'vault' in their issuer.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n| Automated Test Framework (ATF) PRB1539992 [KB1001156](https:\/\/hi.service-now.com\/kb_view.do?sysparm_article=KB1001156) | Rescheduled ATF test suites and parameterized tests consume too much memory                                                                                                                                   | The suite run takes a very long time (several minutes) to resume execution. While waiting for the test to resume execution  more and more memory is consumed. In extreme cases  the instance may run out of memory and kill the test run.                                                                                                                                                                                                                                                                                                                                                       | Refer to the listed KB article for details.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |\n| Change Management PRB1567689                                                                                           | Typo in the JS output from the STTRMTransition where 'conditon' should be 'condition'                                                                                                                         | In the evaluateConditions method when it converts the condition to JS  the key used is 'conditon' instead of 'condition'.                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n| Change Management PRB1613166                                                                                           | Change of behavior required in the 'Change POST' API  in which no arguments should create a default change request in the same UI form                                                                        | It shouldn't give an error requiring the user to specify either a model or a type.                                                                                                                                                                                                                                                                                                                                   ...",
        "QUERY": " 'Agile story field is empty in the new story created'",
        "GROUND_TRUTH": "According to the KB12345 article, the 'Agile Story' field is blank for stories created from the triage board because of the design of the system. When an incident is added to the backlog from the triage board, the agile story field in the incident or problems table is populated, but the agile story is not populated for the wrapper story that gets created for the incident. This is as per the design of the system. However, the 'Agile Story' field is populated correctly when the story is created from the Agile Board > Create story or from the rm_story table (rm_list.do > New)."
    },
    {
        "id": 150,
        "CONTEXT": "\"## In sn_customerservice_case records  saving a form does not always save additional comment values if \n\n## Description\n\nIn a sn_customerservice_case record  if additional comments are set from templates  the value has to be posted using POST button and saving the form does not always save the additional comment value.\n\n## Steps to Reproduce\n\nInstall the Customer Service - com.sn_customerservice plugin.\n\nGo to the following URL and open any existing case.\n\nnav_to.do?uri=sn_customerservice_case_list.do3.\n\nRight-click the form header and choose View \\> Case.\n\nToggle the template bar in the context menu of the form and create a new template with the following values:\n\n* Short description: test\n* Additional Comments: some random text\n\nApply this template on the sn_customerservice_case form and click Save.\n\nAlthough you can see its value in Additional comments  when the form is saved  the value is not always shown in the activities list. (Try clicking POST to save the value.)\n\n## Workaround\n\nThis issue is under review. To receive notifications when more information is available  subscribe to this Known Error article by clicking the **Subscribe** button at the top right of the article. If you are able to upgrade  review the **Fixed In** field to determine whether any versions have a permanent fix.\n\n**Related Problem: PRB1206918**\n\n\n\n## Skill inputs and triggers for Now Assist for Customer Service Management (CSM)\n\n# Skill inputs and triggers for Now Assist for Customer Service Management (CSM) {#ariaid-title1}\n\nUse the inputs and triggers for each skill to configure how and when a skill is used.\n\n## Overview {#now-assist-csm-skill-inputs__section_gtv_fs4_hzb}\n\nDepending on the selected skill  you can configure inputs or triggers. These settings determine how and when a skill is used. An input identifies the data that is used for a skill  such as the table and fields used to generate a case summary. A trigger initiates an action  such as when the system generates a chat summary.\n\n## Chat summarization skill {#now-assist-csm-skill-inputs__section_lrc_tr4_hzb}\n\nFor the chat summarization skill  select the triggers that determine when a chat summary is generated. You can also select the properties that control how a chat summary is displayed.\nThe following table lists the triggers that you can configure for the chat summarization skill of the Chat Assist feature.\n{#d340839e83}\n\n|               Trigger               |                                                       Description                                                        |\n|-------------------------------------|--------------------------------------------------------------------------------------------------------------------------|\n| Virtual Agent to Live Agent handoff | Chat summary that is generated when the chat handoff is done from Virtual Agent to a live agent.                         |\n| Quick action                        | Chat summary that is generated when the live agent performs the `\/summarize` quick action.                               |\n| Chat wrap-up                        | Chat summary that is generated when the live agent ends the chat. The Chat Summary field is updated for the interaction. |\n| Short description                   | Short description field that is updated for the interaction when the live agent ends the chat.                           |\n| Task creation                       | Short description and Description fields that are auto-populated when a task is created.                                 |\nTable 1. Triggers for the chat summarization skill{#now-assist-csm-skill-inputs__table_chat_summary_triggers}\n\nThe following table lists the property that you can select to control how a chat summary is displayed.\n{#d340839e179}\n\n|   Property    |            Description             |\n|---------------|------------------------------------|\n| Bulleted list | Chat summary as an unordered list. |\nTable 2. Property for the chat summarization skill{#now-assist-csm-skill-inputs__table_chat_summmary_property}\n\n## Case summarization skill {#now-assist-csm-skill-inputs__section_ijv_tr4_hzb}\n\nThe case summarization skill includes the inputs that identify the table and fields that are used when a case summary is generated.\n\nIn this release  you can't modify a skill's input data source. The data source contains the tables and fields that the skill relies on.\n\nThe following table lists the inputs for the case summarization skill.\n{#d340839e234}\n\n|    Input     |                                                     Description                                                     |\n|--------------|---------------------------------------------------------------------------------------------------------------------|\n| Input table  | Case \\[sn_customerservice_case\\]                                                                                    |\n| Input fields | * Description * Short description * Work notes * Additional comments {#now-assist-csm-skill-inputs__ul_wht_y54_hzb} |\nTable 3. Inputs for the case summarization skill{#now-assist-csm-skill-inputs__table_case_summary_inputs}\n\n## Resolution notes generation skill {#now-assist-csm-skill-inputs__section_z2l_vr4_hzb}\n\nThe resolution notes generation skill includes the inputs that identify the table and fields that are used when the resolution notes are generated for a case.\n\nIn this release  you can't modify a skill's input data source. The data source contains the tables and fields that the skill relies on.\n\nThe following table lists the inputs for the resolutions notes generation skill.\n{#d340839e313}\n\n|    Input     |                                                     Description                                                     |\n|--------------|---------------------------------------------------------------------------------------------------------------------|\n| Input table  | Case \\[sn_customerservice_case\\]                                                                                    |\n| Input fields | * Description * Short description * Work notes * Additional comments {#now-assist-csm-skill-inputs__ul_cmh_3v4_hzb} |\nTable 4. Inputs for the resolution notes generation skill{#now-assist-csm-skill-inputs__table_resolution_notes_inputs}\n\n\n\n## Create a rule from the Case context record\n\n# Configuring a recommended action to troubleshoot the failed credit card transaction {#ariaid-title1}\n\nLearn how process analysts or business owners configure a recommended action that uses Troubleshoot credit card transaction failure decision tree to troubleshoot the failed credit card transaction.\n\n## Troubleshooting a failed credit card transaction {#ra-troubleshooting-failed-credit-card-transaction__section_wxf_3t2_czb}\n\nPaul  who holds an account with the StellarVest bank is trying to purchase a smart watch using their credit card. But the transaction failed at the time of payment. Paul created a case in the StellarVest bank portal seeking assistance. Agents from the StellarVest bank work on the cases created by customers to resolve their issues.\n\nIn this scenario  the agent assigned to the Paul's case is recommended to use a decision tree to troubleshoot the failed credit card transaction. The decision tree collects user  credit card  and transaction details. If money was debited  transaction tracking is initiated. However  if no money was debited  the agent provides a failure code which determines what guidance is provided. Examples of potential guidances include: reassigning the case  creating a work order  or assigning the case to an IT technician. For more information  see [Decision tree use case](example-decision-tree.html \"This use case demonstrates an end-to-end configuration of a decision tree to help you get started with Guided Decisions. After you configure the decision tree  you can either embed it in a playbook or use it as a recommendation in Recommended Actions.\").\n\n## Create a rule from the Case context record {#ariaid-title2}\n\nCreate a rule to show recommendations for active cases so that agents can resolve cases created by bank customers.\nRole required: sn_nb_action.next_best_action_author  admin\nNew rules can only be created from context records.\n\n1. Navigate to All \\> Recommended Actions \\> Contexts.\n2. Select the Case context.  \n   The Case context is available by default.\n3. In the Rules related list  select New.\n4. Enter a name and a description for the rule.\n5. In the Roles field  select the Edit User Roles ![Edit User Roles icon](..\/image\/icon-pencil-ac.png) icon and then select Customer service agent \\[sn_customerservice_agent\\] and Consumer service agent \\[sn_customerservice.consumer_agent\\].  \n   The selected user roles can see recommendations for this rule.\n6. Select Done on the Roles pop-up window.\n7. In the Condition field  use the condition builder to add the condition \"Active \\| is \\| true\".  \n   Agents can see recommendations for the records in the context table that meet this criteria.\n8. Select Submit.\n{#ex-create-rule__steps_zn1_kbf_ztb}  \n\n## Create a recommendation with decision tree as an action type {#ariaid-title3}\n\nCreate a recommendation to select Troubleshoot credit card transaction failure decision tree as an action type.\nRole required: sn_nb_action.next_best_action_author  admin\nNew recommendations can only be created inside the rules.\n\n1. Navigate to All \\> Recommended Actions \\> Contexts.\n2. Select the Case context.\n3. Select the Active rule from the Rules related list.\n4. In the Recommendations related list  select New.\n5. In the New Recommendation screen  select A guided flow to follow to create a recommendation of type decision tree.  \n   A new Recommendation form is displayed with the Rule  Context  and Action type fields auto populated.\n6. In the Name field  enter Troubleshoot credit card transaction failure.\n7. In the Action field  select the action.  \n   The available actions are determined by the selection in the Action type field.\n   1. In the Action field  select the lookup icon.  \n      In the resulting pop-up window  the Table name field is auto populated with the table that stores the available actions for the selected action type.\n   2. In the Document field  select the Troubleshoot credit card transaction failure decision tree by clicking the lookup icon.  \n      For more information about how to create the decision tree  see [Create Troubleshoot credit card transaction failure decision tree](preparation-for-creating-a-decision-tree.html \"Before configuring a decision tree  document the flow of the decision tree using a flow chart including the questions  answers  associated paths  and guidances.\").\n   3. Select OK.\n   4. Save the record to display the Action inputs form section.  \n      After selecting the action type and action  saving the record displays the required action inputs.\n   {#ex-create-recommendation__substeps_wj3_lnx_f5b}\n8. In the Recommendation hint field  enter Troubleshoot the credit card transaction failure.  \n   The hint explains to agents why a recommendation is being suggested.\n9. Select Update.\n\n\"\n\n",
        "QUERY": " 'Additional comments are not saved in sn_customerservice_case record'",
        "GROUND_TRUTH": "According to the KB12345 article, in a sn_customerservice_case record, if additional comments are set from templates, the value has to be posted using the POST button. Saving the form does not always save the additional comment value. Here are the steps to reproduce this issue:\n\n1. Install the Customer Service - com.sn_customerservice plugin.\n2. Go to the following URL and open any existing case: nav_to.do?uri=sn_customerservice_case_list.do3.\n3. Right-click the form header and choose View > Case.\n4. Toggle the template bar in the context menu of the form and create a new template with the following values:\n   - Short description: test\n   - Additional Comments: some random text\n5. Apply this template on the sn_customerservice_case form and click Save.\n\nAlthough you can see its value in Additional comments, when the form is saved, the value is not always shown in the activities list. (Try clicking POST to save the value.)\n\nThe issue is currently under review. To receive notifications when more information is available, subscribe to this Known Error article by clicking the Subscribe button at the top right of the article. If you are able to upgrade, review the Fixed In field to determine whether any versions have a permanent fix."
    },
    {
        "id": 151,
        "CONTEXT": "\"## Read replicas: Troubleshooting  statistics and configuring secondary databases\n\nRead Replicas \\| Troubleshooting  statistics and configuring secondary databases\n\n# Overview\n\nFor a ServiceNow deployment  a potential performance bottleneck is traffic to the database. To alleviate this bottleneck and improve performance  you can set up secondary replica databases and configure database connection pools for accessing them. In this configuration  the primary database still performs all write operations  and the secondary databases can run queries that are less sensitive to replication lag. Thus  you off-load traffic from the main database  leaving it with more resources to complete time-critical operations.\n\n# Secondary database categories and pools\n\nTo view the configuration of secondary database categories and pools:\n\n1. Hop in to the instance  and go to **System Maintenance** :  \n      Preview Image\n\n**Secondary Database Pools:** shows the lag  replicas  and current state.\n\n**Secondary Database Categories** : shows per category replication lag thresholds and which read replicas are configured (categories are hard set).\n\n{#body_html_insert_image_dialog}\n\n# Reviewing replica statistics\n\nTo review replicas statistics:\n\n1. Hop in to the instance  and go to**System diagnostics \\> table iostats \\> personal list** (add **pool name** field).\n2. Sort by selects  filter out **glide ** and check if the SELECTs are incrementing against the replicas.\n\n\n\n# Troubleshooting read replicas\n\nTo troubleshoot read replicas:\n\n1. Log in to the primary database server and review a MySQL show processlist to confirm if their are any active **select** queries being run. This should not be the case if read replicas are active.\n\nConfirm if the MySQL read replicas are lagging behind the master. If the seconds behind master exceeds what is defined in the category (see Secondary Database Categories above)  this read replica stops receiving traffic.\n\nReview the replica query statistics per \"Reviewing replica statistics\" and ensure the counters are incrementing.\n\nUse the MySQL client on each of the replicas and perform the show processlist.\n\nIf there are very long running UPDATE  DELETE  INSERT or OPTIMIZE statements being run on the master  this could cause the read replicas to lag behind. If they lag behind longer than the counters set within **Secondary Database Categories**   this removes the replica from the pool and causes the related READ queries to be diverted to the master.5. If you see no queries on the read-replicas: ensure the table 'sys_db_pool' has up to date\/consistent configuration i.e the (replica_for field) is pointing to the correct replicas: (i.e has it been recently reseeded and configuration missed? )\n\nmysql\\>use \\<database_name\\>;\nmysql\\> select \\* from sys_db_pool\n\n**Previous issues:**\n\nPost upgrade  an optimize table is run on ts_c% tables. If these tables are very large  they could cause significant lag on each of the replicas. If there is lag on the read replicas that exceeds what is set within the **Secondary Database Categories**  traffic is diverted away from the read replicas. This then causes the load on the primary database to increase exponentially.\n\nOne option is to kill the optimize queries to provide relief to the customer  but you should contact Customer Support and SysAdmin before proceeding.\n\nRead Replica reseeds have been executed  without the sys_db_pool being updated  (5.) above.\n\n# Configuring secondary databases\n\nYou can configure secondary databases to perform these types of queries:\n\n* Reports\n* Homepages\n* Auto-complete\n* Text search\n* Lists\n* Embedded lists\n* Related lists\n* Logs\n* ODBC driver\n* CMDB relationships\n\n## Setting up secondary databases\n\nTo use secondary database pools  set up secondary database servers and enable replication of the primary database. Ensure that your hardware configuration meets these requirements:\n\n* Database replication uses a supported replication technology  such as Tungsten Replicator  MySQL binary logging  Shareplex for Oracle  or Oracle Dataguard. For an example  see the Deploy MySQL Replication section.\n* The secondary database servers are in read-only mode. The secondary database pools feature does not support multiple primary databases.\n* All secondary database names are the same as the primary database name.\n* The primary and secondary database server hard drives have sufficient disk space to accommodate replication logs.\n* \\[Recommended\\] The primary and secondary database servers are located in the same datacenter. Having the secondary databases in the same datacenter improves performance and security.  \n\n|----------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| ![](\/Note_25x.pngx \"Note\") | **Note:** Secondary database pools require that the primary database is always available. ServiceNow recommends enabling high availability and disaster recovery to ensure that a primary database is always available. |\n## Configuring database pools\n\nWhen you have completed the physical set up of secondary databases with replication  you are ready to set up ServiceNow to route queries to the secondary databases:\n\n* Activate the plugin.\n* Define a pool record for each connection pool.\n* Route categories of queries to specific connection pools.  \n\n## Activating the plugin\n\nTo install this feature  activate the Secondary Database Pools plugin. See [ServiceNow instructions for activating a plugin](https:\/\/docs.servicenow.com\/csh?topicname=t_ActivateAPlugin.html&version=latest \"ServiceNow instructions for activating a plugin\").\n\n## Defining database pools\n\nTo define a pool record for each connection pool on every secondary database:\n\n1. Navigate to **System Maintenance \\> Secondary Database Pools**.\n2. Define the pool by completing the below form:  \n   * **Field:** Description\n   * Enter a name for the pool record.\n   * **Database Name:** Enter the database name. **Note:** All secondary database names must be the same as the primary database name.\n   * **Type:** Select the database management system on the secondary database (MySQL  Oracle  or SQLServer).\n   * **Active:** Select the check box to make the connection pool available.\n   * **URL:** Enter a valid JDBC connection URL for the secondary database. For example: jdbc:mysql:\/\/172.16.241.133\/ \n   * **User:** Enter the user name for the database service account.\n   * **Password:** Enter the password for the database service account.\n   * **Min connections:** Define the minimum number of concurrent database connections.\n\n\n   * **Max connections:** Define the maximum number of concurrent database connections. **Note:** Consider using the same minimum and maximum connection settings as you use for your primary database  but do not exceed the connection limit of your database management system. Factor in the number of application servers as well as the number of pools associated with each secondary database. For example  you define two connection pools for a secondary database and set the maximum connections for each pool to 20. If you have two application servers  then you have configured a maximum of 80 total connections for the database (20 to each pool for each application server).\n3. Click **Submit**.\n4. Open the record and click **Test Database Connectivity** to verify that the database connection is defined correctly. If necessary  modify the pool record or database setup to establish connectivity. This example shows an issue with database settings for the maximum allowed packet size.   \n\n## Routing queries to pools\n\nNote: If there are any expensive OOB queries related to TextSeach  Report  PA  etc.  then please log a PRB to Dev-Persistence. They can redirect the PRB to the teams to tag their code to reroute the query to be issued. Setting the query category from the call sites that issues the query is a much safer way than doing it by query pattern because it is hard for a person to understand where a query pattern is used.\n\nNote: In general  tables in sys_metadata family(it includes any table that is extended from sys_metadata) should not be routed to Read-Replica. If there is a good reason to re-route these queries  then please validate with Dev team(Dev-Persistence) before re-routing the query. PRB1352069 fix will try to exclusion list additional tables to the above list so that people don't accidentally route a wrong table. exclusion listing the tables doesn't prevent a new query pattern or new tables that show up in the slow query module.\n\nCAVEAT: It has been requested by development that we open an Case task with the Dev - Persistence team to validate whether we can reroute a query to the read replicas  as we have seen some unexpected results (such as PRB1174630: Infinite loop querying sys_db_category). If Dev does agree to reroute the query  then we should test a restart of a standby node after implementing the reroute to verify that nodes will not fail to restart after implementing the query category. If the node does not restart  then backout the query category routing.\n\nUse categories to route types of queries to one or more specific secondary database connection pools. The categories below are available:\n\n\u0095 Reports (includes scheduled reports and exports)\n\u0095 Homepages\n\u0095 Auto-complete\n\u0095 Text search\n\u0095 Lists (includes exports)\n\u0095 Embedded lists\n\u0095 Related lists\n\u0095 Logs\n\u0095 ODBC driver\n\u0095 CMDB relationships\n\nTo configure categories settings to route queries to pools:\n\n1. Navigate to **System Maintenance \\> Secondary Database Categories**.\n2. Select the category of queries to route to secondary databases (for example  auto-complete).\n3. Configure the category settings by completing the form:  \n   * **Field:** Description\n   * **Name:** This identifies the type of queries you are routing to secondary databases (auto-complete  embedded_list  homepage  list  log  odbc  related_list  reporting  or text search).   \n     (*Note:* Do not change this field value or create a new category.)\n   * **Pool selection:** When multiple connection pools are configured for a category  select the method for determining where to route a query.  \n     * **Round-Robin:** Randomly select but evenly distribute queries among connection pools.\n     * **Priority:** Select a connection pool in a predictable order (determined by the **Order** field. See step 4). Uses the lowest order pool that is available.  \n       *(Note:* When all secondary database pools are unavailable  queries are always routed to the primary database.)\n   * **Active:** Select the check box to route queries to secondary database pools. Query routing is disabled for all categories by default.\n   * **Latency sensitivity:** Select the method for determining whether replication is too far behind to use a secondary database pool:  \n     * **None:** Disregard replication lag and always route this category of queries to an available pool.\n     * **Threshold:** Only route queries to a pool when replication lag is less than the replication lag threshold.\n     * **Threshold \\& users updates:** Only route queries to a pool when the replication lag is less than the threshold and less than the time elapsed since the last update on the queried table during the current session. \"\n\n",
        "QUERY": " 'how to get the secondary db'",
        "GROUND_TRUTH": "To get the secondary database, you need to follow these steps as outlined in the article KB12345:\n\n1. Set up secondary database servers and enable replication of the primary database. Ensure that your hardware configuration meets the requirements mentioned in the article, such as using a supported replication technology, having the secondary database servers in read-only mode, ensuring all secondary database names are the same as the primary database name, and having sufficient disk space to accommodate replication logs.\n\n2. Once you have completed the physical set up of secondary databases with replication, you are ready to set up ServiceNow to route queries to the secondary databases. This involves activating the Secondary Database Pools plugin.\n\n3. Define a pool record for each connection pool on every secondary database. Navigate to System Maintenance > Secondary Database Pools and define the pool by completing the form as described in the article.\n\n4. Test the database connectivity to verify that the database connection is defined correctly.\n\n5. Route categories of queries to specific connection pools. The categories available are mentioned in the article and include Reports, Homepages, Auto-complete, Text search, Lists, Embedded lists, Related lists, Logs, ODBC driver, and CMDB relationships.\n\n6. Configure categories settings to route queries to pools. Navigate to System Maintenance > Secondary Database Categories and select the category of queries to route to secondary databases. Configure the category settings by completing the form as described in the article."
    },
    {
        "id": 152,
        "CONTEXT": "\"## Script Debugger - ITOM\n\nThis article covers Script Debugger topics specific to ITOM products. For non product specific information regarding the Script Debugger  see [Script Debugger](https:\/\/support.servicenow.com\/kb?id=kb_article_view&sysparm_article=KB0815530 \"Script Debugger\").\n\n### Using Script Debugger breakpoints with Discovery inputs {#mcetoc_1hc2hel1l2v}\n\nIn order to use the script debugger  you need to be able to process the payload on the same session as the debugger. The business rule which processes the ECC queue record of interest is reviewed below  with the actual code which processes the payload.\n\nFor Discovery  the business rule calls \"SchedulePriorityECCJob\" and passes \"var job = new DiscoverySensorJob(); job.process();\" as the script. Reviewing \"DiscoverySensorJob.process()\" shows that SncSensorProcessor.process() processes the payload.\n\nThe following will work when debugging script includes  business rules  and pre post scripts (sa_pattern_prepost_script) called by sensors. Sensors do not trigger the debugger directly. Therefore  the business rules and script includes used show that we can process the payload as follows:\n\n1. Get the sys id of the ecc_queue record  \n2. Go to \"System Definition \\> Scripts - Background\"  \n3. Run the following script replacing \\<ecc_queue_sys_id\\> with the correct sys ID\n\nvar eccRecord = new GlideRecord('ecc_queue');\neccRecord.get(\u0091<ecc_queue_sys_id>');\nvar sp = new SncSensorProcessor(eccRecord);\nsp.process();\u00a0\nThe Related link \"Run Again (Debug)\" on the ecc_queue input form can avoid you needing to run this script. It does the same thing  but with one click.\n\n### Discovery Sensor {#mcetoc_1hc2hel1l30}\n\nNot all scripts work with script debugger  discovery_sensor scripts is an example. Furthermore  the debugger is for the current user session only. Therefore scripts running in a different user session will not be debugged. The debugger is used often with Business Rules and Script Includes. As a workaround  create a script include and call it from the code which needs to be debugged. In the following example a script include DebugSensor was created to be called by a discovery sensor in order for it to be caught by the Script Debugger  and thus allow us to see what is in memory via the Script Debugger.\n\nDebugSensor\n\n### Post Sensor\n\nSingle page inputs will trigger the post sensor script. However  multipage inputs will not (except for the last page). For multipage scripts only the last page processed will trigger the post sensor. The Horizontal Discovery Sensor calls resultHandler.isPagingComplete() to determine whether to call the post sensor script. To 'trick' this test to pass  the 'Opt Counter' value in table discovery_multi_page_optimistic_lock can be adjusted.\n\n1. Find the output probe on table discovery_multi_page_optimistic_lock via field 'Output Probe ID'\n2. Set 'Opt counter' value to \\<number of inputs\\> - 1\n3. Reprocess the input via script background or 'Run Again (Debug)' link in the input record\n\n\n\n## Run a Quick Discovery\n\nTo choose the MID Server  supply either the <var class=\"keyword varname\">sys_id<\/var> or name of the MID Server as the argument.  \n   If you do not name a MID Server  the system attempts to find a valid one automatically. A valid MID Server has a status of Up and can discover the given IP address. If the system finds a valid MID Server and runs a Discovery  the discoveryFromIP method returns the <var class=\"keyword varname\">sys_id<\/var> of the Discovery status record. If no MID Server can discover this IP address  the method returns the value undefined.\n\nIf you manually specify the TARGET_MIDSERVER  the system validates the given value and ensures that the MID Server table contains the specified MID Server record. If the validation passes  the discoveryFromIP method returns the sys_id of the discovery status record. If the validation fails  the method return the value undefined.\n\n## Validate discovery results {#ariaid-title4}\n\nValidate the results of your discovery by accessing the ECC queue  analyzing the XML payload  and checking the Discovery log.\nRole required: discovery_admin\nInitial discoveries often reveal unexpected results  such as previously unknown devices and processes or failed authentication. Results should also accurately identify known devices and update the CMDB appropriately. Become familiar with the network that is being discovered and the types of data returned for the different types of discoveries. Use the Discovery Log and the ECC Queue to monitor the Discovery process as data is returned from probes or pattern operations.\n\n1. To view the actual payload of a probe  click the XML icon in a record in the ECC Queue.  \n   Figure 4. ECC Queue ![ECC Queue](..\/image\/DiscoveryECCQueueView.png)\n2. To view the actual payload of a probe  click the XML icon in a record in the ECC Queue.\n3. Use the Discovery Log form for a quick look at how the probes are doing.  \n   To display the Discovery Log  navigate to Discovery \\> Discovery Log.  \n   Figure 5. Discovery Log ![The Discovery log](..\/image\/DiscoveryLog.png)  \n   The Discovery Log provides this information:  \n   {#d256081e1501}\n   |     Column      |                                                                           Information                                                                           |\n   |-----------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------|\n   | Created         | Displays the timestamp for the probe launched. Click this link to view the record for the probe launched in this list.                                          |\n   | Level           | Displays the type of data returned by this probe. The possible levels are: * Debug * Error * Information * Warning {#t_ValidateDiscoveryResults__ul_pq4_bjx_wp} |\n   | Message         | Message describing the action taken on the information returned by the probe.                                                                                   |\n   | ECC queue input | Displays the ECC queue name associated with the log message.                                                                                                    |\n   | CI              | The CI discovered. Click this link to display the record from the CMDB for this CI.                                                                             |\n   | Source          | Displays the probe name that generated the log message.                                                                                                         |\n   | Device          | Displays the IP address explored by the probe. Click this link to examine all the log entries for the action taken on this IP address by this Discovery.        |\n   {#t_ValidateDiscoveryResults__table_gq5_q3x_wp}\nNote: If you cancel an active discovery  note the following information:\n   * Existing sensor jobs that have started processing are immediately terminated.\n   * The existing sensor jobs that are in a Ready state  but have not started processing  are deleted from the system.\n   {#t_ValidateDiscoveryResults__ul_t3h_wzr_ybb}\n4. View the [Discovery Home\n   page](..\/concept\/discovery-home-page.html \"The Discovery Home page provides a summary of discoveries that were triggered by cloud and non-cloud schedules. You can view any errors that occurred during a Discovery and find remediation suggestions. Use the Home page view to examine the cloud resources discovered for the service accounts you selected in a cloud schedule.\") for details about all schedules  cloud resources (virtual machines)  discovered devices  and related errors that might have occurred.  \n[Error details](..\/concept\/discovery-home-page.html#view-ci-discovery-schedule-errors \"From the ServiceNow Home page  you can view the Discovery errors that occurred during a Discovery and get suggestions for resolving these errors. You can view the errors for all schedules or for a single schedule.\") include possible remediation steps.  \n\n## MID Server selection sequence for Discovery schedules {#ariaid-title5}\n\nThe Discovery application follows this sequence to find a MID Server.\n\n### MID Server auto-selection\n\nDiscovery follows this sequence when you select Auto-Select MID Server for the MID Server selection method on the Discovery Schedule form.\nNote: MID Server auto-select is not supported with IPv6.\n\n1. Discovery looks for a MID Server that also has an appropriate IP range configured.\n2. \n\n## MID Server selection sequence for Discovery schedules\n\nIf the system finds a valid MID Server and runs a Discovery  the discoveryFromIP method returns the <var class=\"keyword varname\">sys_id<\/var> of the Discovery status record. If no MID Server can discover this IP address  the method returns the value undefined.\n\nIf you manually specify the TARGET_MIDSERVER  the system validates the given value and ensures that the MID Server table contains the specified MID Server record. If the validation passes  the discoveryFromIP method returns the sys_id of the discovery status record. If the validation fails  the method return the value undefined.\n\n## Validate discovery results {#ariaid-title4}\n\nValidate the results of your discovery by accessing the ECC queue  analyzing the XML payload  and checking the Discovery log.\nRole required: discovery_admin\nInitial discoveries often reveal unexpected results  such as previously unknown devices and processes or failed authentication. Results should also accurately identify known devices and update the CMDB appropriately. Become familiar with the network that is being discovered and the types of data returned for the different types of discoveries. Use the Discovery Log and the ECC Queue to monitor the Discovery process as data is returned from probes or pattern operations.\n\n1. To view the actual payload of a probe  click the XML icon in a record in the ECC Queue.  \n   Figure 4. ECC Queue ![ECC Queue](..\/image\/DiscoveryECCQueueView.png)\n2. To view the actual payload of a probe  click the XML icon in a record in the ECC Queue.\n3. Use the Discovery Log form for a quick look at how the probes are doing. To display the Discovery Log  navigate to Discovery \\> Discovery Log.  \n   Figure 5. Discovery Log ![The Discovery log](..\/image\/DiscoveryLog.png)  \n   The Discovery Log provides this information:  \n   {#d280650e1459}\n   |     Column      |                                                                           Information                                                                           |\n   |-----------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------|\n   | Created         | Displays the timestamp for the probe launched. Click this link to view the record for the probe launched in this list.                                          |\n   | Level           | Displays the type of data returned by this probe. The possible levels are: * Debug * Error * Information * Warning {#t_ValidateDiscoveryResults__ul_pq4_bjx_wp} |\n   | Message         | Message describing the action taken on the information returned by the probe.                                                                                   |\n   | ECC queue input | Displays the ECC queue name associated with the log message.                                                                                                    |\n   | CI              | The CI discovered. Click this link to display the record from the CMDB for this CI.                                                                             |\n   | Source          | Displays the probe name that generated the log message.                                                                                                         |\n   | Device          | Displays the IP address explored by the probe. Click this link to examine all the log entries for the action taken on this IP address by this Discovery.        |\n   {#t_ValidateDiscoveryResults__table_gq5_q3x_wp}\nNote: If you cancel an active discovery  note the following information:\n   * Existing sensor jobs that have started processing are immediately terminated.\n   * The existing sensor jobs that are in a Ready state  but have not started processing  are deleted from the system.\n   {#t_ValidateDiscoveryResults__ul_t3h_wzr_ybb}\n4. View the [Discovery Home page](..\/concept\/discovery-home-page.html \"The Discovery Home page provides a summary of discoveries that were triggered by cloud and non-cloud schedules. You can view any errors that occurred during a Discovery and find remediation suggestions. Use the Home page view to examine the cloud resources discovered for the service accounts you selected in a cloud schedule.\") for details about all schedules  cloud resources (virtual machines)  discovered devices  and related errors that might have occurred.  \n[Error details](..\/concept\/discovery-home-page.html#view-ci-discovery-schedule-errors \"From the ServiceNow Home page  you can view the Discovery errors that occurred during a Discovery and get suggestions for resolving these errors. You can view the errors for all schedules or for a single schedule.\") include possible remediation steps.  \n\n## MID Server selection sequence for Discovery schedules {#ariaid-title5}\n\nThe Discovery application follows this sequence to find a MID Server.\n\n### MID Server auto-selection\n\nDiscovery follows this sequence when you select Auto-Select MID Server for the MID Server selection method on the Discovery Schedule form.\n\n1. Discovery looks for a MID Server that also has an appropriate IP range configured.\n2. If no MID Servers meet these criteria  it looks for a MID Server that has the ALL application that also has an appropriate IP range configured.\n3. If more than one MID Servers meet the criteria  Discovery chooses the first MID Server with the status of Up. If more than one MID Servers are up  it randomly picks one.\n4. If none are up  it uses the default MID Server specified for the Discovery application  assuming it is up.\n5. \"\n\n",
        "QUERY": " 'how to handle external payload in ecc queue'",
        "GROUND_TRUTH": "To handle external payload in the ECC queue, you need to process the payload on the same session as the debugger. The business rule which processes the ECC queue record of interest is \"SchedulePriorityECCJob\". This business rule calls \"DiscoverySensorJob.process()\" which shows that SncSensorProcessor.process() processes the payload. \n\nTo process the payload, follow these steps:\n\n1. Get the sys id of the ecc_queue record.\n2. Go to \"System Definition > Scripts - Background\".\n3. Run the following script replacing <ecc_queue_sys_id> with the correct sys ID:\n\n```\nvar eccRecord = new GlideRecord('ecc_queue');\neccRecord.get(\u2018<ecc_queue_sys_id>\u2019);\nvar sp = new SncSensorProcessor(eccRecord);\nsp.process();\n```\nThe Related link \"Run Again (Debug)\" on the ecc_queue input form can avoid you needing to run this script. It does the same thing, but with one click."
    },
    {
        "id": 153,
        "CONTEXT": "\"## Troubleshooting excessive Service Portal widget reloads\n\n**Background:**\n\nWhen working with ServiceNow instances it is common to see transactions executing where the transaction URL starts with \/API\/now\/sp\/rectangle. For example:\n\n2021-06-23 05:25:23 (739) Default-thread-191 FABB35231B387C90A8ADA9F36B4BCB3F txid=9e5df9e71b74 \\*\\*\\* Start #33598 \/api\/now\/sp\/rectangle\/37cf5aeadbf96450c100e16e13961972  user: admin\n2021-06-23 05:25:23 (766) Default-thread-191 FABB35231B387C90A8ADA9F36B4BCB3F txid=9e5df9e71b74 \\*\\*\\* End #33598 \/api\/now\/sp\/rectangle\/37cf5aeadbf96450c100e16e13961972  user: admin  total time: 0:00:00.037  processing time: 0:00:00.037  SQL time: 0:00:00.007 (count: 14)  business rule: 0:00:00.007 (count: 3)  ACL time: 0:00:00.001  source: 199.91.137.61   type:rest  method:POST  api_name:now\/sp  resource:now\/sp\/rectangle\/{rectangle}  version:Default  user_id:6816f79cc0a8016401c5a33be04be441  response_status:201\n\nThese transactions indicate that a particular Service Portal widget is reloading \/ refreshing. Note that:\n\n* The sys_id in the transaction URL indicates the widget instance which is reloading  i.e.:\n\n\\[db178151.ytz3:\/home\/users\/james.ford\\]$ pbrun snow query empjfordparis \"select sp_column  sp_widget from sp_instance where sys_id = '37cf5aeadbf96450c100e16e13961972'\"\n+------+----------------------------------+----------------------------------+\n\\| Port \\| sp_column \\| sp_widget \\|\n+------+----------------------------------+----------------------------------+\n\\| 3520 \\| 2f2f92eadbf96450c100e16e1396191f \\| 887f56eadbf96450c100e16e139619f1 \\|\n+------+----------------------------------+----------------------------------+\n\n* The sys_id is contained in sp_instance.sp_widget field indicates the underlying Service Portal widget being used  i.e.:\n\n\\[db178151.ytz3:\/home\/users\/james.ford\\]$ pbrun snow query empjfordparis \"select name from sp_widget where sys_id = '887f56eadbf96450c100e16e139619f1'\"\n+------+---------------------+\n\\| Port \\| name \\|\n+------+---------------------+\n\\| 3520 \\| Copy of My Requests \\|\n+------+---------------------+\n\nThe sp_widget table contains further definitions around how the widget operates  i.e.:\n\n* css: Cascading Style Sheet (CSS) data defining aspects of how the widget should be displayed in the end users browser\n* template: An HTML template for the widget defining how end users can interact with the widget\n* script: Server-side JavaScript which generally describes how data to be displayed in the widget is built\n* client_script: Client-side JavaScript which dictates how the widget operates within the end users browser\n\nIt is entirely normal for certain Service Portal widgets to periodically reload however if this happens too frequently it can cause various issues  i.e.:\n\n* Contention on default threads\/semaphore pool on application nodes could cause an impact on multiple end users\n* Multiple \/API\/now\/sp\/rectangle transactions being created for a users session in quick succession - as transactions for a single session are (by default) executed serially these transactions can stack up behind one another and be impacted by session wait - this could cause significant degradation to end user experience\n\nFor example  consider the following scenario - a user initiates a large number of reloads (5) of a given widget instance in a short period of time (136ms):\n\n2021-06-22 08:10:34 (555) http-35 New transaction 8F99C91F1BFC3850078D6397624BCB17 #1405044 \/api\/now\/sp\/rectangle\/dfa819256f8a0380903ed3b0be3ee400\n2021-06-22 08:10:34 (567) http-39 New transaction 8F99C91F1BFC3850078D6397624BCB17 #1405045 \/api\/now\/sp\/rectangle\/dfa819256f8a0380903ed3b0be3ee400\n2021-06-22 08:10:34 (597) http-16 New transaction 8F99C91F1BFC3850078D6397624BCB17 #1405046 \/api\/now\/sp\/rectangle\/dfa819256f8a0380903ed3b0be3ee400\n2021-06-22 08:10:34 (646) http-40 New transaction 8F99C91F1BFC3850078D6397624BCB17 #1405047 \/api\/now\/sp\/rectangle\/dfa819256f8a0380903ed3b0be3ee400\n2021-06-22 08:10:34 (691) http-44 New transaction 8F99C91F1BFC3850078D6397624BCB17 #1405048 \/api\/now\/sp\/rectangle\/dfa819256f8a0380903ed3b0be3ee400\n\nCorresponding transactions are then executed serially - each reloads of the widget takes approximately 1 - 2 seconds so by the time the final transaction completes its performance is significantly degraded due to session wait (8.17s):\n\n2021-06-22 08:10:36 (672) Default-thread-11 8F99C91F1BFC3850078D6397624BCB17 txid=4099919b1b78 \\*\\*\\* Start #1405044 \/api\/now\/sp\/rectangle\/dfa819256f8a0380903ed3b0be3ee400  user: xxxxxx\n...\n2021-06-22 08:10:38 (287) Default-thread-1 8F99C91F1BFC3850078D6397624BCB17 txid=c899119f1b38 \\*\\*\\* Start #1405045 \/api\/now\/sp\/rectangle\/dfa819256f8a0380903ed3b0be3ee400  user: xxxxxx\n...\n2021-06-22 08:10:39 (837) Default-thread-8 8F99C91F1BFC3850078D6397624BCB17 txid=0499919b1b78 \\*\\*\\* Start #1405046 \/api\/now\/sp\/rectangle\/dfa819256f8a0380903ed3b0be3ee400  user: xxxxxx\n...\n2021-06-22 08:10:41 (371) Default-thread-7 8F99C91F1BFC3850078D6397624BCB17 txid=c099dd9b1b78 \\*\\*\\* Start #1405047 \/api\/now\/sp\/rectangle\/dfa819256f8a0380903ed3b0be3ee400  user: xxxxxx\n...\n2021-06-22 08:10:42 (867) Default-thread-1 8F99C91F1BFC3850078D6397624BCB17 txid=409911131b78 \\*\\*\\* Start #1405048 \/api\/now\/sp\/rectangle\/dfa819256f8a0380903ed3b0be3ee400  user: xxxxxx\n...\n2021-06-22 08:10:44 (329) Default-thread-1 8F99C91F1BFC3850078D6397624BCB17 txid=409911131b78 EXCESSIVE \\*\\*\\* End #1405048 \/api\/now\/sp\/rectangle\/dfa819256f8a0380903ed3b0be3ee400  user: xxxxxx  **total time: 0:00:09.637**   processing time: 0:00:01.471  total wait: 0:00:08.166  **session wait: 0:00:08.166**  SQL time: 0:00:01.379 (count: 42)  source: 123.456.789.012   type:rest  method:POST  api_name:now\/sp  resource:now\/sp\/rectangle\/{rectangle}  version:Default  user_id:5935cf7a3776f200ba0694c543990e40  response_status:201\n\nWhilst these transactions execute the user's browser will essentially appear to have hung (i.e. the ServiceNow instance is completely unresponsive) - this does not give a good end-user experience.\n\nIn these cases  it may be necessary to understand why a particular widget is reloading such that this behaviour can be avoided. Reloads are generally triggered by the client script calling a function such as 'server.update()' or 'server.refresh()' - these functions cause the widget server-side script to execute (which in turn refreshes the contents of the widget). Such functions tend to be called (and therefore widget reloads are triggered):\n\n* Periodically due to some kind of interval\/timeout in the client-side script\n* On-demand due to a record watcher being triggered\n\nBoth methods will be discussed further below.\n\n**Periodic widget reloads**\n\nA widget can be reloaded on a periodic basis via code such as the following in the widgets client script:\n\n$interval(function(){\nc.server.refresh($scope);\n}  5000);\n\nThis code means that when the widget is loaded a timer is instantiated in the browser's run time environment. \n\nEach time the timer expires (in this case after 5000ms) the corresponding code (server.refresh()) will be executed. As described above this will cause the widgets server-side script to execute again to update the contents of the widget:\n\n\\[app136164.ytz3:\/home\/users\/james.ford\\]$ tail -f \/glide\/nodes\/empjfordparis001_16502\/logs\/localhost_log.2021-06-23.txt \\| grep FABB35231B387C90A8ADA9F36B4BCB3F \\| grep Start \\| grep rectangle\n2021-06-23 06:03:49 (192) Default-thread-186 FABB35231B387C90A8ADA9F36B4BCB3F txid=0d2686231bf4 \\*\\*\\* Start #33871 \/api\/now\/sp\/rectangle\/37cf5aeadbf96450c100e16e13961972  user: admin\n2021-06-23 06:03:54 (192) Default-thread-189 FABB35231B387C90A8ADA9F36B4BCB3F txid=9a2606eb1b38 \\*\\*\\* Start #33872 \/api\/now\/sp\/rectangle\/37cf5aeadbf96450c100e16e13961972  user: admin\n2021-06-23 06:03:59 (190) Default-thread-186 FABB35231B387C90A8ADA9F36B4BCB3F txid=2f268eeb1b38 \\*\\*\\* Start #33873 \/api\/now\/sp\/rectangle\/37cf5aeadbf96450c100e16e13961972  user: admin\n2021-06-23 06:04:04 (188) Default-thread-189 FABB35231B387C90A8ADA9F36B4BCB3F txid=b836ceef1bf4 \\*\\*\\* Start #33874 \/api\/now\/sp\/rectangle\/37cf5aeadbf96450c100e16e13961972  user: admin\n2021-06-23 06:04:09 (202) Default-thread-186 FABB35231B387C90A8ADA9F36B4BCB3F txid=8a368e2f1bb4 \\*\\*\\* Start #33875 \/api\/now\/sp\/rectangle\/37cf5aeadbf96450c100e16e13961972  user: admin\n...\n\nNote  however  that the above code is not optimal - as the timer is instantiated in the browser run time environment it may not be destroyed even if the user navigates away from the page holding the widget - as a result  the timer continues to fire which can send the constant widget to reload (\/api\/now\/sp\/rectangle) transactions to the corresponding application node. In addition  if the user navigates back to the page holding the widget it's possible for a second timer to be instantiated - now the timer will fire  on average  every 2.5s. In certain cases  this behaviour can quickly spiral out of control!\n\nTo avoid this if a periodic refresh is being used it should create a **'promise'** which is then destroyed when the user navigates away from the page  i.e.:\n\nvar intervalPromise = $interval(function(){\nc.server.refresh($scope);\n}  5000);\n$scope.$on('$destroy'  function() {\n$interval.cancel(intervalPromise);\n});\n\nFundamentally periodically refreshing a widget can work well however it means that users will trigger constant \/API\/now\/sp\/rectangle transactions even if the underlying data displayed in their widget has not changed. Due to this ServiceNow generally recommend refreshing widgets**'on demand'** via a record watcher (discussed below).\n\n**Using a record watcher to trigger widget reloads**\n\nThe client script of a widget can instantiate a record watcher against a given set of records (using an encoded query). If anything to do with that set of records changes (i.e. records are added\/removed or the details of one or more records are modified) the record watcher will be 'fired' at which point some arbitrary code is executed. In the case of a Service Portal widget this code generally just reloads the widget. For example the following code:\n\nSets data.user to the current user sys_id in the server-side script:\n\ndata.user = gs.getUserID();\n\nThe data object can then be accessed in the client side script via the variable 'c' after which a record watcher is instantiated to look at 'sc_request' records where 'requested_for' is set to the current user:\n\nvar c = this;\n...\nspUtil.recordWatch($scope  \"sc_request\"  \"requested_for=\" + c.data.user  function() {\nc.server.refresh();\n});\n\nIf details of this set of records change the record watcher will 'fire' and cause server.refresh() to be executed which then reloads the widget (via a corresponding \/API\/now\/sp\/rectangle transaction).\n\nWhen using record watchers it's necessary to ensure that the encoded query\/filter used in the record watcher is 'well formed' (i.e. it should match a relatively small set of records which do not frequently change and are somewhat 'user specific'). To demonstrate why this is necessary the following was seen in one customer implementation:\n\n* Within their Service Portal header  the customer had instantiated a record watcher which looked for records in the task table and was designated as a 'major incident' (due to a particular boolean field being set to true)\n* The header existed on every single page and  as a result  every single user of the Service Portal had the record watcher instantiated in their browser regardless of the page they were viewing\n* As soon as a task record was designated a 'major incident' the record watcher in the browser of every user of the Service Portal suddenly fired triggering a corresponding \/API\/now\/sp\/rectangle transaction to be sent to application nodes\n* This caused instant (but relatively short-lived) semaphore exhaustion across all application nodes which effectively caused a loss of service\n\nLikewise  even if a record watcher only looks at 'user specific' records if that set of records is large \/ frequently modified it could cause the record watcher to fire far more frequently than expected and cause similar issues.\n\n\n\nIn most cases  if there is evidence of excessive \/API\/now\/sp\/rectangle transactions for a given widget instance its relatively straight forwards to review the corresponding widget  determine how reloads are triggered (i.e. fixed interval or record watcher)  and make recommendations to improve (i.e. in the case of a record watcher 'tighten' the filter by adding additional clauses to the encoded query). In some cases  however  it might not be clear which record watcher is to blame for the reloads. For example  a widget may contain code such as the following which can instantiate multiple record watchers when the widget is loaded:\n\n...\n\/\/ Get list of record watchers\nvar record_watchers = \\[\\];\nif ($scope.data.menu.items) {\nfor(var i in $scope.data.menu.items) {\nvar item = $scope.data.menu.items\\[i\\];\nif (item.type == 'scripted') {\nif (item.scriptedItems.record_watchers)\nrecord_watchers = record_watchers.concat(item.scriptedItems.record_watchers);\n}\nif (item.type == 'filtered') {\nrecord_watchers.push({'table':item.table 'filter':item.filter});\n}\n}\n}\n\/\/ Init record watchers\nfor (var y in record_watchers){\nvar watcher = record_watchers\\[y\\];\nspUtil.recordWatch($scope  watcher.table  watcher.filter);\n}\n...\n\nTo understand which record watchers are instantiated we would need to look at the server side script to understand how 'data.menu.items' is populated  i.e.:\n\n...\nvar menu_id = $sp.getValue('sys_id'); \/\/ instance sys_id\nvar gr = new GlideRecord('sp_instance_menu');\ngr.get(menu_id);\ndata.menu.items = $sp.getMenuItems(menu_id);\n...\n\nThe above code queries essentially query the sp_rectangle_menu_item table looking for records where 'sp_rectangle_menu' is set to the current widget instances sys_id. For example lets say that the above widget instances sys_id is 'dfa819256f8a0380903ed3b0be3ee400' - this would return 6 records from sp_rectangle_menu_item:\n\nselect count(\\*) from sp_rectangle_menu_item where sp_rectangle_menu = 'dfa819256f8a0380903ed3b0be3ee400' and active = 1\n+------+----------+\n\\| Port \\| count(\\*) \\|\n+------+----------+\n\\| 3400 \\| 6 \\|\n+------+----------+\n\nIn the client-side code we look for which of these items have a type of either 'scripted' or 'filtered' - in this case  this returns 4 records:\n\nselect label  type from sp_rectangle_menu_item where sp_rectangle_menu = 'dfa819256f8a0380903ed3b0be3ee400' and type in ('scripted'  'filtered') and active = 1\n+------+-------------+----------+\n\\| Port \\| label \\| type \\|\n+------+-------------+----------+\n\\| 3400 \\| Demands \\| scripted \\|\n\\| 3400 \\| My Requests \\| scripted \\|\n\\| 3400 \\| My Work \\| scripted \\|\n\\| 3400 \\| Incidents \\| scripted \\|\n+------+-------------+----------+\n\nThe code uses the definition of each menu item to instantiate record watchers - for example if we look at the script of the **'My Requests'** menu item we see this instantiates record watchers as follows:\n\nselect record_script from sp_rectangle_menu_item where sp_rectangle_menu = 'dfa819256f8a0380903ed3b0be3ee400' and label = 'My Requests'\n...\nt.record_watchers = \\[\\];\nt.record_watchers.push({'table':'dmn_demand' 'filter':'active=true\\^opened_by=' + u});\nt.record_watchers.push({'table':'rm_story' 'filter':'active=true\\^opened_by=' + u});\nt.record_watchers.push({'table':'sc_req_item' 'filter':'active=true\\^request.requested_for=' + u});\nt.record_watchers.push({'table':'sc_request' 'filter':'active=true\\^requested_for=' + u});\n...\n\nBy working through each menu item (paying attention to conditions under which that menu item is loaded) we can determine which \/ how many record watchers the widget would instantiate. In some cases (such as the above)  however  there can be many record watchers instantiated so how can we work out which record watcher is triggering the widget reloads?\n\nTo answer this let's step back for a second:\n\n* We've already discussed that records watchers 'fire' when there are changes made to the set of records that they 'watch'\n* We've also discussed that the set of records a record watcher watches is defined via a table and encoded query\/filter - for example  the final record watcher above will watch records in the 'sc_request' table which are active and have 'requested_for' set to the current user\n* At a very high level  the way in which this works is that when the record watcher has instantiated the instance will create an AMB channel corresponding to the table \/ encoded query then the client will subscribe to this channel\n* Following this  should there be any modification made to the set of records  a notification will be published on the AMB channel and then delivered to the browser of every client subscribed to the channel - this then causes the record watcher will fire\n\nNote that:\n\n* Record watcher AMB channels are named as follows: \/amb\/rw\/default\/\\[TABLE NAME\\]\/\\[BASE64 ENCODED QUERY\\]\n* An example of a record watcher AMB channel might therefore be: \/amb\/rw\/default\/sc_request\/YWN0aXZlPXRydWVecmVxdWVzdGVkX2Zvcj01OTM1Y2Y3YTM3NzZmMjAwYmEwNjk0YzU0Mzk5MGU0MA--\n* We can immediately see that the channel corresponds to records in the 'sc_request' table and  by decoding the encoded query  can see that it matches records which are active and where 'requested_for' is equal to a given sys_id  i.e.:\n\n% echo \"YWN0aXZlPXRydWVecmVxdWVzdGVkX2Zvcj01OTM1Y2Y3YTM3NzZmMjAwYmEwNjk0YzU0Mzk5MGU0MA--\" \\| base64 -D\nactive=true\\^requested_for=5935cf7a3776f200ba0694c543990e40\n\n* We can then tell that this AMB channel corresponds to the last of the 4 record watchers listed above.\n\n\"\n\n",
        "QUERY": " 'how to troubleshoot portal widget code'",
        "GROUND_TRUTH": "To troubleshoot portal widget code, you need to understand the reasons behind the frequent reloads. In ServiceNow, transactions starting with \/API\/now\/sp\/rectangle indicate that a particular Service Portal widget is reloading or refreshing. \n\n1. Identify the Widget: The sys_id in the transaction URL indicates the widget instance which is reloading. You can use this sys_id to query the sp_instance table and identify the underlying Service Portal widget being used.\n\n2. Understand the Widget Operation: The sp_widget table contains definitions around how the widget operates. It includes CSS data, an HTML template, server-side JavaScript, and client-side JavaScript. \n\n3. Identify the Cause of Reloads: Reloads are generally triggered by the client script calling a function such as 'server.update()' or 'server.refresh()'. These functions cause the widget server-side script to execute, which in turn refreshes the contents of the widget. \n\n4. Periodic Widget Reloads: A widget can be reloaded on a periodic basis via code in the widgets client script. If this happens too frequently, it can cause various issues. To avoid this, you should create a 'promise' which is then destroyed when the user navigates away from the page.\n\n5. Using a Record Watcher to Trigger Widget Reloads: The client script of a widget can instantiate a record watcher against a given set of records. If anything to do with that set of records changes, the record watcher will be 'fired' at which point some arbitrary code is executed. \n\n6. Review the Widget: If there is evidence of excessive \/API\/now\/sp\/rectangle transactions for a given widget instance, review the corresponding widget, determine how reloads are triggered, and make recommendations to improve.\n\n7. Identify the Record Watcher: In some cases, it might not be clear which record watcher is to blame for the reloads. To understand which record watchers are instantiated, you would need to look at the server-side script to understand how 'data.menu.items' is populated."
    },
    {
        "id": 154,
        "CONTEXT": "\"## ServiceNow BCM PDF Generation Installation Steps\n\n**Installation**\n\n1. The following steps should be replicated by a **ServiceNow Admin** in any instance ServiceNow BCM is installed\n2. A ServiceNow BCM Technician or Partner will supply you with a folder which contains 5 Update Sets\n3. Navigate to System Definition: Plugins\n4. Search for Webkit HTML to PDF and make sure it is installed\n\n5. Navigate to Retrieved Update Sets and select the related link Import Update Set from XML\n6. Import the 5 Update sets\n7. Preview and commit the 5 update sets in the following order  \n   1. BCM PDF Global (Note: this update set is in global)\n   2. BCM PDF\n   3. BCM Update PDF UI\n   4. BCM Update PDF UI 2\n   5. BCM PDF Image Update\n\n**Testing**\n\n1. Navigate to All Plans in the Business Continuity application\n2. Open a plan record and select View Plan in the header of the record\n3. In the top right of the Business Continuity UI select Generate PDF\n\n4. A new tab will open  and a progress bar will appear\n5. When the progress bar is completed a pdf will be downloaded which can be opened in the bottom left of the screen\n\n\n\n## Testing Email Issues Using an Employee Instance\n\n### Setup a Test User To Receive Email Notifications\n\n* User Administration-\\>Users-\\>New. Create a new user  e.g john.smith and set its email address to your ServiceNow email  e.g [john.smith@servicenow.com](mailto:john.smith@servicenow.com)\n* Set a password\n* Add at least the itil role to the new user (optionally give it admin as that generally makes changing instance settings when troubleshooting easier)\n* Add the new user to the Service Desk group (so that this user can be assigned incidents  changes etc.)\n* Set user's timezone as desired (e.g your local TZ)\n\n### **Testing Outbound Email**\n\n#### **Notification Test Incident**\n\nSetup an Incident that can be used to test notifications  i.e where an update to worknotes or additional comments triggers an email notification to your user:\n\n* Create Incident with Short Description \"Notification Test Incident\"  Assignment group *Service Desk*  Assigned to \\<the user you created\\>\n* Change OOB Notification *Incident commented for ITIL* to enable *Send to event creator*(switch the view to Advanced when you have the email Notification open)\n* Add a new Additional Comment to the Incident. Check System Logs-\\>Emails and ensure a notification email was generated\n* (Optional) Install the ServiceNow mobile app (Now Mobile or My ServiceNow)  login to the instance on the app  verify that the a push notification is sent out\n\n#### Enabling Email Sending\n\nSystem Properties-\\>Email Properties. Set 'Send all email to this test address...' to your ServiceNow email  e.g [john.smith@servicenow.com](mailto:john.smith@servicenow.com).\n\nTick Yes on Email Sending\n\nWait 2-5 minutes and you should see the emails that were previously in Type=send-ready change to Type=sent and appear in your Outlook mailbox.\n\nYou might want to untick Yes on Email Sending after a while as automatic reports and Scheduled Jobs in the demo data on your instance will regularly send you emails.\n\n### Testing Inbound Email\n\n#### Creating a New Record Using Inbound Actions\n\nSystem Properties-\\>Email Properties. Enable Email Sending and Email Receiving.\n\nSend an email from your Outlook to [instancename@sevice-now.com](mailto:instancename@sevice-now.com) (e.g [emptedwardsv@service-now.com](mailto:emptedwardsv@service-now.com)) with subject whatever you want to be in the subject of the new Incident  and body whatever text you want. Verify it created a new Incident:\n\n#### Updating an Existing Record Using Inbound Actions\n\nReply to the email you got from doing the steps in 'Notification Test Incident' above. It should update the additional comments of your test Incident:\n\n#### Using Inbound Email Triggers in Flow Designer\n\nFollow [Create a flow with an inbound email trigger](https:\/\/docs.servicenow.com\/csh?topicname=create-inbound-email-flow.html&version=latest \"Create a flow with an inbound email trigger\")\n\nFor example:\n\n* Flow name 'Test Inbound Email Incident'\n* Add Trigger Inbound Email  with Receive Type is Reply  Reply Record Type is Incident\n* Add an Action 'Update Record'. For Record drag-drop the Trigger-Inbound Email-\\>Incident Record Data Pill. Table Incident. Fields Additional Comments  drag-drop Trigger-Inbound Email-\\>Body Text Data Pill\n* Save and Activate\n* Reply to the email you got from doing the steps in 'Notification Test Incident' above  putting whatever text in the email body that you want to be added as an Additional Comment to the Incident. Then check Executions in Flow Designer and also confirm that the Incident's Additional Comments were updated.\n\n\n\n## Install Subscription Management v2\n\n[Administer the Now Platform](..\/..\/..\/administer\/general\/concept\/intro-now-platform-landing.html \"As a platform administrator  you have the power of the Now Platform at your fingertips. The Now Platform is an application platform as a service that automates business processes across the enterprise.\") \\> [Getting started on the Now Platform](..\/..\/..\/administer\/general\/concept\/get-started-now-platform.html \"Now Platform is the exclusive ServiceNow platform. It provides a range of options to improve and automate your business processes.\") \\> [Subscription Management v2](..\/..\/..\/administer\/subscription-management\/reference\/subscription-management-landing-page-v2.html \"Subscription Management enables you to proactively manage your subscriptions and monitor subscription usage on your instances.\") \\>\n\n# Install Subscription Management v2 {#ariaid-title1}\n\nIf you have the admin role  you can install the Subscription Management v2 application (com.snc.usage_admin.base).\n\n* Ensure that the application and all of its associated ServiceNow Store applications have valid ServiceNow entitlements. For more information  see [Get entitlement for a ServiceNow\n  product or application](https:\/\/store.servicenow.com\/$appstore.do#!\/store\/help?article=KB0030186).\n* Review the [Subscription Management](https:\/\/store.servicenow.com\/sn_appstore_store.do) application listing in the ServiceNow Store for information on dependencies  licensing or subscription requirements  and release compatibility.\n* Consider enabling the glide.ui.polaris.use system property to optimize the Subscription Management user experience if you've turned off the Next Experience.\n\n{#install-subscription-management__ul_wlc_d2m_5xb}\n\nRole required: admin\nThe following items are installed with Subscription Management v2:\n\n* Plugins\n* Roles\n* Tables\n{#install-subscription-management__ul_ylc_d2m_5xb}\n\nFor more information  see [Components installed with Subscription Management](..\/reference\/installed-with-subscription-management.html \"Several components are installed with activation of the Subscription Management plugin (com.glide.subscription_management)  including user roles and tables.\").\n\n1. Navigate to All \\> System Applications \\> All Available Applications \\> All.\n2. Find the Subscription Management v2 application (com.glide.subscription_management) using the filter criteria and search bar.  \n   You can search for the application by its name or ID. If you can't find the application  you might have to request it from the ServiceNow Store.  \n   In the list next to the Install button  the versions that are available to you are displayed.\n3. Select a version from the list and select Install.  \n   In the Install dialog box that is displayed  any dependencies that are installed along with your application are listed.\n4. If you're prompted  follow the links to the ServiceNow Store to get any additional entitlements for dependencies.\n5. **Optional:** If demo data is available and you want to install it  select the Load demo data check box.  \n   Demo data comprises the sample records that describe application features for the common use cases. Load the demo data when you first install the application on a development or test instance.  \n   Important: If you don't load the demo data during installation  it's unavailable to load later.\n6. Select Install.\n{#install-subscription-management__steps_zlc_d2m_5xb}  \n**Parent Topic:** [Configuring Subscription Management](..\/..\/..\/administer\/subscription-management\/concept\/configuring-subscription-management-v2.html \"Set up and configure Subscription Management.\")\n\n\"\n\n",
        "QUERY": " 'how to test the generated pdf in a ServiceNow BCM?'",
        "GROUND_TRUTH": "To test the generated PDF in a ServiceNow BCM, follow these steps:\n\n1. Navigate to All Plans in the Business Continuity application.\n2. Open a plan record and select View Plan in the header of the record.\n3. In the top right of the Business Continuity UI, select Generate PDF.\n4. A new tab will open and a progress bar will appear.\n5. When the progress bar is completed, a PDF will be downloaded which can be opened in the bottom left of the screen."
    },
    {
        "id": 155,
        "CONTEXT": "\"## Downloaded attachment with encrypted text redirects to page  \"Requested attachment does not exist\"\n\n## Description\n\nAfter impersonating from the admin to a user role  an attempt to export an attachment that has encryption text as xls\/pdf for incident or any other table results in a successful download. However  when opening the downloaded file  the error \"Requested attachment does not exist\" occurs.\n\n## Steps to Reproduce\n\nLog in to an instance as an admin.\n\nActivate the Encryption plugins.\n\nCreate an Encryption context and assign to the admin role.\n\nFor more information  see the product documentation topic [Encryption support](https:\/\/docs.servicenow.com\/csh?topicname=c_EncryptionSupport.html&version=latest).4.\n\nGo to the incident table configuration and make a file (for example  Description) an Encrypted Text field.\n\nImpersonate a user who has an access to a table list such as the incident table.\n\nGo to incident-list.do.\n\nRight-click in the banner  then choose **Export \\> Excel**.\n\nAfter the download is complete  click **Download** to download the file.\n\nCheck the sys_attachment table.\n\nNote that the attachment exists (the attachment is against the sys_poll table).\n\n## Workaround\n\nYou can choose from various workarounds:\n\nDon't use impersonation when exporting data with encrypted-text column. Instead  log in as the user.\n\nSet the **glide.encryption.export_encrypted_data.allowed** property to false\n\nCreate a special decrypt_attachment role that includes the Encrypted Export Attachment context  and add that role to the impersonating user.\n\n**Note** -- In order to be able to see and add that context  disable the \"Hide system contexts\" business rule on sys_encryption_context first.\n\n**Related Problem: PRB1268370**\n\n\n\n## San Diego Patch 7b: Known Errors\n\nSan Diego Patch 7b : Known Errors\n\nThis article presents notable known errors in San Diego Patch 7b  grouped by severity and organized by category. Not all known errors appear on this page. Click the link in the Article column to view details about each known error. To view other families and versions  see the [Known Error Portal](\/kb_view.do?sysparm_article=KB0597477 \"Known Error Portal\").\n\n**Note**: The severity level does not indicate the order in which issues will be fixed  which is determined by a combination of variables.\n\nTo view the release notes for this release family  see San Release Notes( \"San\u00a0 Release Notes\").\n\n# Severity 1\n\nThe following table lists notable Severity 1 known errors in this patch.\n\n|                      Article                       |  Problem   |                 Category                 |                                                                                      Title                                                                                       |\n|----------------------------------------------------|------------|------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| [KB0861651](\/kb_view.do?sysparm_article=KB0861651) | PRB1434839 | Microsoft SMS\/SCCM Integration           | SCCM 2016\/2012 v2\/2007 populates cmdb_ci_pc_hardware instead of cmdb_ci_computer  which differs from Discovery                                                                   |\n| [KB0965271](\/kb_view.do?sysparm_article=KB0965271) | PRB1505867 | PDF Generation                           | The font cache is occupying memory in the heap  which causes memory contention                                                                                                   |\n| [KB0996495](\/kb_view.do?sysparm_article=KB0996495) | PRB1512640 | Language and Translations                | Customised sys_ui_messages are overwritten after upgrade                                                                                                                         |\n| [KB0996745](\/kb_view.do?sysparm_article=KB0996745) | PRB1523413 | PDF Generation                           | HTML2PDF API generated PDF documents and Security Incident PIR reports are removed by scheduled job \"Clean up converted documents genereated by PDF Generation Utilities plugin\" |\n| [KB1208278](\/kb_view.do?sysparm_article=KB1208278) | PRB1547401 | Persistence                              | Gateway database pool can't be instantiated at node startup                                                                                                                      |\n| [KB1156965](\/kb_view.do?sysparm_article=KB1156965) | PRB1596539 | Import \/ Export                          | The import set API fails with a '403 - Failed API level ACL Validation' error if the role 'import_admin' isn't assigned                                                          |\n| [KB1168974](\/kb_view.do?sysparm_article=KB1168974) | PRB1596558 | Antivirus Scanning                       | Invalid requests to antivirus servers impact attachment functionality                                                                                                            |\n| [KB1182003](\/kb_view.do?sysparm_article=KB1182003) | PRB1611377 | Configuration Management Database (CMDB) | CMDB Baseline creation job causing app node out of memory when a CI has a huge number of records referencing it                                                                  |\n| [KB1194249](\/kb_view.do?sysparm_article=KB1194249) | PRB1615702 | Orchestration                            | Orchestration: The Tibco JMS test connection doesn't work when using a custom connectionFactory name                                                                             |\n| [KB1218131](\/kb_view.do?sysparm_article=KB1218131) | PRB1632463 |                                          | batchedGlideAjax can set sysparm_aggregation_size wrongly  and when it ends up a huge number can cause AJAXXMLHttpAggregator to run the app node out-of-memory                   |\n# Severity 2\n\nThe following table lists notable Severity 2 known errors in this patch.\n\n|                      Article                       |  Problem   |                 Category                 |                                                                                                           Title                                                                                                           |\n|----------------------------------------------------|------------|------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| [KB0686716](\/kb_view.do?sysparm_article=KB0686716) | PRB1268370 | Platform Security                        | Downloaded attachment redirects to page \"Requested attachment does not exist\" after impersonating to a user for the attachment which has encryption context                                                               |\n| [KB0870572](\/kb_view.do?sysparm_article=KB0870572) | PRB1437726 | Human Resources Service Management       | WCAG 4.1.2 Campaign Content Library - when the value is announced in VoiceOver  the role is needed for users to understand the context                                                                                    |\n| [KB0870579](\/kb_view.do?sysparm_article=KB0870579) | PRB1457469 | Appsee - Seismic                         | Seismic VA dashboard - 'Conversation' table not shows data after sorting by a topic                                                                                                                                       |\n| [KB0870585](\/kb_view.do?sysparm_article=KB0870585) | PRB1458043 | Application Navigator \\& Banner Frame    | When added to the visible UI  both the current Application and the current Update Set select lists have no accessible label                                                                                               |\n| [KB0870590](\/kb_view.do?sysparm_article=KB0870590) | PRB1458929 | Performance Analytics                    | \\[High chart upgrade 8.2.2\\] Heatmap and geomap visualization are broken                                                                                                                                                  |\n| [KB0870896](\/kb_view.do?sysparm_article=KB0870896) | PRB1459399 | Agent Workspace                          | Process Automation Designer (PAD) - Condition Builder fields are not populated in Modify Condition Widget in PAD Application                                                                                              |\n| [KB0966593](\/kb_view.do?sysparm_article=KB0966593) | PRB1477967 | UI Builder                               | Client Scripts do not save when using UIB in Firefox                                                                                                                                                                      |\n| [KB0962393](\/kb_view.do?sysparm_article=KB0962393) | PRB1483305 | Discovery                                | 'HorizontalDiscoveryProbe' has multiple mutex locks                                                                                                                                                                       |\n| [KB1064739](\/kb_view.do?sysparm_article=KB1064739) | PRB1483528 | Scheduled Job Processing                 | Job scheduler is clogged by interactive jobs because of leaked child jobs                                                                                                                                                 |\n| [KB0966596](\/kb_view.do?sysparm_article=KB0966596) | PRB1485591 | Lists                                    | Simple - List component is not available in the Dashboard Builder                                                                                                                                                         |\n| [KB0959468](\/kb_view.do?sysparm_article=KB0959468) | PRB1489024 | MID Server                               | In Quebec : MID Server upgrade stops if PowerShell tests in Pre-upgrade check failed  PowerConsole remains busy  and closing the PowerShell session fails                                                                 |\n| [KB0961414](\/kb_view.do?sysparm_article=KB0961414) | PRB1490701 | VA-Designer                              | The default custom chat experience in Virtual Agent is set to inactive after upgrading                                                                                                                                    |\n| [KB0966598](\/kb_view.do?sysparm_article=KB0966598) | PRB1492041 | Mobile Platform                          | Mobile Task button actions on WOT form page do not refresh the form once the Task has been accepted\/rejected                                                                                                              |\n| [KB0993213](\/kb_view.do?sysparm_article=KB0993213) | PRB1492353 | Platform Licensing                       |\n\n## NowAttachmentUploadConfiguration init(tableName: String  recordSysId: SysID  fileName: String  conte\n\n# NowAttachmentUploadConfiguration structure - iOS {#ariaid-title1}\n\nThe NowAttachmentUploadConfiguration structure enables you to define the configuration information for an attachment that you are uploading to your ServiceNow instance.\n{#d468535e70}\n\n|       Name        |  Type  |                                                                                                            Description                                                                                                            |\n|-------------------|--------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| contentType       | String | HTTP data content type.                                                                                                                                                                                                           |\n| encryptionContext | String | Optional. Sys_id of an encryption context record. Specify this parameter to allow only users with the specified encryption context to access the attachment. Default: Attached file is not encrypted with any encryption context. |\n| fileName          | String | Name of the attachment file.                                                                                                                                                                                                      |\n| recordSysId       | String | Sys_id of the record on the ServiceNow instance to associate the attachment with.                                                                                                                                                 |\n| tableName         | String | Name of the table on the ServiceNow instance that contains the record specified in the recordSysid parameter.                                                                                                                     |\nTable 1. Properties{#NowAttachUploadConfigiOSStruct__table_vx2_klw_5pb}\n**Parent Topic:** [Mobile SDK API reference - iOS](..\/..\/..\/..\/..\/..\/app-store\/dev_portal\/API_reference\/MobileSDKiOS\/concept\/MobileSDKiOSAPI.html \"The Mobile SDK for iOS provides the classes necessary to interface iOS-based devices with the ServiceNow platform.\")\n\n## NowAttachmentUploadConfiguration init(tableName: String  recordSysId: SysID  fileName:\n\nString  contentType: String  encryptionContext: String?) {#ariaid-title2}\n\nSets the upload configuration values for the specified attachment.\n{#d468535e274}\n\n|       Name        |  Type  |                                                                                                            Description                                                                                                            |\n|-------------------|--------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| tableName         | String | Name of the table on the ServiceNow instance that contains the record specified in the recordSysid parameter.                                                                                                                     |\n| recordSysId       | String | Sys_id of the record on the ServiceNow instance to associate the attachment with.                                                                                                                                                 |\n| fileName          | String | Name of the attachment file.                                                                                                                                                                                                      |\n| contentType       | String | HTTP data content type.                                                                                                                                                                                                           |\n| encryptionContext | String | Optional. Sys_id of an encryption context record. Specify this parameter to allow only users with the specified encryption context to access the attachment. Default: Attached file is not encrypted with any encryption context. |\nTable 2. Parameters{#NAUC-init_S_S_S_S_S__table_shs_cgn_mqb}\n{#d468535e373}\n\n| Type | Description |\n|------|-------------|\n| None |             |\nTable 3. Returns{#NAUC-init_S_S_S_S_S__table_ths_cgn_mqb}\nThe following code example shows how to call this function.\n\n\"\n\n",
        "QUERY": " 'Downloaded attachment redirects to page \"Requested attachment does not exist\" after impersonating to a user for the attachment which has encryption context'",
        "GROUND_TRUTH": "The issue of a downloaded attachment redirecting to the page \"Requested attachment does not exist\" after impersonating a user for the attachment which has encryption context occurs when an admin impersonates a user role and attempts to export an attachment that has encryption text as xls\/pdf for incident or any other table. The download is successful, but when opening the downloaded file, the error \"Requested attachment does not exist\" occurs. \n\nTo reproduce this issue, you would need to log in to an instance as an admin, activate the Encryption plugins, create an Encryption context and assign it to the admin role, go to the incident table configuration and make a file an Encrypted Text field, impersonate a user who has access to a table list such as the incident table, go to incident-list.do, right-click in the banner, then choose Export > Excel. After the download is complete, download the file and check the sys_attachment table. You will note that the attachment exists (the attachment is against the sys_poll table).\n\nThere are several workarounds suggested in the article. You can avoid using impersonation when exporting data with encrypted-text column and instead log in as the user. You can set the glide.encryption.export_encrypted_data.allowed property to false. Or you can create a special decrypt_attachment role that includes the Encrypted Export Attachment context, and add that role to the impersonating user. However, to be able to see and add that context, you need to disable the \"Hide system contexts\" business rule on sys_encryption_context first."
    },
    {
        "id": 156,
        "CONTEXT": "\"## Week Starts on date on the Time Card is not changing\n\nChanging the \"Week Starts On\" field on the Time Card does not save\n\n\n\nJakarta Patch 9c\n\n\n\nNo Time Sheet on the time card so \"Populate week starts on\" Business Rule was running.\n\n\"\n\n",
        "QUERY": " 'Time card with \"week starts on\" not working as expected'",
        "GROUND_TRUTH": "no answer found"
    },
    {
        "id": 157,
        "CONTEXT": "\"## SAMP Reconciliation fails with error 'TypeError: Cannot read property \"install_condition\" from null'\n\n## Description\n\n* The SAMP reconciliation fails after 100% progress due to a race condition when the Import User Subscriptions job is also running at the same time.\n* The progress summary shows:\n\nTypeError: Cannot read property \"install_condition\" from null\n\nat sys_script_include.444d8294c32222006081face81d3aebf.script:514 (anonymous)\nat sys_script_include.444d8294c32222006081face81d3aebf.script:266 (anonymous)\nat sys_script_include.444d8294c32222006081face81d3aebf.script:247 (anonymous)\nat sys_script_include.30bbdf9587f52300923aa75fe5cb0b97.script:413 (anonymous)\nat sys_script_include.30bbdf9587f52300923aa75fe5cb0b97.script:400 (anonymous)\nat sys_script_include.6761b0dd0b1232001a17650d37673a77.script:868 (anonymous)\nat sys_script_include.6761b0dd0b1232001a17650d37673a77.script:824 (anonymous)\nat sys_script_include.6761b0dd0b1232001a17650d37673a77.script:54 (anonymous)\nat sys_trigger.f7006c6e471e59d0e1ce8a12736d4378:1\n\nPossibly related problems:\n[Reconciliation errors in the form 'Cannot read property \\<someSysId\\> from undefined' caused by SamNewPerCoreForMicrosoftLicenseCalculator loading all optimization records and Per Core VM allocations missing relationships](https:\/\/support.servicenow.com\/kb_view.do?sysparm_article=KB1227294 \"Reconciliation errors in the form 'Cannot read property <someSysId> from undefined' caused by SamNewPerCoreForMicrosoftLicenseCalculator loading all optimization records and Per Core VM allocations missing relationships\")\n[Workday reconciliation is failing if subscriptions are pulled from SSO or custom integration profile](https:\/\/support.servicenow.com\/kb_view.do?sysparm_article=KB1316426 \"Workday reconciliation is failing if subscriptions are pulled from SSO or custom integration profile\")\n\n## Steps to Reproduce\n\n1. Schedule the Reconciliation job to run at the same time as the SAM - Import User Subscriptions job.  \n   2. Check Recon Results for recon with status 'Failed' and Progress summary 'Reconciliation Complete'.  \n   3. Check the progress summary for the TypeError: cannot read property 'install_condition' from null.\n\n## Workaround\n\n- Use Case 1 -  \nMake sure the Import User Subscription and Software Reconciliation jobs are never scheduled to run at the same time.\n- Use Case 2 -\n\nIf you have verified both Import User Subscription and Software Reconciliation are running at different times  then consider changing the \"**SamAllocationSuiteEngine** \" Script Include ( \/sys_script_include.do?sys_id=444d8294c32222006081face81d3aebf ) to add the code below line at 435  so that the final code looks like this:\n\ngetSubscriptionSuitesOrComponents: function(publisherId  getSuites) {\n\/\/ First get all the licensable software models of this publisher\nvar subscriptionModels = \\[\\];\nvar subscriptionGa = new SampAggregate('samp_sw_subscription');\nsubscriptionGa.addQuery('software_model.manufacturer'  publisherId);\nsubscriptionGa.addNotNullQuery('user');\nsubscriptionGa.addNotNullQuery('licensable_software_model'); --- We are filtering record where licensable_software_model is empty.\nsubscriptionGa.groupBy('licensable_software_model');\nsubscriptionGa.query();\n\nNOTE: The reconciliation job processes the valid record where you already have a valid licensable_software_model value and show the data. This same job also populates the licensable_software_model value for the records where licensable_software_model is empty. Thus the records skipped in the current run will be automatically processed in the next run.\n\nif you see any record with licensable_software_model as empty and publisher as empty  then those records will not be processed. Those do not have any direct impact on the job execution. This needs to be investigated separately.\n\n**Related Problem: PRB1610760**\n\n\n\n## [Vancouver] Software Asset Management Professional (com.snc.samp) Microsoft Reconciliation Process F\n\n## Description\n\nPost-upgrade to the Vancouver release  some instances with the **Software Asset Management Professional** (com.snc.samp) plugin have experienced failures or stalling of the reconciliation process (recon stuck in progress). When this occurs  the following error messages may appear in the 'Reconciliation Progress Summary ? Reconciliation Progress Details' related list:\n\n1. TypeError: Cannot read property \"sys_id\" from undefined. Please review the stack trace error line  ensuring that the failure involves either the \"hostsToInstallTypes\" or \"vmsToInstallTypes\" object.  \n2. Logs reveal that the 'SAM - Software License Reconciliation Worker' is stuck. The stack trace suggests issues with the script include 'SamNewPerCoreWithCalForMicrosoftLicenseCalculator'.  \n3. JavaException: java.lang.StackOverflowError.  \n\nThis reconciliation failure is specifically linked to \"Microsoft Per-Core\" and \"Microsoft Per-Core (with CAL)\" license metrics.\n\n## Steps to Reproduce\n\nOn a Vancouver instance with \"Microsoft Per-Core\" and\/or \"Microsoft Per-Core (with CAL)\" entitlements  run reconciliation for Microsoft publisher and observe the symptom(s) described in the previous section.\n\n## Workaround\n\nThis problem has been fixed. If you are able to upgrade  review the \"Fixed In\" section to determine the latest version with a permanent fix that your instance can be upgraded to.\n\nIf you are encountering this issue on an affected version of Vancouver  but you are unable to upgrade to a fixed version at this time  kill the ongoing \"SAM - Software License Reconciliation Worker\"  then apply the update set attached to this Known Error article. Ensure that all related script includes are marked with \"replace_on_upgrade = true\" (in the sys_update_xml table) after committing the update set. This will prevent future conflicts with OOB script includes when upgrading to a fixed version.\n\n**FAQ**\n\n**Q:** How can I confirm if my instance is impacted by this issue?\n**A:** If you have the com.snc.samp plugin installed  and you have \"Microsoft per Core\" or \"Microsoft per Core (with CAL)\" entitlements  you can experience this issue when upgrading to an impacted version of Vancouver.\n\n**Q:** Will this affect non-prod instances  prod instances  or both?\n**A:** Both production and sub-production instances can be affected by this issue.\n\n**Q:** Can I implement the workaround myself?\n**A:** Yes. If you are on an impacted version of Vancouver and are experiencing this issue  please follow the steps detailed above and apply the update set attached to this article.\n\n**Q:** Once the workaround has been applied  do I need to do anything else?\n**A:** Once you've applied the update set  we recommend setting \"Replace on Upgrade\" to \"True\" for related script includes to prevent future conflicts when upgrading to a fixed version.\n\n**Q:** Will the solution cause any downtime?\n**A:** Implementing the workaround will not result in any downtime.\n\n**Q:** Which version(s) will contain the fix for this issue?\n**A:** This issue is fixed in Vancouver Patch 4 and above.\n\n**Related Problem: PRB1712246**\n\n\n\n## Reconciliation still fails for Microsoft publisher with the error ' TypeError: Cannot read property \n\n## Description\n\nAllocating VM that is missing a host relationship on a Microsoft Per Core license will cause reconciliation to fail with an error in the form \"Cannot read property \\<property name\\> from undefined\".\n\n## Steps to Reproduce\n\non a Tokyo Patch 7  or utah patch 1 create a vm without host and assign allocations to the its SQL Server license Per Core.\n\nrecon fails for SQL server product with stack trace:\n\nat sys_script_include.1e5ac217b10e0110fa9bf03fa4dd6856.script:1729 (anonymous)\nat sys_script_include.1e5ac217b10e0110fa9bf03fa4dd6856.script:1695 (anonymous)\nat sys_script_include.1e5ac217b10e0110fa9bf03fa4dd6856.script:2177 (anonymous)\nat sys_script_include.1e5ac217b10e0110fa9bf03fa4dd6856.script:2136 (anonymous)\nat sys_script_include.1e5ac217b10e0110fa9bf03fa4dd6856.script:189 (anonymous)\nat sys_script_include.602e129eb0276300fa9b028ca0d3b864.script:43 (anonymous)\nat sys_script_include.8a6dbe2887522300ede6f64936cb0b2c.script:113 (anonymous)\nat sys_script_include.8a6dbe2887522300ede6f64936cb0b2c.script:240 (anonymous)\nat sys_script_include.8a6dbe2887522300ede6f64936cb0b2c.script:114 (anonymous)\nat sys_script_include.8a6dbe2887522300ede6f64936cb0b2c.script:102 (anonymous)\nat sys_script_include.8a6dbe2887522300ede6f64936cb0b2c.script:94 (anonymous)\nat sys_script_include.74ed3a7d8dfed010fa9b4295b8773c71.script:25\nat sys_script_include.8a6dbe2887522300ede6f64936cb0b2c.script:36 (anonymous)\nat sys_script_include.30bbdf9587f52300923aa75fe5cb0b97.script:425 (anonymous)\nat sys_script_include.6761b0dd0b1232001a17650d37673a77.script:896 (anonymous)\nat sys_script_include.6761b0dd0b1232001a17650d37673a77.script:836 (anonymous)\nat sys_script_include.6761b0dd0b1232001a17650d37673a77.script:57 (anonymous)\nat sys_trigger.03ee762b975eedd0fbb979971153af9e:1\n\nit is a discovery issue since no host was stamped for this vm (type Virtualizes By missing in cmdb_rel_ci  where child = host  parent = vm)\n\nas a workaround  either customer needs to delete those allocations  or modify code in SamNewPerCoreForMicrosoftLicenseCalculator (sys_script_include_1e5ac217b10e0110fa9bf03fa4dd6856.xml) with\n\n_onlyUnique: function(value  index  self) {\n\nreturn self.indexOf(value) === index \\&\\& !gs.nil(value); \/\/ ADDED part from '\\&\\&'\n} \n\n## Workaround\n\nUpgrade to Utah Patch 5 or Tokyo Patch 10  backports were made as part of [BAK0071941](https:\/\/buildtools1.service-now.com\/x_snc_defect_backport.do?sys_id=00f3266d93eaa9d0b8d9b0627cba1015&sysparm_view=Scrum&sysparm_record_target=x_snc_defect_backport&sysparm_record_row=1&sysparm_record_rows=2&sysparm_record_list=parent_defect%3D5dde2900dbf8a114c7304b7c13961934%5EORDERBYDESCsys_updated_on) \/ [BAK0071942](https:\/\/buildtools1.service-now.com\/x_snc_defect_backport.do?sys_id=f4b4e2e1932ea9d0b8d9b0627cba10c7&sysparm_view=Scrum&sysparm_record_target=x_snc_defect_backport&sysparm_record_row=2&sysparm_record_rows=2&sysparm_record_list=parent_defect%3D5dde2900dbf8a114c7304b7c13961934%5EORDERBYDESCsys_updated_on).\n\n**Related Problem: PRB1665109**\n\n\"\n\n",
        "QUERY": " 'typeerror: cannot read property \"install_condition\" from null software reconciliation'",
        "GROUND_TRUTH": "The error 'TypeError: Cannot read property \"install_condition\" from null' occurs during the SAMP reconciliation process when the Import User Subscriptions job is running simultaneously. This is due to a race condition. To reproduce this error, you need to schedule the Reconciliation job to run at the same time as the SAM - Import User Subscriptions job. Then, check the Recon Results for recon with status 'Failed' and Progress summary 'Reconciliation Complete'. Finally, check the progress summary for the TypeError.\n\nThere are two workarounds suggested in the article. The first one is to ensure that the Import User Subscription and Software Reconciliation jobs are never scheduled to run at the same time. The second workaround involves modifying the \"SamAllocationSuiteEngine\" Script Include to add a specific code below line at 435. This code filters records where the licensable_software_model is empty. The reconciliation job processes the valid record where you already have a valid licensable_software_model value and show the data. This same job also populates the licensable_software_model value for the records where licensable_software_model is empty. Thus the records skipped in the current run will be automatically processed in the next run."
    }
]